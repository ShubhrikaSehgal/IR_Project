Springer.tar//Springer//Springer\10.1007-s00138-012-0447-z.xml:Texture-independent recognition of facial expressions in image snapshots and videos:Facial expression recognition Static and dynamic classifier Subspace learning Human machine interaction:  1 Introduction  Facial expressions play an important role in recognition of human emotions. Psychologists postulate that facial expressions have a consistent and meaningful structure that can be backprojected to infer people inner affective state [14, 15]. Basic facial expressions typically recognized by psychologists are: happiness, sadness, fear, anger, disgust and surprise [13]. In the beginning, facial expression analysis was essentially a research topic for psychologists. However, recent progresses in image processing and pattern recognition have significantly motivated research works on automatic facial expression recognition [16, 20, 36]. In the past, a lot of effort was dedicated to recognize facial expression in still images. For this purpose, many techniques have been applied: neural networks [32], Gabor wavelets [4] and active appearance models [30]. A very important limitation to this strategy is the fact that still images usually capture the apex of the expression, i.e., the instant at which the indicators of emotion are most marked. In their daily life, people seldom show apex of their facial expression during normal communication with their counterparts, unless for very specific cases and for very brief periods of time. More recently, attention has been shifted particularly towards modelling dynamical facial expressions [35, 27]. This is because the differences between expressions are more powerfully modelled by dynamic transitions between different stages of an expression rather than their corresponding static key frames. This is a very relevant observation, since for most of the communication act, people rather use ‘subtle’ facial expressions than showing deliberately exaggerated poses in order to convey their message. In [3], the authors found that subtle expressions that were not identifiable in individual images suddenly became apparent when viewed in a video sequence. Dynamical classifiers try to capture the temporal pattern in the sequence of feature vectors related to each frame such as the hidden Markov models (HMMs) and dynamic Bayesian networks [37]. In [5], parametric 2D flow models associated with the whole face as well as with the mouth, eyebrows, and eyes are first estimated. Then, mid-level predicates are inferred from these parameters. Finally, universal facial expressions are detected and recognized using the estimated predicates. In [36], a two-stage approach is used. Initially, a linear classification bank was applied and its output was fused to produce a characteristic signature for each universal facial expression. The signatures thus computed from the training data set were used to train discrete HMMs to learn the underlying model for each facial expression. In [29], the authors propose a Bayesian approach to modelling temporal transitions of facial expressions represented in a manifold. However, the fact that the method relies heavily on the gray level of the image can be a serious limitation. In [34], the authors explore Gabor motion energy filters (GME) as a biologically inspired representation for dynamic facial expressions. They show that GME filters outperform the Gabor energy filters, particularly on difficult low intensity expression discrimination. In [18], the authors combine some extracted facial feature sets using confidence level strategy. Noting that for different facial components, the contributions to the expression recognition are different, they propose a method for automatically learning different weights to components via the multiple kernel learning. In [21], the authors use two types of descriptors motion history histogram (MHH) and histogram of local binary patterns (LBPs). LBP was applied to each frame of the video and was used to capture local textural patterns. Based on these two basic types of descriptors, two new dynamic facial expression features are proposed. In [23], the authors uses weak classifiers are formed by assembling edge fragments with chamfer scores. An ensemble framework is presented with all-pairs binary classifiers. An error correcting support vector machine (SVM) is utilized for final classification. In [38], the authors construct a sparse representation classifier (SRC). The effectiveness and robustness of the SRC method are investigated on clean and occluded facial expression images. Three typical facial features, i.e., the raw pixels, Gabor wavelets representation and LBPs are extracted to evaluate the performance of the SRC method. In [22], a sequential two stage approach is taken for pose classification and view dependent facial expression classification to investigate the effects of yaw variations from frontal to profile views. LBPs and variations of LBPs as texture descriptors are investigated. Multi-class SVMs are adopted to learn pose and pose-dependent facial expression classifiers. As can be seen, most of the proposed expression recognition schemes require a frontal view of the face. Moreover, most of them rely on the use of image raw brightness changes. The recognition of facial expressions in image sequences with significant head motion is a challenging problem. It is required by many applications such as human computer interaction and computer graphics animation [8, 25, 26] as well as training of social robots [6, 7]. This paper introduces a novel scheme for dynamic facial expression recognition that is based on the appearance-based 3D face tracker [11]. It has two main contributions. First, we introduce a view- and texture-independent scheme that exploits facial action parameters estimated by an appearance-based 3D face tracker. We represent the learned facial actions associated with different facial expressions by time series. Second, we compare this dynamic scheme with a static one based on analyzing individual snapshots and show that the former performs better than the latter. We provide evaluations of performance using three subspace learning techniques: linear discriminant analysis (LDA), non-parametric discriminant analysis (NDA) and SVMs. Compared to existing dynamical facial expression methods, our proposed approach (first contribution) has several advantages. First, unlike most expression recognition systems that require a frontal view of the face, our system is view independent since the used tracker simultaneously provides the 3D head pose and the facial actions. Second, it is texture independent since the recognition scheme relies only on the estimated facial actions—invariant geometrical parameters. Third, its learning phase is simple compared to other techniques (e.g., the HMM). As a result, even when the imaging conditions change, the learned expression dynamics need not to be recomputed. In this work, we compare the proposed approach for dynamic facial expression against individual frame-based recognition methods. This comparison shows a clear superiority in terms of recognition rates and robustness. The most related works are our previous works&amp;nbsp;[10] and&amp;nbsp;[12]. These works utilize the intensities of facial actions for the dynamic facial expression recognition. However,&amp;nbsp;[10] proposes a dynamic classifier based on a brute force matching of temporal trajectories. [12] proposes a dynamic classifier that is not based on examples. It is based on an analysis–synthesis scheme exploiting learned predictive models (second order Markov models). The current paper provides a substantial novelty since it addresses dynamic and static recognition schemes with moderate and high magnitude facial expressions. Moreover, the current work utilizes and compares several subspace learning techniques for both the dynamic and static recognition frameworks such as LDA, NDA and SVM. The rest of the paper is organized as follows. Section&amp;nbsp;2 briefly presents the used 3D face and facial action tracker. Section&amp;nbsp;3 describes the proposed recognition scheme. In Sect.&amp;nbsp;4 we report some experimental results and method comparisons. Finally, in Sect.&amp;nbsp;5, we present our conclusions and some guidelines for future work.    2 3D facial dynamics extraction  In our work, we use the 3D face model  &amp;nbsp;[ 2]. This 3D deformable wireframe model was first developed for the purpose of model-based image coding and computer animation. The 3D shape of this wireframe model is directly recorded in coordinate form. It is given by the coordinates of the 3D vertices   where   is the number of vertices. Thus, the shape up to a global scale can be fully described by the  -vector  ; the concatenation of the 3D coordinates of all vertices  . The vector   is written as:       where   is the static shape of the model,   is the animation control vector, and the columns of   are the animation units (AUs). The static shape is constant for a given person. In this study, we use six modes for the facial AUs matrix  . We have chosen the following AUs or facial actions: lower lip depressor, lip stretcher, lip corner depressor, upper lip raiser, eyebrow lowerer, and outer eyebrow raiser. These facial actions are enough to cover most common facial animations. Moreover, they are essential for conveying emotions. Thus, for every frame in the video, the state of the 3D wireframe model is given by the 3D head pose parameters (three rotations and three translations) and the internal face animation control vector  . This is given by the 12-dimensional vector  :       where:     , , and  represent the three angles associated with the 3D rotation between the 3D face model coordinate system and the camera coordinate system.    , , and  represent the three components of the 3D translation vector between the 3D face model coordinate system and the camera coordinate system.   Each component of the vector  represents the intensity of one facial action. This belongs to the interval  where the zero value corresponds to the neutral configuration (no deformation) and the one value corresponds to the maximum deformation. In the sequel, the word “facial action” will refer to the facial action intensity. , , and  represent the three angles associated with the 3D rotation between the 3D face model coordinate system and the camera coordinate system. , , and  represent the three components of the 3D translation vector between the 3D face model coordinate system and the camera coordinate system. Each component of the vector  represents the intensity of one facial action. This belongs to the interval  where the zero value corresponds to the neutral configuration (no deformation) and the one value corresponds to the maximum deformation. In the sequel, the word “facial action” will refer to the facial action intensity. In order to recover the facial expression, one has to compute the facial actions encoded by the vector  which encapsulates the facial deformation. Since our recognition scheme is view independent, these facial actions together with the 3D head pose should be simultaneously estimated. In other words, the objective is to compute the state vector  for every video frame. For this purpose, we use the tracker based on Online Appearance Models&amp;nbsp;[ 11]. This appearance-based tracker aims at computing the 3D head pose and the facial actions, i.e. the vector  , by minimizing a distance between the incoming warped frame and the current   appearance of the face (a geometrically normalized frontal face). This minimization is carried out using a gradient descent method. The statistics of the   appearance as well as the gradient matrix are updated every frame. This scheme leads to a fast and robust tracking algorithm. The remaining of this section will describe the main features of the used 3D face tracker&amp;nbsp;[ 11]. These features are also illustrated in Fig.&amp;nbsp;  which shows the model fitting for every video frame. Given a video sequence depicting a moving head/face, we would like to recover, for each frame, the 3D head pose and the facial actions encoded by the control vector . In other words, we would like to estimate the vector  (Eq.&amp;nbsp;2) at time  given all the observed data until time , denoted . In a tracking context, the model parameters associated with the current frame will be handed over to the next frame. For each input frame  , the observation is simply the warped texture patch (the shape-free image) associated with the geometric parameters  . We use the   symbol for the tracked parameters and textures. For a given frame  ,   represents the computed geometric parameters and   the corresponding shape-free patch (shown in the upper part of Fig.&amp;nbsp; ), i.e.,       where   is a piece-wise affine transform that maps the input image underneath the projected 3D mesh to the shape-free face image. The model of face appearance (shape-free face images) is given by a multivariate Gaussian with a diagonal covariance matrix . The choice of a Gaussian distribution is motivated by the fact that this kind of distribution provides simple and general model for additive noises. In other words, this multivariate Gaussian is the distribution of the facial patches . Let  be the Gaussian center and  the vector containing the square root of the diagonal elements of the covariance matrix .  and  are d-vectors ( is the size of the shape-free face image ). The estimation of  from the sequence of images will adopt the following steps (illustrated in Fig.&amp;nbsp;): Acquire a new image .         &amp;nbsp;           Set the initial solution for  as  (the solution estimated at the previous image).         &amp;nbsp;                 Iteratively update the solution        by minimizing the error between the current warped texture (based on       ) and the current texture model (shape free textures). A Gauss–Newton like minimization is used. The error is set to the        distance between the warped texture and the current face texture model (Eq.&amp;nbsp;      4). This minimization process is illustrated in the nested loop (Fig.&amp;nbsp;      ).                                                         &amp;nbsp;           At convergence, use the current solution  for computing the current shape-free facial image .         &amp;nbsp;                 Use the estimated shape-free facial image        for updating the texture model (the mean and the covariance of the multivariate Gaussian) using the following equations:                                                                                       where        is a positive scalar controlling how fast the past observations are forgotten.              &amp;nbsp;           Compute the current gradient matrix  by the finite difference method. This gradient matrix encodes the texture variation with respect to all geometrical parameters (face pose and the facial actions).         &amp;nbsp;           Hand over the current face pose and facial actions, the gradient matrix and the face texture model to the next frame. Set . Go to step 1.         &amp;nbsp;  Acquire a new image . Set the initial solution for  as  (the solution estimated at the previous image). Iteratively update the solution   by minimizing the error between the current warped texture (based on  ) and the current texture model (shape free textures). A Gauss–Newton like minimization is used. The error is set to the   distance between the warped texture and the current face texture model (Eq.&amp;nbsp; 4). This minimization process is illustrated in the nested loop (Fig.&amp;nbsp; ). At convergence, use the current solution  for computing the current shape-free facial image . Use the estimated shape-free facial image   for updating the texture model (the mean and the covariance of the multivariate Gaussian) using the following equations:                           where   is a positive scalar controlling how fast the past observations are forgotten. Compute the current gradient matrix  by the finite difference method. This gradient matrix encodes the texture variation with respect to all geometrical parameters (face pose and the facial actions). Hand over the current face pose and facial actions, the gradient matrix and the face texture model to the next frame. Set . Go to step 1.    3 Facial expression recognition  In order to learn the spatio-temporal structures of the facial actions associated with facial expressions, we have used a simple supervised learning scheme that consists in two stages. In the first stage, video sequences depicting different facial expressions are tracked using the appearance-based tracker. The retrieved facial actions   are represented by time series. In other words, an example (expression going from neutral to apex) is encoded by a sequence of facial actions  . In the second stage, in order to get the same dimension for all training examples, all facial action sequences are registered in the time domain using the dynamic time warping (DTW) technique&amp;nbsp;[ 24]. DTW is a well-known technique to find an optimal alignment between two given (time dependent) sequences under certain restrictions. An illustration of DTW is given in Fig.&amp;nbsp; . This temporal alignment is needed in order to subsequently use machine learning tools that require the observations to have the same dimension. The motivation behind this process, resides in the fact that the generation of a facial expression, in terms of speed and intensity, is person-specific and furthermore, it is different for the same person, depending on the context. For this reason, we have to make sure that all observations we acquire are conveniently resized in the time domain for their computational analysis. For example, a given example (expression) is always represented by a feature vector obtained by concatenating the vectors   belonging to the registered temporal sequence. Video sequences have been picked up from the CMU database&amp;nbsp;[ 19]. These sequences depict five frontal view universal expressions (surprise, sadness, joy, disgust and anger). Each expression is performed by seven different subjects, starting from the neutral one. Altogether, we select 35 video sequences composed of around 15–20 frames each, that is, the average duration of each sequence is about half a second. The learning phase consists of estimating the facial action parameters   (a 6-element vector) associated with the frames of each training sequence, that is, the temporal trajectories of the action parameters. Figure&amp;nbsp;  shows the retrieved facial action parameters associated with three sequences: surprise, anger, and joy. The training video sequences have an interesting property: all performed expressions go from the neutral expression to a high magnitude expression by going through a moderate magnitude around the middle of the sequence. Therefore, using the same training set, we get two kinds of trajectories: (1) an entire trajectory which models transitions from the neutral expression to a high magnitude expression, and (2) a truncated trajectory (the second half part of a given trajectory) which models the transition from small/moderate magnitudes (half apex of the expression) to high magnitudes (apex of the expression). Figure&amp;nbsp;  shows the half apex and apex facial configurations for three expressions: surprise, anger, and joy. In the final stage of the learning, all training trajectories are aligned in the time domain using the DTW technique by fixing a nominal duration for a facial expression. In our experiments, this nominal duration is set to 18 frames. In the recognition phase, the 3D head pose and facial actions are estimated online from the video sequence using the appearance-based face and facial action tracker. We infer the facial expression associated with the current frame  by considering the estimated trajectory, i.e. the sequence of vectors  within a temporal window of size 18 centered at the current frame . This trajectory (feature vector) is then classified using classical classification techniques that rely on the learned examples. We have used three different classification methods: (1) LDA, (2) NDA&amp;nbsp;[17], and (3) SVMs with a radial basis function&amp;nbsp;[9]. It is worth noting that the static recognition scheme will use the facial actions associated with only one single frame. In this case, the training examples correspond to the apex of the expression or to its half apex.    4 Experimental results and method comparisons 
Springer.tar//Springer//Springer\10.1007-s00138-012-0448-y.xml:Detection of moving objects with a moving camera using non-panoramic background model:Moving objects detection Spatio-temporal Gaussian model Background modeling Moving camera Non-panoramic background:  1 Introduction  In these days, monitoring of public and private spaces is required because of the steady increase in crimes and safety issues [17]. As a result, large numbers of camera are deployed in various sites, such as airports, infrastructures, hospitals, and homes and trained people are watching real-time videos through closed-circuit TV (CCTV) system to find any interesting things. However, humans are not capable of watching many cameras simultaneously, and this results in ineffective/failed surveillance scenarios. For this reason, automated visual surveillance system is widely researched and actually implemented in various areas from the perspective that vision-based security systems are easy to set up, inexpensive, and non-obtrusive [20]. Automated visual surveillance system generally starts with detection of moving objects from the scene with the assumption that nothing might happen in areas without any motion. The detected moving objects might be turned over to the tracking process as initial positions [25] or used as regions to be classified in recognition. Moreover, moving object detection itself can alert interesting areas where it is required to be focused on (intrusion, line cross, etc.). Extraction of moving objects is an important and fundamental research topic of surveillance system in these senses [12]. Among traditionally proposed algorithms to extract moving objects, the background subtraction technique is one of the most successful approaches in this area [7, 11, 21–23, 27]. These methods build statistical background model and extract moving objects by finding regions which do not have similar characteristics to the background model. However, they have limitation that they are only applicable with the stationary cameras: the cameras should not move (even mechanical vibrations can make detection errors) and their fields of view are fixed, which lead to increasing number of required cameras to cover large areas. Detection of moving objects with moving cameras (including pan–tilt–zoom and hand-held cameras) has been researched to overcome this limitation of installment and inefficiency. The most typical method for detecting moving objects with non-stationary cameras is the extension of background subtraction method [ 4,  6,  8– 10,  14,  18,  19]. In these methods, panoramic background models are constructed by applying various image registration techniques [ 1– 3,  13] to input frames and the position of current frame in panoramas is found by image matching algorithms. Then, moving objects are segmented in a similar way to the fixed camera case. Cucchiara et al. [ 6] and Robinault et al. [ 19] estimated camera motion matrix by comparing input frame to background mosaic and did foreground segmentation. They used different type of motion model, but both of them do not consider image registration error by parallax effect. Kang et al. [ 10] built background mosaic considering internal parameters of cameras. However, camera internal parameters are not always available and possible registration errors are still not considered in this method. To solve the problem of registration errors and segment moving object robustly, Bhat et al. [ 4] used cylindrical mosaics and Ren et al. [ 18] proposed spatial distribution of Gaussian. Hayman et al. [ 9] considered one pixel as spatial mixture of random processes, sum of signal and noise, to solve possible registration errors. Mittal [ 14] and Guillot et al. [ 8] improved adaptability of background mosaic by modifying background update and using key point matching method. These algorithms commonly require a panoramic background mosaic before extracting moving objects. However, mosaic-based approaches cannot avoid several limitations. The underlying assumptions for using panorama include sufficiently accurate motion model, accurately estimated parameters, and distortion-free lenses [ 18]. Moreover, they suffer from several difficulties including background adaptation, stitching error accumulation, slow initialization, and large computation memory and time problems. These problems are illustrated in Fig.&amp;nbsp; . The second method to detect moving objects with moving camera is optical flow [24, 28]. They assume that the motions of backgrounds and foreground objects are divided by different optical flows and reduce moving object detection to motion segmentation problem. This scheme has been adopted in the following approaches. Zhang et al. [28] used focus of expansion and its residual map to segment moving objects in the scene. Thakoor et al. [24] used dense optical flow and detected moving objects by comparing it to the estimated camera motion. However, in the perspective of detecting moving objects, dense optical flow requires heavy computation and camera motions should be relatively small. In this paper, a robust and novel moving object detection algorithm is proposed to solve the existing problems in traditional approaches. The proposed method detects moving objects with a moving camera robustly without panoramic background model. This allows real-time and online computation, and problems of panoramic mosaic (stitching error accumulation and background model adaptation) can also be avoided. Background model is updated with spatial and temporal information, and several pre- and post-processing methods are effectively combined to improve the performance of the proposed algorithm.    2 Overall flow of algorithm  The framework of the proposed method is illustrated in Fig.&amp;nbsp; . The highlight of this paper is the part of spatio-temporal Gaussian model update scheme for adaptation of non-stationary background due to moving camera. In addition, several techniques including a proportional-integral-derivative (PID) control are utilized systematically as shown in Fig.&amp;nbsp; . Because camera induces its own motion, the estimation of camera motion, which is the same as background motion, is required to prevent background regions from falsely being detected as foregrounds. In the proposed method, Lucas Kanade Tracking method (LKT) [ 13] is adopted to estimate the camera motion between current frame and background model. After the motion of camera is estimated, difference of Gaussian filter (DoG) sharpens both current frame and background model, and the calculated camera motion warps the filtered background model to the filtered current frame. This warping divides the current input frame into two regions: overlapped region and newly covered region. Moving objects are extracted only in the overlapped region and newly covered regions are directly modeled as a part of background. The main difference between traditional panorama-based approaches and our algorithm is that those approaches actually stitch several frames to build large panoramic background, while we simply use image registration method to find overlapped region and newly covered region in the current frame. The extraction of moving objects is processed by background subtraction based on pixel-wise spatio-temporal distribution of Gaussian. In Fig.&amp;nbsp; , temporary foreground mask map is illustrated under moving object detection (MOD) part. Here, white region is extracted foregrounds and black region is background part. The detection results shown here, however, still have silhouette distortion problem. This can be caused by two reasons: foreground colors similar to background regions and label (foreground or background) decision based on local neighborhoods. Hence, the foreground segmentation result is further refined by PID control-based smoothing/estimation process and probabilistic morphology and, finally, the regions labeled as background are used to update the current background model in the spatio-temporal sense. The details of the methods are explained in each section below.    3 Preprocessing  In the proposed method, 2D DoG filter [ 26], a well-known image-sharpening algorithm is applied to captured frames before main processes. This filter strengthens edges on image and makes them more noticeable. The effect of this filter and its shape is shown in Fig.&amp;nbsp; , and the general equation for 1D DoG filter is            where   is pixel’s intensity value. For each pixel, the difference of its values in two Gaussian distributions with different standard deviations increases the visibility of edges. Figure&amp;nbsp; a shows a walking person in the field, but it is hard to recognize because of low-contrast video quality. However, in Fig.&amp;nbsp; b, we can see that edges of person are strengthened after applying DoG filter. By building background model with filtered image, we get more discriminative background models even for the low-contrast video. Every step in the proposed method is processed with DoG filtered image except calculating motion of camera between current frame and background model in image registration part. Finding camera motion between consecutive frames is preferred to be done in gray channel by experiments. Since DoG filter strengthens the edges on current input and background image, it makes detection of small moving foregrounds even in low-contrast scenes as the video seq. 4 and the video seq. 8 in Fig.&amp;nbsp;  possible. However, it may cause false alarms near edges and misclassify background pixels as foreground ones as seen in Fig.&amp;nbsp; , where a portion of background is labeled as foreground. The performance gain and loss by DoG filter on several test clips is given in Table&amp;nbsp; 1. By comparing precision and recall values in Table&amp;nbsp; 1, we can see that precision values is almost same in both cases because the number of true positive and false positive are decreased together by removing DoG filter in our process. However, the recall values are decreased significantly due to a large number of false-negative samples, arising from turning off the DoG filter. This high drop of recall values can be seen especially in videos with low-contrast/small objects. More than the precision and the recall values, the shapes of foregrounds are enhanced by DoG filter in many cases as shown in Fig.&amp;nbsp; .    4 Warping and background modeling  To detect moving objects with non-stationary camera, most approaches build panoramic background model, find areas where current frame is corresponding to, and, then, compare current frame to corresponding background model. However, there are several drawbacks in this type of approaches. Without information of intrinsic and extrinsic camera parameters, building large panorama suffers from accumulated stitching errors and it is hard to find the accurate position of current frame in the panorama. Moreover, some part of panorama is not updated frequently because of obsolete background information, which leads to incorrect moving objects detection. Finally, the system takes initialization time to build panorama by sweeping all possible ranges of camera motions, and the system itself is very heavy. To avoid these problems, the proposed method does not make large panoramic background model, but constructs only a small background model corresponding to the current frame in an on-line manner. The first task for moving objects detection with the small size background model is to find overlapped regions between background model and current frame. The first task of conventional approaches was to find corresponding part to a current input image in the large panoramic background. In the proposed method, not all pixels are overlapped between background model and current frame after camera motion because background size is the same as camera input. For this reason, a task for dividing current frame into overlapped regions and newly covered region is required and separate procedures for the two different types of regions should be implemented. In our method, the overlapped region between current frame and background model is chosen by simple feature tracking method; extracting and tracking features by LKT [ 13]. If we define   as  th extracted feature on background model and   as  th corresponding tracked feature on current frame, then, we can solve the below equation for transform matrix   between background model and current frame as,                         where  , and   are actual positions of feature points. The transform matrix   is a   matrix and it describes the relationship between background model and current frame in least square sense. By multiplying estimated transform model   on background model, we can warp background model with respect to current frame and find overlapped region between them as in Fig.&amp;nbsp; . The remaining region defined as newly covered region of current frame (black area in Fig.&amp;nbsp; ) is firstly labeled as background regions. In the proposed method, the estimation of transform model  is processed globally; all feature points are used to estimate single  for the whole image plane. Conventional methods using global estimation method have suffered from estimation error and its accumulation. In most cases, the estimation error mainly arises from inexact matches of some pixels from least squares solution  and subpixel accuracy. However, in our method, this estimation error can be ignored because our method suppresses the influence of the inexact matches, which is usually within few pixels, by considering local neighborhood of pixels in background modeling and label decision of pixels. Moreover, the accumulation of error does not happen in our method that does not stitch images to build large panoramic background model, which is made by repeated multiplication of previous s and comparison between transformed panoramic model and current frame. After image warping step, moving objects are extracted from the overlapped regions in current frame. For fixed camera case, this step is quite reliable even when using conventional approaches. However, extracting moving objects with moving camera has to deal with several problems. The first problem is image registration error. It is well known that zero registration error is not possible because of many factors, such as parallax effect and sub-pixel accuracy. This registration error leads to false labeling of motionless pixels as foregrounds. In the proposed method, these false alarms are removed by considering neighbor pixels in the decision of the labels (foreground or background) of a pixel as follows. For a pixel   in current frame, candidate anchor point   in background model is computed by       where   is the transform matrix between current frame and background model, calculated in the Sect.&amp;nbsp; 4.1. Then, the label of the pixel   is determined by comparing   to the neighbor pixels of  , as                                      where   and   are the standard deviation and the mean value in background model, and   stands for the intensity value in current frame.  , which is a point in background model, is actual corresponding pixel of  . Large difference between   and   means that transform matrix   is not precise and registration error occurs in image warping step.   is a threshold to decide the labels of pixels and it is set to 2.5 for all experiments in the proposed method. The illustration of decision concept by considering neighbor pixels is shown in Fig.&amp;nbsp; , and the difference between   and   (black arrow in right-most image) occurred by registration error. This soft comparison method deals with general registration errors and significantly reduces the number of false alarms. Figure&amp;nbsp;  is the result of detecting moving objects with/without considering neighbor pixels. It is easy to see that the number of false alarms is crucially reduced by using neighbor pixels concept. However, considering pixels within local area has a drawback. The silhouettes of moving objects might be distorted and their size can be reduced when the detected moving objects have similar color to their neighbor background pixels or objects to detect are small. To handle this problem, we use PID control-based tracking and probabilistic morphology refinement step, which are described in Sect.&amp;nbsp; 5. The second problem is how to deal with newly covered regions. Because panoramic background model is not constructed in our approach, motions of camera cause newly covered region in current frame, which does not have corresponding background model to compare with. For this reason, the pixels in this region are labeled as background directly and are used to update background model. Although foreground objects located in this area might be modeled as background at their first appearances, we experimentally check that they are detected as foreground in a short time because of their different motions to the camera motion. As fixed camera case, the background update stage is required to build adaptive model for illumination changes and view-point changes in detection with moving camera. To represent background model, we modify single Gaussian model, which has one mean and one variance per pixel. Similar strategy to [ 22], online K-means approximation, is applied, but only background-labeled pixels are used to update background model. The conventional single Gaussian model, however, is not enough to solve accumulated error in image registration when camera itself has motions. To handle this problem, we propose spatio-temporal background modeling method in this paper. By adding spatial and temporal terms to mean and variance update equations, we improve flexibility of background model for registration error and adaptiveness for changing views of cameras. The key concept for temporal term is to use different learning rate depending the age of background. Newly presented background with small age is rapidly updated with large learning rate (the example of age is shown in Fig.&amp;nbsp; ). For a pixel   in current frame and its corresponding pixel   in background model, background update equation is                                      where   and   are mean and variance in warped background model at time   described in Sect.&amp;nbsp; 4.1, respectively. The important notation to notice is that  , rather than  , is used. This spatial soft decision decreases the number of falsely detected pixels, which are generally caused by registration errors in the range of few pixels.   in Eq.&amp;nbsp;( 10) is defined as how many frames we have information on certain pixel   and this is illustrated in Fig.&amp;nbsp; . It works as the learning rate in general background modeling method. However, the learning rate in conventional approaches has limitation because it is usually constant for all pixels over entire time range. On the other hand, the proposed learning rate scheme (age) is varying over temporal and spatial domain. In Fig.&amp;nbsp;a we can see that the age values are initially set to 1 for entire images. When camera is moved to right direction in the second frame, the overlapped regions are located in the left parts of current frame and right parts are newly covered regions. Then, the age values in overlapped areas are increased and those in the newly covered region are set to one again. The numbers in Fig.&amp;nbsp; are changing age values in consecutive frames. Different ages in pixels adjust adaptiveness of background to current frame, and this makes newly covered region directly modeled as background ( and  are both 1 for newly covered region). There are several advantages to the proposed spatio-temporal background modeling method. First, modeling itself considers spatial window and increases the effects of consideration of neighbor pixels in decision of the labels of pixels. The background model becomes less accurate without spatial background model update. Second, temporal information, depending on the age in background model, reduces false alarms and increases the adaptiveness of background model. Moreover, dynamic learning rate does not have delay in detecting moving objects, whereas a constant learning rate requires background setup time and causes slow system initialization.    5 Tracking and refinement  As described before, objects detection by considering neighbor pixels might cause holes or distortions in detection masks. In this paper, this problem is solved with aid of object tracking methodology. We track extracted moving objects (maintain their labels) by matching detected blobs between consecutive frames through data association method. Detected blobs in the previous frame and those in the current frame are matched and share their labels considering their mask appearances, positions, and sizes. After labels of objects are decided, their positions and sizes are smoothed. The positions and the sizes of the detected blobs usually have noisy values from drastic changes of shape and size of detected object blobs. In order to visualize the smooth motion of the object and improve the matching performance, it is necessary to filter out noisy observations (positions and sizes) of the objects. In our paper, we propose a filtering scheme mimicking the discrete proportional-integral-derivative (PID) control method [ 15]. The proposed method is depicted as in Fig.&amp;nbsp;  for the case of the center position of an object. The reference input of the control system is given by the current position of the detected blob,  . This implies that the output   of the PID control system tracks the observation   smoothly. The control input   is given by PID control strategy with negative feedback as shown in Eq.&amp;nbsp;( 12). Then the smoothed output   is obtained by where , and  are PID coefficients, and  is the frame time index. By the above equations,  is minimized smoothly over time and we can get smoothed position of the object by using  instead of  for the object position at time . The same scheme is applied for the case of size (width and height) of each object. After the label of an object is maintained and its expected position and size are determined by PID control method, holes or distortions in detection masks are removed with the following steps. If a detected blob maintains its label more than   number of frames with tracking method, then, the object can be regarded as a solid object, which is not generated by image registration error. In this case, the spatial soft decision is not required to be applied for the object’s expected region in the next frame because the spatial soft decision is designed to suppress image registration error. Without spatial soft decision on the region of the detected blob, the detected blob can have better silhouette (less holes and distortions). Consequently, possible distortions of object silhouette from excessive consideration of neighbor pixels can be reduced through the PID scheme and selective application of the spatial soft decision on the evidence of object labeling. The scheme is illustrated in Fig.&amp;nbsp; , and, for the experiments, the PID coefficients   were set to (0.2, 0.003, 0.03), and the   was 15 in this paper. More than PID tracking method, probabilistic morphology is additionally applied to the extracted moving objects for better silhouettes of objects. When the object has similar color to its background model, detection masks might have holes and it decreases the performance of tracking and recognition processes. For this reason, in conventional approaches, various morphological operations are applied to detected foreground regions. However, simple morphological operation does not recover whole objects, or, sometimes, falsely increases the size of masks and distorts the true shape of the original object. In our paper, we propose probabilistic morphological operation to build better silhouette of moving objects. When moving objects are extracted as foreground pixels, we can define foreground labeled pixel set in current frame&amp;nbsp;as       and its surrounding candidate set as            where   is the label of pixel (foreground/background) and   is the Euclidean distance between pixel   and   in frame domain. In the proposed method,   is set to 3 and not changed. In Fig.&amp;nbsp; b, the pixels in the set   are white and the pixels in the set   are grey as they are defined by equation&amp;nbsp;( 14) and ( 15). The label of pixels in the set   is currently background, but those pixels are candidates to be transformed as foreground-labeled. While conventional morphology uses detected mask information only, proposed morphology method uses colors for better refinement. In detail, the candidate pixels are transformed to foreground when they have similar color values to the currently detected foreground pixels in neighborhood more than to nearby background pixels. For each pixel   in the set  , its label   is transformed to foreground with following equation:                                                                where   is the pixel value in current frame, and   is the mean value in background model.   and   stands for foreground and background respectively. The transformed pixels (from background to foreground) are grey pixels in Fig.&amp;nbsp; c.   
Springer.tar//Springer//Springer\10.1007-s00138-012-0449-x.xml:Classifying web videos using a global video descriptor:Video descriptors Action recognition Frequency spectrum Spatio-temporal analysis:  1 Introduction  Due to the massive number of videos uploaded online each day, recognizing actions and scenes in videos is an important problem in computer vision research. In the literature, several approaches have been proposed for the recognition of actions in diverse real-world videos. Recent survey papers [4, 5] provide a detailed overview of the present approaches, challenges and the available datasets. Laptev et al. [6], addressing the demand for datasets of large number of realistic action classes, introduced Hollywood dataset with eight action classes, collected from the movies. Liu et al. [7] introduced the UCF YouTube dataset consisting of 11 categories of actions collected from YouTube and personal videos. UCF50 [2] extended the 11 action categories of the UCF YouTube dataset to a total of 50 action categories. Kuehne et al. [3] presented the very challenging HMDB51 dataset of 51 action classes with video clips collected from a wide range of sources. Provided these datasets, several approaches of human action recognition that fall into two broad groups were proposed: global methods and local spatio-temporal interest point-based methods. Global methods represent the actions based on holistic information about the action and scene. These methods often require the localization of the human body through alignment, background subtraction or tracking. Some common representations of holistic methods are silhouettes [8, 9] and motion [10, 11]. Holistic models perform well in a controlled environment; however, on datasets of low-quality web videos, conditions such as the presence of clutter in the background, occlusion, variance in illumination and viewpoint changes make the use of these methods impractical. Recently, trajectory-based methods were proposed for action recognition. Wu [12] captured ensemble motions of a scene on a set of dense Lagrangian particle trajectories, which were computed by numerical integration of optical flow over time. Then actions were described by the use of chaotic features extracted on object motion trajectories, which were obtained after low rank minimization and clustering of all trajectories. Similarly, Wang et al. [13] represented the videos by computing motion boundary histograms, HOG, HOF and trajectory descriptors along the dense foreground motion trajectories. Then, they utilized the standard bag-of-features model for the final representation of the videos. Both of these methods only extracted features on the dynamic trajectories belonging to the scene foreground and did not capture the context information such as the objects present in the background or scene properties, which may be helpful for recognizing actions. In addition, computation and analysis of trajectories may be computationally expensive. Spatio-temporal interest point-based methods represent the scene and the performed actions as a combination of local descriptors, which are computed in a neighborhood of interest points. The neighborhood can be selected as an image patch or as a spatio-temporal volume, called cuboids, in a video. The spatio-temporal interest point-based methods have received a lot of attention in the vision community due to their robustness to scale and viewpoint invariance. Laptev et al. [14] introduced the Space–Time Interest Point detector, a three-dimensional (3-D) variant of the Harris corner detector [15], which identified the points with high variations in intensity and motion. They used a bag-of-features representation on histogram of oriented gradients (HOG) and histogram of optical flow (HOF) descriptors for recognizing natural human actions [6]. Observing the sparseness of the detected STIP interest points, Dollar [16] proposed an alternative feature detector, which computes the response after the application of separable Gaussian filters in space and a quadrature pair of Gabor filters in the time domain for each pixel, followed by the computation of local maxima. Depending on the response, they simply compute gradients or optical flow on cuboids, then flatten them and finally apply principal component analysis (PCA) for dimension reduction. Klaser et al. [17] presented a video descriptor which is based on histograms of oriented 3-D spatio-temporal gradients. Scovanner et al. [18] proposed 3-D SIFT, an extension of the SIFT descriptor to spatio-temporal data. The local descriptor approaches are less sensitive to noise or occlusion; however, they require the detection of sufficient and relevant interest points and lack the capability of modeling the global geometrical or temporal information. Furthermore, these approaches, often utilize the bag-of-features model, requiring the quantization of large amount of data. Even though the interest points and the features are computed locally, each sequence is represented by a global histogram, which does not carry any spatial or temporal information. There are also recent works aimed at capturing the relationships between actions and the scene. Ikizler-Cinbis et al. [19] extracted multiple features on the human, objects and scene, and utilized a multiple-instance learning framework for human action recognition on YouTube videos. However, their approach required motion compensation for foreground estimation, and also the detection and tracking of the human in the scene. Liu et al. [7] extracted a combination of motion and static features and utilized the PageRank algorithm to prune the static features using motion cues as an alternative way to motion compensation. The hybrid use of motion and static features improved the performance of their approach. In our work, we aimed to design a descriptor which captures both motion and scene information without the need of motion compensation. The “gist” proposed by Torralba et al. [ 20] is a global scene descriptor based on power spectrum features, and it has the state-of-the-art performance for scene classification. However, it is not suitable for action recognition as it does not capture the motion information. We believe the computation of frequency spectral components of videos may provide useful scene and motion information for action classification. In this paper, we propose a global descriptor for videos, to be used for the action classification of challenging datasets such as UCF50 and HMDB51 with a large number of action classes, some of which are illustrated in Fig.&amp;nbsp; . Our descriptor is generated by applying a bank of 3-D spatio-temporal filters on the frequency spectrum of a sequence. The bandpass nature of these filters alleviates the need for motion compensation. Furthermore, as opposed to the approaches which apply bag-of-features model, our approach preserves the spatial and temporal information, as we perform quantization in fixed spatio-temporal sub-volumes after application of each filter on the frequency spectrum and taking the inverse Fourier transform. As the filter responses for all filters on all sub-volumes are concatenated, the ordering and the length of each feature vector are identical for all represented video clips. The framework of our approach is shown in Fig.&amp;nbsp; . In summary, our main contribution in this paper is the presentation of a new global motion and scene descriptor for the classification of realistic videos. We compared the performance of a state-of-the-art local descriptor to our global descriptor. Our global descriptor obtained the highest classification accuracies on two of the most complex datasets, UCF50 and HMDB51, among all published results, to the best of our knowledge. Moreover, the combination of these local and global descriptors resulted in a further improvement in the results. The structure of our paper is as follows. In Sect.&amp;nbsp;2, we present the basics of our approach. We describe the implementation details in Sect.&amp;nbsp;3. In Sect.&amp;nbsp;4, we present the quantitative results on the KTH, UCF50 and HMDB51 datasets. Finally, we conclude our work in Sect.&amp;nbsp;5.    2 Gist of a video  Feature extraction and the computation of descriptors are crucial tasks in action recognition and the classification of videos. In this paper, we introduce a 3-D global descriptor for real-world videos, such as those included in the UCF50 and HMDB51 datasets. Videos which involve similar actions tend to have similar scene structure and motion. The regularities in the appearance or motion can be used to pinpoint the type of actions involved in the video, and can be useful in the classification of videos. The frequency spectrum computed for a video clip could capture both scene and motion information effectively [20–22], as it represents the signal as a sum of many individual frequency components. In a video clip, the frequency spectrum can be estimated by computing the 3-D discrete Fourier transform (DFT). The motion is an important element which can be representative of the type of performed action in a scene. It can be explained in a straightforward way by considering the problem in the Fourier domain [ 22]. The frequency spectrum of a two-dimensional pattern translating on an image plane lies on a plane, the orientation of which depends on the velocity of the pattern. Given a 2-D image  , we can create a volume, space–time image sequence, by translating   with a velocity   over time. This volume is then expressed as       The three-dimensional Fourier transform of   over space and time is computed as            where   and   are the width, height and length of the clip, and   and   are the spatial positions and time of each point in the created volume. Here, the 3-D DFT of the volume will have the same size as the volume itself. After substituting Eq.&amp;nbsp; 1 in Eq.&amp;nbsp; 2 and rearranging the terms, the Fourier transform formula would be            The inner term in Eq.&amp;nbsp; 3 is actually the 2-D Fourier transform of  ; hence, the equation may be simplified to            where   represents the 2-D Fourier transform of  . The Fourier transform of the complex exponential term is a Dirac delta function, hence we obtain       where   is the Dirac delta function. Thus   will have non-zero values on a plane passing through the origin, as the delta function will be non-zero only when  , as shown in Fig.&amp;nbsp; a–c. This derivation shows that analyzing the Fourier transform of a signal, the motion in a sequence can be estimated by finding the plane which contains the power. Furthermore, multiple objects with different motion will generate frequency components in multiple planes as depicted in Fig.&amp;nbsp; d–r. Since the motion can occur in different directions and frequencies, in our work we use 3-D Gabor filters of different orientations and center frequencies to effectively capture the motion information in a video clip. By filtering the frequency spectrum with a certain oriented filter and taking the inverse Fourier transform, the motion and scene components which are normal to the orientation of the filter are pronounced, as illustrated in the example in Fig.&amp;nbsp; .    3 Implementation  A flowchart describing the implementation of our method is depicted in Fig.&amp;nbsp;. Our goal is to represent each video sequence by a single global descriptor and perform the classification. For the current implementation, we extract K uniformly sampled clips of a fixed length from each given video. As the second step, we compute the 3-D DFT and obtain the frequency spectrum of each clip as given by Eq.&amp;nbsp; 2. In order to capture the components at various intervals of the frequency spectrum of a clip, we apply a bank of narrow band 3-D Gabor filters with different orientations and scales. The transfer function of each 3-D filter, tuned to a spatial frequency   along the direction specified by the polar and the azimuthal orientation angles   and   in a spherical coordinate system, can be expressed by            where  ,   and  . The parameters  ,   and   are the radial and angular bandwidths, respectively, defining the elongation of the filter in the spatio-temporal frequency domain. The combination of our filters is selected to cover only half of the total volume of the frequency spectrum due to the symmetrical nature of the discrete Fourier transform. 3-D plots of these filters are shown in Fig.&amp;nbsp; . Applying each generated 3-D filter on the frequency spectrum of the clip, we compute the output       where   is the output when the  th filter is applied. Then we take the inverse 3-D DFT       By quantizing the output volume in fixed sub-volumes and taking the sum of each sub-volume and performing the same computation for each filter in our filter bank, we obtain a long feature vector which represents a single clip. This feature vector has the advantage of preserving the spatial information as the response of each filter on each sub-volume contributes to an element in the concatenated feature vector. The last step is to apply PCA, a popular method for dimensionality reduction, in order to generate our global video descriptor.    4 Experimental results  To test the performance of our approach, we used publicly available datasets: KTH, UCF50 and HMDB51. UCF50 and HMDB51 are the two most challenging datasets with the largest number of classes, which are collections of thousands of low-quality web videos with camera motion, different viewing directions, large interclass variations, cluttered backgrounds, occlusions and varying illumination conditions. For all experiments, we picked three key clips of 64 frames from each video and downsampled the frames of clips to a fixed size ( ) for computational efficiency. Picking more than three clips did not result in a further improvement in performance as depicted in Fig.&amp;nbsp; ; hence, we report results based on three clips. Next, we computed the 3-D DFT to compute the frequency spectrum of the clips of each video and then applied the generated filter bank. Our generated filter bank, described in Sect.&amp;nbsp; 3, consisted of 68 3-D Gabor filters, which corresponded to 2 scales and 37 and 31 orientations for the first and second scales, respectively, in the spatio-temporal frequency domain. The filters are shown in Fig.&amp;nbsp; . The selection of filters was designed experimentally to capture the frequency components effectively as shown in Fig.&amp;nbsp; ; the cumulative power spectrum of 500 videos of different actions was computed and our filter bank captures more than 99&amp;nbsp;% of the total power. There was no need for another set of filters, which captures very high frequencies with negligible power. However, increasing the number of filters in the pass band makes the filters become narrower and the descriptor to have a finer response, with a penalty of higher computational requirements. As an experiment, we tested a three-scale filter set with 64 narrower filters per scale and obtained an additional 2.5&amp;nbsp;% performance improvement on UCF50. Considering the computation time trade-off, we did not use this configuration for the reported results. In our experiments, the central frequencies for the two scales of filters were set to 38.8 and 19 with radial bandwidths 14.2 and 8.6, respectively. The angular bandwidths  and  were set to 0.2 and 0.1, respectively. Each of the filters we computed had  as the size of the frequency spectrum of clips. After the application of the filters, we computed the average response of filters on 512 uniformly spaced  sub-volumes, to quantize and generate the global feature vector for the clip. The length of the feature vector in our experiments was 104,448, as there are 68 filters, 512 sub-volumes and 3 key clips. We reduced the dimensionality of the feature vectors to 2,000 using PCA [23]. To achieve even higher performance, we tested combining our descriptor with a state-of-the-art local descriptor [24], Space–Time Interest Points (STIP). We computed dense STIP features, and generated 1,000 and 2,000-dimensional codebooks to represent each sequence as a histogram. For classification, we trained a multi-class support vector machine (SVM) [25] using the linear kernel for our descriptor and histogram intersection kernel for STIP. We performed cross validation by leaving one group out for testing and training the classifier on the rest of the dataset and performing the same experiment for all groups on UCF50. For HMDB51, we performed cross validation on the three splits of the dataset. We did not include any clips of a video in the test set if any other clip of the same video was used in the training set. The discriminative power of our descriptor can be seen clearly in the example in Fig.&amp;nbsp; . This basic experiment was done using four sequences from a public dataset. For each of the four sequences, we computed the descriptors. Each entry in the matrix in Fig.&amp;nbsp; c is the normalized Euclidean distance between the computed descriptors of the four sequences. As seen in the matrix, the descriptor distances between the jumping actions in two different scenes is comparably lower than the other distances, which shows that our descriptor can generalize over intra-class variations. The distances are high when different actions are performed in different scenes, such as the ones labeled by blue arrows in Fig.&amp;nbsp; a. To illustrate the advantage of the 3-D global descriptor, we compare our descriptor with the popular descriptors: GIST [ 20] (on UCF50 dataset) and STIP[ 14,  24] (on KTH, UCF50 and HMDB51 datasets) which involve the computation of histograms of oriented gradients (HOG) and histograms of optical flow (HOF). For comparison, we also list the performance of a low-level descriptor based on color and gray values [ 3] (on UCF50 and HMDB51 datasets), and the biologically motivated C2 features [ 3,  26] (on KTH and HMDB51 datasets). Figure&amp;nbsp;  shows the comparison of performance over three datasets. The KTH dataset includes videos captured in a controlled setting of six action classes with 25 subjects for each class. As depicted in Table&amp;nbsp; 1, our descriptor has a classification accuracy of 92.0&amp;nbsp;%, which is comparable to the state of the art. Figure&amp;nbsp;  shows the confusion table for this dataset. This experiment shows that our descriptor is able to discriminate between the actions with different motions appearing in similar scenes. This dataset includes unconstrained web videos of 50 action classes with more than 100 videos for each class. As depicted in Table&amp;nbsp; 2, our descriptor has an accuracy of 65.3&amp;nbsp;% over 50 action classes, which outperforms GIST and STIP. For evaluating the performance of the GIST descriptor, we have used various numbers (3, 20, 40) of sampled frames for each video and performed classification after concatenating the computed descriptors for each frame. The accuracy increased up to 42.4&amp;nbsp;% when 40 frames were used. Figure&amp;nbsp;  shows the confusion tables for GIST, STIP and our descriptor, GIST3D. Using the combination of STIP and GIST3D by late fusion resulted in a classification accuracy of 73.7&amp;nbsp;%, which is another 8&amp;nbsp;% improvement in the performance. Figure&amp;nbsp; d depicts the confusion table for the combined classification. For comparison of our descriptor to STIP, we also analyzed the average similarities of descriptors among action classes of UCF50. We computed the Euclidean similarity for our descriptors and histogram intersection as the similarity measure for STIP. Our descriptor has higher intra-class similarity and lower inter-class similarity than STIP as shown in Fig.&amp;nbsp; . This clearly explains why our global descriptor (GIST3D) performs superior than STIP. The HMDB51 dataset includes videos of 51 action classes with more than 101 videos for each class. As depicted in Table&amp;nbsp; 3, our descriptor has a classification accuracy of 23.3&amp;nbsp;% over 51 action classes, which outperforms STIP by 5&amp;nbsp;%. The late fusion classifier of these two descriptors resulted in a 6&amp;nbsp;% improvement in the performance over using just our descriptor GIST3D. Figure&amp;nbsp;  shows the confusion tables for STIP, our descriptor and the fused classifier. The actions in the video sequences of HMDB51 are not isolated; multiple actions may be present in a single video sequence despite a given single class label for the sequence. There is also large intra-class scene variation. Therefore, classifying actions on this dataset is more challenging and the performances of the mentioned methods are lower. By comparing our classification accuracy with the tested descriptors and analyzing the Tables&amp;nbsp;2 and&amp;nbsp;3, we found out that GIST, being a scene descriptor, suffered from the lack of captured motion information. For example, as observed in Fig.&amp;nbsp;b, GIST cannot differentiate between the actions of playing guitar and violin which happen in similar indoor scenes. The motion is discriminative for these videos and our descriptor (GIST3D) is able to differentiate these actions, as shown in Fig.&amp;nbsp;c. Conversely, STIP suffered from locality, as it did not capture the global scene structure and did not carry spatio-temporal information due to the global histogram representation. For example, walking with dog and horse riding actions have similar translating motion, and STIP is not as discriminative as our descriptor for these two actions, as depicted in Fig.&amp;nbsp;a. The horse riding videos mostly have rural scenes with periodic vertical and horizontal components such as fences, whereas the walking with dog videos contain urban or park scenes. Our descriptor encodes the useful scene information and is able to discriminate between these two actions. &amp;nbsp;    5 Conclusion 
Springer.tar//Springer//Springer\10.1007-s00138-012-0450-4.xml:Recognizing 50 human action categories of web videos:Fusion Action recognition Web videos:  1 Introduction  Action recognition has been a widely researched topic in computer vision for over a couple of decades. Its applications in real-time surveillance and security make it more challenging and interesting. Various approaches have been taken to solve the problem of action recognition&amp;nbsp;[20]; however, the majority of the current approaches fail to address the issue of a large number of action categories and highly unconstrained videos taken from web. Most state-of-the-art methods developed for action recognition are tested on datasets like KTH, IXMAS, and Hollywood (HOHA), which are largely limited to a few action categories and typically taken in constrained settings. The KTH and IXMAS datasets are unrealistic; they are staged, have minor camera motion, and are limited to less than 13 actions which are very distinct. The Hollywood dataset&amp;nbsp;[ 9], which is taken from movies, addresses the issue of unconstrained videos to some extent, but involves actors, contains some camera motion and clutter, and is shot by a professional camera crew under good lighting conditions. The UCF YouTube Action (UCF11) dataset&amp;nbsp;[ 10] consists of unconstrained videos taken from the web and is a very challenging dataset, but it has only 11 action categories, all of which are very distinct actions. The UCF50 dataset, which is an extension of the UCF11 dataset, also contains videos downloaded from YouTube and has 50 action categories. The recently released HMDB51 dataset&amp;nbsp;[ 8] has 51 action categories, but after excluding facial actions like smile, laugh, chew, and talk, which are not articulated actions, it has 47 categories compared to 50 categories in UCF50. Most of the current methods would fail to detect an action/activity in datasets like UCF50 and HMDB51 where the videos are taken from web. These videos contain random camera motion, poor lighting conditions, clutter, as well as changes in scale, appearance, and viewpoints, and occasionally no focus on the action of interest. Table&amp;nbsp; 1 shows the list of action datasets. In this paper, we study the effect of large datasets on performance, and propose a framework that can address issues with real-life action recognition datasets (UCF50). The main contributions of this paper are as follows:        We provide an insight into the challenges of large and complex datasets like UCF50.         &amp;nbsp;           We propose the use of moving and stationary pixel information obtained from optical flow to obtain our scene context descriptor.         &amp;nbsp;           We show that as the number of actions to be categorized increases, the scene context plays a more important role in action classification.         &amp;nbsp;           We propose the idea of early fusion schema for descriptors obtained from moving and stationary pixels to understand the scene context, and finally perform a probabilistic fusion of scene context descriptor and motion descriptor.         &amp;nbsp;      To the best of our knowledge, no one has attempted action/activity recognition on such a large-scale dataset (50 action categories) consisting of videos taken from the web (unconstrained videos) using only visual information. We provide an insight into the challenges of large and complex datasets like UCF50. We propose the use of moving and stationary pixel information obtained from optical flow to obtain our scene context descriptor. We show that as the number of actions to be categorized increases, the scene context plays a more important role in action classification. We propose the idea of early fusion schema for descriptors obtained from moving and stationary pixels to understand the scene context, and finally perform a probabilistic fusion of scene context descriptor and motion descriptor. The rest of the paper is organized as follows. Section&amp;nbsp;2 deals with the related work. Section&amp;nbsp;3 gives an insight into working with large datasets. In Sects.&amp;nbsp;4 and 5, we introduce our proposed scene context descriptor and the fusion approach. In Sect.&amp;nbsp;6, we present the proposed approach, followed by the experiments and results with discussions in Sect.&amp;nbsp;7. Finally, we conclude our work in Sect.&amp;nbsp;8.    2 Related work  Over the past two decades, a wide variety of approaches has been tried to solve the problem of action recognition. Template-based methods&amp;nbsp;[1], modeling the dynamics of human motion using finite state models&amp;nbsp;[6] or hidden Markov models&amp;nbsp;[21], and Bag of Features models&amp;nbsp;[4, 10, 11, 22] (BOF) are a few well-known approaches taken to solve action recognition. Most of the recent work has been focused on BOF in one form or another. However, most of this work is limited to small and constrained datasets. Categorizing large numbers of classes has always been a bottleneck for many approaches in image classification/action recognition. Deng et al.&amp;nbsp;[3] demonstrated the challenges of doing image classification on 10,000 categories. Recently, Song et al.&amp;nbsp;[17] and Zhao et al.&amp;nbsp;[19] attempted to categorize large numbers of video categories by using text, speech, and static and motion features. Song et al.&amp;nbsp;[17] used visual features like color histogram, edge features, face features, SIFT, and motion features and showed that text and audio features outperform visual features by a significant margin. With the increase in number of action categories, motion features alone are not discriminative enough for reliable action recognition. Marszalek et al.&amp;nbsp;[13] introduced the concept of context in action recognition by modeling the scenes. 2D-Harris detector is used to detect salient regions from which SIFT descriptor is extracted and bag-of-features framework is used to obtain the static appearance descriptor. Han et al.&amp;nbsp;[5] detects person, body parts, and the objects involved in an action and used the knowledge of their spatial location to design contextual scene descriptor. Recently, Choi et al.&amp;nbsp;[2] introduced the concept of “Crowd Context” to classify activities involving interaction between multiple people. In all the proposed methods [2, 5, 13], the performance depends on the detectors used. Extracting reliable features from unconstrained web videos has been a challenge. In recent years, action recognition in realistic videos was addressed by Laptev et al.&amp;nbsp;[9] and Liu et al.&amp;nbsp;[10, 11]. Liu et al.&amp;nbsp;[10] proposed pruning of the static features using PageRank and motion features using motion statistics. Fusion of these pruned features showed a significant increase in the performance on the UCF11 dataset. Ikizler et al.&amp;nbsp;[7] used multiple features from the scene, object, and person, and combined them using a Multiple MIL (multiple instance learning) approach. Fusion of multiple features extracted from the same video has gained significant interest in recent years. Work by Snoek et al.&amp;nbsp;[14] compares early and late fusion of descriptors. There has been no action recognition work done on very large datasets, using only visual features. In this paper, we propose a method which can handle these challenges.    3 Analysis on large-scale dataset  UCF50 is the largest action recognition dataset publicly available, after excluding the non-articulated actions from the HMDB51 dataset. UCF50 has 50 action categories with a total of 6676 videos, and with a minimum of 100 videos for each action class. Samples of video screenshots from UCF50 are shown in Fig.&amp;nbsp; . This dataset is an extension of UCF11. In this section, we perform a baseline experiment on UCF50 by extracting the motion descriptor and using the bag of video words approach. We use two classification approaches:         BoVW-SVM:              &amp;nbsp;            BoVW-NN:              &amp;nbsp;        Which motion descriptor do we use? BoVW-SVM: support vector machines (SVM) to do classification. BoVW-NN: nearest neighbor approach using SR-Tree to do classification. Due to the large scale of the dataset, we prefer a motion descriptor which is faster to compute and reasonably accurate. To decide on the motion descriptor, we performed experiments on a smaller dataset KTH with different motion descriptors, which were extracted from the interest points detected using Dollar’s detector&amp;nbsp;[ 4]. At every interest point location   , we extract the following motion descriptors:     Gradient: At any given interest point location in a video , a 3D cuboid is extracted. The brightness gradient is computed in this 3D cuboid, which gives rise to three channels  that are flattened into a vector, and later PCA is applied to reduce the dimension.    Optical flow: Similarly, Lucas–Kanade optical flow [12] is computed between consecutive frames in the 3D cuboid at  location to obtain two channels . The two channels are flattened and PCA is utilized to reduce the dimension.    3D-SIFT: Three-dimensional SIFT proposed by Scovanner et al.&amp;nbsp;[15] is an extension of SIFT descriptor to spatio-temporal data. We extract 3D-SIFT around the spatio-temporal region of a given interest point . Gradient: At any given interest point location in a video , a 3D cuboid is extracted. The brightness gradient is computed in this 3D cuboid, which gives rise to three channels  that are flattened into a vector, and later PCA is applied to reduce the dimension. Optical flow: Similarly, Lucas–Kanade optical flow [12] is computed between consecutive frames in the 3D cuboid at  location to obtain two channels . The two channels are flattened and PCA is utilized to reduce the dimension. 3D-SIFT: Three-dimensional SIFT proposed by Scovanner et al.&amp;nbsp;[15] is an extension of SIFT descriptor to spatio-temporal data. We extract 3D-SIFT around the spatio-temporal region of a given interest point . All of the above descriptors are extracted from the same location of the video and the experimental setup is identical. We use BOF paradigm and SVM to evaluate the performance of each descriptor. From Table&amp;nbsp; 2, one can notice that 3D-SIFT outperforms the other two descriptors for codebook of size 500, whereas gradient and optical flow descriptors perform the same. Computationally, the gradient descriptor is the fastest and 3D-SIFT is the slowest. Due to the time factor, we will use gradient descriptor as our motion descriptor for all further experiments. We also tested our framework on the recently proposed motion descriptor MBH by Wang et al.&amp;nbsp;[18]. The MBH descriptor encodes the motion boundaries along the trajectories obtained by tracking densely sampled points using optical flow fields. Using the code provided by the authors&amp;nbsp;[18], MBH descriptors are extracted for UCF11 and UCF50 datasets and used in place of the above-mentioned motion descriptor for comparison of results with&amp;nbsp;[18]. In this experiment, we show that increasing the number of action classes affects the recognition accuracy of a particular action class. Since the UCF11 dataset is a subset of UCF50, we first start with the 11 actions from the UCF11 dataset and randomly add new actions from the remaining 39 different actions from the UCF50 dataset. Each time a new action is added, a complete leave-one-out cross validation is performed using bag of video words approach on motion descriptor and SVM for classification on the incremented dataset using a 500-dimension codebook. Performance using BoVW-SVM on the initial 11 actions is 55.46&amp;nbsp;% and BoVW-NN is 37.09&amp;nbsp;%. Even with the increase in the number of actions in the dataset, SVM performs significantly better than the nearest neighbor approach. Figure&amp;nbsp;  shows the change in performance by using BoVW-SVM on the initial 11 actions as we add the 39 new actions, one at a time. Increasing the number of actions in the dataset has affected some actions more than others. Actions like “soccer juggling” and “trampoline jumping” were most affected; they have a standard deviation of  7.08 and  5.84&amp;nbsp;%, respectively. Some actions like “golf swing” and “basketball” were least affected with a very small standard deviation of  1.35 and  2.03&amp;nbsp;%, respectively. Overall, the performance on 11 actions from UCF11 dropped by  13.18&amp;nbsp;%, i.e., from 55.45 to 42.27&amp;nbsp;%, by adding 39 new actions from UCF50. From Fig.&amp;nbsp; , one can also notice that 8 of 11 actions have standard deviation of more than  4.10&amp;nbsp;%. Analysis of the confusion table shows a significant confusion of these initial 11 actions with newly added actions. This shows that the motion feature alone is not discriminative enough to handle more action categories. To address the above concerns, we propose a new scene context descriptor which is more discriminative and performs well in very large action datasets with a high number of action categories. From the experiments on UCF50, we show that the confusion between actions is drastically reduced and the performance of the individual categories increased by fusing the proposed scene context descriptor.    4 Scene context descriptor  In order to overcome the challenges of unconstrained web videos and handle a large dataset with lots of confusing actions, we propose using the scene context information in which the action is happening. For example, skiing and skateboarding, horse riding and biking, and indoor rock climbing and rope climbing have similar motion patterns with high confusion, but these actions take place in different scenes and contexts. Skiing happens on snow, which is very different from where skateboarding is done. Similarly, horse riding and biking happen in very different locations. Furthermore, scene context also plays an important role in increasing the performance on individual actions. Actions are generally associated with places, e.g., diving and breast stroke occur in water, and golf and javelin throw are outdoor sports. In order to increase the classification rate of a single action, or to reduce the confusion between similar actions, the scene information is crucial, along with the motion information. We refer to these places or locations as scene context in our paper. As the number of categories increases, the scene context becomes important, as it helps reduce the confusion with other actions having similar kinds of motion. In our work, we define scene context as the place where a particular motion happens (stationary pixels), and also include the object that creates this motion (moving pixels). Humans have an extraordinary ability to perform object detection, tracking and recognition. We assume that humans tend to focus on objects that are salient or the things that move in their field of view. We try to mimic this by coming up with groups of moving pixels which can be roughly assumed as salient regions and groups of stationary pixels as an approximation of non-salient regions in a given video. Moving and stationary pixels:  Optical flow gives a rough estimate of velocity at each pixel given two consecutive frames. We use optical flow   at each pixel obtained using Lucas–Kanade method&amp;nbsp;[ 12] and apply a threshold on the magnitude of the optical flow to decide if the pixel is moving or stationary. Figure&amp;nbsp;  shows the moving and stationary pixels in several sample key frames. We extract dense CSIFT&amp;nbsp;[ 14] at pixels from both groups and use BOF paradigm to get a histogram descriptor for both groups separately. We performed experiments using CSIFT descriptor, extracted on a dense sampling of moving pixels   and stationary pixels  . For a 200-dimension codebook, the moving pixels CSIFT histogram alone resulted in a 56.63&amp;nbsp;% performance, while the stationary pixels CSIFT histogram achieved 56.47&amp;nbsp;% performance on the UCF11. If we ignore the moving and stationary pixels and consider the whole image as one, we obtain a performance of 55.06&amp;nbsp;%. Our experiments show that concatenation of histogram descriptors of moving and stationary pixels using CSIFT gives the best performance of 60.06&amp;nbsp;%. From our results, we conclude that concatenation of   and   into one descriptor   is a very unique way to encode the scene context information. For example, in a diving video, the moving pixels are mostly from the person diving, and the stationary pixels are mostly from the water (pool), which implies that diving will occur only in water and that this unique scene context will help detect the action diving. Why CSIFT? Liu et al.&amp;nbsp;[10] show that using SIFT on the UCF11 dataset gave them 58.1&amp;nbsp;% performance. Our experiments on the same dataset using GIST gave us a very low performance of 43.89&amp;nbsp;%. Our approach of scene context descriptor using CSIFT gave us a performance of 63.75, 2.5&amp;nbsp;% better than motion feature and 5.6&amp;nbsp;% better than SIFT. It is evident that color information is very important for capturing the scene context information. Key frames: Instead of computing the moving and stationary pixels and their corresponding descriptor on all the frames in the video, we perform a uniform sampling of k frames from a given video, as shown in Fig.&amp;nbsp; . This reduces the time taken to compute the descriptors, as the majority of the frames in the video are redundant. We did not implement any kind of key frame detection, which can be done by computing the color histogram of frames in the video and considering a certain level of change in color histogram as a key frame. We tested on the UCF11 dataset by taking different numbers of key frames sampled evenly along the video. Figure&amp;nbsp;  shows that the performance on the dataset is almost stable after three key frames. In our final experiments on the datasets, we consider three key frames equally sampled along the video to speed up the experiments. In this experiment, a codebook of dimension 500 is used.
Springer.tar//Springer//Springer\10.1007-s00138-012-0451-3.xml:Markerless tracking and gesture recognition using polar correlation of camera optical flow:Optical flow Polar correlation Markerless Multi camera:  1 Introduction  Motion tracking is a critical aspect of many virtual and augmented reality applications and there is a wide variety of different tracking technologies and approaches. Current tracking technologies include accelerometers, gyroscopes, electromagnetic trackers, infra-red trackers, GPS, and ultrasonic trackers. Each of these approaches has limitations: IMU and GPS systems are generally expensive, electromagnetic trackers have a low range and accelerometers and gyroscopes are subject to drift. As such, hybrid approaches use multiple sensor modalities to compensate for individual sensor weaknesses&amp;nbsp;[46]. One approach that has gained in popularity in recent years is vision-based tracking. Cameras are inexpensive, have large range, and provide raw images which are rich in information. Additionally, there are other auxiliary benefits of using cameras, such as face recognition that could be used to identify a user and personalize a system using their profile. Vision-based tracking systems can be classified into two approaches: inside-looking-out, in which the optical sensor is placed on the moving object and the scene is stationary&amp;nbsp;[45] and outside-looking-in, in which the optical sensor is stationary and observes the moving object&amp;nbsp;[11]. Traditionally, vision-based tracking requires markers as reference points or a pre-loaded model of the physical environment. Unfortunately, markers can clutter the physical environment and preloaded models can be time-consuming to create. To overcome these limitations, we present a real-time, markerless, vision-based tracking system employing a rigid orthogonal configuration of two pairs of opposing cameras&amp;nbsp;[14]. We show how opposing cameras enable cancellation of common components of optical flow, which leads to an efficient tracking algorithm. Our prototype is low cost, requires no setup, obtains high accuracy and has a large range span. Our prototype tracker is an example of an inside-looking-out optical tracking system. Other related approaches&amp;nbsp;[27, 28] use omnidirectional cameras that give a  view of the environment. Omnidirectional cameras are generally expensive, whereas our device prototype is built using inexpensive off-the-shelf webcams. We can extract tracking information by exploiting cameras placed on two ends of a diameter and facing away from each other. Our approach is based on polar correlation of optical flow to calculate egomotion, in this case, the rotation and translation of a camera between every two frames. Our tracking system can be used in various applications like 3D spatial interaction, body tracking, and in robotic applications for large terrain tracking. To explore our tracker’s effectiveness in an application, we explored how well it would perform as a handheld gesture recognition device. Handheld devices used in gesture recognition applications are often built using accelerometers and gyroscopes (for example, the Nintendo Wii Remote). A less common, yet promising approach are vision-based inside-looking-out systems. Markerless inside-looking-out camera systems have to recover motion parameters. Recovering motion parameters of a moving camera is similar to the autonomous robotic problem, in the sense that both have to solve for the egomotion of the camera. It is advantageous to use a multi-camera based approach to solve this problem (i.e., recovering motion parameters) because it increases the field of view and also introduces additional constraints that could be used to recover the motion parameters. Several SFM and egomotion algorithms [7, 20–22, 26] have been developed for multi-camera navigation tasks (see Sect.&amp;nbsp;2). For the task of gesture recognition, where it is not necessary to obtain perfect egomotion, we believe a multi-camera vision-based device has significant potential. In the next section, we discuss work related to our prototype tracking system. Section 3 focuses on the details behind the device and the optical flow-based tracking algorithm. Section 4 presents some implementation details. Section 5 describes the gesture recognition algorithm used to evaluate the applicability of our device in gesture recognition applications. Section 6 discusses our experimental results on tracking performance and gesture recognition accuracy. Finally, Sect.&amp;nbsp;8 concludes the paper.    2 Related work  There are many vision-based outside-looking-in tracking systems.&amp;nbsp;[24] presents a technique for both single camera and two camera on-screen item selection by finger pointing, without any 3D inference, using image morphing and line intersection. [36] presents a hand gesture interface using two cameras while &amp;nbsp;[47] discusses a system for hand pointing and gesture recognition using optical flow techniques to model the motion of the hand. [48] presents a system for tracking 3D finger position using a single camera so a user’s finger can be used as an input device. In work related to video games, [9] presents a perceptual user interface for a responsive dialog-box agent by recognizing user acknowledgment from head gestures. Sony’s EyeToy&amp;nbsp;[11] and Microsoft’s Kinect are examples of commercial vision-based outside-looking-in systems in gaming applications.&amp;nbsp;[13] describes several vision algorithms for applications such as vision-based computer games, hand signal recognition, and television set control through hand gestures. [12] presents specialized algorithms tailored for particular hardware for game interactions while&amp;nbsp;[15, 17] uses computer vision and a hearing-based user interface specifically for children’s games. [39] presents a markerless multi-camera motion capture system. In&amp;nbsp;[44], theoretical work on perceptual user interfaces is presented along with algorithms for head tracking, hand tracking, and full body tracking. As examples of other approaches, [31] uses multiple stationary cameras to observe the user’s interaction, [35] utilizes a glove with colored markers that assist in tracking, [4] tracks faces for enhanced interaction, and [8, 23] are real-time markerless object tracking systems for augmented reality applications. The major difference in these approaches and our approach is that they employ stationary cameras while we use a rigid set of moving cameras as a controller. However, from a computer vision point of view, these two approaches are similar because the fundamental algorithms needed to process the motion in the camera images remain the same. For example, the same optical flow algorithm can be applied to image frames whether a scene object is moving or the camera is moving. There are fewer inside-looking-out systems for user interface applications, even though there are many algorithms for structure from motion (SFM) and simultaneous localization and mapping (SLAM), both used in robotics or general purpose vision applications. SLAM and SFM methods include both single camera&amp;nbsp;[10] and multiple camera&amp;nbsp;[19] approaches. [16] presents the eight-point algorithm for structure from motion. [32] presents an algorithmic solution to the classical five point pose problem. This algorithm is suited for numerical implementation that relates to the inherent complexity of the problem. Due to the heuristic and ad hoc nature of the procedure, the implementation is not trivial for non-expert users. [25] presents a much simpler algorithm based on the hidden variable resultant technique. [38] presents a system for interactively browsing and exploring large unstructured collections of photographs. The underlying solution for structure from motion is formulated as a minimization problem, a non-linear least squares problem, and solved with algorithms such as Levenberg-Marquardt&amp;nbsp;[33]. [7] presents a multi-camera 6 DOF motion estimation system. The approach first calculates 5 DOF using the five point algorithm of Nister&amp;nbsp;[32] and then calculates the scale of translation using a multi-camera scale constraint. [20] presents a multi-camera motion estimation algorithm using a global optimization technique. It computes an optimal estimate of the essential matrix by searching the rotation space and an optimal solution for translation using linear programming and a Branch and Bound algorithm. For multi camera systems&amp;nbsp;[34] presents the generalized camera model (GCM). [26] uses the GCM to formulate the generalized epipolar constraint (GEC) and then solves it to get rotation and translation. [21] compares two algorithms in the context of GCM. The first algorithm uses a linear solution and the second a geometric algorithm. [22] developed a multi-camera rig and a motion estimation algorithm, assuming a spherical imaging system, for the multi-camera rig (i.e., the cameras share a common center of projection). An approach for computing egomotion using optical flow is described in&amp;nbsp;[43], but the experimental results show that the technique works for only very small motions, which is not practical for user interface applications. A multi-camera 6 DOF pose tracking algorithm is presented in&amp;nbsp;[40], but tested only on synthetic data. In other work, [2] presents a real-time tracking system using a cluster of outward-looking custom integrated circuits as smart optical sensors. [45], uses LED panels in a room ceiling to provide markers for tracking; this cumbersome setup limits its applicability as a convenient tracking system, especially in outdoor environments. [18] presents a subspace method for recovering the observer’s motion and the depth structure of the scene. The algorithm involves splitting the motion equations into separate equations for translational direction, rotation velocity, and relative depth. It was shown that the resultant equations can be solved successively, starting with the equations for translational direction. In&amp;nbsp;[41], Simoncelli presents a linear structure from motion algorithm using spherical cameras. Spherical projection has the advantage of treating all viewing directions homogeneously, and it allows for a simple form of the optical flow equation. The approach we take in our work is similar to&amp;nbsp;[18] and [41]. The primary difference is that we show that it is possible to cancel rotation terms in opposite cameras in a multi-camera rig arrangement. [5] presents the instantaneous model of optical flow and [29] shows that translational optical flow lies in the direction of the line connecting the point and the focus of expansion. This work forms the theoretical foundation of our tracking algorithm.    3 Tracking algorithm  The schematic design of our tracking device is shown in Fig.&amp;nbsp; a. Our device is designed as a multi camera rig with four cameras   (for   = 1 to 4), placed as a rigid orthogonal configuration of two pairs of opposing cameras. Figure&amp;nbsp; b shows the position   and orientation   of each camera with respect to the rig coordinate system. Figure&amp;nbsp; c shows a prototype of the device that we built using off-the-shelf webcams for testing purposes. This section presents our algorithm for conducting five degrees of freedom tracking, by computing the direction of translation and angular velocity. This information is integrated over time to get position information of the device at each time instant. Given two successive images of a scene, the motion of each pixel in the first image to the second image is defined as a vector  , called optical flow, where   and   are velocity components in the   and   direction, respectively. Using the instantaneous model of optical flow&amp;nbsp;[ 5], for a camera   the optical flow vector   at point   can be written&amp;nbsp;as:                                      where   are the translational components and   are the rotational components of optical flow,   is the translation,   is the angular velocity of camera  , and   is the   component (depth) of the 3D point corresponding to the image point  . To account for the perspective transformation induced by any camera system, we define the equation for perspective projection between an object point   and a corresponding image point  . The coordinates for   are       where   is the focal length of the camera. This can be written linearly in homogeneous coordinates as:            where   is a scale factor. The focal length can be obtained by using standard camera calibration techniques. [ 42] provides a high accuracy camera calibration technique for off-the-shelf cameras that we use in our prototype. Equations ( 4) and ( 5) can then be used to obtain the optical flow equations needed to account for the perspective transformation using homogeneous coordinates. For simplicity we assume focal length of 1 in this paper. Figure&amp;nbsp; b shows the position and orientation of each camera in the rig. Following&amp;nbsp;[ 43], for a camera shifted from the origin:                         where   is the translation and   is the angular velocity of camera  , placed at position   with orientation  , and   is the translation and   is the angular velocity of the rig. For camera 1:            Substituting Eq. ( 8) in Eqs. ( 6) and ( 7), we get:       Using Eq. ( 1), and substituting Eq. ( 9) in Eqs. ( 2) and ( 3), we get:                         Equations ( 10) and ( 11) represent the optical flow in camera 1 in terms of the rig motion parameters   and  . Similarly equations for camera 2 can also be derived as                         The equations for cameras 3 and 4 are defined similarly. Consider four symmetric points of the form    ,   and  . Let the flow vector at these symmetric points for camera   be   (for   = 0–3). The equations for flow vectors at these symmetric points in camera 1 and camera 2 can be obtained by substituting the coordinates of these points in terms of   and   in Eqs.&amp;nbsp;( 10) and ( 11) for camera 1 and Eqs.&amp;nbsp;( 12) and ( 13) for camera 2 (cameras 3 and 4 are defined similarly). The equations for optical flow at these symmetric points in camera 1 are:                                                                                                        We compute a quantity   for camera   as:                         Similarly we can derive the equations for   for cameras 2, 3 and 4. For example, the equations for camera 2 are            Next we compute a quantity   as:                                      By substituting Eqs.&amp;nbsp;( 23) and ( 24) in Eq.&amp;nbsp;( 25) we get:       Note that   has reduced to a scaled version of  . By substituting   for all four cameras in Eqs.&amp;nbsp;( 26) and ( 27), we get:                            is the scaled version of translation   of the rig. The computation of   cancels all the rotation terms and we are left with only translation terms. This is the concept of Polar Correlation, which says that opposing cameras have a common component of optical flow, which we show can be canceled out to get the direction of translation of the rig. It should be noted that two pairs of opposing cameras are needed because with camera 1 and 2 only   and   can be computed, to compute   we need another pair of opposing cameras. After computing the scaled version of translation of the rig, we can obtain the translation   of each camera   using       where   is the orientation of camera   and   is the translation of the rig. Using the translation of each camera, we can generate a synthetic translational field   for each camera as       We generate a synthetic translational flow vector for all the points where we have computed the optical flow. In&amp;nbsp;[ 29], it was shown that the translational component of optical flow at point   always lies on the line joining the focus of expansion (FOE) and the point  . Therefore, by using the synthetic translational flow at each point we can get the direction of the line joining the point   and the FOE. When a camera   is shifted from the rig center, an optical flow vector in that camera has three components,  , where   is a translational component due to translation of the rig,   is a translational component due to rotation of the rig [the   term in Eq.&amp;nbsp;( 6)], and   is a rotational component due to rotation of the rig. The component of optical flow perpendicular to the line connecting the FOE and the point   is the projection of the translational component   due to the rotation of the rig and rotational component   due to rotation of rig. This concept is pictorially represented in Fig.&amp;nbsp; . Let   be the vector going from the FOE ( ) to a point   on the image plane. Then,       Let   be a vector perpendicular to  , obtained by   rotation. Then, Now we normalize the vector   to obtain            The translational component of the optical flow always lies on the line connecting the image point and the FOE. Therefore,   will always lie on the line joining point   and the FOE. Now we take the component of   perpendicular to  , by taking a dot product between   and  . The motive behind normalizing   is that the component of   perpendicular to   can be found directly by computing a dot product between   and  . Using the computed optical flow over the image frames from the four cameras on selected interest points, we can obtain the numerical value       by taking the projection of the flow vector perpendicular to the direction of the line connecting the point to the FOE. By extracting the rotational component and translational component due to the rotation of the rig from Eqs.&amp;nbsp;( 10) and ( 11) for camera 1, we get                         Substituting Eqs. ( 35), ( 37) and ( 38) in Eq. ( 36) we get an equation of the form:       where   and   are in terms of   and  . Similarly we can also get equations for cameras   and 4. Thus each point in a camera gives us one equation and 3 unknowns   and  . We choose a uniform constant value for depth  . Using all the points in all the four cameras we obtain a family of equations of the form:       where   is a   matrix and   is a   vector of known quantities, for a total of N interest points. Pre-multiplying both sides of Eq. ( 40) by   gives us       Since   is a   matrix and   is a   vector, we get three linear equations and three unknowns, which can be easily solved to get   and  , which is the angular velocity of the rig. We assume that initially the multi-camera rig is aligned with and placed at the origin of the world coordinate system. Thus, it has a starting position   and orientation   (identity matrix). We obtain a translation vector   from the calculated scaled version of translation and a rotation matrix   from the calculated rotation terms for each time frame  . The rotation matrix can be obtained from the calculated rotation terms by treating them as Euler angles   of rotation using:            where   and  . The position   and the orientation   of the multi-camera rig can be calculated using:                         Using Eqs.&amp;nbsp;( 43) and ( 44) we integrate the calculated relative translation and rotation to obtain the pose of the multi-camera rig at each time frame.    4 Implementation  The configuration of the cameras in the device is shown in Fig.&amp;nbsp;a. We used off-the-shelf Logitech webcams to build the prototype, as shown in Fig.&amp;nbsp;c, making the device low-cost. The cameras are rigidly fixed together and we used a powered USB hub with an extension cable to connect the cameras to the computer. This configuration gave us the ability to move around in a large space with no additional setup time. The alignment of the four cameras in orthogonal position is done manually by measuring angles for the prototype build. An industrial grade system could be employed for perfect alignment. The prototype of the system described in this paper is designed with 4 cameras, placed in a orthogonal configuration of opposing pairs. The opposing camera pairs are needed to determine image points with optical flow correlation. The two pairs are arranged in an orthogonal configuration to cover full 3D space and obtain six DOF motion parameters. Other camera configurations are possible by stacking one or more sets of four cameras on top of each other. This design could provide additional points of reference to further reduce ambiguity in motion and more robust tracking. We use Intel’s OpenCV implementation of&amp;nbsp;[ 3] to compute optical flow, which is a real time, iterative approach using image pyramids. It gives as output a sparse flow field because it calculates optical flow only on selected feature points. The feature points are selected using the approach of&amp;nbsp;[ 37]. This approach selects points in an image, with specific characteristics. At point   in an image  , the image intensity gradients are defined as:        Given            a point is selected if the minimum eigenvalue of matrix H for that point is greater than a threshold. This will give points which are shaped like a corner. These kinds of points are helpful to overcome the aperture problem&amp;nbsp;[ 1] in computing optical flow and lets us utilize a markerless implementation. Note that other more robust feature point selection methods, like&amp;nbsp;[ 30] could also be used and is an area for future work. After computing optical flow in each camera, flow vectors from each frame are passed through the quadrantization step to get an estimate of optical flow at symmetric points   and   to use polar correlation. As shown in Fig.&amp;nbsp; , each frame is divided into 4 quadrants. The center points of each quadrant are called quadrantization points   (for   = 0–3) for camera  . Each quadrantization point is associated with a vector with some uniform constant magnitude   and angle as the average of all flow vectors’ angles in that quadrant. This process is used to associate features across opposing camera pairs. It combines the flow vectors in each quadrant to estimate the optical flow at the quadrantization points, helping to overcome noise in the optical flow features in the image frame, making the tracking more robust. Initial 3D location of the device is chosen to be (0,0,0). Repeat steps 1 through 10 for each set of four frames from all the four cameras.    Step 1: Find interest points of the image frames from all the four cameras, Sect.&amp;nbsp;4.2.   Step 2: Calculate optical flow on the interest points&amp;nbsp;[3], Sect.&amp;nbsp;4.1.   Step 3: Estimate the flow at the quadrantization points, Sect.&amp;nbsp;4.3.   Step 4: Using the estimated flow at the quadrantization points, calculate , Eq.&amp;nbsp;(22).   Step 5: Calculate , using Eq.&amp;nbsp;(25) through (27).   Step 6: Normalize  to get direction of translation.   Step 7: Find focus of expansion using the computed direction of translation, Sect.&amp;nbsp;3.2.   Step 8: For each interest point, using the generated synthetic translational flow vector and the computed flow at that point, calculate the value of  and construct Eq.&amp;nbsp;(36).   Step 9: Solve the family of equations to get angular velocity, Eq.&amp;nbsp;(41).   Step 10: Using the computed direction of translation and angular velocity track the current 3D location of the device, Sect.&amp;nbsp;3.3. Step 1: Find interest points of the image frames from all the four cameras, Sect.&amp;nbsp;4.2. Step 2: Calculate optical flow on the interest points&amp;nbsp;[3], Sect.&amp;nbsp;4.1.
Springer.tar//Springer//Springer\10.1007-s00138-012-0452-2.xml:Hierarchical stereo matching with image bit-plane slicing:Stereo matching Disparity computation Variate bit-rate image processing:  1 Introduction  Stereo matching is considered as one of the most challenging and unsolved problems in computer vision. Due to the broad applicability to many application domains such as multimedia, 3D display and robotics, it has attracted researchers’ attention for over several decades. In the past few years, a fairly large amount of computational algorithms has been proposed to cope with the stereo matching problem [1, 5, 7]. The objective is usually to obtain an accurate disparity map from a pair of images. Based on the well-studied camera and scene geometry [11], the 3D structure of the scene can then be derived using the image formation parameters. In the early development of stereo algorithms, the block matching techniques exploiting local constraints have been extensively investigated [2, 6, 17]. Recent advances on stereo matching, in contrast, are mostly based on the energy minimization framework for global optimization [23, 24]. Some well-known techniques include graph cuts [4, 15, 19], belief propagation [14, 22], dynamic programming [3, 18], etc. Those methods and the variations have been shown to provide significant disparity estimates on the Middlebury stereo evaluation datasets [21]. However, the computational cost is fairly high, in terms of processing time and data allocation. In addition to the methodologies for matching cost computation and disparity optimization, there also exist some frameworks such as multi-resolution stereo matching. This hierarchical stereo matching technique can be implemented using image pyramids to improve the disparity estimates based on the coarse-to-fine paradigm [13, 27]. It can also be adopted to generate the aggregation window with a pyramid shape for better depth discontinuity handling and hardware-accelerated computation [8, 10, 26]. Although the hierarchical matching strategy has been investigated in terms of spatial image resolution change, no related research has been done based on the modification of image intensities. In this work, we address the feasibility of hierarchical stereo matching in the intensity domain. Similar to the conventional image pyramids, a series of images with less and less information encoded is constructed hierarchically [25]. The lower-level image in the hierarchy contains more detailed information present in the higher level. However, the information reduced is in terms of the representation for image intensity, instead of image resolution. By creating an image pyramid in the intensity domain, low bit-rate stereo matching is possible with a smaller memory allocation for each image. This is extremely useful for high bit-rate image processing, since those images may contain up to 16&amp;nbsp; bits per pixel for each channel. To implement the intensity-based hierarchical matching technique, a variate bit-rate image processing approach similar to bit-plane slicing is proposed. The stereo matching algorithm is first carried out on a low bit-rate image pair. If the resulting disparity meets some quality requirements, the stereo matching process is terminated. Otherwise, an extra bit-plane is provided for the image pair to perform higher bit-rate stereo matching. This process continues until the disparity estimate is satisfactory or the finest level in the hierarchy is reached. The proposed technique can thus be used to reduce data usage. To the best of the author’s knowledge, the idea of intensity-based hierarchical matching technique has never been presented before. The proposed variate bit-rate matching scheme can be considered as a framework and used to combine with the existing stereo matching algorithms. It is shown in this paper that the quality of disparity estimates does not drop significantly when the incomplete information with low-intensity quantization is provided. Thus, the trade-off between performance and data bit-rate for stereo matching is worth investigating. The main contribution of this work is initiating the variate bit-rate stereo matching approaches. We validate the feasibility of hierarchical matching based on image bit-plane slicing, and evaluate the performance in terms of bad matching rate and average image bit-rate. In addition to the applicability on 8-bit images (per channel), the proposed technique will also greatly benefit the stereo matching on the images with higher bits per pixel.1 In this paper, we are interested in the relationship between the data bit-rate and stereo matching results. Thus, it is not our primary objective to improve the computation cost. However, due to the data alignment nature of bit-plane image processing, the proposed framework can be implemented using GPU [12, 16] or FPGA [9] for hardware acceleration. The paper is organized as follows. Section&amp;nbsp;2 introduces the idea of variate bit-rate representation for stereo matching. In Sect.&amp;nbsp;3, we present the algorithm for hierarchical stereo matching technique. Experimental results and performance evaluation are provided in Sect.&amp;nbsp;4. Finally, in Sect.&amp;nbsp;5, some conclusions are drawn.    2 Variate bit-rate matching  Given a pair of images for stereo matching, most existing algorithms take the image intensity values as raw data input for disparity computation. For a general grayscale image with 256&amp;nbsp;quantization levels, each pixel is represented by an 8-bit integer. Stereo matching is carried out on the images with full 8-bit intensity depth. However, as the number of bits per pixel increases for high-quality image representation, it might be unnecessary or impractical to perform stereo matching using the full intensity depth. The investigation on how to use the reduced intensity depth for stereo matching has become an important issue. More precisely, if an -bit stereo image pair is given, is it possible to derive a comparable or better disparity map using only the -bit information where \(m &amp;lt; n\)? To deal with the problem of stereo matching on the image pairs obtained from various quantization levels, we propose a hierarchical stereo matching technique based on a variate bit-rate image representation. The objective is not only to derive a disparity map, but also to simultaneously address the following issues:      The variate bit-rate correspondence matching is performed regionally for the pixels with higher-order bits. Different quality thresholds can be applied to the regions of interest to generate the disparities with different levels of detail.     The initial disparity estimates derived from low bit-rate images is gradually improved with higher-order bits. The quality of disparity maps can be controlled by the image bit-rate, and without degradation if the full intensity depth is used.  For an  -bit grayscale image, the intensity value of any pixel can be represented by       using bit-plane decomposition, where   is the  -th bit of the intensity value. If the image is to be approximated with an intensity depth less than   bits, then the higher-order bits should be used since they contain more significant information. Based on these facts, the variate bit-rate representation for a  -bit image approximation is given by       where  . An image represented by   can be thought as the one with   quantization levels on the intensity value. The variate bit-rate correspondence matching is performed regionally for the pixels with higher-order bits. Different quality thresholds can be applied to the regions of interest to generate the disparities with different levels of detail. The initial disparity estimates derived from low bit-rate images is gradually improved with higher-order bits. The quality of disparity maps can be controlled by the image bit-rate, and without degradation if the full intensity depth is used. Using the image representation given by Eq.&amp;nbsp;(2), a variate bit-rate stereo matching technique can be carried out for disparity computation from a pair of -bit images. Instead of the full intensity depth, we can use only the  most significant bits of the  bit image pair for stereo matching. If the derived disparity estimate does not meet some predefined criteria, then stereo matching is performed again on either the entire or certain parts of the image using , the  most significant bits of the pixels. This process may continue until the disparity is satisfactory. It is clear that the worst case happens when  and the entire image is processed. In this case, the result is identical to the one computed from the original -bit image pair. One major advantage of the proposed variate bit-rate stereo matching technique is its capability on bitwise processing of the image data. As illustrated in Fig.&amp;nbsp; , once the image pair of bit-plane   is obtained, stereo matching can be performed immediately for disparity computation. The quality of disparity estimates can then be gradually improved by providing lower-order bit-planes “upon request”. Thus, the data rate for disparity computation in terms of the number of bits per pixel is generally less than   when processing the  -bit images. The framework of hierarchical stereo matching using the variate bit-rate image representation does not involve any specific stereo matching algorithms. To take advantage of hierarchical matching exclusively on the pixels with increased intensity depth (bit-rate), it is preferable to adopt the algorithms which are capable of regional processing. However, as shown in the experiments, the global matching algorithms provide good disparity estimates for low bit-rate image pairs and can be used to initialize the variate bit-rate hierarchical matching.    3 The Algorithm  Given a pair of -bit images,  and , the corresponding -bit image representation,  and , can be obtained from Eq.&amp;nbsp;(2), where . Let the image pair with -bit quantization levels be denoted by , then the associated disparity map , which consists of the disparity  for each pixel , can be computed by stereo matching algorithms using the two -bit images. Since the objective is the low data rate disparity computation, it is natural to use the smallest  to perform stereo matching on , provided that the quality of  is satisfactory. For variate bit-rate stereo matching, the quality of disparity estimate obtained from   is defined as follows. First, the disparity error rate of an image region   is given by       where the pixel  , and   represents the ground truth disparity with  . The quality of disparity estimate can then be written as the reciprocal of disparity error rate, i.e.,  . In general, a more intensity quantization level implies less stereo mismatch. Thus,   can be used for quality assessment by writing Eq.&amp;nbsp;( 3) as       if the ground truth disparity is not available. In this case, when all of the   bits per pixel are used, we have   as expected. The variate bit-rate stereo matching is to find the smallest   for each pixel   such that Eq.&amp;nbsp;( 4) is equal to zero or less than a quality threshold. More precisely, we will solve for the required intensity depth       with the disparity error   for each pixel  . Since   cannot be computed until the last image bit-plane is available,   is estimated using an iterative technique by comparing the disparity estimates obtained from   and  , where  . If the disparity difference for a pixel   is less than a threshold, i.e.,       then   is selected. Otherwise,   is increased until the condition ( 6) holds. Mathematically, Eq. ( 5) can be modified as       with  . It should be noted that Eq. ( 7) is derived pixelwise and the error disparity threshold   takes on the integer values  . If the spatial proximity is considered for disparity computation, then the variate bit-rate representation can be obtained regionally as       where   takes on the region   and  . To increase the bit-rate on specific pixels of the image pair for iterative stereo matching, a binary mask is created according to the disparity difference given by Eq. (6). That is, the stereo matching on the -bit image pair  is carried out only on the regions assigned by the binary mask . Depending on the tolerance of the error disparity threshold  might contain many small isolated regions. In order to preserve the local depth continuity of the scene, they are eliminated using morphological erosion and dilation operations. The associated disparities on the regions are updated with the disparity of the surrounding pixels given by the latest disparity estimate  obtained from the stereo image pair . In addition to providing the  -th bit information to the reference image (the left image in our case) for stereo matching, it is also required to assign the search ranges for all pixels covered by the binary mask   in the left image. The search range is used to decide the regions in the right image to have the  -th bit information for stereo matching. Similar to the mask   used for the reference image, a binary mask   is generated to update the  -th bit information for the right image. Since stereo matching is carried out along the image scanlines for rectified image pairs,   is created by taking the union of possible matches for all of the regions in  . More specifically, suppose   consists of   connected components, i.e.,       and the maximum disparity in the region   obtained from the disparity map   is  . Then   is given by the union of   regions       where   indicates the translation of a set by   pixels along the image scanlines. The proposed hierarchical stereo matching technique is carried out iteratively by providing more information using the next image bit-plane. It is clear that the binary mask for the left (reference) image possesses the property . Moreover, we have  for the right image since the correspondence search region  in the right image  is constrained by the maximum disparity obtained from performing stereo matching on the region . Thus, it is guaranteed that, while the disparity estimate is gradually improved, the amount of extra bits to be added to the image pair is decreased for each iteration. Now, suppose the additional information from bit-plane  is added to form the image pair . The extra-bit is updated on the regions  and  for the left and right images, respectively. Each pixel of the images contains various bits of data representation and has different levels of intensity quantization. To derive the disparity map  from the image pair , two different matching schemes, performing on the bit-rate updated pixels only or on the whole image, are adopted. For the first case, stereo matching is carried out only on the newly updated region of the left image given by . Thus, as the image bit-rate increases, the computational cost of each iteration decreases for this  scheme. For the second case, even only part of the image is updated with new pixel values, stereo matching is performed on the whole image which contains mixed bit-rate pixels. This  scheme is similar to the conventional stereo matching method and has a fixed computational cost for all iterations. Since the image region without extra-bit update can still gain some information from the bit-rate update region  while evaluating the cost function using a local matching window, the disparity estimates using partial and full matching schemes will be slightly different. As shown in the experiments, the full matching scheme provides better results at the cost of more computation involved. In Eq. (7), the threshold  means that the disparity computation gives the same result when the information from an additional bit-plane is incorporated. This is a fairly strong condition since it requires the exact quality to be obtained without the extra bit. A slightly weak condition is to set a small and non-zero threshold. Because an additional bit-plane provides more intensity quantization levels, it generally benefits the high contrast or textured region instead of homogeneous or textureless region. Thus, the trade-off between the quality of disparity estimates and using low bit-rate image pairs for less computation can be controlled by the threshold setting. One important issue for the hierarchical variate bit-rate matching technique is how to select the lowest bit-rate and use the associated image pair for initial stereo matching. It is clear that the images with very low bit-rate, such as binary images or images with low-intensity quantization levels, are not informative for stereo matching. If no additional information is available, an initial disparity estimate can be derived using  with  for an -bit image pair. Equation (7) is then carried out to improve the disparity quality with higher bit-rate image representation. As shown in the experiments, there is a significant improvement in terms of bad matching pixels on 3-bit image pairs. Thus, a 4-bit stereo image pair can be used to derive a suitable initial disparity map and identify the regions for further disparity computation. Since the hierarchical stereo matching is carried out iteratively on different bit-rate image pairs, it is possible to perform different matching algorithms for each disparity computation. More specifically, global matching algorithms can be used to derive the initial disparity maps followed by local matching algorithms for regional improvement. This hybrid stereo matching strategy takes the low bit-rate images for global matching and high bit-rate images for local matching. Consequently, the final disparity result can be improved with a reduced average image bit-rate for overall computation.    4 Implementation and discussion  In this section, we describe the implementation and experiments of the stereo matching technique based on bit-plane slicing. The hierarchical matching framework serves as a basis for conventional stereo matching approaches. Thus, two local algorithms— (SAD) and  (NCC), and two global algorithms— (GC) [4] and  (BP) [22] are used to test the effectiveness of our method. Eight standard Middlebury stereo datasets, Tsukuba, Venus, Teddy, Cones, Bowling, Lampshade, Midd and Wood, are used in our experiments [20]. The evaluation details on the percentage of bad matching pixels and the average image bit-rate versus the pixel bit-rate are presented using the Tsukuba dataset. We first begin with performing the stereo matching algorithms on fixed bit-rate image pairs. The objective of this experiment is twofold: (1) to evaluate the performance of different stereo matching algorithms when low bit-rate image pairs (with less intensity quantization levels) are used for disparity computation, and (2) to select suitable low bit-rate image pairs and use them to derive initial disparity estimates for iterative stereo matching. To compare the performance of stereo matching on different bit-rate image pairs, bit-plane slicing is first used to generate the image pairs with a fixed bit-rate. The four stereo matching algorithms are then carried out to compute the disparity maps associated with the fixed bit-rate image pairs. Figure&amp;nbsp;  shows the disparity estimates of Tsukuba dataset, from the top to the bottom, derived using the image pairs with   bits per pixel (i.e.,   quantization levels). The percentage of bad matching pixels compared to the ground truth disparity is illustrated in Fig.&amp;nbsp;  for the stereo matching algorithms performed on different bit-rate image pairs. As shown in Figs.&amp;nbsp; and , the local algorithms SAD and NCC give poor disparity estimates for low bit-rate stereo matching. The mismatches are mainly due to insufficient quantization in image intensity. However, the performance is improved as the image bit-rate increases. The SAD and NCC curves in Fig.&amp;nbsp; also indicate that the disparity map derived from the 6-bit image pair is almost the same as those derived from higher bit-rate image pairs (in terms of the percentage of bad matching pixels). This result suggests that, for the local matching algorithms, the images with less quantization levels can still give comparable performance with the 8-bit images, and further quantization does not necessarily reduce the mismatching rate.
Springer.tar//Springer//Springer\10.1007-s00138-012-0453-1.xml:Automated identification of thoracolumbar vertebrae using orthogonal matching pursuit:Vertebrae identification SOMP Dictionary-based classification CT images:  1 Introduction  Accurate identification of the vertebral column structures is a crucial step in planning an image-guided spine surgery. Inaccurate numbering or labeling may lead to the risk of performing an interventional procedure or surgery at an unintended level and cause serious consequences. The classic groupings of vertebral column structure include 7 cervical, 12 thoracic, 5 lumbar, 5 sacral, and 4 coccygeal vertebrae. The common abbreviations C, T, L, and S for referring to cervical, thoracic, lumbar, and sacral vertebrae are used throughout this paper. For example, C2 signifies the second cervical vertebra; T12, L5, and S1 denote the twelfth thoracic vertebra, the fifth lumbar vertebra, and the first sacral vertebrae, respectively. A typical practice of labeling the vertebral column is either to count in a caudad direction from C2 or to count in a cephalad direction from L5, assuming the normal structure of the vertebral column. Due to the high prevalence of anomalies in the lumbosacral region, such as the sacralization of L5 and the lumbarization of S1 [1], the accurate detection of L5 and S1 is not trivial; therefore, counting from C2 inferiorly is usually considered more reliable. However, not all the imaging routines cover the C2 or even the cervicothoracic region especially for the diagnosis in the abdomen and/or pelvic region. In these scenarios the correct identification of thoracolumbar vertebrae, in particular, the last thoracic vertebra and the first lumbar vertebra, can be very helpful in making a definitive labeling. Because of the resemblance in the last thoracic vertebra and the first lumbar vertebra, and other factors such as the low spatial resolution and contrast in the imaging, an automated algorithm to assist differentiating the last thoracic and the first lumbar vertebra would be challenging but valuable. Anatomy of the vertebral column can be found in detail in major anatomic references, e.g., [2]. The most distinguishing feature of thoracic vertebrae is that they all articulate with at least one pair of ribs and therefore have facets on the sides of the bodies. There are also other observable characteristics of typical thoracic vertebrae. For example, the vertebral foramen is of a circular shape, whereas the shape of vertebral foramen is more triangular in the lumbar and cervical vertebrae. Although these characteristics are quite pronounced in the middle of the thoracic region, the bodies of the thoracic vertebrae do look similar to those in the cervical and lumbar regions at the respective ends. For example, “the spinous processes of the lower thoracic vertebrae resemble those of lumbar vertebrae in that they are shorter, stouter, and project more posteriorly [3].” “There is an abrupt transition in the orientation of articular processes at the level of T12 where the inferior articular process becomes oriented to match those of lumbar vertebrae with an anterolaterally directed facet.” [3]. Because of the resemblance in the last thoracic vertebra and the first lumbar vertebra, plus other factors such as the low spatial resolution and contrast in the imaging, it is not always an easy task to differentiate the last thoracic and the first lumbar vertebra. An automated algorithm to assist the identification and interpretation would be a valuable tool for radiologists. In this work, we put the development of such an algorithm in a scenario where an automated or semi-automated spine labeling system for CT images is available and the purpose of having such an algorithm is to provide additional opinions or cross-validate the labels returned by the spine labeling system. Ideally, if such an algorithm is shown to be sufficiently reliable and fast, then it can be integrated into a spine labeling system as a useful module to improve the overall labeling accuracy. The stated problem can be formulated as a classification problem. The classification algorithm takes the input from a 3D CT volume and the vertebra labeling results obtained from an automated spine labeling system. The labeling results are represented by a sequence of labeled vertebral bodies, each with the estimated center location and the orientation along the spinal curve geometry.For each labeled vertebra that lies in a specified thoracolumbar region (e.g., from T10 down to L2), the classification algorithm checks if the input vertebra looks more like a thoracic vertebra or a lumbar vertebra. Finally, a voting scheme combines the classification results on these neighboring vertebrae and decides if the input labeling is correct or not. Figure   shows an example output of a spine labeling algorithm on the thoracolumbar region from a CT image. There is a large body of research work on automating the analysis and interpretation of the spine image acquired on various imaging modalities; to name a few recent work, Yao et al. [4] reported a multiple-stage segmentation method to extract the whole vertebral column from CT images without labeling individual vertebrae; Schmidt et al. [5] presented a parts-based graphical model for spine detection and labeling in MR images; Dong and Zheng [6] employed a graphical model-based solution for vertebra identification from X-ray images, with a focus on the cervical and thoracic vertebrae; Huang et al. [7] described a learning-based vertebra detection and iterative normalized-cut segmentation for spinal MR images; Ma et al. [8] studied the segmentation and identification of the thoracic vertebrae in CT images using a learning-based edge detector and a deformable surface model; Klinder et al. [9] proposed a comprehensive solution for detecting, identifying, and segmenting vertebrae in CT images; Kelm et al. [10] applied the marginal space learning method to automatic detection and labeling of the intervertebral disks in MR images; Zhan et al. [11] presented a robust algorithm for spine labeling and intervertebral disc detection in 3D MR scout images using a hierarchical learning framework and a local articulated model. Other proposed techniques on the detection and segmentation of spinal structures can be found in [8, 12–20]. However, to our best knowledge, none of these methods was devoted to address the need and the development of accurately identifying the vertebrae in thoracolumbar region especially in the presence of vertebral anomalies or defects. The purpose of our study was to develop and test a computer algorithm for the identification of thoracolumbar vertebrae towards a seamless integration into an automatic spine labeling system. In this work, we put the development of such an algorithm in a scenario where an automated or semi-automated spine labeling system for CT images is available and formulate a classification problem that takes the input from a 3D CT volume and the vertebra labeling results returned by an automated spine labeling system. For each labeled vertebra that lies in a specified thoracolumbar region (e.g., from T10 down to L2), the classification algorithm checks if the input vertebra looks more like a thoracic vertebra or a lumbar vertebra. Finally, a voting scheme combines the classification results on these neighboring vertebrae and decides if the input labeling is correct or not. The rest of this paper is organized as follows: the technical description of the proposed method is elaborated in Sect.&amp;nbsp;2. Section 3 presents the experimental results on hundreds of CT volumes collected from various clinical sites. Finally, we draw conclusions in Sect.&amp;nbsp;4 with a brief discussion on the limitations and potential extensions of the proposed method.    2 Methods  In this paper, we present a method to identify the thoracolumbar vertebrae using simultaneous orthogonal matching pursuit (SOMP) which is a recently developed greedy matching pursuit algorithm [21] and has shown excellent discriminative power in solving sparse approximation problems arising from many recognition applications [22–24]. The proposed method is essentially a dictionary-based classification approach. At the training stage, dictionaries that capture the appearance characteristics of thoracic and lumbar vertebrae are trained independently from the human-annotated samples. At the testing stage, testing samples are identified by comparing the reconstruction errors after they are projected on the dictionary of each class. The details of different modules adopted in our approach, including the maximum intensity projection (MIP) generation, vertebra detection, dictionary building and the final classification, are described in the remaining part of this section. Aiming to a highly efficient classification algorithm, we decide to investigate a 2D approach which can significantly reduce the computational time, compared to a fully 3D approach. By further considering the variability in the 2D axial views because of the projecting direction of ribs as shown in Fig.  , we choose to look into the MIP images. The projection is performed in a volume of interest (VOI) containing the target vertebra body. The assumption is that, for each vertebra to be identified, we already have a good estimate of the body center location and the orientation along the spinal curve geometry, which can be used to estimate the boundary of the VOI as well as the projection direction for the MIP generation. After the MIP generation, the next step is to extract the vertebra regions which are normalized to the same scale and provide the compact and representative features in the classification stage. The automatic extraction is achieved by a classic adaboost-based detection approach [ 25] in which a cascade of vertebra detectors is learned during an off-line training stage and then applied in the real-time detection. In the off-line training stage, positive training samples are obtained by (1) collecting cases with the manually labeled thoracolumbar vertebrae; (2) generating the MIP images using the aforementioned procedure; (3) annotating the vertebra regions inside the MIP images. Note that during the MIP generation, we add small random in-plane rotations and translations to accommodate the offsets that exist in both the human annotations and the algorithm-generated labeling results. In the testing stage, a scanning window approach is applied to the input 2D MIP image to achieve a multi-scale detection of the vertebra region. At each level of the cascade the classifier is tuned to achieve a high true positive rate ( \({&amp;gt;}99.9\,\%\)) on the training set. Meanwhile, a moderate false alarm rate ( \({&amp;lt;}0.1\,\%\)) can be tolerated since the false positive detection results can be easily discarded by taking advantage of the prior that the target vertebra body region should be roughly positioned near the center of the MIP image in most cases since these centers used in the MIP generation are either marked by human or detected by a spine labeling algorithm. In practice, we pick the detected region which is the nearest to the center of the image among all detected regions of reasonable sizes. Figure&amp;nbsp;  demonstrates two typical MIP images, one from T12 and one from L1, as well as the extracted vertebra regions. More examples from T12 vertebrae and L1 vertebrae are shown in Fig.&amp;nbsp; . In case no vertebra is detected, a fixed-size region around the image center is cropped as a failure protection. Finally, the detected regions are resized to  matrices and then vectorized to provide input for the consequent training and testing stages. The basic assumption we make in the proposed approach is that the thoracic and lumbar vertebrae images lie on two different low dimensional linear subspaces, following the linear subspace approach [ 26– 28] widely used in computer vision for image recognition. Previous work on compressive sensing [ 28,  29] has shown that if there are sufficient training images of same class, then a testing image   drawn from this class can be effectively approximated by a sparse linear combination of the training images under certain conditions. Since the samples of the same class lie in a low dimensional linear subspace, it does not need all the plausible images for training. Mathematically speaking, given a testing image  , its sparse representation vector   can be obtained by solving the following optimization problem:       where   is the so-called dictionary matrix whose columns are training images, the 0-norm of  , which is also called the sparsity of  , is defined as the number of non-zero elements in  , and   is the desired mean square error. In real world applications, it is not practical to solve the problem in Eq.&amp;nbsp;( 1) as it is an NP-hard problem. Tropp et al. [ 21] showed that if the sparsity   of the image   is known, an approximation to the solution can be obtained by the SOMP algorithm which instead solves the following problem:       According to the linear subspace assumption, a quite well estimation could be obtained for a testing image   if   belongs to the same class of the training images in the dictionary   . Hence, it is reasonable to expect that under the same sparsity   the residual magnitude   is small if the columns of   consist of only the samples from the same class as  ; on the other hand, the error can be large if the columns of   are taken from samples not in the same class as  . Note that in this classification problem, we are not interested in finding the exact sparse representation which may not be unique [ 30], instead we are more interested in the residual magnitude which indicates how well a testing image can be represented by a class specific dictionary. In our experiment, the types of the cropped vertebrae images were manually labeled by experts. We divided the images into a training set and a testing set. Our objective is to identify the types of the testing vertebrae images given the training set. We build one dictionary, , for the thoracic vertebrae and one dictionary, , for the lumbar vertebrae using the training images, such that the column vectors of the dictionaries are the vectorized training images. At the testing stage, the SOMP finds the best presentation of the testing images given a class-specific dictionary  , which consists of   normalized and vectorized training images that belong to the same class and  . Let  , where  ,   denote a set of   testing image. Let   denote a sub-matrix of   consisting of the columns   with  ,  , and   denote the sparsity. The steps of finding the residual using the SOMP method are as follow:        The residual  and the selected column set  are initialized as  and ø;         &amp;nbsp;                 Find the column        in        that is most strongly correlated with the residual        such that                             and add the column        to the selected column set by updating       ;              &amp;nbsp;                 Calculate the projection matrix       , which projects vectors onto the linear space spanned by the selected columns using the equation:                                          &amp;nbsp;           Update the residual: . Repeat step 2 to 4 for  times and obtain the residual matrix .         &amp;nbsp; The residual  and the selected column set  are initialized as  and ø; Find the column   in   that is most strongly correlated with the residual   such that         and add the column   to the selected column set by updating  ; Calculate the projection matrix  , which projects vectors onto the linear space spanned by the selected columns using the equation: Update the residual: . Repeat step 2 to 4 for  times and obtain the residual matrix . The SOMP algorithm is summarized in Algorithm 1. In our experiment, the sparsity   is determined empirically and set to 20. Let  denote the th column of . Then, the residual magnitude  of projecting the testing image  onto the dictionary  is . The identification result is obtained by comparing the residual magnitudes after projecting the testing image onto all dictionaries. For example, if for a testing vertebra image , the residual  after projecting it on the  is less in magnitude than the residual  from the , then the testing sample is identified as a thoracic vertebra, otherwise it is identified as a lumbar vertebra. To further measure the confidence associated with the identification result, one can define the identification score as the ratio between the largest residual magnitude and the second-largest residual magnitude and only trust the identification result if this ratio is greater than a pre-specified threshold. To improve the identification performance, one can also build a cascade of SOMP classifiers using different dictionaries. One simple-to-implement cascading strategy is to check the identification score at each stage, and if this score is lower than the threshold set for this stage, then the classifier in the next stage is triggered, and the identification score returned by the last used classifier will be used as the final criterion to determine the identification result.    3 Experimental results  We tested our algorithm on three datasets. Dataset I consists of 205 CT volumes with the thoracolumbar regions carefully annotated from 3D images. These volumes were selected from studies acquired at different institutions and have a good amount of variability in patient populations and imaging protocols. Dataset II contains 149 high-resolution colon scans of slice thickness between 1 and 2mm. Dataset III contains 265 low-dose whole-body CT scans obtained in PET/CT studies and most images were acquired at 5mm slice thickness. For each case in these datasets, we first ran an automatic spine labeling algorithm and then applied the preprocessing step described in Sect.&amp;nbsp; 2.1 to produce vertebra MIP images for classification. Examples of the resulting MIP images from dataset I, II, and III are shown in Fig.&amp;nbsp; . Note that dataset I is a subset of the training set that was used for training the automatic spine labeling algorithm. The thoracolumbar annotations from dataset I were also used in training the 2D vertebral region detector from MIP images as described in Sect.&amp;nbsp; 2.1. The cascaded SOMP dictionaries were also built on the samples from dataset I. The first stage of the cascade was built using about half of the samples selected from dataset I, then it was tested on all samples from dataset I. The threshold that determines if the identification decision can be made at the first stage or should be escalated to the second stage is determined empirically. Helped by a radiologist, we manually examined the spine labeling and the vertebrae classification results obtained from dataset II and dataset III. Among the 149 T12 and 149 L1 vertebrae in dataset II given by the spine labeling algorithm, the classification corrected the labeling mistakes on three studies, two with sacralization, and one with lumbarization, all leading to the wrong detection of L5. Meanwhile, the classification did not catch one mislabeling caused by the lumbarization and also misclassified L1 as T12 in two studies where the presence of short lumbar rib structures attached to L1 was found, which may cause the confusion. However, we also noticed that the three corrections all had the final scores greater than 1.15 and the three misclassifications were among the 11 vertebrae with final scores lower than 1.12. Therefore, the negative effects to the labeling system caused by the misclassification can be largely reduced by raising the final decision score threshold to a certain level without lowering the sensitivity on most cases. We also checked how the vertebrae classification results agree with the spine labeling algorithm on dataset III. The conflicts on 29 vertebrae were found. The classification results were correct on five cases where the mislabelings due to the sacralization were confirmed. Among the 24 instances of misclassification, two had scores higher than 1.2 and both exhibit bilateral lumbar rib structures, three had scores between 1.1 and 1.2, and all others had scores below 1.1, including two cases with implants, one case with visible vertebral fracture and two cases with lumbar ribs on one side. We also noticed that among these 24 misclassifications there were seven failures of the vertebral region detection due to the low image quality. Figure&amp;nbsp;  shows two studies where the labeling errors due to the sacralization of L5 were corrected by the vertebra classification module. Figure&amp;nbsp;   shows two studies where the vertebra classification module gave the correct results in the presence of lumbarization of S1. Figure&amp;nbsp;  shows several examples of misclassified vertebrae due to different reasons. All our experiments were performed on a PC with 2GB of RAM and a 2.0&amp;nbsp;GHz Intel CPU. The average time for MIP generation and 2D vertebral region detection is less than 0.2&amp;nbsp;s per vertebra. The average time spent on one SOMP classifier is around 0.3&amp;nbsp;s, and in our experiments, about 17&amp;nbsp;% of the testing samples were sent to the second stage SOMP classifier, which may take additional 0.3&amp;nbsp;s. Overall the average processing and classification time per vertebra is about 0.55&amp;nbsp;s.     1. Konin, G.P., Walz, D.M.: Lumbosacral transitional vertebrae: classification, imaging findings, and clinical relevance. Am. J. Neuroradiol. 195, 465–466 (2010) 2. Gray, H.: Gray’s Anatomy. Running Press, Philadelphia (1901) 3. Karageanes, S.J.: Thoracic spine. In: Principles of Manual Sports Medicine, pp. 263–294. Lippincott Williams&amp;amp; Wilkins, Baltimore (2005) 4. Yao, J., O’Connor, S.D., Summers, R.M.: Automated spinal column extraction and partitioning, pp. 390–393. In: ISBI (2006) 5. Schmidt, S., Kappes, J.H., Bergtholdt, M., Pekar, V., Dries, S.P.M., Bystrov, D., Schnörr, C.: Spine detection and labeling using a parts-based graphical model. In: IPMI, pp. 122–133 (2007) 6. Dong, X., Zheng, G.: Automated vertebra identification from X-Ray images. In: ICIAR, issue 2, pp. 1–9 (2010)
Springer.tar//Springer//Springer\10.1007-s00138-012-0454-0.xml:SLAC: Statistical total lesion metabolic activity computation by fuzzy unsupervised learning of PET images:Finite mixture models Positron emission tomography Laplace distribution Lesion activity estimation Convex combination of random variables Gaussian distribution:  1 Introduction  Over the past decade, positron emission tomography (PET) imaging has been accentuating its role in the field of oncology&amp;nbsp;[1]. Cancer fatalities are expected to outscore other mortalities in the coming decade, which makes the future of oncology more prominent. PET can record the metabolic response of several trillions of cells in the human body by the use of radiotracers such as F-fluorodeoxyglucose (FDG). The tumor lesion, being more metabolically active, shows a higher tracer uptake in the PET scan, thereby enabling quantification of metabolic activity information compared to anatomic imaging schemes like magnetic resonance imaging (MRI), computed tomography (CT), etc. For a wide range of clinical practices, spanning from radiotherapy treatment to tumor staging and patient follow-up evaluations, FDG PET has established itself to be an indispensable tool&amp;nbsp;[2]. Apart from F-FDG, other radio tracers like F-fluorothymidine (FLT) have also shown their capability in assessing tumor metabolic activity changes&amp;nbsp;[3]. Early response detection in tumor therapy may lead to avoiding the costs of ineffective therapy, earlier application of alternative and possibly more effective treatment, and hence reduced patient morbidity. In recent clinical studies&amp;nbsp;[4–8], PET volume delineation and the subsequent quantification of tumor lesion activity drew immense interest among oncologists. In some cases, PET imaging detects changes in the tumor metabolic activity even hours after drug administration, and may therefore be capable of predicting therapy response well before the current imaging gold standard, CT&amp;nbsp;[5, 6]. Bearing this in mind, Wahl et al.&amp;nbsp;[4] proposed PET response criteria in solid tumors (PERCIST) for clinical PET studies as an alternative to response evaluation criteria in solid tumors (RECIST) in CT. Radiotherapy, on the other hand, needed better tumor localization so as to selectively annihilate the proliferating cancerous lesions. For tumor staging and therapy follow-up evaluations, better reproducibility and repeatability in lesion activity quantification is considered vital rather than precision in tumor delineation. Several measures&amp;nbsp;[1, 3, 9–13] have been used in the literature for the longitudinal evaluation of tumor proliferation. Standard clinically relevant quantification metrics for PET are most often expressed in a normalized scale known as the standardized uptake value (). Typical metrics are  (maximum lesion activity uptake),  (average lesion activity uptake),  (activity uptake in  spherical region of interest (ROI) around the &amp;nbsp;[4]),  (total lesion volume) and  (total lesion activity uptake), associated to a tumor lesion. TLA is also known as total lesion glycolysis (TLG)&amp;nbsp;[1, 9–12], or total glycolytic volume (TGV)&amp;nbsp;[3, 13], in the context of F-FDG PET, or total proliferative volume (TPV)&amp;nbsp;[3], in the context of F-FLT PET. We coined total lesion activity (TLA) as a generic term to represent the totality of tumor lesion metabolic activity in a PET image irrespective of the radiotracer involved. The clinical relevance of TLA can be found in&amp;nbsp;[3, 9–13]. The change in  between pre- and post- treatment PET scans, estimated for therapy response evaluations, was referred to as Larson-Ginsberg index (LGI)&amp;nbsp;[9, 11]. TLA calculation typically involves tumor delineation, to find , followed by the quantification . A quantitative comparison of various delineation methods to estimate TLV can be found in&amp;nbsp;[14]. In Fig.&amp;nbsp; , the limitations in computing   from the delineations are demonstrated. Line profiles through a typical reconstruction and the ground truth activity distribution in spherical lesions with diameter 37, 28, 22, 17, 13 and 10&amp;nbsp;mm are plotted in red and black lines, respectively. A perfect PET volume delineation algorithm would end up with a tumor locus exactly equal to the ground truth line profile. Nevertheless, in doing so, the total activity inside the delineated profile (light gray shaded area) will be an underestimation of the actual activity (dark gray shaded area). This underestimation gets increasingly evident with decrease in tumor size as more and more activity is “spilled” outside the true delineation profile. One way to handle this is to incorporate a partial volume correction (PVC) method as a preprocessing stage prior to delineation&amp;nbsp;[ 15,  16]. However, investigators have been reluctant to incorporate these non linear data transformations in their clinical studies&amp;nbsp;[ 4]. This is because slight errors made in the tumor size measurements to correct for partial volume effects could adversely affect the quantitation, especially in the case of smaller lesions. Besides, recent investigations on the impact of PVC on esophageal cancer prognosis proved to be insignificant&amp;nbsp;[ 17]. We introduce a statistical scheme of lesion activity computation designed with focus on accurate activity estimation rather than tumor delineation. Our proposition is that “the delineation and the quantification are not two separate entities, instead to be done hand in hand”. The main contribution of this paper is the proposal involving statistical analysis and modeling for a robust estimate of TLA. This paper is an extended version of the work we discussed in&amp;nbsp;[18]. To model the partial volume effects (PVE) in the reconstructed PET image, we revisit some existing methodologies related to it. Caillol et al. proposed a fuzzy Gaussian mixture model using a stochastic expectation maximization (SEM)&amp;nbsp;[19] framework to model the partial pixels for remote sensing applications in&amp;nbsp;[20, 21]. Van Leemput et al. independently proposed a similar unified approach to model partial voxels in magnetic resonance images using Gaussian mixture-based expectation maximization (EM)&amp;nbsp;[22, 23] algorithm with a Markov random field&amp;nbsp;[24]. Hatt et al.&amp;nbsp;[25] applied the fuzzy SEM approach to model partial volumes in the PET image. In addition, a locally adaptive prior computation was introduced between SEM iterations to better model the PET image. This framework was further extended for modeling heterogeneous tumors in&amp;nbsp;[26]. However, the scope of both&amp;nbsp;[25] and&amp;nbsp;[26] was limited to tumor delineation. In our statistical lesion activity computation (SLAC) approach, we use the fuzzy SEM algorithm from&amp;nbsp;[21, 25] to model the PET image suffering from PVE as a fuzzy convex combination of tumor and background voxels. Then, instead of going for delineation as in the fuzzy locally adaptive Bayesian (FLAB) method&amp;nbsp;[25], we derive a statistical relationship between TLA and the modeled finite mixture parameters. TLA is thus estimated directly, without a tumor delineation step. To evaluate the performance, TLA estimated using the SLAC is compared to the TLA estimated from tumor delineations obtained with three state-of-the-art segmentation methods including gradient-based&amp;nbsp;[27], adaptive threshold-based&amp;nbsp;[28] and stochastic methods&amp;nbsp;[25]. The remainder of the paper is organized as follows. Section&amp;nbsp;2 presents the fundamental concepts related to the statistical modeling of the reconstructed PET image, whereas Sect.&amp;nbsp;3 details the proposed framework for the direct statistical estimation of total lesion activity. Section&amp;nbsp;4 deals with the experimental set up. Section&amp;nbsp;5 demonstrates the results obtained with the simulated studies. The benefits, limitation and scope of the proposed method are discussed in Sect.&amp;nbsp;6, while Sect.&amp;nbsp;7 winds up with the concluding remarks.    2 Fuzzy unsupervised learning  Fuzzy unsupervised learning investigates how systems can learn to represent (fuzzy representation) particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns (observed PET image). Let the observed PET image and the hidden statistical structure be realizations   and   of the random fields   and  , respectively, where   is the set of voxels. Let  , where  , be the   class labels associated with the hidden classification map. Then   takes its values in  , or  . The unsupervised learning problem consists of estimating the unknown model parameters and the hidden   from the observed noisy version of   in  . From Bayes’ theorem, the a&amp;nbsp;posteriori distribution of   given the available information   is       where   is the likelihood function of   conditionally with respect to the hidden true value  .   is the a&amp;nbsp;priori information about  . For the sake of simplicity, consider two “hard” classes viz. tumor and background, i.e.,  with “” representing the tumor lesion and “” the background. To model the blur and partial volume effects imposed by the PET acquisition system and reconstruction, a fuzzy measure is introduced as in&amp;nbsp;[20, 21, 25], which represents the partial volume voxels having joint membership from both the tumor “hard” class and the background “hard” class. This model allows co-existence of tumor only, background only and partial volume voxels. While the statistical part of the algorithm models the uncertainty in the classification, the fuzzy part models the imprecision in voxel membership. Now  in the fuzzy unsupervised learning model takes its value in the closed interval . This is achieved by simultaneously using Dirac and Lebesgue measures in the fuzzy model&amp;nbsp;[21]. Then the new measure will be  on , where  and  are the Dirac measures on  and , and  is the Lebesgue measure on the fuzzy interval . The a&amp;nbsp;priori distribution of   with respect to the measure   on   is defined by the density   as                                      where   satisfies       In our modeling, we assume   to be non stationary, which means that   and   in&amp;nbsp;( 2)–( 4) depend on  :  . Here, we use an adaptive support window to estimate the a&amp;nbsp;priori information locally as discussed in&amp;nbsp;[ 20,  21,  25]. Let   be the learned hidden classification map at a particular iteration, then       where   is a   local support window around voxel   is the Kronecker delta function. Card  is the cardinality of  . To find the conditional density of  given the hidden classification , consider two independent and identically distributed (iid) random variables,  and , associated to the tumor hard class (“”) and the background (“”) with densities  and  defining the distributions of  conditional to  and , respectively. Then the partial volume activities can be modeled by convex combinations of these independent random variables as , for . Here,  and  model the noise associated with the tumor, the background and the partial volume activities, respectively. If  and  are Gaussian distributed with densities  and , respectively (see “Gaussian distribution” in Appendix A), then  is again Gaussian distributed with density  conditional to , such that  and  (see Gaussian distribution in Appendix B). To study the variability in the assumption on probability distribution, we also investigated the case of modeling with Laplace distribution (see “Laplace distribution” in Appendix A). Laplace distribution is selected since at high acquisition blur, the histogram of the reconstructed PET image resembles the shape of a Laplace density function. Nevertheless, the convex combination of two Laplace random variables is not necessarily Laplace distributed. In this case,  is characterized by the density function given in Laplace distribution in Appendix B with . The SEM algorithm&amp;nbsp;[ 19] is an iterative method used for estimating the parameters of the model. It is a stochastic version of the EM algorithm&amp;nbsp;[ 22,  23] making use of stochastic expectation and maximization steps to efficiently model the PET image. The initialization and the stopping criteria of the SEM are discussed in the next section. At each iteration  , a realization   is sampled (see below) according to the posterior distribution&amp;nbsp;( 7) computed using the current values of the parameters. Then the parameters of the density functions   are updated from the “hard” classes in  , and   are updated using&amp;nbsp;( 6). The estimation formulae for the Gaussian and Laplace distributions can be found in Appendix A. The a&amp;nbsp;posteriori distribution of   with respect to   given the noisy observation   is given by             for  , which are obtained by numerical integration. The “fuzzy” label , representing the blur and PVE, can be assigned either by solving the least squares equation  or by sampling the fuzzy class into discrete levels. Here, we follow the latter case. If  fuzzy levels have been used, then  with “fuzzy” class labels , where . If, for the tumor “hard” class and the background “hard” class,  and , respectively, then  takes the classification labels from , where . To obtain the hidden variable , we sample the posterior distribution  taking values from  and making use of the maximum posterior likelihood (MPL) method proposed in&amp;nbsp;[20, 21]. MPL integrates a dual sampling stage as follows. If  and  are the a&amp;nbsp;posteriori densities associated with the fuzzy class, the tumor class and the background class, respectively, then the classification rule assigns to each voxel  the label  in  which has the maximum a&amp;nbsp;posteriori density, i.e., . If , i.e., when the classifier opts for hard class labels, the decision rule halts. Otherwise, i.e., when , the classification proceeds to the second stage decision rule given by  to sample one of the fuzzy labels  in .    3 Statistical lesion activity computation  Revisiting Sect.&amp;nbsp; 1, total lesion activity (TLA) is devised as a composite of lesion metabolic activity and tumor volume [ 1,  3,  9– 13]. Here, we derive the mathematical interpretation of TLA and its statistical counterpart as follows. Mathematically,   is  . If   is the fraction of the true lesion activity concentration   in voxel  , TLA can be expressed as Beginning from the total PET activity ( ), constituting contributions from the lesion ( ) and the background activities, we deduce the relationship between   and the SEM model parameters. From the observed PET image voxels   is given by       where   is the observed histogram and   is the set of observed PET activity levels,  . In other words, the total activity in   is the same as the sum of all activity levels   scaled by respective frequencies of occurrence  . Based on the fact that normalization of the observed histogram   gives the associated probability mass function  , without the loss of generality,   can be defined as       where   is the number of voxels. Extending&amp;nbsp;( 10) to the continuous domain and integrating over the whole activity,   can be related to the density   defining the distribution of   as       From Riemann–Stieltjes integral,   can be derived from the 1st moment of   as       Consider the case of a   class classifier to model   as a finite mixture of probability distributions. Let  , where  , be the   labels associated with the classification. Then   can be modeled as a mixture of conditional densities such that  , where   stands for the mixing probabilities   and   denotes the density defining the distribution of   conditional to  . If   represents the conditional expectation of   given  , then the   can be modified as Referring back to the SEM detailed in Sect.&amp;nbsp; 2, the conditional expectation of   given   is            applying the linearity property of the expectation operator and the statistical independence between the random variables   and  . After the fuzzy unsupervised learning linking the tumor, background and its partial mixture, the tumor in the reconstructed PET image is represented by two “hard” classes and   “fuzzy” classes as shown in Fig.&amp;nbsp; . In that scenario, the classification takes labels in  , where   (see Sect.&amp;nbsp; 2). Then&amp;nbsp;( 13) can be written as               and   correspond to the weights of the background,   fuzzy mixtures and the tumor class, respectively. Substituting&amp;nbsp;( 14) in&amp;nbsp;( 15),       In the   estimator in&amp;nbsp;( 16), the first term originates from the background activity  , while the second term represents the lesion activity  . Adding up the contributions from the tumor class only, we propose that, the statistical   can be expressed by       In&amp;nbsp;( 17) activities from   “hard” tumor class and   “fuzzy” classes, representing the tumor and the partial volume voxels respectively, are summed up. To compute the   in&amp;nbsp;( 17), the expected value   is   for both Gaussian   and Laplace   distributed   (see Appendix A). Thus   estimator needs estimations of only the mixing proportions  , fuzzy weights   and the expected mean of the tumor class  . Figure&amp;nbsp; a and b illustrate a typical class allocation   after fuzzy unsupervised learning. Figure&amp;nbsp; c embodies the actual histogram, in black, being modeled as a finite mixture of tumor, background and partial mixture distributions. The tumor class and the background class distributions are represented in red and blue, respectively. The contributions of each class towards the final estimate of   are pointed out. The steps involved in the proposed statistical lesion activity computation (SLAC) algorithm are listed in Algorithm&amp;nbsp;(1). To deal with initialization, we use random starts. In each of these random starts, a fuzzy C-means clustering&amp;nbsp;[29] routine looks for the initial feasible solution consisting of three clusters representing tumor, background and partial volume mixture. The model parameters are estimated to compute the message length&amp;nbsp;[30]. The one giving the minimum message length () is selected to initialize the SEM algorithm&amp;nbsp;[19]. In each iteration, the a&amp;nbsp;posteriori probabilities  are estimated as in&amp;nbsp;(7) using numeric integration. This forms the expectation part of the SEM module.  are then sampled with the decision rule mentioned in Sect.&amp;nbsp;2 to estimate the intermediate labels . The maximization part involves the distribution parameter estimation and the local adaptive prior  computation. Gaussian parameter estimation involves finding the sample mean  and the sample variance , while Laplacian parameters are the sample median  and the mean absolute deviation from the sample median .  is finally statistically calculated using&amp;nbsp;(17).    4 Experiment  To evaluate the performance, two types of phantoms with hot lesions were simulated. First, a warm, water-filled cylinder (200&amp;nbsp;mm diameter, 155&amp;nbsp;mm long) with six hot spheres was simulated. The spheres were located in the central plane on a circle with a diameter of 110&amp;nbsp;mm. The diameters ( ) of the spheres were 37, 28, 22, 17, 13, and 10&amp;nbsp;mm (see Fig.&amp;nbsp; ). The cylinder was filled with 7.4&amp;nbsp;kBq/cc. The lesion-to-background ratio was set to 4:1. Next, realistic FDG uptake values were assigned to the various organs and tissues of the NCAT phantom&amp;nbsp;[31] based on a clinical FDG PET scan. Eight non spherical tumors were inserted in the liver, lungs, spleen and lymph node (see Fig.&amp;nbsp;). In all tumors, the activity was set to 18.2&amp;nbsp;kBq/cc. The activity in the liver, spleen, lungs and body was 6.3, 5.5, 0.9 and 2.5&amp;nbsp;kBq/cc, respectively. For the hot spheres phantom, 30 noisy fully 3D acquisitions of 1&amp;nbsp;min on an ECAT Exact HR+ (Siemens Healthcare) were simulated using an analytical projector. To accurately model the hot lesions,  voxels were used during simulation of the PET projection. The scanner resolution was modeled by a Gaussian with a full width half maximum (FWHM) of 5&amp;nbsp;mm. Attenuation was modeled as well. The detector sampling was 2.25&amp;nbsp;mm in transaxial direction and 2.425&amp;nbsp;mm in axial direction. To evaluate the algorithms on a more realistic dataset, 30 3&amp;nbsp;min FDG PET scans of the NCAT phantom were simulated using a Monte Carlo simulator (PET-SORTEO&amp;nbsp;[32]), which models among others the spatially variant point spread function (PSF) of the ECAT Exact HR+ scanner. The voxel size used to generate the phantom was . Attenuation and scatter were also modeled. For both phantoms, the projection data were reconstructed using the MLEM algorithm&amp;nbsp;[33] with ordered subsets. As in clinical routine,  iterations over  subsets were performed . In addition, MAP reconstructed images were calculated using an edge preserving prior minimizing the absolute differences between neighboring voxels&amp;nbsp;[34]. For better convergence,  iterations over  subsets were performed . The reconstruction voxel size was set to . During reconstruction of both datasets, the system resolution was recovered by modeling the PSF as an isotropic Gaussian with 5&amp;nbsp;mm FWHM (as done in clinical reconstructions). To evaluate the robustness of the  estimation, the MLEM images were also post-smoothed with 1–10&amp;nbsp;mm Gaussian FWHMs. In the MAP algorithm, various weights () were assigned to the prior ( = 0.05–0.55 in steps of ). Three competent PET volume delineation methods were selected for this study, namely a stochastic (FLAB)&amp;nbsp;[25], a gradient-based (GDM)&amp;nbsp;[27] and an adaptive threshold-based (ATM)&amp;nbsp;[28] method. These methods were selected as their performance has been reported to be superior to the threshold-based region growing method, commonly used in clinical trials, over a wide variety of lesions. The FLAB&amp;nbsp;[25] method uses the same fuzzy SEM algorithm as we use in the SLAC method, but then to delineate lesions accurately. From the  classification labels, the binary segmentation map is created by allocating  “fuzzy” classes closer to the tumor “hard” class as belonging to the tumor and the rest to the background.The adaptive threshold method (ATM)&amp;nbsp;[28] is optimized for uniform spherical objects: it computes the optimal threshold as a function of the sphere size, the object-to-background activity ratio and the spatial resolution of the scanner. This relation can be computed and/or determined from phantom measurements. The assumption that the tumor is uniform and spherical is its main limitation. In addition to the above-mentioned automated methods, a 40&amp;nbsp;% threshold-based region growing method (T40) was evaluated. To estimate the , a  region of interest (ROI) containing the tumor lesion and sufficient background was selected as mentioned in&amp;nbsp;[25]. The ROI size should be broad enough to include sufficient background so as to properly model the distribution parameters. On the contrary, it should be narrow enough just to avoid the learning machine from labeling those partial mixtures closer to the tumor class as belonging to the tumor. The ROI does not necessarily need to be of the shape of a proper rectangular cuboid; however, could be of any size or shape. In the case of NCAT phantom lesions, to deal with the background heterogeneities, we restrict the ROI to include only two “hard” classes of data and its mixtures. Here, a finite mixture model is estimated from the ROI and the least prominent background class is removed from further modeling with SLAC. Lesions  and  (see Fig.&amp;nbsp;) are typical cases of multi class data in realistic tumors. SLAC was iterated  times unless the cut-off condition, 0.1&amp;nbsp;% change in , was reached. To study the variability of the SLAC method with the number of fuzzy levels, it was varied from  over , and  till . Likewise, various adaptive windows sizes  were used to investigate its influence. The reconstructed PET image is modeled using either Gaussian or Laplace distributions to study the dependency on the probability density assumption. For each of the compared delineation schemes, optimal parameters were selected from our previous investigations&amp;nbsp;[14, 18]. FLAB was iterated  times unless the cut-off condition,  parameter change, was reached. The noise was modeled as Gaussian and three fuzzy levels were used along with two hard levels for the analysis. To obtain the binary delineation map, the two fuzzy levels closest to the tumor class were taken as tumor and the other fuzzy level as background. For GDM, the denoising was iterated twice whereas the deconvolution ran over  iterations&amp;nbsp;[27]. ATM required source and background activities. The source was computed as the mean intensity within a region grown inside the hot tumor lesion. The background activity was averaged over the background region selected within the phantom. ATM was set to run  iterations unless the cut-off condition,  relative threshold level (RTL) change, was reached. FWHM, needed for the GDM and the ATM methods, is estimated from simulation experiments for both  and  reconstructions. The parameters of these algorithms were tuned to optimize the delineation accuracy. For quantification of the accuracy of the   estimate, the error in   was computed in   as          stands for   estimated using either a delineation method or the proposed direct approach (SLAC).   stands for the actual   calculated based on a resampled fuzzy ground truth knowledge of the tumor location   and the true lesion activity.
Springer.tar//Springer//Springer\10.1007-s00138-012-0455-z.xml:A novel online boosting algorithm for automatic anatomy detection:Histogram Kalman filter Anatomy detection Online boosting:  1 Introduction  Automated detection of anatomic structures is an essential functionality for navigating through large 3D image datasets and supporting computer-aided diagnosis (CAD). Several approaches have been proposed for localizing anatomic structures. Zheng et al.&amp;nbsp;[22] trained a probabilistic boosting tree with steerable features to detect the heart chambers. Zhou et al.&amp;nbsp;[24] combined the concepts of prediction and detection to more efficiently and precisely localize the left ventricle in ultrasound images. A framework allowing simultaneous localization of several anatomic structures was proposed by Liu et al. &amp;nbsp;[10]. Criminisi et al.&amp;nbsp;[2] introduced context-rich visual features and utilized regression forests to estimate the organ position and the corresponding bounding box. Despite their tremendous success, the existing approaches are all off-line and thus restrained by the generic limitations associated with off-line training as explained below:        The off-line approaches require all training samples to be available before the commencement of training. Clearly, it is not practical to collect a complete set of representative training samples in advance; it is inevitable to encounter some variants that are not captured in the original training set; for instance, shape changes in anatomic structures caused by diseases or image artifacts.         &amp;nbsp;           The detector trained using off-line approaches is fixed and cannot be improved given a set of new training samples. One way to overcome this problem is to collect and store all the previously used and upcoming training samples and then re-train the detector. However, such a practice may face a number of practicalities and is limited by the amount of memory and computation time.         &amp;nbsp;      Certainly, it would be ideal if a detector could be improved dynamically according to the feedback from physicians. Such feedback is available because in current clinical practice, all CAD systems operate in a closed-loop environment—any CAD finding must be approved or rejected by a physician. Therefore, we aim to develop an online learning framework. The off-line approaches require all training samples to be available before the commencement of training. Clearly, it is not practical to collect a complete set of representative training samples in advance; it is inevitable to encounter some variants that are not captured in the original training set; for instance, shape changes in anatomic structures caused by diseases or image artifacts. The detector trained using off-line approaches is fixed and cannot be improved given a set of new training samples. One way to overcome this problem is to collect and store all the previously used and upcoming training samples and then re-train the detector. However, such a practice may face a number of practicalities and is limited by the amount of memory and computation time. Research in online learning (e.g., [4–6, 11–15, 21]) is largely motivated by the challenges of object tracking. The first online boosting algorithm was proposed by Oza and Russell&amp;nbsp;[12]. In this method, the sequential arrival of samples was modeled by a Poisson distribution. Inspired by Oza and Russell’s method, Grabner and Bischof&amp;nbsp;[4] developed a boosting-based feature selection method for object tracking. The updating mechanism discards any weak learner with error rate greater than or equal to 0.5 (random guess) and replaces them with new weak learners from the feature pool. Parag et al.&amp;nbsp;[13] proposed an online boosting framework based on a fixed set of linear regressors and developed a forgetting mechanism for improving adaptation to new samples. Wu and Nevatia&amp;nbsp;[21] suggest an unsupervised online learning approach capable of learning from a small labeled training set. Liu and Yu&amp;nbsp;[11] proposed a unified framework for both feature selection and weak learner updating in which features evolve based on a gradient descent method to improve handling of the object tracking task. Although the existing online learning methods have proven to be effective for object tracking, they are not suitable for incrementally training a detector for anatomy detection. Fundamental to successful object tracking is rapid and efficient handling of changes in statistical properties of the target object. Such a requirement demands learning methods with continual adaptation to the varying appearance of the target over time. However, such a high-level adaptation is achieved at the expense of sacrificing the classification accuracy. In other words, tracking methods tend to forget the information of past samples as they learn from the new samples. In the literature, this phenomenon is referred to as “catastrophic forgetting”. Although this phenomenon might not dramatically affect a tracking method as it manages the current state of the object, it degrades the anatomy detectors since they require strong long-term memory of samples. In fact, the anatomic structure detector must maintain consistent performance with the old samples while simultaneously acquiring additional information from the new data. Therefore, the online tracking methods cannot be directly used for the task of anatomy detection. This paper proposes a novel online boosting algorithm for automatic detection of anatomic structures. The proposed method eliminates the need for storing historical training samples and is capable of continually improving performance with new samples. We evaluate our method with three major thoracic structures: the pulmonary trunk, carina, and aortic arch (Fig.&amp;nbsp;  method&amp;nbsp;[ 4] and yields results comparable to off-line counterpart&amp;nbsp;[ 20]. The key contributions of our work are summarized as follows:        A distinctive online boosting method for object detection: The proposed method has a unique online learning structure coupled with more accurate weaker learners. Rather than the traditional Kalman filter, we use histograms as the weak learner. Histograms allow improved sample tracking compared with the Kalman filter. Our method builds upon a fixed feature pool, and neither replaces nor discards any features from the pool, resulting in more efficiency and stability.         &amp;nbsp;           An evaluation framework for investigating the properties of online learning approaches: We focus on four key properties, including computational efficiency, stability, adaptability, and “catastrophic forgetting”. Corroborated with experiments, the proposed method has advantages over the “gold standard” G–B method&amp;nbsp;[4]. The proposed method is not only faster and more stable, but also improves handling of “catastrophic forgetting” with a satisfactory level of adaptability.         &amp;nbsp;           Three specific applications of our methodology to anatomy detection: The proposed online boosting method provides results comparable to off-line AdaBoost&amp;nbsp;[17]. To our knowledge, we are the first to address anatomical structure detection with online approaches; other existing approaches are all off-line.         &amp;nbsp; A distinctive online boosting method for object detection: The proposed method has a unique online learning structure coupled with more accurate weaker learners. Rather than the traditional Kalman filter, we use histograms as the weak learner. Histograms allow improved sample tracking compared with the Kalman filter. Our method builds upon a fixed feature pool, and neither replaces nor discards any features from the pool, resulting in more efficiency and stability. An evaluation framework for investigating the properties of online learning approaches: We focus on four key properties, including computational efficiency, stability, adaptability, and “catastrophic forgetting”. Corroborated with experiments, the proposed method has advantages over the “gold standard” G–B method&amp;nbsp;[4]. The proposed method is not only faster and more stable, but also improves handling of “catastrophic forgetting” with a satisfactory level of adaptability. Three specific applications of our methodology to anatomy detection: The proposed online boosting method provides results comparable to off-line AdaBoost&amp;nbsp;[17]. To our knowledge, we are the first to address anatomical structure detection with online approaches; other existing approaches are all off-line.    2 Proposed method  The proposed approach is an online boosting method, which continually updates a linear classifier. Given a set of training samples, the method dynamically updates a pool containing  features and returns a subset of  best features (\(N&amp;lt;M\)). Each best feature is associated with a voting weight. The final classifier is a weighted linear combination of the  best features. Each feature in the pool is assigned one weak learner,2 which comprises two histograms: one for positive samples and the other for negative samples. Since samples arrive sequentially, the two histograms are incrementally assembled over time. To continually update the histograms, the range of each feature in the pool must be known in advance. To fulfill this requirement, each feature’s range is estimated by examining feature values computed from a temporary set of samples. This temporary set is subsequently discarded and is not used in the training stage. The estimated ranges are then equally divided into 100 bins. Updating. For each feature in the pool, when a training sample arrives, depending on the label, the pertinent histogram is selected, the bin to which the sample belongs is determined, and the frequency of the associated bin is updated. Note that future training samples whose feature values fall beyond the obtained ranges are assigned to either the first or last bin. Once the histograms are updated, the decision thresholds are defined at the intersections of positive and negative samples’ histograms. These thresholds are chosen among the histograms’ bins such that a maximum classification rate is obtained. Figure   shows the pseudocode of the proposed online boosting approach. Stage 1: When a new sample arrives, the corresponding histograms of all weak learners are updated and the new decision thresholds are calculated. Stage 2.1: Next, each updated weak learner classifies the sample and is then rewarded or punished with the sample’s importance weight for a correct or wrong classification, respectively. The importance weight  is initially set to 1 and is updated during the learning process (See stage 2.4). The rewards and punishments that a weak learner receives are accumulated in  and  which are further used to calculate the error rate of each weak learner (See Stage 2.1 of the pseudocode). Stage 2.2: Given the error rates of all weak learners, the selector chooses the best weak learner with the least error rate. The selected weak learner is added to the set  that contains the best weak learners selected so far. The members of the set  are ignored when choosing the “next best” weak learners. In fact, each weak learner is only selected once and there are no multiple instances of a weak learner in the set . Stage 2.3: Once a weak learner is chosen, its corresponding voting weight is computed. The voting weight explains to what extent the selected weak learner contributes to the final classifier. Recall that the learning algorithm is to return a linear classifier that is a weighted combination of the best weak learners. The voting weight  (), corresponding to the th best weak learner, is given by , where  denotes the weak learner index (). Stage 2.4: Next, according to the classification outcome and error rate of the selected weak learner, the importance weight  is updated. The importance weight increases for a wrong classification and decreases for a correct classification. The more the importance weight, the harder it is to classify the sample. The weak learner, depending on the error rate, can reduce the importance weight by up to 50&amp;nbsp;% or increase boundlessly. The rationale behind using and updating  is to select the “next best” weak learner in a manner that corrects for difficult training samples which have been misclassified by the previously chosen weak learners. Stage 3: Given the  best weak learners, the final linear classifier is defined as  where  is the output returned by the th best weak learner,  is the voting weight of th best weak learner, and  denotes the set containing the indices of selected best weak learners. The classifier outputs 1 if the sample contains the desired object; otherwise it returns 1. In case of misclassification, the learning process is repeated with the current sample until either a correct classification is obtained or the maximum number of classification attempts  is reached. In this section, we first compare our online boosting method with some other variants of online boosting, and then explain our contributions in contrast with the work of Grabner and Bischof&amp;nbsp;[4]—the closest work to our methodology. The first online boosting method was suggested by Oza and Russell&amp;nbsp;[12]. In their work, not only the combination rule, but also the classifiers themselves were incrementally updated given new training samples. This learning structure was further extended by Wu and Nevatia&amp;nbsp;[21] to the case of cascade structured detectors. In comparison, while our methodology is an incremental feature selection method, Oza’s approach uses the totality of feature pool and, thus, is not applicable to the task of feature selection. The online boosting algorithm suggested by Parag et al. [13] is also not applicable to the task of feature selection, since the features underlying the regressors are fixed and all are in use during the learning process. Liu and Yu [11] proposed a gradient-based feature selection scheme for online boosting, in which not only the weak learners but also the features evolve to capture the ongoing changes of the object being tracked. Although highly adaptive and thus apt for tracking purposes, their algorithm is not expected to maintain a long memory of samples, since the features themselves undergo continuous change during the learning process. Such a learning algorithm is not suitable for the task of anatomy detection. The closest methodology to our approach is the one suggested by Grabner and Bischof&amp;nbsp;[4], in which incremental feature selection is realized through an online boosting scheme. The existing variants (e.g., [9, 18]) of this learning algorithm mainly differ in terms of number of target categories and the updating mechanism of the importance weight, with the layered structure and other main principles remaining the same. In our paper, we therefore choose to compare our methodology with &amp;nbsp;[4]. In comparison, both algorithms borrow the idea of online boosting to implement incremental feature selection, but our methodology differs from the method of Grabner and Bischof &amp;nbsp;[4] in both (1) the learning structure and (2) the adopted weak learner. 1. New learning structure. Figure&amp;nbsp;  compares the proposed learning method with the one suggested by Grabner and Bischof&amp;nbsp;[ 4]. As seen in Fig.&amp;nbsp; , the learning structures of the proposed method and that of Grabner and Bischof differ in terms of the number of feature pools, the number of selectors, and the assessment process, as reviewed below.         Number of feature pools: As seen in Fig.&amp;nbsp;, our method employs only one feature pool compared to  feature pools in G–B method where  is the number of desired weak learners. When a sample arrives, all weak learners must be updated. The proposed method requires updating only  weak learners, independent of the number of weak learners to be selected (). However, the layered structure requires updating  weak learners ( feature pools each containing  weak learners). In fact, computational complexity is a function of the desired number of weak learners (), and thus significantly increases when the classifier builds upon a large number of weak learners.         &amp;nbsp;            Number of selectors: Using only one feature pool, we need to employ only one selector to choose  best weak learners. The G–B method requires  feature pools and thus  selectors () each selecting one best feature. Adopting  selectors increases the time and computational complexity and, as we show later, is also the main cause of instability of the G–B method.         &amp;nbsp;            Assessment process: We do not discard training samples until either a correct classification is made or a maximum number of classification attempts is reached (See stage 3 in Fig.&amp;nbsp;). Traditional online methods designed for tracking typically discard training samples after updating the detector once. Such a practice may not be problematic for tracking methods which rapidly adapt to new samples, but would highly restrain methods with less adaptability. Our method tends to keep longer memory of old samples and thus exhibits less adaptation to the new samples. To enhance adaptability and to maximize the information captured from a sample, we feed each sample into the detector up to “” times, significantly enhancing the adaptability while slightly affecting the detector’s long-term memory. Indeed, rather than adopting many feature pools— which accounts for high adaptability of the G–B method—we feed each sample into the detector several times.         &amp;nbsp;        2. Histogram-based weak learners. In contrast to the G–B method&amp;nbsp;[ 4], which employs Kalman filter, we use histogram-based weak learner. Kalman filter estimates the mean and standard deviation of positive and negative samples and returns a single decision threshold. It makes a Gaussian assumption and operates optimally for features with bi-modal distribution. However, Kalman filter is not capable of handling potentially useful features with multi-modal distributions and dismisses them as ineffective features. In other words, Kalman filter fails to fully utilize the feature pool. Compared with Kalman filter, histograms maintain a more comprehensive presentation of samples and allow improved handling of multi-modal features. Number of feature pools: As seen in Fig.&amp;nbsp;, our method employs only one feature pool compared to  feature pools in G–B method where  is the number of desired weak learners. When a sample arrives, all weak learners must be updated. The proposed method requires updating only  weak learners, independent of the number of weak learners to be selected (). However, the layered structure requires updating  weak learners ( feature pools each containing  weak learners). In fact, computational complexity is a function of the desired number of weak learners (), and thus significantly increases when the classifier builds upon a large number of weak learners. Number of selectors: Using only one feature pool, we need to employ only one selector to choose  best weak learners. The G–B method requires  feature pools and thus  selectors () each selecting one best feature. Adopting  selectors increases the time and computational complexity and, as we show later, is also the main cause of instability of the G–B method. Assessment process: We do not discard training samples until either a correct classification is made or a maximum number of classification attempts is reached (See stage 3 in Fig.&amp;nbsp;). Traditional online methods designed for tracking typically discard training samples after updating the detector once. Such a practice may not be problematic for tracking methods which rapidly adapt to new samples, but would highly restrain methods with less adaptability. Our method tends to keep longer memory of old samples and thus exhibits less adaptation to the new samples. To enhance adaptability and to maximize the information captured from a sample, we feed each sample into the detector up to “” times, significantly enhancing the adaptability while slightly affecting the detector’s long-term memory. Indeed, rather than adopting many feature pools— which accounts for high adaptability of the G–B method—we feed each sample into the detector several times. In summary, employing one feature pool and one selector results in high computational efficiency; adopting the assessment strategy improves adaptability; and utilizing histogram-based weak learners enhances the overall performance.    3 Experiments  In this section, we first evaluate the performance of our proposed method compared with the G–B method&amp;nbsp;[4] and AdaBoost&amp;nbsp;[17], and then investigate its four online learning properties, comparing with the G–B method&amp;nbsp;[4]. We conduct three experiments with three thoracic structures: the pulmonary trunk, carina, and aortic arch. The rationale for our anatomic choices resides in their high diagnostic value. Detecting the pulmonary trunk is an essential step of pulmonary artery–vein separation, which significantly enhances the performance of CAD systems designed for pulmonary embolism detection. Carina localization is critical for our pulmonary embolism detection system&amp;nbsp;[25] because it facilitates airway extraction. Parallel with the pulmonary artery, airway indirectly assists in elimination of false positives generated in the azygos vein. Finally, locating the aortic arch is fundamental to any automatic method for detecting and measuring aortic calcification (e.g.,&amp;nbsp;[8, 23]). The experiments exploit 157 CT pulmonary angiogram datasets, of which 80 are used for training the detector and the remainder 77 are reserved for testing. Each dataset contains 450–600 slices. For each of the three experiments, we carry out the following procedure to construct the training samples. From each training dataset, 50 positive and 50 negative samples are extracted and resized to  pixels. The positive samples are selected up to 5 pixels away from the ground truth and negative samples are randomly selected such that the desired structures are not included. Having 80 training datasets, one can obtain 8,000 training samples which can be further enhanced by additional 12,000 bootstrap negative samples. The bootstrap set of samples is generated by the cascade detector&amp;nbsp;[ 20] which is trained with the current 8,000 samples as the initial training set. The AdaBoost classifier is built at each stage of the cascade scheme. Decision stumps are used as the weak learner and 2D Haar features&amp;nbsp;[ 20] are chosen to form the feature pool. We adopt Haar features because they are computationally efficient and particularly well suited for the task of object detection&amp;nbsp;[ 1,  3,  16,  19]. Using four Haar patterns of different positions, scales, and aspect ratios, we obtain 101,400 features for each training sample. At each stage, when Adaboost is trained, the resulting detector scans all training datasets and returns false positive samples, which are later collected as the bootstrap samples to train the next stage of the cascade. We resize all bootstrap samples generated through the training process of the cascade detector, select 12,000 samples randomly, and add these samples to the initial training set, resulting in 20,000 training samples for each thoracic structure. Figure&amp;nbsp;  shows examples of the bootstrap samples. We train AdaBoost, the G–B method&amp;nbsp;[4], and the proposed detector with the pre-constructed training sets for each thoracic structure. AdaBoost uses all 20,000 samples at once, while the two online learning methods receive samples sequentially over time. Each detector consists of 100 weak learners chosen from a feature pool of 1000 features. The feature pool is randomly selected from 101,400 Haar features. The trained detectors are evaluated using the test datasets. Since the structures may appear at different scales (See Fig.&amp;nbsp; ), a multi-scale search is performed to locate the three anatomic structures. During the scanning process, the detector grades image patches of different scales and returns the center of the patch with the highest classification score. If the distance between the detection point and the ground truth location is less than 30mm, the object is regarded as “detected”; otherwise, the object is regarded as a misclassification. Table&amp;nbsp; 1 summarizes the detection rates for the proposed method, AdaBoost&amp;nbsp;[ 17] and the G–B method&amp;nbsp;[ 4]. Comparison with the method of Grabner and Bischof&amp;nbsp;[4]: For carinal evaluation, the proposed method slightly outperforms the G–B method&amp;nbsp;[4]. The small variation in scale and orientation accounts for the high carina detection rates. However, the situation for the pulmonary trunk and aortic arch is quite different. Table 1 shows a decreased detection rate for both online approaches, although the proposed method still achieves satisfactory performance. The large variation in the pulmonary trunk and aortic arch’s scale and orientation accounts for this degradation. Difficulties caused by object variation demands a detector with strong long-term training sample memory. Allowing only short-term memory, the approach of Grabner and Bischof exhibits significant performance degradation.
Springer.tar//Springer//Springer\10.1007-s00138-012-0456-y.xml:Classification of small lesions in dynamic breast MRI: eliminating the need for precise lesion segmentation through spatio-temporal analysis of contrast enhancement:Topological texture features Dynamic breast MRI Mutual information Minkowski Functionals Support vector regression:  1 Introduction  Breast cancer is among the leading causes of mortality for women in North America&amp;nbsp;[1]. In this regard, dynamic contrast-enhanced MRI (DCE-MRI) is emerging as a promising diagnostic modality in the detection and evaluation of suspicious mammographic lesions. However, DCE-MRI exams typically require acquisitions in both spatial and temporal domains making subjective evaluation of clinical findings a challenging task for the radiologist. As a result, breast cancer diagnosis on DCE-MRI has been the subject of research in the area of computer-aided diagnosis (CADx)&amp;nbsp;[2–11]. Previous work has showcased the ability of CADx applications to achieve a high diagnostic sensitivity (up to 97&amp;nbsp;%) and reasonable specificity (76.5&amp;nbsp;%) in the task of classifying suspicious lesions on DCE-MRI through the use of dynamic criteria, morphologic criteria, or combinations of both&amp;nbsp;[12]. However, not many studies have focused on evaluating the value of DCE-MRI in  lesions which may not exhibit typical characteristics of benign and malignant tumors&amp;nbsp;[13]. Accurate diagnosis of such small lesions is clinically important for improving disease management in patients, where evaluating the dignity of breast lesions as being benign or malignant is specifically challenging. In this regard, Leinsinger et al. [14] reported a diagnostic accuracy of 75&amp;nbsp;% in breast cancer diagnosis through cluster analysis of signal intensity (SI) time curves. More recently, Schlossbauer et al.&amp;nbsp;[13], attempting to classify a dataset of small lesions (mean size 1.1&amp;nbsp;cm), reported AUC values of 0.76, 0.61 and 0.72 when using dynamic criteria, morphologic criteria and combinations of both, respectively. The primary goal of this work is to improve the classification performance of such small, diagnostically challenging lesions on breast DCE-MRI. Improved classification performance can contribute to reducing (1) the likelihood of performing “false positive” biopsies of benign lesions thereby eliminating the surgical risks associated with the biopsy and (2) missed breast cancers developing from misdiagnosed malignant lesions, while also enabling earlier diagnosis of suspicious lesions. In this work, we focus on the use of texture features to capture properties of the lesion enhancement pattern. Previous studies have investigated the use of standard second-order statistical features derived from gray-level co-occurrence matrices (GLCM) in capturing properties of lesion enhancement&amp;nbsp;[8–10]. Other studies have focused on characterizing the lesion enhancement pattern through dynamic evaluation of spatial variation&amp;nbsp;[15–17]. More recently, combining such GLCM-derived texture features from all post-contrast images of the lesion was also proposed to improve classification performance [18, 19]. Most of these studies have noted that texture features capturing aspects of heterogeneity or homogeneity of the enhancement pattern contribute to the best classification performance. This is not surprising given that heterogeneity has been previously identified as a key criterion indicative of lesion malignancy&amp;nbsp;[20, 21]. Given these observations, we investigate the use of more novel and advanced integral geometry texture analysis techniques that could be better suited to capturing such aspects of lesion enhancement heterogeneity. Specifically, we investigate the use of Minkowski Functionals, which can capture topological properties of the underlying gray-level pattern&amp;nbsp;[22]. This approach has been previously investigated in pattern recognition problems in medical imaging, specifically related to fibrotic tissue classification on lung CT&amp;nbsp;[23, 24] and prediction of hip-fracture in post-menopausal women&amp;nbsp;[25]. The primary motivation for investigating the use of such topological texture features in the problem of lesion character classification is illustrated in Fig.&amp;nbsp;. As seen here, when subject to several thresholds, malignant and benign lesions exhibit different changes to topology as a function of the gray-level threshold. Minkowski Functionals could be used to capture such differences and hence distinguish between benign and malignant lesions. While this figure shows larger lesions, we investigate the effectiveness of such techniques using a dataset of small lesions (mean lesion diameter 1.05&amp;nbsp;cm) as motivated by the clinical need described earlier.  Previous studies have also shown that pursuing standard GLCM approaches to texture analysis for purposes of lesion classification on breast MRI require  of the lesion for satisfactory classification performance. However, the precise segmentation of small lesions, such as those used in this study (example shown in Fig.&amp;nbsp;), is a challenging task for the radiologist. Manual segmentation of such lesions is subject to significant inter-reader variability between different radiologists owing to difficulties in visualizing lesion margins, combined with the limited spatial resolution provided by MRI. Automated segmentation techniques could assist in this task, but these methods usually have free parameters that need to be optimized with respect to the dataset being analyzed. Moreover, verification of results achieved by automated segmentation is challenging in itself, given the difficulty in establishing a ‘ground truth’ for segmentation stemming from the absence of accepted reference standards for such clinical imaging data&amp;nbsp;[26]. For these reasons, we aim to pursue a texture analytic strategy that characterizes the lesion enhancement pattern without segmenting the lesion . Specifically, every annotated ROI (on all post-contrast images) is subjected to several thresholds and each such threshold is used to create a binary image. Strictly speaking, this may be considered as a special case of segmentation. However, it is accomplished as part of the texture feature extraction process itself without requiring any additional human or algorithmic input. The goal is then to quantify the morphological changes observed in the ROI as a function of the gray-level threshold. Extraction of such spatio-temporal features is achieved in this study using two approaches: (1) topological texture features derived from Minkowski Functionals and (2) thresholded GLCM, i.e. GLCM features computed from such binary images as a function of the gray-level threshold. This thresholding suppresses the effects of healthy surrounding tissue, thereby removing the need for precise segmentation of the lesion. We investigate the classification performance of such spatio-temporal texture features when extracted from unsegmented lesions, and compare this to conventional GLCM features extracted from segmented lesions. In this work, texture analysis using Minkowski Functionals and GLCM is performed on all five post-contrast images of a dynamic breast MRI exam and the extracted features are combined to form lesion characterizing high-dimensional feature vectors which are used for lesion classification. For the machine learning task, support vector regression (SVR) is used&amp;nbsp;[27]; SVR extends the use of support vector machines to regression analysis and is used in this study as a function approximator that predicts the class label of texture feature vectors extracted from lesions of unknown character. While support vector machines can themselves be used in a classification task&amp;nbsp;[28], they provide binary outputs for class predictions. SVR provides fuzzy class labels that can subsequently be used to generate ROC curves, which is the accepted metric of classification performance in previous studies. These processing steps were used to optimize the free parameters of texture features derived from Minkowski Functionals and GLCM, study the effects of segmentation on the classification performance of such texture features, and compare their performance to that achieved by conventionally used GLCM-derived texture features, as discussed in the following sections.    2 Data  Sixty lesions were identified from a representative set of DCE-MRI exams from 54 female patients by two experienced radiologists who came to a consensus on evaluation of clinical findings. The mean patient age was 52 with a standard deviation of 12 and a range of 27–78. In all cases, histo-pathologically confirmed diagnosis from needle aspiration/excision biopsy was available prior to this study; 32 of the lesions were diagnosed as benign and the remaining 28 as malignant. Mean lesion diameter was 1.05&amp;nbsp;cm (standard deviation of 0.73&amp;nbsp;cm). An example of such a lesion is showed in Fig.&amp;nbsp;. The histological distribution of the 32 benign lesions is as follows: 3 fibroadenoma, 10 fibrocystic change, 5 fibrolipomatous change, 7 adenosis, 1 papilloma and 6 non-typical benign disease. The histological distribution of the 28 malignant lesions is as follows: 18 invasive ductal carcinoma, 5 invasive lobular carcinoma, 3 ductal carcinoma in situ and 2 non-typical malignant disease. Patients were scanned in the prone position using a 1.5T MR system (Magnetom Vision, Siemens, Erlangen, Germany) with a dedicated surface coil to enable simultaneous imaging of both breasts. Images were acquired in the axial slice orientation using a T1-weighted 3D spoiled gradient echo sequence with the following imaging parameters; echo repetition time (TR) = 9.1&amp;nbsp;ms, echo time (TE) = 4.76&amp;nbsp;ms and flip angle (FA) = 25. Acquisition of the pre-contrast series was followed by subsequent administration of 0.1&amp;nbsp;mmol/kg body weight of paramagnetic contrast agent (gadopentate dimeglumine, Magnevist, Schering, Berlin, Germany). Five post-contrast series were then acquired, each with a measurement time of 83&amp;nbsp;s, and at intervals of 110&amp;nbsp;s. All MR datasets were acquired, based on routine clinical indication only, with informed consent from the patients. Purely retrospective use of strictly anonymized MR data was performed according to the IRB guidelines of Ludwig Maximilians University, Munich, Germany. In the collection of patient data used in this study, images in the dynamic series were acquired with two different settings of spatial parameters; for 19 patients, the images were acquired as 32 slices per series with a  matrix, &amp;nbsp;mm in-plane resolution and 4&amp;nbsp;mm slice thickness, while in other cases, the same images were acquired as 64 slices per series with a  matrix, &amp;nbsp;mm and &amp;nbsp;mm slice thickness. To maintain uniform image data for texture analysis, the images acquired with a  matrix were reduced to a  matrix through bilinear interpolation.    3 Methods  With the exception of two patients, where two separate lesions were chosen for analysis, one primary lesion was selected from all patients for analysis. In four cases, these lesions were captured with two non-overlapping ROIs; a single encapsulating ROI was used to capture the lesion in the rest. Each identified lesion was extracted as a 2D square region of interest (ROI) of dimensions  pixels on the central slice of the lesion from the pre-contrast and all post-contrast images of the T1 dynamic series. This choice of ROI size was utilized by the radiologists to reduce the amount of healthy tissue surrounding the lesion captured in the ROI. The ROI annotations were made on difference images created by subtracting the fourth post-contrast image with the pre-contrast image (acquired as part of the imaging protocol); the ROI was subsequently translated to all five post-contrast images. The annotated lesions were then segmented to ensure that the surrounding healthy tissue did not adversely affect subsequent texture analysis, specifically that involving GLCM. A fuzzy C-Means (FCM) approach previously proposed for lesion segmentation in DCE-MRI was used to accomplish this task&amp;nbsp;[29]. The FCM algorithm is an unsupervised learning technique that creates fuzzy clustering assignments to separate an input set of data points into a specified number of clusters. In the lesion segmentation problem, the FCM algorithm was used to evaluate fuzzy cluster assignments for each pixel time series (from five post-contrast images of the lesion) in the 2D square ROI to partition them into two clusters i.e. lesion and healthy tissue. A detailed description of the FCM application to lesion segmentation can be found in [29]. In this study, the FCM approach failed to segment two lesions; these were manually segmented by consensus of the two expert radiologists. Lesions were enhanced on each post-contrast ROI () by subtracting and dividing the th post-contrast ROI , with the corresponding ROI annotated on the pre-contrast lesion . This step effectively suppresses the healthy tissue that surrounded the lesion in the ROI but can be problematic if patient motion during the acquisition results in improper registration between the various post- and pre-contrast images. Datasets used in this study had only negligible motion artifacts over time and thus, compensatory image registration steps were not required. were computed by first binarizing each ROI through the application of several thresholds between its minimum and maximum intensity limits&amp;nbsp;[ 22]. The number of thresholds applied was a free parameter; a suitable choice for this parameter was investigated here. Once the binary images were obtained, three Minkowski Functionals, i.e. area, perimeter and Euler characteristic, were computed from each ROI as follows:                                      where   is the total number of white pixels,   the total number of edges and   is the number of vertices. The area feature recorded the area of the white pixel regions in the binary image, the perimeter measure calculated the length of the boundary of white pixel areas and the Euler characteristic was a measure of connectivity between the white pixel regions. Once computed for every binary image derived from a specific ROI, these values were stored in three high-dimensional vectors corresponding to each Minkowski Functional. Such computations were performed for the same ROI on all five post-contrast images of the lesion and then combined. The dimension of such texture feature vectors was given by  , where   is the number of thresholds used to binarize each ROI and   is the number of post-contrast images (5 in this study). were extracted from the lesion ROIs as described in [30, 31]. An inter-pixel distance of  was used in generating the GLCMs. On each ROI, GLCMs were generated in the four principal directions and then summed up element-wise resulting in one non-directional GLCM from which 11 Haralick features were computed. While 14 such texture features were described by Haralick et al. [30], features f3, f12 and f14 were undefined owing to the small lesion size in the dataset and were omitted from the analysis. Each texture feature was computed on every post-contrast image and then combined into a texture feature vector; 11 such texture feature vectors with a dimension of 5 were computed in this manner for every individual lesion ROI. Prior to GLCM texture analysis, the ROIs were re-binned to 32 gray-level histogram bins between the minimum and maximum intensity limits following lesion enhancement. The minimum and maximum intensity limits were defined ‘globally’, i.e. from all lesion ROIs, within each post-contrast set. The choice of 32 bins between ‘globally’ chosen intensity limits was recommended by previous work&amp;nbsp;[9]. features were computed in this study using thresholded binarization as an initial processing step, similar to the computation of Minkowski Functionals. Each ROI was first binarized through the application of several thresholds between its minimum and maximum intensity limits. Conventional GLCM features were then extracted from each of the binary images created. Such computations were performed for the same ROI on all five post-contrast images of the lesion and then combined. The dimension of such texture feature vectors was given by , where  is the number of thresholds used to binarize each ROI and  was the number of post-contrast images (5 in this study). The number of thresholds applied was a free parameter; a suitable choice for this parameter was guided by experiments performed with features derived from Minkowski Functionals. Re-binning, normally a pre-processing step for the extraction of conventional GLCM features described above, was not required here since GLCM features were extracted from binary images. Feature selection involves identifying a subset of features from the input feature space that makes the most relevant contribution to separating the two different classes of data points in the machine learning step. This study used mutual information analysis, which measures the information content of each feature with regard to the decision task to be performed&amp;nbsp;[32]. In this study, mutual information was used to identify a subset of features for each of the high-dimensional texture feature vectors (extracted from all five post-contrast images) that best contributed to lesion character classification. Mutual information (MI) is a measure of general independence between random variables&amp;nbsp;[ 33]. For two random variables   and  , the MI is defined as:       where entropy   measures the uncertainty associated with a random variable. The MI   estimates how the uncertainty of   is reduced when   has been observed. If   and   are independent, their MI is zero. For the   texture feature vectors of dimension  , the MI between the each dimension of the texture feature vectors  , and the corresponding class labels  , was calculated by approximating the probability density function of each variable using histograms  :       Here, the number of classes   and the number of histogram bins for the texture features   was determined adaptively according to       where   is the estimated kurtosis and  , the number of ROIs in the data set&amp;nbsp;[ 32]. The extraction of texture features and subsequent feature selection was followed by a supervised learning step where the lesion patterns were classified as benign or malignant. In this work, support vector regression with a radial basis function kernel was used for the machine learning task&amp;nbsp;[27]. Here, SVR treats the texture features as independent variables and their labels as the dependent variable and acts as a function approximator; this function is then used in conjunction with the texture features of the test data points to predict their labels. The SVR implementation was taken from the libSVM library&amp;nbsp;[34]. In this study, 70&amp;nbsp;% of the data was used for the training phase while the remaining 30&amp;nbsp;% served as an independent test set. The training data were sub-sampled from the complete dataset in such a manner that atleast 40&amp;nbsp;% of each class (benign and malignant) were represented. Special care was taken to ensure that lesion ROIs extracted from the same patient was used either as training or as test data to prevent any potential for biased training. To ensure the integrity of the independent test set, global intensity limits for pre-processing (for GLCM texture features) were determined using lesion ROIs from the training data alone. The best features of the texture feature vectors were selected by evaluating the mutual information criteria of the training data alone; this ensured that label information for the test data was not used prior to the classification task. In the training phase, models are created from labeled data by employing a random sub-sampling cross-validation strategy where the training set is further split into 70&amp;nbsp;% training samples and 30&amp;nbsp;% validations samples; the purpose of the training was to determine the optimal classifier parameters, i.e. those that best capture the boundaries between the two classes of lesion patterns. The free parameters for the classifier used in this study were the cost parameter for SVR and the shape parameter of the radial basis function kernel. A wide range of values were investigated for these free parameters; for the cost parameter, the values were in the range of 0.001–20 while for the shape parameter, the values were in the range of 0.001–1. The specific value chosen for each of these parameters was determined by optimization through random sub-sampling cross-validation. During each run of the cross-validation process, the optimum value for the free parameter was chosen from the specified range to yield the best separation between benign and malignant classes of the training data alone. Then, during the testing phase, the optimized classifier predicted the label (benign or malignant) of lesion ROIs in the independent test dataset; an ROC curve was generated and used to compute the area under the ROC curve (AUC) which served as a measure of classifier performance. This process was repeated 100 times resulting in an AUC distribution for each feature set. A Wilcoxon signed-rank test was used to compare two AUC distributions corresponding to different texture features. Significance thresholds were adjusted for multiple comparisons using the Holm–Bonferroni correction to achieve an overall type I error rate (significance level) less than  (where )&amp;nbsp;[35, 36]. Texture, classifier and statistical analysis were implemented using Matlab 2008b (The MathWorks, Natick, MA).    4 Results  Figure&amp;nbsp;  shows the classification performance obtained for the three Minkowski Functionals when extracted with different number of binarizing thresholds. The best classification performance was observed when 20 such thresholds were used to binarize the ROI of the lesion (on each post-contrast image). The classification performance did not improve when more thresholds were used; all other results reported in this study used 20 thresholds when computing topological texture features derived from Minkowski Functionals from each post-contrast image. The overall dimension of each Minkowski Functional feature vector was 100. Figure&amp;nbsp; shows a comparison in classifier performance when Minkowski Functional feature vectors were extracted from segmented and unsegmented lesions. The best classification performance was observed with area () and perimeter () when the feature vectors were extracted from unsegmented lesions. Such features significantly outperformed their counterparts extracted from segmented lesions (\(p&amp;lt;0.001\)). The Minkowski Functional Euler Characteristic, however, improved in classification performance when extracted from segmented lesions but failed to achieve comparable performance in distinguishing between benign and malignant lesions. Figure&amp;nbsp;  shows a comparison in classifier performance when thresholded GLCM feature vectors were extracted from segmented and unsegmented lesions. Twenty thresholds were used to compute thresholded GLCM features from each post-contrast image and the overall dimension of each thresholded GLCM feature vector was 100. The best classification performance was observed with f8 ( ) and f9 ( ) when the feature vectors were extracted from unsegmented lesions. Such features significantly outperformed their counterparts extracted from segmented lesions ( \(p&amp;lt;0.001\)). In all, 6 of 11 features showed significantly improved performance when extracted from unsegmented lesions ( \(p&amp;lt;0.05\)). Only two features, f5 and f6, significantly improved in performance when extracted from segmented lesions but they still failed to achieve comparable performance in distinguishing between benign and malignant lesions. Figure&amp;nbsp;  shows a comparison of classification performance between the best Minkowski Functional and thresholded GLCM feature vectors extracted from  , and conventional GLCM-derived features extracted from   lesions. As seen here, comparable classification performance is achieved by Minkowski Functionals: area and perimeter, thresholded GLCM feature vectors [f8 (sum entropy) and f9 (entropy)], and conventional GLCM feature vectors [f4 (sum of squares: variance) and f6 (sum average)].
Springer.tar//Springer//Springer\10.1007-s00138-012-0457-x.xml:Automated morphological classification of lung cancer subtypes using H&amp;amp;E tissue images:Morphological classification Squamous carcinoma Adenocarcinoma Computer vision Tissue microarray H&amp;amp;E:  1 Introduction  Lung cancer (LC) is the leading cause of cancer-related death worldwide and accounts for over 1 million deaths annually [7]. The majority of patients with NSCLC present with locally advanced or metastatic disease for which systemic treatment with chemotherapy is the standard of care. Those patients with earlier stage disease (I and II predominantly) are eligible for surgery with curative intent. NSCLC is a highly drug-resistant cancer and response rates are poor, particularly in the second line setting where response rate is \(&amp;lt;\)10&amp;nbsp;%. LC is classified on the basis of histology into two main subtypes. These are non-small cell lung cancer (NSCLC), which represents around 80–90&amp;nbsp;%, and small cell lung cancer (SCLC). For NSCLC, 5-year survival is around 5–10&amp;nbsp;% and has not changed for over two decades. NSCLC is subdivided into squamous carcinoma (SC) and non-squamous histological subtypes, and the non-squamous includes adenocarcinoma (AC) and undifferentiated carcinoma. AC accounts for about 40&amp;nbsp;% of all lung cancers, whereas SC accounts for 25–30&amp;nbsp;% of all lung cancers [3]. Overall, AC and SC account for 65– 70&amp;nbsp;% of all lung cancer patients. An important development in the treatment of metastatic NSCLC has been the tailoring of therapy for patients with NSCLC on the basis of histology. In the recent JMRB phase III clinical trial [7], pemetrexed with cisplatin has been shown to improve survival in patients with AC, but has adverse effect on SC patients. SC tumours are contraindicated for treatment with antiangiogenesis agents where tumour-related bleeding can lead to mortality, and FDA has restricted the use of pemetrexed to non-SC NSCLC patients only. In contrast, SC patients fared better with the gemcitabine combination. Separation of efficacy based on histology has also been shown for bevacizumab in the ECOG 1499 study [18], in which patients with AC only benefited from the combination with paclitaxel–carboplatin. Conversely, early data on IGF inhibitors suggests that SC are more susceptible compared with AC. Accordingly, optimizing therapy for NSCLC in the first-line setting now warrants robust, rapid and economically efficient approaches for classification of adenocarcinoma and squamous cell carcinoma types for therapeutic purposes. Hence, a pathology diagnosis of “non-specified NSCLC” is no longer routinely acceptable, and a robust and efficient approach for classification of AC and SC histotypes is needed for optimizing therapy. However, due to complex tissue patterns, to the authors’ best knowledge, there is no automated method in classification of AC and SC based on morphological tissue patterns of hematoxylin and eosin (H&amp;amp;E) staining images. The World Health Organization classification of lung tumours is based on the morphological appearances in resected specimens. However, the principal difficulty in applying this in the respiratory system is that unlike other organ systems, only a small percentage of lung tumours are resected, and even when very experienced pulmonary pathologists examine small biopsy specimens against agreed criteria for squamous or glandular differentiation, there are still insufficient features present to classify the subtypes accurately in around 50&amp;nbsp;% of cases [8, 21]. Recent studies indicate that NSCLC histology itself may direct therapeutic decision making [7]. The general practice is for pathologists to look at routine low-cost H&amp;amp;E slides and search for visible tissue structures, involving musin and glandular architecture of adenocarcinoma and solid sheet structure of squamous carcinoma. However, if the visible patterns are confusing, further special chemical tests like pas-D or mucicarmine are required. After applying the special chemical tests, if the visible patterns are still not clear to the human eyes, expensive immunohistochemistry is applied. Ullmann et al. [20] presented an approach to classify AC and SC using a number of biomarker expression based on immunohistochemistry; the tissue expression levels of 86 different proteins were manually scored by pathologists and analyzed using hierarchical clustering and principal component analysis to investigate protein expression profiles in the two variants of NSCLCs. Although Ullmann’s approach showed that the two lung carcinoma subtypes can be classified using multiple antibody analysis, the method is time-consuming and very expensive. While most studies have focussed on histotype-specific proteins and their detection using immunohistochemistry, we explored the possibility of an automated method to deal with challenging H&amp;amp;E tissue patterns. A tremendous number of studies have been conducted on the association of biomarkers with cancers (a broad review on recent developments using TMA technology can be referred to [5]). However, due to complex tissue patterns and variations on the tissue architecture, color, shape and size, automated analysis using routine H&amp;amp;E tissue samples alone is still an unexplored field. In this paper, a robust computer vision approach to automatically classify AC and SC in real time using low-cost H&amp;amp;E tissue images alone is introduced. The presented method includes a feature enhancement approach designed for H&amp;amp;E colour space, a tissue morphological pattern extraction function and a machine learning algorithm. The proposed technique is fully automated, computationally efficient and has been demonstrated to robustly achieve high classification accuracy in our experiments. The outline of this paper is as follows. Section&amp;nbsp;2 describes the materials in this study, and Sect.&amp;nbsp;3 presents the proposed classification method, including a tissue image pattern enhancing approach, a feature extraction model and machine learning algorithms. Section 4 shows the experimental results on two types of data set for evaluation, including a tissue microarray data set and a full-face tissue section data set. Section 5 concludes the paper.    2 Materials  A total of 114, 5&amp;nbsp;m tissue sections were taken from paraffin-embedded samples and stained with H&amp;amp;E. Regions of interest of tissue sections were defined by experienced pathologists, and three tissue microarrays (TMA) were generated using an MTA1 tissue arrayer (Beechers Instruments) with a core size of 0.6&amp;nbsp;mm in diameter. On one array, each primary tumor subject was represented by four cores; on the other two, each primary tumor subject was represented by three cores. Cores that have fallen out or contain less than 10&amp;nbsp;% were excluded as missing cores; in total, there were 97 AC and 272 SC tumour cores of the three TMAs. In evaluation, two types of tissue samples were collected (see Table&amp;nbsp; 1). The first data set contains 369 tissue samples (97 AC and 272 SC) from the three TMAs. These samples were obviously small, round and consistent in size and shape. To further evaluate the robustness of the proposed approach, 284 randomly selected tissue samples (191 AC and 93 SC) with various sizes and shapes were collected from the tumour regions of original tissue sections. The tumour regions were first delineated by an experienced histopathologist, within which regions were randomly selected for measurement. In comparison with the tissue microarray data, the full-face tissue section data provide larger areas, containing more tumour tissues. Some sample images from TMAs and tissue sections are displayed in Fig.&amp;nbsp; . All the TMA slides and tissue sections were scanned using Aperio Scanscope CS2 (Aperio Technologies Inc. San Diego USA), at   objective magnification.    3 Methods  The proposed method includes: (1) an image processing method (cw-HE) for generating distinctive hematoxylin and eosin staining patterns, reducing noisy information and producing discriminative image features; (2) a feature extraction model to produce 37 densitometric and Haralick’s [12] features from the H&amp;amp;E tissue sample images; and (3) a machine learning algorithm (cw-Boost) for pattern discovery. The three major contributions of cw-HE are to produce distinctive H&amp;amp;E staining patterns, reduce noise and generate discriminative image data (see Fig.&amp;nbsp; ). In our initial experiments, the machine learning algorithms do not produce good classification models based on the raw H&amp;amp;E image features (accuracies of all trained models are below 90&amp;nbsp;%; row one of Table  3 shows the best classification model that achieves 86.18&amp;nbsp;% accuracy for the TMA data set, and Table&amp;nbsp;  4 shows the best classification model that achieves 89.83&amp;nbsp;% accuracy for the tissue section data set). We observe that at low resolution, the samples appear all red and pink and not easy to differentiate by human eye. Therefore, an image feature enhancement method, named cw-HE, is specifically designed for H&amp;amp;E images. In H&amp;amp;E staining, hematoxylin induces blue staining of nuclei, and eosin induces red/pink staining of cytoplasm. Our design concept is to enlarge the difference between the nuclear and cytoplasmic expression in color space and therefore further produce more distinctive nuclear architecture patterns, which is also an important information in differentiating AC from SC for pathologists. The H&amp;amp;E staining patterns can be greatly reinforced by stretching the histogram over the available dynamic range separately in the red channel and in the blue channel in RGB color space (see Fig.  ). To verify the contribution of cw-HE, separate experiments have been conducted utilizing the raw H&amp;amp;E grayscale images and the cw-HE processed images as presented in Tables&amp;nbsp; 3 and  4, showing that cw-HE greatly improves the classification accuracies for all machine learning methods. Moreover, our preliminary exploration showed that applying histogram equalization in RGB color space performed better in separating the nuclei from the cytoplasm than in HSL color space. In Fig.&amp;nbsp; , the tissue image enhanced in RGB space showed better separation between the nuclei and cytoplasm and less nuclear misdetections than the tissue image enhanced in HSL space in evaluation with Otsu clustering&amp;nbsp;[ 15] segmentation technique. cw-HE is described below. Given an H&amp;amp;E image  , two histograms of intensity values   were first calculated separately from the red channel   and from the blue channel   and then transformation applied independently. Regarding the histogram distribution of image intensities as a probability distribution,  , where   is the number of pixels having intensity   is the total number of pixels, the upper limit   and lower limit   of the available dynamic range are defined as the 95th and 5th percentile in the histogram. Given the input histograms  , we obtain their associated dynamic range,  . Next, using the input   and   and valid intensity scale 0– , the outputs   and   are given by:                         The output enhanced H&amp;amp;E tissue image  . A total of 37 image features (Table&amp;nbsp; 2) were extracted from each tissue core image, including 11 Haralick texture features [ 12], 11 mean values of individual Haralick features and 15 densitometric features, utilizing Zeiss KS400 imaging system (Carl Zeiss, Oberkochen, Germany). Haralick features are popularly adopted for texture classification; the calculation of the Haralick features is based on a co-occurrence matrix  , which is used to describe the patterns of neighboring pixels in an image at a given distance,  . The definitions of the Haralick features are described in the “Appendix” section. Densitometric features have been utilized for morphological classification in recent studies [6, 24]. Each feature was normalized to provide a value from 0 to 1 and used as the basis for tissue histology discrimination between NSCLC variants. Importantly, in our preliminary exploration, we found that the blue channel information offers higher discriminative information in classification of AC and SC than the composite grayscale, red or green channel information. As hematoxylin induces the blue staining of nuclei, the blue components of H&amp;amp;E tissue images offer better quality of nuclear architecture information. This is consistent with general medical practice as nuclear architecture information is useful for classification by histopathologists. Hence, the blue channel from each image was used for developing the NSCLC subtype classifier. For building tissue pattern classification models, a low variance error boosting algorithm (cw-Boost) from the author’s previous effort&amp;nbsp;[22] in classification of high-dimensional features, but low numbers of instances such as gene expression data is utilized, in combination with alternative decision tree (ADTree) as the base learner. The algorithm of cw-Boost is described below. Given a training set   with labels  , a base learner I, the number of base models to be built   and an integer   (maximum number of times to perturb data; in experiments we use  &amp;nbsp;=&amp;nbsp;10) produce the cw-AdaBoost classifier   by the following steps.        Create a new set  with instance weight  where           &amp;nbsp;                 for        = 1 to                               set  = 0                   &amp;nbsp;                          perturb  using cw-Resample                   &amp;nbsp;                          build a base model  = I()                   &amp;nbsp;                          increment  by 1                   &amp;nbsp;                                               &amp;nbsp;                          if , go to step 2.2.                   &amp;nbsp;                          if \((E_i &amp;gt; 0.5) \vee (E_i = 0)\), deduct 1 from  and abort loop.                   &amp;nbsp;                                               &amp;nbsp;                          for each , if , then multiply  to                     &amp;nbsp;                          Normalize weights                   &amp;nbsp;                               &amp;nbsp;                      &amp;nbsp;         Given a data set S: a sequence of instances with weights: , we produce a new data set with the following steps.        Generate M random number:           &amp;nbsp;                      &amp;nbsp;                      &amp;nbsp;           set  and           &amp;nbsp;           let           &amp;nbsp;           let           &amp;nbsp;           f \((a&amp;gt;M)\wedge (b&amp;gt;M)\)              &amp;nbsp;                 if       \(g(a)&amp;lt;s(b)\),                       select instance  into the output data set                   &amp;nbsp;                          increment  by 1                   &amp;nbsp;                          go to step 5                   &amp;nbsp;                               &amp;nbsp;                 otherwise,                       increment  by 1                   &amp;nbsp;                          go to step 5.                   &amp;nbsp;                               &amp;nbsp; Create a new set  with instance weight  where for   = 1 to           set  = 0         &amp;nbsp;           perturb  using cw-Resample         &amp;nbsp;           build a base model  = I()         &amp;nbsp;           increment  by 1         &amp;nbsp;                      &amp;nbsp;           if , go to step 2.2.         &amp;nbsp;           if \((E_i &amp;gt; 0.5) \vee (E_i = 0)\), deduct 1 from  and abort loop.         &amp;nbsp;                      &amp;nbsp;           for each , if , then multiply  to           &amp;nbsp;           Normalize weights         &amp;nbsp; set  = 0 perturb  using cw-Resample build a base model  = I() increment  by 1  if , go to step 2.2. if \((E_i &amp;gt; 0.5) \vee (E_i = 0)\), deduct 1 from  and abort loop.  for each , if , then multiply  to Normalize weights  Generate M random number:   set  and let
Springer.tar//Springer//Springer\10.1007-s00138-012-0458-9.xml:Extraction of left ventricle borders with local and global priors from echocardiograms:AdaBoost Level set Active contours Contour extraction Prior Echocardiogram:  1 Introduction  Extraction of the inner and outer cardiac borders (endocardium and epicardium) from echocardiographic images produces crucial information about the cardiac functions and cardiac morphology like the size and thickness of the cardiac wall, the volume of chambers, the ejection fraction (blood pumping capacity), etc. In the current clinical practice, the cardiac borders are generally delineated manually by sonographers [ 15]. However, the manual delineation process is user-dependent, time consuming, ineffective, and results in variations between experts (Fig.&amp;nbsp; ). As a result, automation of the cardiac wall extraction is considered important for increasing the accuracy of measurements and speeding up the cardiac assessment. Conventional contour extraction techniques (such as [14, 18, 20]) cannot be directly employed for the automatic extraction of the cardiac borders because of ultrasound modality related problems. The literature partially addresses these problems by employing different regularization strategies through the incorporation of domain related information into the extraction process in the form of shape priors [5, 7, 26, 31]. The overall results improve significantly for the borders with poor image information. However, the shape prior is sometimes not adequate to recover a shape with very low image gradient magnitude. Chen et al. [6] suggest using intensity profiles of object contours besides the shape prior by training an intensity model from a set of images. A similar intensity and curvature profile information is used by Leventon et al. [17]. Bresson et al. [3] propose a variational model that uses a shape model learned with Principle Component Analysis, image gradient, and region information in its energy functional. Although using such priors are very useful for the extraction of object boundaries, it is difficult to use other image and non-image based features (e.g. motion and distance to a reference point) together as the prior information with the contour extraction methods. The machine learning methods learn different types of features for detecting the target structures. AdaBoost [12] is one of the popular machine learning methods used heavily by the medical imaging community for the detection of anatomical structures. Carneiro et al. [4] detect the fetal anatomies and Feng et al. [11] detect fetal faces from ultrasound images through a probabilistic boosting tree. Georgescu et al. [13] use AdaBoost for extracting the endocardial borders of the left ventricle with structure detection. AdaBoost is also used for locating the vessel boundaries [27] and tracking the cardiac borders [28]. In this paper, we present a novel approach for the automatic extraction of the cardiac borders with AdaBoost and active contours. We first detect the candidate cardiac borders by learning local information with AdaBoost and then extract the final borders by incorporating global shape information. We classify the incorporated knowledge as local and global priors because they include different types of information about the target structure. The global prior refers to the classical shape prior [5, 7] that incorporates the geometric shape requirements into the extraction process. Local priors include locally definable features of the target borders such as image information, time related properties, local geometric shape, distance from a shape reference point, etc. Since there are different types of local features, it is hard to learn and combine them with classical regularization techniques. In this study, different local features are all trained and learned via AdaBoost easily without using any regularization techniques. In the testing phase, score images that include the local information are produced. We employed the global prior incorporation method of [25] in bringing the local and global priors together. Oktay and Akgul [25] incorporates the global prior by re-initializing the best matching expert contour on the score image at regular intervals. Preliminary work on this study was presented in [23, 24]. We extend [23, 24] by utilizing the motion information more efficiently and by providing a more complete&amp;nbsp;system validation. The proposed method is tested on short axis view echocardiograms of end-systole and end-diastole phases to extract endocardium and epicardium. It is hard to evaluate the accuracy of the system and effectiveness of the filters with real echocardiograms due to the lack of proper ground truth. Therefore, experiments on the synthetic images are also performed to compare our method with the methods of the literature and to show the robustness of our method under various controlled noise levels. The rest of this paper is organized as follows. The local priors and the AdaBoost scoring process are described in Sect. 2. Section 3 includes the method for the incorporation of the global prior. The experiments are presented in Sect.&amp;nbsp;4. Finally, we provide concluding remarks in Sect.&amp;nbsp;5.    2 Local priors  Incorporation of local priors is necessary when the target object has different characteristics at different spatiotemporal image locations. We extract these local, spatial, and motion features after converting the echocardiographic images into polar coordinates (Fig.&amp;nbsp; ). After training the extracted features with AdaBoost, each pixel of the target image is given two scores to be used with global priors. The architecture of AdaBoost scoring system is shown in Fig.&amp;nbsp; . We convert the echocardiographic images into polar coordinates [16] using the linear interpolation technique. The left ventricle center is chosen as the center of the polar coordinates. Although the left ventricle center can be estimated automatically as in [30], we use the centers of expert contours during the AdaBoost training and manually extract the centers during the testing. In the echocardiographic images, the left ventricle is usually in the middle of the image and the midpoint of the image is near to the polar center. So, in our system the polar center does not affect the system performance very much. However, for the images in which target organ is not in the center of the image, the performance is dependent on the polar center. The employment of polar coordinates (Fig.&amp;nbsp;) has advantages for the local feature extraction. First, the translational differences between different images are minimized because the left ventricle centers of all images are registered to the origin in the polar coordinates. As a result, the distance from the heart wall to the polar origin becomes a valuable feature that can be used as a local prior. Second, the heart wall orientations are similar (all almost horizontal) in the polar coordinates. This allows us to use a smaller number of filter orientations to capture the contour orientations. In the echocardiograms, the upper and lower parts of the cardiac wall have higher image gradient magnitudes than the side walls because of the signal dropout. In order to address this modality related problem, we divide the polar images into a number of overlapping angle ranges  where  and  is the number of ranges (Fig.&amp;nbsp;b). The features in each  range are extracted and trained separately so that different characteristics of the cardiac borders including the signal dropout rates at specific positions can be handled appropriately. The incorporation of temporal information is crucial for detecting the cardiac borders [1, 21]. The sonographers utilize the temporal information in the manual extraction of the cardiac borders by viewing the echocardiograms as movies [15]. We incorporate the temporal information using local features in the form of 3D Haar-like filters [19], hence our system avoids any explicit spatiotemporal deformable models which are complicated and expensive to minimize. This solution allows us to conveniently use our local prior enforcement method in imposing spatiotemporal constraints. Three types of rectangular Haar-like filter sets  ,  , and   are used for the image feature extraction (Fig.&amp;nbsp; ). For each filter type, the feature value is calculated by subtracting the total intensity value of the darker area from the total intensity value of the lighter area [ 29]. The 2D filter set  extracts the spatial appearance information. The 3D spatiotemporal filter set , which is the stacked version of type , is used to extract motion and appearance information in the time dimension. Similarly, the filters in set  are stacked versions of the type D filters, but while stacking these filters, each filter is shifted depending on the anticipated heart wall motion. The  filters are shifted up (down) for the neighboring frames of the end-systole (end-diastole). For the cardiac images in polar () space, the border positions have the smallest  value in the end-systole and the largest in the end-diastole phase. The borders of the neighboring frames of the end-systole phase have  values which are typically 1 to 3 pixels greater than the  value of the end-systole. In order to register the contour motion of the end-systole, the  filters shift up for each neighboring frame 1, 2, or 3 pixels. Similarly for the end-diastole, the  filters shift down for each neighboring frame 1, 2, or 3 pixels. Since we do not know the exact amount of border motion between the frames, we try different shift amounts in order to capture the cardiac motion. These filters describe motion information better and capture motion patterns effectively. The spatiotemporal filter sets  and  are very useful in addressing the ultrasound noise since they use the motion information of neighboring frames. We use various versions of the filters with different sizes (from the size of  to ), gray level inverses, and rectangular shapes. The filters are efficiently calculated by using the integral images [10, 29]. The calculation of  and  filters with integral volumes are the same as [19]. In order to train the local prior model, we extract a number of features  where  is the feature type. The features are calculated separately for each possible (-) pair where  is the angle range and  is the border type (epicardium or endocardium). The first feature type   is the distance of the pixel   from the center of the left ventricle:       where   returns the   position of pixel   in polar space. The other feature types are all image related:       where   and   are the sets of all   and   filters applied to pixel  , respectively.   filters are different from the sets   and   in that they have to specify shift amounts for the neighboring frames. Since we do not know the amount of shift in the cardiac movies, for a given   filter and pixel  , we measure the responses of all possible shift values and choose   that gives the maximum value. The classical AdaBoost assigns a weight to each feature and sums them linearly which is used in our handling of   and   filter sets. However in our system, not all   filters are assigned weights. Instead, we only assign weights for a special subset of   filters (maximum  ) which makes our approach non-linear. This is a novel method that we use to effectively address the unknown contour movement values of borders between the cardiac frames. Consider a pair   for a pixel   and class   as the classification label of  . We define   as the set of features extracted with Eqs.  1 and  2 for the pixel  . AdaBoost uses weak simple classifiers   and assigns weights   inversely proportional to the accuracy of the classification. Different from the classical AdaBoost, instead of using the discrete class value  , we employ   directly as the score value in our local prior estimations. The final strong score value is built by       where   is the total number of weak classifiers. For training, the polar converted echocardiographic images are divided into   ranges where  is selected empirically. The expert contour positions are used as positive examples and 10&amp;nbsp;% of the other positions in the same angle range are used as negative examples. We train and test only the end-systole and the end-diastole frames in the cardiac cycle for which the expert delineations are available. Each  pair is trained separately because they include different types of local knowledge, i.e., for each  tuple we train a separate classifier . For the scoring process, each pixel of the angle range   is assigned two score values with   and   by Eq.&amp;nbsp; 3. These two scores are used to form two polar images   and  . An example   image is shown in Fig.&amp;nbsp; . These two images are converted back to the rectangular representation which are linearly normalized to the [0–255] range and used by the active contour energy functional that will be defined in Sect.&amp;nbsp;  3.    3 Global priors with level set method  Although the local priors incorporate valuable information into the contour extraction process, we still need to impose more global constraints such as geometric shape. Such information would guide the contours towards better positions where there is no sufficient local data. For example, the sections of the score image in Fig.&amp;nbsp;, which do not carry any local information, can only be extracted with global information. The score images  and  are formed only with the local information from the target images. We use the prior shape incorporation method of [25] which is based on incorporating the shape prior during re-initialization of the level set surface. Note that, at this step our goal is to bring the local and global priors together rather than proposing a new global prior incorporation method. In addition, it is possible to use other contour extraction methods like active shape models [9] and active appearance models [8] with the score images. The global prior incorporation method is based on re-initializing the level set surface formed with the most similar expert contour [22, 25]. We prefer to use this technique because it does not require any training and it is fast. For the cardiac wall extraction, let  and  be two closed curves evolving on the plane  with time  for extracting the endocardium and epicardium, respectively. Consider   as the set of points on   and  . Let   be the position vector and   be a signed distance function defined as:            where   is the shortest Euclidian distance to   from point  x. and   are the normalized rectangular images that include the score of each pixel assigned by AdaBoost. The surface   evolves on image   which is defined as            The 3D surface   evolves under the influence of internal and external energy terms according to the variational level set formulation [ 18]. The incorporation of global information is achieved by stopping the surface deformations and re-initializing the surface under the influence of the shape prior at regular intervals. During the re-initialization, the expert contours are warped onto the evolving contour and the best matching contour is found. The best matching contours for the endocardium and epicardium are found separately. The surface that contains the global shape information is constructed by embedding the best matching warped expert contours (endocardium and epicardium) into the zero level of the surface with Eq.&amp;nbsp; 4. The constructed surface is re-initialized on the image   and thus the global shape information is incorporated by the re-initialization. [ 25] explains the details of the global shape incorporation method.    4 Experimental results  The system is tested and validated using real echocardiographic and synthetic images. Experiments on the echocardiograms are performed to show the effectiveness of the method and its practical applicability. Experiments on synthetic images are performed to demonstrate the robustness of the local priors under varying controlled levels of noise. The proposed method is validated on the echocardiographic images of the left-ventricular short-axis transthoracic views. The data set includes sequences of frames during the cardiac cycle of 20 different people. Each cardiac cycle includes one end-systole frame and one end-diastole frame whose endocardial and epicardial borders are delineated by 4 different experts (Fig.&amp;nbsp;). The frames between the end-systole and end-diastole phases exist in the dataset. However, they are not annotated by the experts in medical practice. There are 20 end-systole and 20 end-diastole frames for 20 different subjects. Each end-systole and end-diastole frame is annotated by 4 different experts which makes a total of 160 annotations for 40 images. Each endocardial and epicardial contour is represented by 100 points. We performed a non-overlapping subset of leave-five-out cross validation experiment for 20 systole and 20 diastole frames separately. In each cross validation sub-experiment, we used 15 cardiac images (each delineated by 4 experts) for training and the remaining 5 cardiac images for testing. The total number of features extracted for each pixel is 1081. We used the expert detected endocardium and epicardium contours in the training set as the global shape prior. We do not have a proper ground truth data available for the validation of the system. Therefore, the delineations of 4 different experts are used for the evaluation of our extraction results. The expert delineations are also compared with each other to obtain the variations between the experts. The average pixel differences are calculated between expert-expert and expert-system contours. Given two contours to be compared,   and  , the Chamfer distance   [ 2] is calculated by       where   is the minimum Euclidean distance between the point   and the contour  . We run our method on 40 echocardiograms in the dataset (20 systole and 20 diastole images). The average of expert-expert differences (mean   standard deviation) are shown in Table  1. The smallest average pixel difference between the experts is 3.10 pixels (Expert 1 and Expert 3) and the largest is 6.72 pixels (Expert 1 and Expert 4). Our system results are between 3.54 and 5.10 pixels for endocardium and epicardium.
Springer.tar//Springer//Springer\10.1007-s00138-012-0459-8.xml:Breast cancer diagnosis from biopsy images with highly reliable random subspace classifier ensembles:Reject option Biopsy image Random subspace ensemble Combined feature Breast cancer diagnosis:  1 Introduction  Breast cancer accounts for nearly 1 in 4 cancers diagnosed in US women, it is also the most common type of cancer in women and the fifth most common cause of cancer death worldwide [1]. There is substantial evidence that there is a worldwide increase in the occurrences of breast cancer, especially in Asia, for example, China, India and Malaysia have recently experienced rapid increase in breast cancer incidence rates [2]. A recent study predicted that the cumulative incidence of breast cancer will increase to at least 2.2 million new cases among women across China over the 20-year period from 2001 to 2021 [3]. The most noticeable symptom of breast cancer is typically a lump or a tumor that feels different from the rest of the breast tissue. However, it is not easy to distinguish a malignant tumor from a benign one because there are structural similarities between the two. To accurately identify the structural differences, physicians have to cautiously study a patient’s clinical history and make various medical examinations supported by imaging using mammography or ultrasound. However, the precise diagnosis of a breast tumor can only be obtained through some form of biopsy where by a small sample of cells or tissue is removed for examination. Typical biopsy processes for breast cancer analysis include Fine-Needle Aspiration (FNA), core needle, and excisional biopsy [4]. Among these FNA is the most convenient because it involves the use of very small needles (smaller than those used for blood tests) [5]. This deterministic diagnosis is vital as the potency of the cytotoxic drugs administered during treatment can be life threatening. As there is always a subjective element related to the pathological examination of a biopsy, an automated technique will provide valuable assistance for physicians. Recent years have witnessed a large increase in research related to computer assisted breast cancer diagnosis. A large focus with respect to biopsy image analysis has been on automated cancer type classification. Many recent studies have revealed that biopsy images can be properly classified, without requiring perfect segmentation if suitable image feature descriptions are chosen [6–8]. Tabesh et al. aggregated color, texture, and morphometric cues at the global and histological object levels for classification, achieving 96.7&amp;nbsp;% classification accuracy in classifying tumor and non-tumor images [9]. The wavelet package transform coupled with local binary patterns were used for meningioma subtype classification in [10]. This research, and similar work, demonstrated that by combining different image description features it is possible to improve medical image classification performance. A great number of machine learning methods have been proposed to design accurate classification systems for various medical images [11]. Among them, ensemble learning has attracted much attention due to the good performance from many applications in medicine and biology [12]. Ensemble learning is concerned with mechanisms to combine the results of a number of weak learning systems. A weak learner is defined to be a classifier which is only slightly correlated with the true classification, it can label examples better than random guessing. In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification [13]. In the case of ensemble classification, ensemble learning is concerned with the integration of the results of a number of classifiers (often called as ‘base classifiers’) [14] to develop a strong classifier with good generalization performance, therefore, ‘base classifiers’ are also referred as ‘weak classifiers’. Among the representatives of ensemble learning, the Random Subspace (RS) method [15] is often quoted as an efficient way of combining the results of a number of classifiers. A recent application of RS for functional Magnetic Resonance Imaging (fMRI) classification has shown promising results [16]; here RS outperformed single classifiers as well as some of the most widely used alternative classifier ensemble techniques such as bagging, AdaBoost, random forests and rotation forests. The same outcome has also been reported in the context of RS ensemble based gene expression classification [17]. RS divides the input feature space into subspaces; each subspaces is formed by randomly picking features from the entire space, features may be repeated across subspaces. In previous studies of medical images classification, accuracy was the only objective; the aim was to produce a classifier that featured the smallest error rate possible. In many applications, however, it is more important to address the reliability issue in classifier design by introducing a reject option which allowed for an expression of doubt. The objective of the reject option is thus to improve classification reliability by leaving the classification of “difficult” cases to human experts. Since the consequences of misclassification may often be severe when considering medical image classification, clinical expertise is desirable so as to exert control over the accuracy of the classifier in order to make reliable determinations. Classification with a rejection option has been a topic of interest in pattern recognition. Multi-stage classifiers are ensembles where individual classifiers have a reject option [18]. Cascading [19] is a scheme to support multi-stage classification. At the first stage of a cascading system, the system constructs a simple rule using a properly generalized classifier; based on its confidence criterion, it is likely that the rule will not cover some part of the space with sufficient confidence. Therefore, at the next stage, cascading builds a more complex rule to focus on those uncovered patterns. Eventually there will remain few patterns which are not covered by any of the prior rules, these patterns can then be dealt with using an instance-based nonparametric technique which is good at unrelated, singular points [20]. Many cascading multi-stage classifier architectures have been proposed [21–24] and plenty of promising results have been achieved in medical and biological classification applications, such as microarray data classification [25] and gene expression data classification [26]. In this paper, we propose and evaluate a novel cascade scheme, comprised of two random subspace ensembles, to be applied to microscopic biopsy image classification with a reject option. The first stage of our cascade scheme consists of an ensemble of SVMs with reject option to classify patterns with high level of confidence. The more complex and slower second stage, which is an ensemble of MLPs, deals with the rejected patterns from stage 1, and is designed to make further classifications or rejections. Compared with some earlier cascading classifier paradigms, our proposed system is composed of two different ensembles. In the first stage, a one-vs-all SVM ensemble is employed to classify “straight forward” samples (thus obtaining high accuracy) and reject those which are less straight forward or ambiguous. Only samples for which the ensemble’s confidence score, in terms of consensus degree, is greater than a certain threshold will be classified. The second stage consists of a random subspace ensemble of MLPs which operates using majority voting, any samples that have a low consensus degree will be rejected for further consideration by human experts. It is suggested that classification with the proposed cascaded ensembles will provide an efficient means to simultaneously reduce the error rate and enhance the reliability by controlling the accuracy-rejection trade-off. The rest of this paper is organized as follows: the breast cancer biopsy image dataset used in our work and the image feature extraction methods are introduced in Sect.&amp;nbsp;2. In Sect.&amp;nbsp; 3, we described and theoretically analyzed the proposed two-stage ensemble cascading system in detail. In Sect.&amp;nbsp; 4, the experimental results are given based on the adopted benchmark image dataset. We compared the proposed cascading system with its component classifiers as well as some widely used aggregation techniques, such as bagging and AdaBoost. The paper ends with some conclusions in Sect.&amp;nbsp;5.    2 Biopsy images and feature descriptions  In this section we will first introduce, in Sect.&amp;nbsp;2.1, the benchmark breast cancer biopsy image dataset. The proposed image feature extraction methods are then introduced in Sect.&amp;nbsp;2.2. The choice of features for describing the initial biopsy images depends on the nature of the input images. For biopsy image classification, calculating global features to estimate the global appearance of the images is an effective approach. In this work, we propose to use three image descriptors for biopsy image feature extraction: (1) local binary patterns (Sect.&amp;nbsp;2.2.1), (2) gray level co-occurrence matrixes (Sect.&amp;nbsp; 2.2.2) and (3) the curvelet transform (Sect.&amp;nbsp;2.2.3). With respect to the work described in this paper a breast cancer benchmark biopsy images dataset from the Israel Institute of Technology 1 was used. The image set consists of 361 samples, of which 119 were classified by a pathologist as normal tissue, 102 as carcinoma in situ, and 140 as invasive ductal or lobular carcinoma. The samples were generated from breast tissue biopsy slides, stained with hematoxylin and eosin. They were photographed using a Nikon Coolpix  995 attached to a Nikon Eclipse  E600 at magnification of   to produce images with resolution of about   per pixel. No calibration was made, and the camera was set to automatic exposure. The images were cropped to a region of interest of   pixels and compressed using the lossy JPEG compression. The resulting images were again inspected by a pathologist to ensure that their quality was sufficient for diagnosis. Figure   presents three sample images of healthy tissue, tumor in situ and invasive carcinoma. Shape feature and texture feature are critical factors for distinguishing one image from another. For the biopsy image discrimination, shapes and textures are also quite effective. As we can see from Fig.&amp;nbsp;, three kinds of biopsy images have visible differences in cell externality and texture distribution. Thus, we use Local Binary Patterns (LBPs) for extracting local textural features, Gray Level Co-occurrence Matrix (GLCM) statistics for representing global textures and the Curvelet Transform for shape description. Local Binary Patterns (LBPs) were first introduced as a texture descriptor for summarizing local gray-level structures [27, 28], LBPs are generated by taking a local neighborhood around each pixel into account, thresholding the pixels of the neighborhood at the value of the central pixel and then using the resulting binary-valued image patch as a local image descriptor. In other words, a binary code of 0 or 1 is assigned to each neighborhoods pixel. The binary code of each pixel in the case of a  neighborhoods would form an 8 bits code. In this manner, a single scan through an image can generate LBP codes for each pixel. Formally, the LBP operator takes the form            where   is the gray value of the central pixel,   is the value of its neighbors,   is the total number of neighbors and   is the radius of the neighborhood. A useful extension to the original LBP operator is the so-called uniform patterns [27]. An LBP is “uniform” if it contains at most two bitwise transitions from 0 to 1 or vice versa when the binary string is considered circular. For example, 11100001 (with two transitions) is a uniform pattern, whereas 11110101 (with four transitions) is a non-uniform pattern. The uniform LBP describes those structures which contain at most two bitwise (0 to 1 or 1 to 0) transitions. Uniformity represents important structural features such as edges, spots and corners. Ojala et al. [27] observed that although only 58 of the 256 eight-bit patterns are uniform, nearly 90&amp;nbsp;% of all observed image neighborhoods are uniform. We use the notation  for the uniform LBP operator, meaning a neighborhood of  sampling points on a circle of radius . The superscript  stands for using uniform patterns and labeling all remaining patterns with a single label. The number of labels for a neighborhood of 8 pixels is 256 for standard LBP and 59 for . A common practice when applying an LBP coding over an image is to generate a histogram of the labels, where a 256-bin histogram represents the texture description of the image and each bin can be regarded as a micro-pattern. The distribution of these patterns represents the whole structure of the texture. The number of patterns in an LBP histogram can be reduced by only using uniform patterns without losing much information. As noted above, there are 58 different uniform patterns in an 8-bit LBP representation, the remaining patterns can be assigned in one non-uniform binary number, thus representing the texture structure with a 59-bin histogram instead of using 256 bins. LBP has been shown to be an efficient image texture descriptor. Recently, a complete modeling of the local binary pattern operator was proposed and the associated Complete LBP (CLBP) scheme developed for texture classification [ 28]. Different to traditional LBP, in CLBP, a local region is represented by its center pixel and a Local Difference Sign-Magnitude Transform (LDSMT). With a global thresholding, the center pixel is coded by a binary code and the binary map is called  . Two other complementary components are also obtained by LDSMT: the difference signs and the difference magnitudes, two operators   and   are used to code them. The framework of CLBP is presented in Fig.  . The CLBP could achieve much better rotation invariant texture classification results than conventional LBP based schemes. We briefly review three operators in CLBP here, namely,   and  . Given a central pixel   and its   neighbors  , the difference between   and   can be calculated as  . The local difference vector   describes the image local structure at   can be further decomposed into two components:       where  , when  , otherwise,  .   is the magnitude of  . Equation&amp;nbsp;( 2) is called the local difference sign-magnitude transform (LDSMT). The  operator is defined as the original LBP operator in Eq. (1). The   operator is defined as:            where   is a threshold set as the mean value of   from the whole image. The   operator is coded as:       where   is defined in Eq. ( 3) and   is a threshold set as the average gray level of the whole image. In this work, we use the 3D joint histogram of these three operators to generate textural features of breast cancer biopsy images, according to [28], the joint combination of the three components gives better classification than conventional LBP and provides a smaller feature dimension. Global texture distribution is one of the important characteristics used in identifying objects or regions of interest in an image. The co-occurrence probabilities provide a second-order method for generating texture features [ 29]. The basis for features used here is the gray level co-occurrence matrix, the matrix is square with dimension  , where   is the number of gray levels in the image. Element   of the matrix is generated by counting the number of times a pixel with value   is adjacent to a pixel with value   and then dividing the entire matrix by the total number of such comparisons made. Each entry is therefore considered to be the probability that a pixel with value   will be found adjacent to a pixel of value   [ 30], the matrix can be seen in Eq.&amp;nbsp;( 5). With respect to the work described in this paper, a total of 22 features were extracted from gray level co-occurrence matrices in our work, these are listed in Table  1. Each of these statistics has a qualitative meaning with respect to the structure within the GLCM, for example, dissimilarity and contrast measure the degree of texture smoothness, uniformity and entropy reflect the degree of repetition amongst the gray-level pairs, and correlation describes the correlation between the gray-level pairs. For details of these statistical features, see [ 29– 32]. The Curvelet transform [33–37] is one of the latest developments in non-adaptive transforms. Compared to the wavelet transform, the curvelet transform provides a more sparse representation of an image, with improved directional elements and better ability to represent edges and other singularities along curves. Sparse representation usually offers better performance with its capacity for efficient signal modeling. So far, successful applications of the curvelet transform have been found in many medical and biological image analysis tasks, including digital mammogram analysis [38] and phenotype recognition [39]. In the curvelet transform, fine-scale basis functions are long ridges; the shape of the basis functions at scale   is   by   so the fine-scale bases are skinny ridges with a precisely determined orientation. The curvelet coefficients can be expressed by:       where   denotes the curvelet function, and   and   are the variables of scale, orientation, and position, respectively. In the last few years, several discrete curvelet transforms have been proposed. The most influential approach is based on the Fast Fourier Transform (FFT) [ 36]. In the frequency domain, the curvelet transform can be implemented with   by means of the window function  . Defining a radial window   and an angular window   as follows:                         where   is a frequency domain variable and   and   are polar coordinates within the frequency domain. For each  ,   is defined over the Fourier domain by:       where   denotes the integer part of  . The fastest curvelet transform currently available is curvelets via wrapping [36], which will be used for our work. From the curvelet coefficients, some statistics can be calculated from each of these curvelet sub-bands. In this paper, the mean , the standard deviation  and the entropy  are used as the simple features. If  curvelets are used for the transform,  features  are obtained, where  and  . A  dimensional feature vector can be used to represent each image in the dataset. Each feature extracted from the above three descriptors characterizes individual aspects of image content. The joint exploitation of different image descriptions is often necessary to provide a more comprehensive description in order to produce classifiers with higher accuracy. Using 5 levels of the curvelet transform, 82 sub-bands of curvelet coefficients are computed, therefore, a 246-dimensional curvelet feature vector is generated for each image. With a 64 gray-level quantization, we used 10 different relative interpixel distances to generate 10 different gray level co-occurrence matrices for each image. The 22 statistics listed in Table 1 are computed for each of these 10 gray level co-occurrence matrices, thus, we have a 220-dimensional GLCM feature vector for each image. The CLBP feature vector of each image has a dimension of 200. The three feature vectors are normalized, respectively, into the range of , then concatenated together to produce a 666-dimensional feature vector of each image for classification. One of the difficulties of multiple feature aggregation lies in the high dimensionalities of the feature space. However, by using Random Subspace classifier ensembles (see following section) this problem can be resolved due to its dimension reduction capability. Due to the differences existing in different molecular imaging devices and staining methods, histology images of biopsies may change significantly in colors and intensities. The above explained feature extractors can cope with this situation effectively, since all of them work in the grayscale color space. Before feature extraction, all the biopsy images will be converted from a chromatic color space to a grayscale color space with an intensity interval from 0 to 255. The conversion eliminates the adverse effects from color and intensity variations because the feature extractors work in the same space. Moreover, as the features are extracted from the whole images, the distribution and structures of different patterns such as tissues, ducts, fat and tumors, will be automatically described by the feature extractors, thus, they will not affect the performance of the combined feature.    3 Serial fusion of random subspace ensembles  Although many supervised learning algorithms such as neural networks, the -nearest neighbor algorithm and SVM have been extensively applied to many medical image classification problems, few of them has addressed the issue of classification reliability (the extent that one can rely upon a given prediction). Note that we are interested in the assessment of a classifier’s performance on a single example such as the diagnosis associated with am individual patient. In such cases an overall quality measure of a classifier (e.g., classification accuracy) would not provide the desired information, even where good accuracies are achieved using some state-of-art method. With respect to some real applications, such as medical diagnosis, highly reliable classifiers are required so that a correct therapeutic strategy can be selected. Therefore, it is desirable to have a reject option in order to avoid making a wrong decision when classifier is presented with ambiguous input, i.e., an option to withhold a classifier decision. In this paper a new two-stage classifier for the breast cancer biopsy image classification, consisting of a random subspace ensembles with reject option, is proposed. With respect to the work described in this paper, we adopted the definitions of recognition rate, rejection rate and reliability proposed in [ 40], as presented below, so as to facilitate the performance evaluation of classifiers with a reject option:    Recognition rate (RR)&amp;nbsp;&amp;nbsp;no. of correctly recognized images/no. of testing images   Rejection rate (ReR)&amp;nbsp;&amp;nbsp;no. of rejected images/no. of testing images.   Reliability (RE)&amp;nbsp;&amp;nbsp;(no. of correctly recognized images + no. of rejected images)/no. of testing images.   Error rate (ER)&amp;nbsp;&amp;nbsp;100&amp;nbsp;%  reliability.  From the above we can see that Reliability = Recognition rate&amp;nbsp;+&amp;nbsp;Rejection rate. According to this definition of reliability, high reliability can be achieved with an appropriate trade-off between error rate and rejection rate. Recognition rate (RR)&amp;nbsp;&amp;nbsp;no. of correctly recognized images/no. of testing images Rejection rate (ReR)&amp;nbsp;&amp;nbsp;no. of rejected images/no. of testing images. Reliability (RE)&amp;nbsp;&amp;nbsp;(no. of correctly recognized images + no. of rejected images)/no. of testing images. Error rate (ER)&amp;nbsp;&amp;nbsp;100&amp;nbsp;%  reliability.
Springer.tar//Springer//Springer\10.1007-s00138-012-0460-2.xml:A unifying framework for automatic and semi-automatic segmentation of vertebrae from radiographs using sample-driven active shape models:Segmentation Vertebrae Radiographs Spine Osteoporosis ASM:  1 Introduction  Osteoporosis is a skeletal disease in which the bone mass is reduced and the bone microarchitecture is disrupted, leading to an increased risk of bone fragility and fractures. It has been estimated that, in the industrialized countries, approximately 40&amp;nbsp;% of the postmenopausal women at the age of 60 and up to 70&amp;nbsp;% of the women at the age of 80 have osteoporosis [39]. Prevalent vertebral fractures have been established as a strong risk factor for future fractures [24]. The presence of a single vertebral fracture increases the risk of subsequent fractures—in any bone—fivefold [28]. Consequently, measurements of the vertebral fractures have been adopted in osteoporosis pharmaceutical trials, where the presence of a fracture is used as a patient entry criterion and incident vertebral fractures are the current gold standard in evaluating treatment efficacy. In practice, fractures are identified by trained radiologists or technicians from radiographic images. Each vertebra is manually evaluated using either a semi-quantitative [18] or a fully quantitative [6, 15, 27] method. This process is labor-intensive, particularly for clinical studies involving multiple visits for thousands of patients. All of the currently accepted fracture assessment methods are, either implicitly or explicitly, centered around measurements of the posterior, middle, and anterior vertebral heights. However, the representation of three heights might not capture all osteoporotic vertebral shape variations, such as fractures per se, pre-fractures, and osteophytes. Moreover, this simplified representation does not allow for discrimination between osteoporotic fractures and non-osteoporotic fractures. Expressive, high-dimensional representations of the full vertebral outlines could be essential in the design of novel imaging biomarkers for prognosis, diagnosis, and estimation of treatment efficacy in osteoporosis. Already, promising diagnostic markers based on full vertebral delineations have been proposed by de Bruijne et al. [9] and Roberts et al. [37]. Moreover, such representations can be used to develop novel procedures for fracture assessment which can more accurately characterize the amount of bone loss. Manual delineation of each vertebra is impractical for clinical trials, and consequently, automatized segmentation is necessary. Because of the multi-site nature of clinical trials, and the large number of patients involved, fully automatic solutions are of particular interest. Furthermore, since radiography is currently the accepted “gold standard” in vertebral fracture assessment, segmentation accuracy on radiographic images is especially important. In this paper, we propose a fully automatic vertebra segmentation technique. The primary contributions of this work are (1) a novel hierarchical formulation of the vertebra segmentation problem linking initial vertebral column segmentation to individual vertebrae segmentations; (2) a procedure for alignment of these coarse and fine vertebra shape models, ensuring both point correspondence of the individual high- and low-resolution models as well as correspondence between the two models; (3) an ASM formulation which employs multiple ASM initializations and a nonlinear appearance model; and (4) a thorough evaluation on a data base of lateral radiographs acquired for fracture reading. The remainder of this paper is organized as follows. In the following, we provide a brief overview of spinal imaging and the previous work in vertebra segmentation. In Sect.&amp;nbsp;2, the proposed segmentation procedure is described. Section&amp;nbsp;3 presents experimental results on a challenging data base of digitized radiographs, along with a comparison to previous automatic and semi-automatic techniques. Finally, we provide concluding remarks in Sect.&amp;nbsp;4. The human vertebral column is divided into several regions. The cervical vertebrae (C1–C7) are located at the top of the spine, followed by the thoracic vertebrae (T1–T12) and the lumbar vertebrae (L1–L5). Assessment of vertebral fractures is conventionally performed from lateral radiographs of the spine. Normally, radiologists inspect the T4–T12 and L1–L4 vertebrae for fractures. Since the entire spine cannot be imaged on a single film using conventional radiography, separate images are taken of the lumbar and thoracic vertebrae. Note that newer X-ray systems, such as the EOS imaging system, allow for imaging of the entire spine in one image. These are, however, not used routinely for fracture assessment. A sample lumbar radiograph with manual annotations is given in Fig.&amp;nbsp; . The vertebrae of the spinal column are very similar in appearance and differ mostly in size and “aspect ratio”. In practice, the ambiguity of the correct vertebra level is resolved by relating the location of the vertebrae to the position of the sacrum. Occasionally, even trained radiologists displace all vertebrae one level up or down; we refer to this as . Due to the divergent beam of conventional radiography and slight rotations of the vertebrae, the radiographic images are subject to various projection effects. Most prominently, the vertebrae furthest from the centering point sometimes appear to have dual edges along the end-plates. We refer to this phenomenon as the  effect. In severe cases, the vertebrae might even appear to inter-penetrate. We note that considerable research has been undertaken on osteoporosis biomarkers in other imaging modalities, such as CT, qCT, low-dose CT, dual energy X-ray absorptiometry (DXA), and MRI. DXA images are of special interest, as they allow for imaging of the spine without parallax effects and are used in computing bone mineral density scores. CT is an emerging modality in vertebral fracture assessment which has shown high sensitivity and specificity [2]. Conventional radiography is, however, still the “gold standard” in fracture reading and has the advantage of being cheap, fast, and low dose. Moreover, the availability of large collections of historical data makes radiographs ideal for data mining purposes. The vertebra segmentation problem has received considerable attention in the medical imaging community. The research has focused on many modalities, including MR [11, 32], videofluoroscopy [42], CT [25], and qCT [26]. Several techniques have also been proposed for 3D reconstruction of the spine from biplanar radiography [4, 5, 29]. Segmentation of the vertebral contours in X-ray images is a particularly challenging task. The images suffer from poor signal-to-noise ratios and depict a projection of all layers in the organism. The vertebrae are often subject to projection effects and tend to vary greatly depending on patient positioning and anatomy. Furthermore, X-ray images are sensitive to scanner calibration. As a consequence, statistical models of object shape and appearance variation, based on evidence from manually annotated training sets, have been used extensively to ensure robust segmentation of the vertebrae. Previous work in segmentation of vertebrae from two-dimensional images can be divided into two categories: semi-automatic and automatic techniques. Semi-automatic techniques require input from a trained operator, such as the approximate location of vertebra centers or corners. For some problems, such as the analysis of cohorts involving thousands of patients, the requirement of manual operator interaction might be too demanding. Automatic techniques, on the other hand, tend to be both less accurate and are known to often suffer from level-shifting [10, 38]. The first attempt at semi-automatic segmentation of the spine was presented by Smyth et al. [40]. Their technique is based on active shape models (ASMs) [13] and require the manual annotation of three pre-specified locations along the spine. In [36], Roberts et al. presented a semi-automatic segmentation technique centered around the sequential fitting of a series of active appearance models (AAMs) [12], each of which models a triplet of neighboring vertebrae. Prior to segmentation, the center of each vertebra must be annotated by the operator. AAMs were also employed in the work of Howe and Long [21], in which a customized generalized Hough transform (GHT) was used to initialize the segmentation. Most recently, Iglesias and de Bruijne [22] proposed an ASM formulation using a conditional model of the vertebral shape variation given six manual landmark points for each vertebrae. The first automatic vertebra segmentation technique was proposed by Zamora et al. [41]. The proposed method determines an initial segmentation estimate from a customized GHT and computes a final segmentation using traditional ASM search. GHT was also used for locating the vertebrae in digital videofluoroscopic images in the work of Zheng et al. [42]. In [10], de Bruijne and Nielsen proposed a novel shape-constrained segmentation methodology based on particle filtering [23]. The most recent work in automatic vertebra segmentation was presented by Roberts et al. [38]. The proposed technique applies a parts-based model in computing a first estimate of the spine location; this estimate is subsequently used to initialize a round of AAM-search, similar to that previously proposed by the same authors in [36].    2 Segmentation technique  In this section, we present a fully automatic, hierarchical technique for segmentation of vertebral contours. In addition, we show that the procedure is easily extended to incorporate manual initialization of the segmentation in a semi-automatic fashion. The proposed technique relies on both high- and low-resolutional manual annotations of the vertebrae, as visualized in Fig.&amp;nbsp; . We propose to perform a segmentation in the following 3 steps.          Initially, the location of the spine is estimated using a shape model containing the low-resolutional landmarks for all modeled vertebrae, resulting in, e.g., 24 landmarks for a model containing 4 vertebrae. The six-point representation gives a detailed description of the location and orientation of the spine, but only a coarse description of the shapes of the individual vertebrae. The coarse spine model is fitted to the image using a sequential Monte Carlo (SMC) sampler on shapes [33].         &amp;nbsp;             The coarse segmentation the individual vertebrae is used to initialize a high-resolutional vertebra shape model. The mapping from the coarse segmentation to the high-resolutional shape model is determined by a conditional shape model [22].         &amp;nbsp;             The high-resolutional representation given by the conditional shape model is fitted to the image using active shape model segmentation for each vertebra individually [13].         &amp;nbsp;      In the following, each component of the proposed technique is described. Firstly, we present a short review of traditional statistical shape models. Secondly, we give a detailed description of the three steps involved in the segmentation procedure. Finally, we present an extension for manually constraining one or several landmarks of the low-resolutional spine model. For reference, an illustration of the segmentation pipeline is given in Fig.&amp;nbsp; . Initially, the location of the spine is estimated using a shape model containing the low-resolutional landmarks for all modeled vertebrae, resulting in, e.g., 24 landmarks for a model containing 4 vertebrae. The six-point representation gives a detailed description of the location and orientation of the spine, but only a coarse description of the shapes of the individual vertebrae. The coarse spine model is fitted to the image using a sequential Monte Carlo (SMC) sampler on shapes [33]. The coarse segmentation the individual vertebrae is used to initialize a high-resolutional vertebra shape model. The mapping from the coarse segmentation to the high-resolutional shape model is determined by a conditional shape model [22]. The high-resolutional representation given by the conditional shape model is fitted to the image using active shape model segmentation for each vertebra individually [13]. We incorporate traditional linear Point Distribution Models (PDMs), as originally proposed by Cootes et al. [ 13]. Consider a training set of   shapes, where each shape,  , is parametrized by a sequence of   corresponding landmarks, such that  . To establish a frame of reference in which all training shapes are aligned, each training shape undertakes a translation  , a scaling  , and a rotation  . We refer to these transformations as the   and parametrize them as  . The transformation results in a representation,  , in the space of normalized shapes, defined by    where   represents the   matrix of  ,   denotes the rotation matrix parametrized by  , and   denotes the transformation which vectorizes  . The optimal alignment transformations are determined by generalized Procrustes analysis (GPA) [ 19]. The aligned shapes are summarized in a  . This shape facilitates transformations between the image space and the space of normalized shapes. A compact shape representation which retains most of the shape variation in the training set is computed by principal component analysis (PCA). Accordingly, the  ,  , which account for pose-free shape variation, are given&amp;nbsp;by    where   denotes the mean shape in the space of normalized shapes and   denotes a matrix containing a subset of the shape covariance eigenvectors. The linear transformation is chosen such that a certain proportion,  , of the shape variation is explained. Under this model of object shape and pose, segmentation of an unseen image amounts to an optimization of some measure of correspondence between image and shape with respect to the model parameters  . We propose to model an initial estimate of the spine location by a collection of shape samples using an SMC sampler on shapes [ 33]. The SMC sampler on shapes is an iterative, sampling-based segmentation technique which models the object of interest using a global shape model and a local classification-based appearance model. In accordance with the hierarchical approach, the overall spine shape is modeled by a low-resolutional spine PDM, based on the vertebra corners and end-plate mid-points of all modeled vertebrae. A visualization of sample manual annotations for the spine PDM of the L1 through L4 vertebrae is given in Fig.&amp;nbsp; a. The overall idea of the SMC sampler on shapes can be summarized as follows. In each iteration, the method maintains a set of  samples, or particles, , with associated weights . The particles contain the shape and pose parameters associated with the spine PDM and represent the belief of the system on the location of the spine in the current iteration. The method iteratively resamples the particle set under the weighting  using a static SMC sampler [30]. The resulting new particle sets represent new sets of segmentation hypotheses and they are assigned a weight according to the likelihood of the segmentation given the image evidence. This procedure is reiterated for a fixed number of iterations. We note that the SMC sampler on shapes is similar to that of shape particle filtering—as originally proposed in [10]—albeit with two substantial improvements: (1) the SMC sampler on shapes applies a statistically motivated formulation of the shape weights; and (2) the static SMC sampler exploits the static nature of images, as opposed to the particle filter which is designed for dynamical processes. In order to determine the likelihood of a candidate segmentation, the SMC sampler on shapes relies on a pixel classification-based model of the appearance inside, and surrounding, the spine. This model requires a template labeling function  which constructs a map of anatomical labels in the image domain on the basis of a given shape from the spine shape model. In our implementation,  does not have a parametric functional form but is calculated algorithmically from the spine shape using ad hoc rules of the location of six anatomical regions. The template labeling results in all pixels in a given image being assigned to one of six classes: anterior background, posterior background, intra-vertebral space, inter-vertebral space, vertebral boundary, and overall background. A visualization of the template label assignment scheme is given in Fig.&amp;nbsp;. In the model training phase, a pixel classifier is trained to discriminate between the six classes based on a feature representation of the individual pixels. For an unseen image, the pixel classifier is used to generate a full probability map for all six classes and all pixels. The quality of a candidate shape segmentation  can be summarized by multiplying the likelihood of the individual pixels under the template labeling implied by the shape . In our implementation, the pixel classifier was trained using a random forest classifier [7] and a feature representation based on Gaussian derivatives up to third order on scales 0.17, 0.56, and 1.7 mm [16]. Our experiments showed that the random forest classifier outperforms the traditional -Nearest Neighbours approach on the presented problem in both classification and segmentation accuracy. Having outlined the SMC sampler on shapes, we now take a step back and consider a statistical formulation of the posterior probability of the spine segmentation given the image data. This formulation will motivate the use of Monte Carlo sampling for the spine segmentation problem and give us a principled way of assigning the sample weights. Let us first define the space of class labels, , such that each pixel belongs to one of  classes  for . For efficiency, only a subset of  pixels from each template is considered. Let us furthermore define the input data to the pixel classifier as , with associated labels . In [ 33], it is shown that, under the assumption of i.i.d. data   and conditional independence of  , the posterior   can be formulated as        The likelihood term   is computed from the trained pixel classifier. The prior term   is estimated from the shape training data under the assumption of normally distributed pose and shape. The   term is simply computed from the template labeling associated with   and can be thought of as a class-indicator. Since the prior of the data  is unknown,  can only be determined up to a constant of normalization. Direct integration of  is infeasible for high-dimensional problems. The SMC sampler [30] allows us to efficiently generate samples from  while exploiting the static image space using an annealing scheme. In the following, we describe how the spine segmentation samples are generated using the static SMC sampler. The static SMC sampler samples from a sequence of distributions  . This sequence is designed as a bridge between the proposal distribution   and the target distribution   through a sequence of intermediate artificial distributions. Following [ 31], these artificial distributions are constructed using an annealing scheme, such that    where the annealing parameters   must satisfy  \(0 \le \beta _1 &amp;lt; \cdots &amp;lt; \beta _T = 1\). In this manner, samples are gradually shifted towards regions of high density. Initially, the samples set   is simply generated by sampling the shape and pose prior   and weighted according to  . During subsequent iterations, all sample weights   are first computed according to    Following this, the sample set   is resampled with replacement according to the normalized weights. Duplicate samples are redrawn from a PDM of the local shape variation in the neighborhood of that sample. When the procedure terminates, the sample set  will approximate the model posterior , under the weighting of . The complete sample set can either be retained for further computations or, alternatively, be summarized as the maximum a posteriori (MAP) estimate by retaining the sample of maximum weight or employing a generic mode seeking method, such as the mean-shift algorithm [17]. Given a coarse SMC-based estimate of the spine pose and shape, we want to be able to estimate the most likely outline of the individual vertebrae as well as the uncertainty associated with this estimate. That is, we wish to specify the conditional distribution of the vertebra segmentation in a high-dimensional shape space given the segmentation in a low-dimensional shape space. Previous work on conditional vertebra models indicate that this is efficiently modeled as a conditional normal distribution on the shape parameters [9, 22]. We follow the same approach. This model will allow us to initialize the following ASM search in the mean of the conditional distribution and constrain the search according to the conditional covariance. The statistical model relies on the parametrization of two PDMs based on the individual vertebrae:          The high-resolutional PDM is computed from the complete delineations of the individual vertebrae. The model is based on a data base of manual full contour annotations. To ensure point-to-point correspondence, all landmarks are recomputed. First, reference positions for the corner- and mid-points are defined on the basis of the ground truth six-point annotations. Secondly, landmarks are placed equidistantly between these reference points by linear interpolation of the original landmarks.         &amp;nbsp;             The low-resolution PDM is computed from the individual vertebrae of the coarse segmentations, i.e., each training example has six landmark points.         &amp;nbsp;      Since the conditional model only models the shape parameters of the PDMs, it is essential that the pose parameters can be directly inherited from the low- to the high-dimensional model. Consequently, pose correspondence must be established between the reference shapes of the two PDMs. We propose to achieve pose correspondence using the following two-step approach:        Two intermediate high-resolutional reference shapes are computed: (1) a pose-corresponding reference shape, , where the pose transformations have been inherited from the low-resolutional model and (2) an independent reference shape, , based on regular alignment of all landmarks in the high-dimensional training set.         &amp;nbsp;           The final reference shape, , is computed by aligning the independent reference shape, , to the pose-corresponding reference shape, .         &amp;nbsp;      By constructing the high-dimensional PDM from the resulting reference shape, we achieve both pose-correspondence between the high- and low-dimensional models and landmark correspondence in the high-dimensional model. The high-resolutional PDM is computed from the complete delineations of the individual vertebrae. The model is based on a data base of manual full contour annotations. To ensure point-to-point correspondence, all landmarks are recomputed. First, reference positions for the corner- and mid-points are defined on the basis of the ground truth six-point annotations. Secondly, landmarks are placed equidistantly between these reference points by linear interpolation of the original landmarks. The low-resolution PDM is computed from the individual vertebrae of the coarse segmentations, i.e., each training example has six landmark points. Two intermediate high-resolutional reference shapes are computed: (1) a pose-corresponding reference shape, , where the pose transformations have been inherited from the low-resolutional model and (2) an independent reference shape, , based on regular alignment of all landmarks in the high-dimensional training set. The final reference shape, , is computed by aligning the independent reference shape, , to the pose-corresponding reference shape, . Having established the required shape models, we now consider the conditional model on the shape parameters. Let   and   denote the high- and low-resolutional shape parameters, respectively. We model  , the conditional distribution of the shape parameters, as the Gaussian density    The conditional mean   and the conditional covariance   are computed as    and    where  ,  ,  , and   denotes the mean shapes and shape covariances and   and   denotes the shape cross-covariances. The covariance matrix   is, in general, singular due to multicollinearity. We regularize this matrix by ridge regression [ 20], such that  . Visualizations of the first three modes of variation for initialization of both a normal and a fractured vertebra are given in Fig.&amp;nbsp; .
Springer.tar//Springer//Springer\10.1007-s00138-012-0461-1.xml:Linear feature selection in texture analysis - A PLS based method:Machine learning Classification Feature selection Texture analysis Partial least squares Feature extraction:  1 Introduction  Texture analysis has been applied to numerous fields within image analysis [18, 24, 36, 37]. By applying it to magnetic resonance images (MRI), one may capture early tissue changes and provide the means for obtaining information that might not be assessed visually [35]. This could be used as support for medical decisions. For example, the comparison by Herlidou et al. [17] suggests that diagnoses established by visual image analysis have large variability and automated texture analysis provides a more accurate discrimination between control and pathologic subjects with skeletal muscle dystrophy. Some texture analysis approaches combine different features at various scales in a generic bank of features to allow, for instance, a broad representation of the image. Kovaleva et al. [24] applied extended multisort co-occurrence matrices to analyze structural brain asymmetry. The reported texture method was based on intensity, gradient and anisotropy features. The investigation presented in [18] employed four automated methods of texture representation (gray level histogram, co-occurrence, runlength and gradient matrices) for structural characterization of trabecular bone. Another example is the study of Sørensen et al. [36], where a general uncommitted statistical machine-learning framework was used for measuring emphysema in images of the lungs. The drawback of a bank of features is a potentially high-dimensional representation of data. Beyer et al. [4] showed that increasing the number of features, and thus the dimensionality of the data, leads to a loss of meaning for each feature and possibly decreases the model accuracy. In many situations, a large number of features are strongly correlated and do not introduce new information to improve the ability to analyze the images. Feature selection and feature extraction (as feature space transformation) are effective approaches for dimensionality reduction. Feature selection is the process of reducing the dimensionality by selecting a subset of the original variables. Feature extraction is a transformation of the input data which possibly reduces the dimensionality by some functional mapping of -dimensional data into a -dimensional data, where (  ) [31], e.g., by principal component analysis (PCA). These techniques can decrease the model complexity and eliminate noisy subspaces. Partial least squares (PLS) regression is another example of multivariate data analysis technique that has been used for dimensionality reduction [19, 23, 33]. Techniques such as PCA and PLS minimize a least squares cost function. The regression can focus on the atypical observations instead of describing the model represented by the majority of the data [12]. In this study, we introduce a robust PLS-based dimensionality reduction (DR) method in a texture analysis framework applied to diagnosis of knee osteoarthritis (OA). The framework was applied to classify between healthy subjects and OA patients by quantification of the tibial knee bone structure. The framework implemented the following steps: Initially, a generic bank of texture features was extracted from the images. As a pre-processing step to the dimensionality reduction, the outliers were identified and eliminated. Next, a feature relevance index (FRI) was applied to the original features to order them according to their importance to the model. Then, the method iteratively selected the features and transformed them to an orthogonal space. In the final step, the transformed features were used as input to a linear classifier that returned for each knee the probability of having&amp;nbsp;OA.    2 Background  Fisher linear discriminant analysis (LDA), [26] is a classical classification tool that considers both classification and dimensionality reduction. The method decomposes the total covariance into between-class covariance and within-class covariance and maximizes the ratio of between-class to within-class scatter. The low-dimensional discriminative space is estimated based on the resulted linear transformation. This method is known to perform quite well in low-dimensional settings. However, in high dimensions LDA cannot be applied directly because the covariance matrix estimate is singular and cannot be inverted. Although an approximated inversion can be calculated, in this case, the LDA method can suffer from high variance, resulting in poor performance [38]. To deal with this, the framework presented in this paper used PLS to apply two dimensionality reduction approaches to the data set: feature selection and feature extraction. The resulted reduced space was sent as input to the LDA classifier. The following sections briefly explain the PLS method and describe some PLS based feature selection approaches. Multivariate data analysis techniques like PCA and PLS have a common principle; they are constrained to find a linear transformation that produces mutually uncorrelated components. PCA is a classic technique used for dimensionality reduction by feature extraction. Concerning classification problems, we defined the data matrix as X (referred to as predictive or explanatory variables in factor analysis) and the classification classes as vector Y (the dependent or response variables). PCA linearly transforms the original features into uncorrelated features. These new features are ranked, loosely speaking, according to the amount of variability of X they explain. In this approach, no importance is given to how each feature may be related to the classes. PLS does not require matrix inversion to obtain the coefficient matrix. Rather, the factors are computed by successive 1-D linear regression, assuming that the relationship between features and the classes is influenced by a few underlying variables, called latent variables or factors. The features and classes are assumed to be realizations of these underlying variables [41]. Therefore, PLS regression suggests the use of supervised dimensionality reduction by considering both X and Y information. This regression returns a linear combination of the features, the latent factors (X-scores), which are used to predict the classes based on Y-scores. Figure&amp;nbsp;  exemplifies how observations from a synthetic data set (see the data scatter plot in Fig.&amp;nbsp; ) are represented in terms of the principal component coefficients created by PCA and the latent factors estimated by PLS. The PLS plot shows that PLS considered the correlation of the variables to the classes to compute the latent factors. The first variable had a weak relation to the class separability, so when computing the first latent factor, PLS almost ignored this variable while PCA considered the three variables nearly equally. Barker et al. [3] presents a formal statistical explanation clarifying that the dimensionality reduction provided by PLS is determined by between-groups variability, while by PCA is determined by total variability. Therefore, PLS potentially performs better than PCA for dimensionality reduction when classification is the ultimate goal. The PLS model also supports selection of the latent factors; considering  the total number of factors generated by the regression, by selecting only the first  (\(k&amp;lt;h\)) to compose the new feature set, one can reject the noisy information. There are various algorithms for calculating PLS regression. The NIPALS algorithm [1] is the standard, but the SIMPLS [21] is very popular because it is faster. Our framework used the SIMPLS algorithm for the PLS regression. In the following paragraphs, we briefly introduce the PLS regression for calculating the coefficients B and the factors T. Although PLS supports multiple response variables, we considered a single response Y. For   samples and   predictors or features, a PLS regression model decomposes  X and  Y to produce the following bilinear representation of the data:               where the input data and classes are in  X   and  Y   matrix, respectively. The  X-scores,  T  , contain the transformed features in the orthogonal space while the matrix  U   has the transformed classes ( Y-scores). The scores matrices contain the information on how the samples relate to each other.  P   and  Q   are the loading matrices, which represent the regression coefficient of  X on each column of  U and the regression coefficient of  Y on each column of  T, respectively. Loadings contain the information on how the features relate to each latent factor. The matrices  E   and  F   contain the residuals. The latent factors (Eq.&amp;nbsp; 3) and regression coefficients (Eq.&amp;nbsp; 4) are computed based on a weight matrix  W   that expresses the correlation of each  X-column with the  Y variable. Thereby, entries in  W with values close to zero express less important features.                         PLS is typically not robust towards outliers due to use of least squares regressions. Furthermore, the SIMPLS algorithm is based on the empirical covariance matrix, where outliers can have a damaging effect on the estimates. To reduce the effect of outliers, some approaches, such as the method presented in [5], propose to replace the empirical covariance matrix by a robust covariance estimator. In [11], Daszykowski et al. presented a robust version of PLS by applying a weighting scheme to down-weight the negative influence of outliers upon the model. The outliers were detected based on standardized leverage and residual distances exceeding a cut-off value. For a more complete review on the leading robust PLS algorithms, see the work by Kruger et al. [25]. An alternative approach to robust PLS algorithms is to detect the outliers and eliminate them before defining the final model. According to Wold et al. [40], the initial outcome of a PLS regression can be used itself to detect outliers. Moderate outliers can be identified by the residuals of Y and X (E and F in Eqs.&amp;nbsp;1 and 2). The guidelines from the authors claimed that samples that deviate outside of four times the standard deviation (SD) of the Y-residuals can be considered an outlier. For the X-residuals, one needs to summarize all  values for each sample. This is proportional to the distance between the data point and the model plane in X-space, the so-called DModX (distance to the model in X-space). A sample with DModX larger than 2.5 times the SD of the X-residuals indicates an outlier. The strong outliers are found by analyzing the Hotelling T of the latent factors, the scores T in Eq.&amp;nbsp;3. The Hotelling T is proportional to the leverage, which is a measure of the influence of a sample on the PLS model [27]. Samples with confidence level less than a certain percentage can be considered outliers. Usually the limit level is between 95 and&amp;nbsp;99&amp;nbsp;%. When a model includes a high-dimensional set of features, usually several of them are correlated. Besides providing nearly the same information to predict the classes, highly correlated features can imply model convergence problems, overfitting and the “curse of dimensionality” in general [29]. Therefore, an important part of learning-based processes is to identify a subset of the weakly correlated features that relate to the class discrimination. Several PLS-based feature selection approaches have been proposed [2, 32], mainly to estimate the features more related to the underlying latent data structure. In a recent work, Li et al. [28] proposed the use of the absolute values of the regression coefficients of the PLS model as an index for evaluating the importance of wavelengths of multi-component spectral data. The proposed algorithm, called competitive adaptive reweighed sampling (CARS), first ranks the variables according to their absolute PLS coefficients. Based on this ranking, the algorithm sequentially selects  subsets of variables from  Monte Carlo sampling runs. In each sampling run, a fixed ratio (e.g., 90&amp;nbsp;%) of samples is randomly selected as training data. Next, based on the regression coefficients, a two-step procedure is adopted to select the key variables. The first step uses an exponentially decreasing function and the second step uses an adaptive reweighed sampling method to remove the unimportant variables. Finally, a cross validation (CV) approach is applied to choose the subset with the lowest root mean square error. Another example is the interactive variable selection (IVS) [30] that modifies the PLS algorithm by doing a dimension-wise selective reweighting of single values in each column of the weight matrix W. The investigation presented two techniques that use a threshold, defined by CV, to replace some values in W to zero. Their experiments showed that the elimination of either small or large values in W improved the model, but no clear explanation was given justifying the elimination of large W-values. Moreover, the author used a small simulated data set and the method was in part interactively evaluated. Wold at al. [ 40] introduced the variable importance in the projection (VIP), which is a score that summarizes the importance of each variable for the latent factors projections. The score is a weighted sum of squares of the weights in  W, with the weights calculated from the amount of  Y-variance of each PLS latent factor (see Eq.&amp;nbsp; 5):            where  . Building on Wold’s work, Bryan et&amp;nbsp;al. [6] presented the MetaFIND application that implements a post-feature selection method based on VIP and correlation analysis of metabolomics data. The features were ranked, but the threshold that defines the selected features was a user-defined parameter. The “greater than one rule” is generally used as a criterion for variable selection, since the average of squared VIP scores is equal to 1, but Chong et al. [8] showed that the proper cut-off value may be higher than 1 under uneven class distributions, high correlation, or an equal coefficients structure. Their investigation also explored the nature of the VIP method compared with other methods. Although PLS combined with VIP scores is often used when multicollinearity is present among variables [30, 40], there are few guidelines about how to use it [8]. To address this issue, our developed framework introduces a robust PLS-based strategy for dimensionality reduction (DR) that includes outlier detection, feature selection and feature extraction.    3 Framework  The overall texture analysis framework is summarized in the following steps:        Segmentation of the region-of-interest         &amp;nbsp;           Features computation         &amp;nbsp;                 Dimensionality reduction                                  Pre-processing steps:                                      Auto-scaling and initial PLS regression                             &amp;nbsp;                                         Initial feature ranking                             &amp;nbsp;                                         Identification of the outliers                             &amp;nbsp;                                         Re-computation of FRI                             &amp;nbsp;                                                        &amp;nbsp;                                     Iterative forward feature selection                                      Incremental feature selection                             &amp;nbsp;                                         Intermediate evaluation                             &amp;nbsp;                                                        &amp;nbsp;                               &amp;nbsp;           Classification         &amp;nbsp;           Evaluation         &amp;nbsp;      The steps 1, 2, 4 and 5 are described in Sect.&amp;nbsp; 4, where the application of the texture analysis framework to OA diagnosis is detailed. Section  3.1 explains the step 3, the proposed dimensionality reduction method. Segmentation of the region-of-interest Features computation Dimensionality reduction              Pre-processing steps:                       Auto-scaling and initial PLS regression                   &amp;nbsp;                          Initial feature ranking                   &amp;nbsp;                          Identification of the outliers                   &amp;nbsp;                          Re-computation of FRI                   &amp;nbsp;                               &amp;nbsp;                 Iterative forward feature selection                       Incremental feature selection                   &amp;nbsp;                          Intermediate evaluation                   &amp;nbsp;                               &amp;nbsp; Pre-processing steps:        Auto-scaling and initial PLS regression         &amp;nbsp;           Initial feature ranking         &amp;nbsp;           Identification of the outliers         &amp;nbsp;           Re-computation of FRI         &amp;nbsp; Auto-scaling and initial PLS regression Initial feature ranking
Springer.tar//Springer//Springer\10.1007-s00138-012-0462-0.xml:Accurate prediction of AD patients using cortical thickness networks:Alzheimer’s disease Classification Network Cortical thickness:  1 Introduction  Alzheimer’s disease (AD) is a progressive neurodegenerative disease which is characterized by memory loss, poor judgment, language deterioration and so forth. As the major form of dementia, AD affected 26.6&amp;nbsp;million people worldwide in 2006 and was predicted to affect 1 in 85 people by 2050 [1]. Mild cognitive impairment (MCI), commonly defined as a subtle but measurable memory loss, is considered as the transition stage between normal aging and dementia; and patients with MCI have been reported more prone to progress to AD than healthy people [2, 3]. A number of studies have indicated that structural changes such as gray matter atrophy can be identified in MCI and AD patients [4–7], which, in structural magnetic resonance imaging (MRI), might be reflected by brain volume atrophy [4, 5] or cortical thickness thinning [6, 7] of all or specific regions of the patients’ brain. These studies demonstrate the feasibility of discovering clinical biomarkers or developing an automatic diagnosis system to distinguish AD or MCI patients from the healthy elder. Recently, many efforts have been made to utilize MRI neuroimaging to develop an accurate prediction system for MCI and AD through modern machine learning algorithms. Statistical learning techniques such as support vector machines (SVMs) [8, 9] are commonly used classifiers in neuroimaging community, which have the capability of dealing with high dimensional data and perform well on generalization. Generally speaking, when using T1-weighted MRI, classification approaches can be roughly divided into two categories according to the type of features extracted from the original images, including (1) brain volume features such as gray matter probability (GMP) or hippocampal volumes [10, 11, 13–16], (2) brain surface features such as cortical thickness or curvature [17–19]. Further, depending on the sampling type, classification approaches can also be categorized into voxel-based (or vertex-based) and regions-of-interest-based (ROI-based). In voxel-based (or vertex-based) methods, the much higher dimensional features are usually noisy and most of them might be not related to the disease. Thus, techniques such as smoothing, downsampling and feature selection are needed to reduce the dimensionality so that the classifiers can be more effective and efficient [13]. Another solution is to register and group the voxels (or vertices) into an anatomical atlas, such as automated anatomical labeling (AAL) atlas, and calculate the mean value of all the voxels (or vertices) in each region as the features [16]. Nonetheless, the existing atlas might not be adapted to a specific disease. The authors of [10] have proposed an approach through adaptive region parcellation, in which the brain is divided using most discriminative voxel clusters. This method has been first used to predict schizophrenia [10], and then in several studies for AD prediction [11, 12]. In addition, several popular machine learning algorithms, such as relevance vector machine (RVM) [20], boosting [14] and multi-kernel learning (MKL) [21], have been introduced in neuroimaging community. These approaches also achieved remarkable results on AD classification. However, none of these studies considered the network architecture of the brain, defined as the relationship, or connection, between voxels, vertices or ROIs. Yet, it is widely reported that brain is a complex network and recently, brain connectivity research—which aims to analyze the brain network through graph theoretical approaches—has been increasingly popular [22–24]. Studies such as [25–29] constructed a network for each group of subjects and investigated the abnormality of network measures in patients, providing new clues for understanding the pathology as well as new avenues for diagnosis of brain diseases. Different from the group-level analysis (a network for each group) in [25–29], studies such as [30–33] established a network for each subject, namely individual network, and these networks were used as features for classification. The authors of [30] proposed an approach to predict temporal lobe epilepsy (TLE). They established a connectivity matrix for each subject using ROI-based cortical thickness and curvature through the so-called robust L1-norm models. Graph edit distance (GED) was calculated to measure the distances between any two individual networks and the pairwise GEDs were embedded into a Cartesian feature space. The classification was performed on the embedded feature space using linear discriminant analysis (LDA). In [31], six different on-fiber physiological parameters of diffusion tensor imaging (DTI) were used to establish six networks for each subject. Local clustering coefficients of all six networks were computed as features. A hybrid feature selection that combined Pearson correlation with labels and SVM-RFE [34] was applied for dimensionality reduction and SVM was trained for classification. In [32], individual networks was constructed based on the correlation between the longitudinal thickness changes of different ROIs. These networks along with other features were used to train an SVM to predict AD and MCI patients. Another study [33] established hierarchical anatomical networks for MCI patients using volumetric measures of ROIs across different scales and applied partial least square (PLS) for dimensionality reduction of network features for classification. All of these articles argued that network features were more reliable due to the informative network topology. In this study, we present a novel method for establishing individual anatomical networks using cortical thickness and attempt to use these networks to distinguish AD (and MCI) patients from the normal controls (NC). To obtain a connectivity matrix for each subject, we first calculate the distance of pairs of cortical regions using mean cortical thickness; then a kernel function is applied to the distance to obtain the connection weight. For feature selection, the hybrid method is used including: (1) a filter method for fast and rough dimensionality reduction and (2) a wrapper method for further precise feature selection. Three types of filters and two wrappers form are combined to form six different hybrid feature selection methods, and the results of these methods are compared. Classification is performed via a kernel-based SVM. We apply our method to 83 subjects from the OASIS AD database and report the highest cross validation accuracy of 90.4&amp;nbsp;%, at least 10&amp;nbsp;% higher than that using the raw cortical thickness as features. The remainder of this paper is organized as follows: in Sect.&amp;nbsp;2, we introduce the experimental dataset and image pre-processing pipeline. Then we present in detail the network construction, feature selection and classification framework. The experimental results are described in Sect.&amp;nbsp;3, followed by discussions in Sect.&amp;nbsp;4 and conclusion and limitations in Sect.&amp;nbsp;5.    2 Materials and methods  Data used in this study are taken from Open Access Series of Imaging Studies (OASIS) database ( http://​www.​oasis-brains.​org/​). The OASIS database consists of a cross-sectional collection of right-handed subjects aged from 18 to 96. About one hundred subjects over 60&amp;nbsp;years old have been clinically diagnosed with very mild to moderate AD. In our study, 83&amp;nbsp;subjects are chosen from the database for experiment, including 39&amp;nbsp;subjects with MCI and moderate AD (22 females, 17 males, age   SD   5.64), and 44&amp;nbsp;age-matched normal controls (30 females, 14 males, age   SD   7.29). The summary of demographical information is showed in Table  1. The control subjects have a Clinical Dementia Rating (CDR) of zero, and Mini-Mental State Examination (MMSE) scores between 26 and 29. The MCI/AD patients have a CDR of 0.5, 1 or 2 and MMSE scores between 15 and 28 (most of patients have MMSE scores less than 26). We divide the subjects into two groups based on their CDR scale, that is, all subjects with CDR   0.5 (patients) are in class 1, while all subjects with CDR   (healthy controls) are in class 0. All T1-weighted images in this study are corrected for intensity non-uniformity using N3 algorithm [ 35] and automatically registered into ICBM 152 template through 9-parameter linear transformation [ 36]. Cortical thickness of each subject is measured using the method described in [ 37,  38]. The registered images are classified into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) using an artificial neural network classifier [ 37]. Then the inner (GM/WM) and outer (pial) cortical surfaces are extracted automatically from the segmented images through the Constrained Laplacian-based Anatomic Segmentation with Proximities (CLASP) algorithm [ 38]. Both surfaces consist of 40,962&amp;nbsp;vertices and 81,920&amp;nbsp;polygons in each hemisphere. Cortical thickness is calculated as the Euclidean distance between the linked vertices on the two surfaces. To increase the signal-to-noise ratio, cortical thickness were smoothed using a surface-based diffusion smoothing with 20&amp;nbsp;mm full-width-at-half-maximum (FWHM) [ 39]. Finally, the brain of each subject is segmented using Automated Anatomical Labeling (AAL) atlas [ 40] which divides the whole brain volume into 116 anatomical regions. The cortical parcellation is produced by finding the highest occurrence at each vertex on the cortical surface; 40&amp;nbsp;cortical regions are identified for each hemisphere (see Table  2). The mean cortical thickness of regions is estimated by averaging the cortical thickness values over all vertices in the corresponding region, and used for construction of cortical connectivity for each subject. Figure&amp;nbsp;  shows the pipeline of image pre-processing and mean cortical thickness extraction. A network, or graph, is typically defined as  , where   is the set of vertices (or nodes) and   is the set of edges (or links). For this work, we assume that a node is a cortical region as defined by the AAL atlas and an edge is a certain similarity of the mean cortical thickness between a pair of nodes. Thus each individual network, or graph, shares the same set of 80&amp;nbsp;vertices. This facilitates comparisons using only the weight of edges, which is measured by the kernel below. The mean cortical thickness features of all subjects form an   feature matrix, where   is the number of subjects and   is the number of vertices. Let   denote the cortical thickness of the  th ROI of the  th subject; then the connection weight   is defined&amp;nbsp;as,       where   is a kernel function, and   is a function that measures the distance between the  th and the  th ROI. Specifically, the exponential kernel   is used here, which contains an input parameter   that determines how locally data is analyzed, and the distance function   is defined as the square of difference between the mean cortical thickness of any two regions,       Other kernel and distance functions could also be used to construct the networks. We choose the exponential kernel because it has several desirable properties, including non-negativity and monotonicity, and bounded range. The smaller the absolute difference of cortical thickness, the larger the connection weight between two regions. If two regions have the same value of cortical thickness, they have the strongest connection whose weight equals 1. The kernel width   partially determines the distribution of edge weight or the network density. In this study, we chose a set of different  ’s to examine its influence on the final classification performance. According to the above method, we obtained   individual symmetric and hollow (zeros along the diagonal) connectivity matrices, each with   labeled vertices. These networks are pruned to reduce noise: edges with weight less than 0.01 are eliminated. Our aim is to make prediction of AD patients using the edge weight features of individual cortical networks. Each network has  edges, and their weights are rearranged to form a long feature vector for each subject. Because of the high-dimensionality of the network features and small number of samples, namely the curse of dimensionality, dimensionality reduction is considered to reduce the variance and improve the performance of classifiers. Because we are interested in interpretable results, we consider only canonical dimensionality reduction, a.k.a., feature selection (as opposed to feature extraction, in which new dimensions are constructed as functions of the original dimensions such as principle component analysis). Feature selection algorithms can be grossly subdivided into two categories: filter methods and wrapper methods&amp;nbsp;[41]. A filter method such as test directly evaluates feature subsets through their information content while a wrapper method iteratively optimizes the performance of a specific classification algorithm. Thus filter methods tend to be more computationally efficient; they also tend to be less effective than wrapper methods for a specific classification problem. Hence, a hybrid approach is intuitively desirable to improve the accuracy of feature selection at relatively low computational cost. In our study, we try six different combinations of three filters and two wrappers and compare their impacts on classification performance. Filter methods are used to filter the features on the original feature dimensions to eliminate features with small discriminative power. We consider three filter methods: (1) a two-sample  test, (2) Pearson’s correlation with classification labels, and (3) the ratios of between-group sum of squares to within-group sum of squares  . First, a two-sample   test is used to test whether there exists a statistically significant difference between the means of two data samples. For a two-tailed test, the null hypothesis is that the mean value of two groups are equal. The   test yields a   value—the probability of mistakenly rejecting the null hypothesis—for this hypothesis testing. We assume the data are normally distributed and apply two-sample   test on each feature dimension to retain the features which have a   value smaller than some threshold. Second, Pearson’s correlation coefficient between a feature dimension and clinical labels may reveal the discriminative power of the feature; the larger the absolute Pearson correlation coefficient is, the more relevant a feature is to classification. Let   denote the  th feature of the  th subject, and   denote the label of the  th subject; Pearson’s correlation coefficient of the  th feature dimension is defined as follows:       where   is the mean value of the  th feature dimension across subjects and   is the mean value of labels. Pearson’s correlation coefficients of each feature dimension are ranked and features with larger   are retained. Third, the within-group sum of squares (WSS) measures the within-class scatter while the between-group sum of squares (BSS) measures the variation between the group means. A discriminative feature should have a larger BSS and a smaller WSS, or a larger ratio of BSS to WSS. Also we assume that   is the value of the  th feature of the  th sample and   is the group label (+1 or  1); the ratio   of the  th feature is defined as       where   is the mean value of the  th feature across subjects in class   and   is the mean value of the  th feature across all subjects; the index function   equals 1 if the  th subject belongs to the group   and equals 0 otherwise. The above three filter methods calculate the ranking scores independently for each feature. However. they do not take into account the relationship (redundant or complementary) between features, in other words, they do not consider the features as a whole or do not optimize the feature subset for a specific classification problem. Thus the selected features might not be the most optimal feature subset for the classification. In fact, features with weak discriminative power may contribute to the performance of classification if they are complementary to others, while those with strong power may affects the performance if they are redundant. To avoid this problem, we retain a little more features than in the common situation when using filter method, and then apply wrapper method (normally subset-based) for further feature selection. One of the wrapper methods we use in this work is the well-known SVM-RFE [34]. It uses the selected feature subset to iteratively train an SVM. In each iteration, the weight  for each feature in the feature subset is calculated during the SVM training process (e.g., for linear SVM we can simply consider the weight of features as the  satisfies , however, the SVM-RFE algorithm uses the radial basis function kernel, which is slightly complicated for estimation of the feature weight), and is considered as the score of the features. A portion of features with small score are eliminated in each iteration of SVM training until the classification accuracy is over a threshold, or the number of remaining features is smaller than a threshold. Note that SVM-RFE uses the accuracy of cross validation to estimate the goodness of feature subset, which may avoid overfitting problem efficiently. Another wrapper method is a local-learning-based feature selection method [42], which can iteratively optimize the weight of features to maximize a defined margin between the groups. In this algorithm, the distance of a sample to the nearest sample in the same group (nearest hit) and the distance to the nearest sample in the opposite group (nearest miss) are calculated; the margin of a sample is defined as the difference of the two distances. Because a weight is set to each feature, we should find the nearest hit and nearest miss in the weighted feature space, which cannot be estimated as in the original feature space according to the author. Thus, it is assumed that every sample might be the nearest sample, and the probability is estimated using the weight of feature. Then the margin is considered as the expectation of margins calculated using all possible nearest samples (for detail, see [42]). This method is especially suitable for high dimensional data that contain lots of irrelevant features. We use SVMs with radial basis function (RBF) kernel for classification. The RBF kernel is defined as follows: where   are the two feature vectors, and   controls the width of the kernel. In order to obtain optimal prediction model, the hyper-parameter of SVM (such as the penalized coefficient   and the RBF kernel width  ) should be carefully chosen. We select the optimal hyper-parameters through grid search and cross validation (CV), that is, in each iteration we change the parameters and estimate the ten-fold CV accuracy; we select the ones that achieve best results. This CV is named inner CV because it is nested in another CV called outer CV, which is used for the estimation of generalization of the method [ 43]. The outer CV we use in this work is a leave-one-out cross validation (LOOCV). In each fold of outer CV, one sample is kept out for validation and the remaining are used for feature selection and training the classifier; then the held-out sample is validated using the selected features and parameters obtained in the training process. We obtain the LOOCV accuracy after all the samples are validated and consider it as the estimation of generalization of our method. This nested CV method can avoid overestimation of the method and obtain unbiased results. The pipeline of our classification framework is presented in Fig.  .    3 Results  The LOOCV classification accuracy is showed in Fig.&amp;nbsp; , in which we report the accuracy of classifying individual cortical networks (CTNet) that are constructed using kernel widths from 0.008 to 0.024. Each line represents the result using a different hybrid feature selection method that combines a filter and a wrapper. The filter we used includes two-sample   test (tTest), Pearson correlation with labels (Corr) and ratio of BSS to WSS (BSSWSS), while the wrapper includes local-learning-based feature selection (LLB) and SVM-RFE (RFE). Note that our method automatically selects features and tunes classifier hyper-parameters within each fold of outer LOOCV; thus the selected features and hyper-parameters may change every time in each fold. This nested CV pipeline provides a good estimate of generalization error. Figure&amp;nbsp;  shows that the performance of the predictive system is better and more stable when using LLB as the wrapper than RFE. The best LOOCV classification accuracy is 90.4&amp;nbsp;% when using the kernel width   for network construction and the feature subset selected by the combination of   test and local-learning-based feature selection methods. The best accuracy using six different hybrid feature selection methods and the corresponding   are listed in Table  3. Although the prediction accuracy is commonly used for the evaluation of classification system, it might be biased and not comprehensive when considering class priori. Thus other methods are introduced for a more comprehensive evaluation. One evaluation method is to use true positive rate (TPR, or sensitivity) and false positive rate (FPR, or specificity). TPR and FPR can evaluate the prediction performance without considering class priori, in other words, for a classification problem, the prediction accuracy may change when class priori varies, while TPR and FPR remain consistent. We can easily visualize and organize the performance of a set of discrete classifiers (predicting discrete labels such as +1 or  ) using TPR and FPR to plot the receiver operator characteristic (ROC) graph (note that ROC graph is different from ROC curve), in which each classifier is represented by a point (FPR, TPR). In ROC space, the (0,1) point represents a perfect classifier (all samples are correctly predicted); thus the nearer a point is to the (0,1) point (more northwest), the better a classifier is. The convex hull of a set of classifiers in ROC graph consists of point (0,0), (1,1) and several “more northwest” points. A classifier is potentially optimal if and only if it lies on the convex hull regardless of class priori. Figure&amp;nbsp;  shows the ROC graphs of classification using different hybrid feature selection schemes, in which each point represents classification using different kernel width   for network construction. We aim to find the best kernel width   in terms of ROC graph and list it in Table&amp;nbsp; 3. On the other hand, ROC curve can be used for evaluation of ranking or continuous classifier, in which case each classifier is represented by a curve. This is because a classifier that exports continuous value can be thresholded to make discrete binary prediction (such as SVMs commonly use a threshold of zero), and by varying the threshold a set of classifiers with different (FPR, TPR) can be produced so that a curve can be traced. A single measure of classification performance can be derived by calculating the area under ROC curve (AUC), e.g., a larger AUC indicates a better classifier. In Table&amp;nbsp;  3, we also list the best AUC. For a review of ROC-based graphs, see [ 44]. The proposed method for network construction actually projects the mean cortical thickness of AAL ROIs to much higher-dimensional network connectivity features (in this study from 80 to 3,160), which may be the major cause of the “curse of dimensionality”. This motivates us to reduce feature dimensionality in order to improve the performance of classifiers. Fortunately, modern sophisticated feature selection methods are powerful enough for large-scale dimensionality reduction, making it possible for learning more complex classifiers. To further demonstrate the effect of the proposed network method, we compare the prediction results of classification using individual cortical networks (CTNet) and raw mean cortical thickness (RawCT). Because the number of subjects is similar to that of RawCT features (83 vs. 80), we can apply only wrapper methods instead of hybrid method for feature selection, or even directly train the classifier using RawCT features without feature selection. Therefore we implement the following three strategies before training an SVM with RBF kernel using RawCT features: (1) without feature selection (RawCT-None); (2) using local-learning-based feature selection (RawCT-LLB); (3) using SVM-RFE for feature selection (RawCT-RFE). The LOOCV prediction accuracy and AUC of the three methods are summarized in Table&amp;nbsp; 4. Obviously, the highest LOOCV classification accuracy using network edge features exceeds that using raw cortical thickness by more than 10&amp;nbsp;%. The ROC graph in Fig.&amp;nbsp; a visualizes the best classification results based on network features, as well as that based on raw cortical thickness. The convex hull consists of point (0,0), A1 and (1,1), indicating that the proposed network-based method using tTest&amp;amp;LLB feature selection (A1) is better than other methods regardless of class priori. In Fig.&amp;nbsp; b, we compare the ROC curve under the best condition using network features and raw cortical thickness features. For network features, we find the features selected by local-learning-based method are more stable than SVM-RFE in the leave-one-out analysis (this will be discussed in the Sect.&amp;nbsp; 4). Here we only examine the most discriminative features under the best condition (CTNet-tTest&amp;amp;LLB,  ), in which condition we use   for network construction, and use 2-sample   test and local-learning-based method for feature selection. The number of features selected by the tTest&amp;amp;LLB feature selection in leave-one-out (LOO) analysis ranges from 10 to 15; in total, 22 different edges were selected in at least one of the 83&amp;nbsp; folds of cross validation. We calculate the reproducibility ratio of each feature selected across LOO iterations to find which of them are almost consistent, or are chosen in most feature selections out of the 83&amp;nbsp;folds of cross validation, because these consistent features might encode the class-conditional signal. In Table  5 we list some edges that are frequently selected in LOO analysis and point out the cortical regions they connect. We find the selected edges connect several regions that are believed to be related with Alzheimer’s disease [ 4– 7,  25,  26,  45,  46], including hippocampus and parahippocampus, anterior and middle cingulum, superior temporal gyrus, fusiform gyrus, precuneus, lingual gyrus, calcarine, supramarginal gyrus, and several regions in the frontal lobes such as orbitofrontal region. In addition, one of the discriminative edges (No. 697 in Table&amp;nbsp; 5) is consistent with the abnormal connection in a studies about structural networks of AD patients [ 25]. The most discriminative edges with high reproducibility ratio ( \(&amp;gt;\)90&amp;nbsp;%) are visualized in Fig.&amp;nbsp; .    4 Discussion  The proposed approach establishes individual cortical network for each subject through a kernel function using the mean cortical thickness of AAL regions. In fact, the raw cortical thickness features are transformed to high-dimensional network edge features, which makes complicated feature selection essential to the classification system. This results in a more complex prediction system and demands more computation time for training, nevertheless it does work as presented in the last section. We explain the reasons why the proposed method works from three aspects. (1) The reliability and the stability of the network patterns: we consider that the network features can reflect the patterns of relationship between different ROIs, which are more reliable and stable than the original mean cortical thickness. This is because the human brain is so complex a non-linear system and has severe individual differences, in others words, the raw brain measures such as cortical thickness might not be easily modeled by a linear model with respect to age, gender, intracranial volume (ICV) and so forth. The individual differences might affect the accuracy of classification model, and moreover linear regression might not solve this problem well. An intuitive solution is that we can find patterns that are hardly affected by the individual differences. We consider that network features are just this kind of patterns, because networks focus on the relationships but not on the cortical thickness values, and obviously the relationships are much more consistent across subjects than the values regarding individual differences. (2) Sensitivity to the disease: we think the network patterns are more sensitive to the changes caused by the diseases. Consider that a neuropathy may cause some organic changes such as gray matter atrophy or cortical thinning of a specific region. Such change might not be discovered by group analysis using raw cortical measures due to the individual differences, nonetheless it might be easily reflected in the variation of connection weight because the weight of edges connected to all other regions will be altered—those to regions with larger thickness will decrease and those to the regions with smaller thickness will increase. (3) The perspective of kernel-based learning: for kernel-based learning, the original features are first projected to high-dimensional space, in which data might be better organized or represented and prone to be separated by linear model. The proposed method for network construction is sort of like this kernel trick. The kernel used for establishing network is actually a variant of RBF kernel. Although these two kinds of kernel tricks differ, they have the same intention: a better organization and representation of the data. Further investigation for the kernel trick used in network construction will be done in the future work. Hybrid feature selection combines filter and wrapper methods: the filter method quickly removes most irrelevant features that have little discriminative power; while the wrapper method searches for the best feature subsets for classification. This hybrid feature selection method avoids the drawbacks of both methods to some extent. Because the filter method selects features according to the content information but not the classification performance, although each of selected features has a better discriminative power when considered independently, they might not be the optimal feature subset for a specific classification problem. On the other hand, although a wrapper method can optimize the feature subset for a specific problem, it is quite time-consuming; in addition, the overfitting problem is easily caused by high-dimensional data when using the wrapper. Thus, the thought of combining both methods is quite intuitive. When examining the results of six combinations of feature selection, we find that the filter hardly influences the classification performance if adequate features are retained for the wrapper, in other words, it is the wrapper that determines the results of hybrid feature selection. However, the filter is indispensable for accelerating the wrapper and reducing the probability of overfitting problem (because of the lower dimensional data). For the dataset in our experiment, local-learning-based feature selection (LLB) is much more stable and reliable than SVM-RFE, as shown in the results section. SVM-RFE iteratively eliminates features to obtain a feature subset that has the best cross validation accuracy, which might be prone to cause the overfitting problem; while the local-learning-based method aims to find the maximal margin in the deduced weight space. In addition, SVM-RFE is much shower than LLB because of the cross validation process. Thus, LLB is much more suitable for this AD dataset. However, we cannot say that LLB is better than SVM-RFE in all circumstances. In fact, we find in other studies that, if the dataset is not prone to be separated, LLB can hardly output any feature, and thus, a forward or backward search method such as SVM-RFE might be more suitable.    Acknowledgments     1. Brookmeyer, R., Johnson, E., Ziegler-Graham, K., Arrighi, H.: Forecasting the global burden of alzheimer’s disease. Alzheimer’s Dement. 3(3), 186–191 (2007)
Springer.tar//Springer//Springer\10.1007-s00138-012-0463-z.xml:Learning class-specific dictionaries for digit recognition from spherical surface of a 3D ball:Rotation invariance Digit ball recognition Sparse coding Circle detection Dictionary learning:  1 Introduction  Digit recognition has attracted extensive attentions and has been attacked from various directions in the computer vision community, for example, hand-written digit recognition and vehicle license plate detection and recognition. In the literature, almost all the digit recognition researches hold a common assumption that the digits to be recognized are placed on a plane surface. However, many cases in the real world are that the digits usually appear on non-plane surfaces, such as spherical surface of a ball. As a particular example of detecting and recognizing the digits on non-plane surfaces, in this paper, we introduce a digit ball detection and recognition system to detect and recognize 3D digit balls. Here, the so-called digit ball is a 3D ball which carries Arabic number on its spherical surface, as illustrated in Fig.&amp;nbsp; . The ultimate goal of our system is to recognize the digits appearing on the balls. Our system works for a practical application purpose, concisely, it is used as an interface of a man-machine interactive game and the recognition results can be immediately observed by human. Therefore, the particular application requires the system to keep on working without any recognition errors in a real-time manner.1 As well, along with the specific application, the work environment can be weakly controlled, i.e. using the diffuse reflection dark plane as the background, as illustrated in Fig.&amp;nbsp; is used, and the illumination reflection changes are allowed in the real-world applications. More importantly, the particular application requires the real-time performance of the system to achieve nearly 100&amp;nbsp;% accuracy in recognizing these balls, which makes the digit ball recognition an extremely challenging task. Besides its realization of this goal, our system has other practical significance. For example, it can be used for lotto&amp;amp;lottery monitoring and used as an automatic assistance (tracking and commentator) in Snooker broadcasting. As demonstrated above, despite its real-world significance, our system is confronted with several intrinsic problems to recognize the digit on the ball, such as the arbitrary rotations of the 3D balls and diverse viewpoints of the digit, which can be easily seen from Figs.&amp;nbsp;  and  . Obviously, the arbitrary rotation of the ball not only deforms the digit shape that is projected onto 2D image, but also poses us the difficulty to recognize the digit from various viewpoints. However, the rotation issue is just one intrinsic challenge for our system to recognize the digit on the ball, and the diverse viewpoints of the digit region will make it much more difficult. Therefore, for better recognition performance, how to determine a desirable viewpoint of the digit is another problem to attack. Moreover, as only one digital camera is used in our work, the global information of the digit balls cannot be captured and we can only obtain the biased digit information on the spherical surface. For this reason, most of digit recognition approaches will not work on this situation, even if they are well adapted to print or hand-written digit recognition, such as OCR&amp;nbsp;[ 20], geometric moments&amp;nbsp;[ 27], contour profile&amp;nbsp;[ 11], mathematical morphology&amp;nbsp;[ 6,  21– 23,  28] and so on&amp;nbsp;[ 12,  17]. As we cannot directly run recognition process on the arbitrarily-rotated digit balls, we ought to seek for other solutions. One possible way to tackle this problem is to use 3D reconstruction technique to map the 2D projection of the ball into closely hemispherical surface. Then, we can adopt the spherical correlation to implement pose estimation and the sequential recognition process. However, the spherical correlation usually needs to capture the global information in advance and the related algorithm is very time-consuming&amp;nbsp;[ 24,  25]. This explains that our aim is not to solve the general 3D object recognition problem, but to recognize the shape-deformed digit on the spherical surface. The last but not the least, other inevitable factors also have negative impact on the recognition performance, such as various illumination reflection changes, motion of the ball and so on. Focusing on the above issues, instead of adopting the 3D reconstruction technique, we turn to extracting some informative representations of the balls. Concretely, we first extract the digit region of the detected ball according to the geometrical property, then regularize the digit region, and finally transform the regularized digit region into some efficient and effective representations. In this paper, we study two rotation-invariant feature descriptors, i.e. spin image&amp;nbsp;[9, 13] and rotation-invariant feature transformation&amp;nbsp;[13]. These descriptors are generated from the regularized digit region of the ball, and are invariant to arbitrary rotations. However, as a conclusion of our experiments on the descriptors, the recognition accuracies on them are not satisfactory, even though the descriptors enable us to avoid any calibration process of the digit region to save computation time. In addition, we introduce another effective representation of the arbitrarily rotated digit balls—the polar image. The polar image not only provides us with the robustness to illumination reflection, as we can see from experiments, but also help us to alleviate the arbitrary rotation problem by merely dealing with the single-direction translation problem of the polar image. In our system, the polar image of each detected digit ball is calibrated according to some criterion. This calibration process needs additional computation time, but is very fast and brings out very decent recognition rate. Therefore, in our system, we use a coarse-to-fine recognition strategy for the final recognition: (1) using spin image to coarsely select several candidate digit labels, and (2) using the calibrated polar image for the final determination of the label. As for recognition, we exploit a dictionary learning-based method to learn a class-specific dictionary for each kind of the digit ball, and use the reconstruction error&amp;nbsp;[26] to recognize the novel balls. Beyond this method, we adopt a Bayesian-based multiple sampling strategy to boost the recognition performance. The experiments demonstrate that, with the help of this strategy, our system indeed achieves 100&amp;nbsp;% recognition accuracy for all the detected balls under the experiment environment. Actually, in the real-world application, there is no report on any misclassification of the balls yet, and any running errors involve no issues related to our system (the detection and recognition process). Besides, another critical step is digit ball detection, whose quality will directly affect the recognition performance. We consider the problem of detecting several 3D balls in one image (dubbed multi-ball detection), and develop a novel method to tackle this problem. As well, from experiments, we can see the effectiveness of our detection method. Even when the balls are densely placed in the image, our method can still detect and segment the balls out of the image accurately. To the best of our knowledge, it is the first work to address such kind of multi-ball detection and recognition task in the literature. To summarize, the contributions of our work include the following:      We propose to use the intensity-domain spin image&amp;nbsp;[13] and the polar image to represent the digit ball, which can eliminate the difficulties caused by arbitrary rotation.   We adopt a coarse-to-fine recognition strategy, which is a two-stage process: using spin image for coarse prediction and using the polar image for the final determination of the label.   We advocate learning a class-specific sub-dictionary for each digit ball (its polar image), and using them with the Bayesian-based multiple sampling strategy for the final classification. Experimental results demonstrate our system can indeed achieve 100&amp;nbsp;% recognition accuracy of all the detected digit balls. We develop a novel method for multi-ball detection task, and demonstrate its effectiveness through experiments even when the balls are very densely placed in the image. We propose to use the intensity-domain spin image&amp;nbsp;[13] and the polar image to represent the digit ball, which can eliminate the difficulties caused by arbitrary rotation. We adopt a coarse-to-fine recognition strategy, which is a two-stage process: using spin image for coarse prediction and using the polar image for the final determination of the label. We advocate learning a class-specific sub-dictionary for each digit ball (its polar image), and using them with the Bayesian-based multiple sampling strategy for the final classification. Experimental results demonstrate our system can indeed achieve 100&amp;nbsp;% recognition accuracy of all the detected digit balls. As our system works for particular application under the weakly controlled environment, the prior knowledge of the size of the ball is given, i.e. its spherical radius  , and the distance between the camera and the diffuse reflection dark plane is fixed. In our system, the industrial camera is calibrated based on Zhang’s method&amp;nbsp;[ 30]. The system consists of two inseparable parts, one is the training part, which aims to learn the sophisticated class-specific dictionaries for the sequential recognition stage, i.e. the second part. Both of the two parts mainly share three crucial steps: (1) digit ball detection; (2) digit region detection, regularization and calibration; and (3) feature extraction and representation. Apart from the three common steps, the training part also has the most important step, i.e. dictionary learning. The overall flowchart of our system is plotted in Fig.&amp;nbsp; . In the rest of the paper, we will elaborate these four important steps and validate our system through experiments. Finally, we conclude our paper with remarks.    2 Digit ball detection  Actually, there are two different purposes related to digit ball detection in our system, one is used for learning class-specific dictionaries in the training part, and the other for recognition, i.e. the multi-ball detection. Each of the training images only contains one digit ball, while the images in the recognition usually have several balls to detect, and this multi-ball detection task is much more difficult. We elaborate the two detection approaches in this section. As illustrated in Sect.&amp;nbsp;1, our system works in a weakly controlled environment, hence the carefully chosen diffuse reflection dark plane makes it easy to detect the ball on each training image. Moreover, as a result of that the distance of the camera and the plane is fixed without any shifting and the spherical radius  of the balls is given, we can simply use an enclosing circle fitting method to detect the ball. In detail, we use Canny algorithm to detect the edges of the image with binarization process, refine the binary image by omitting the isolated point set, draw a minimum circle to enclose the existing points, and finally estimate the center of the circle (the detected ball). Empirically, we also try Hough transform-based method to detect the ball, however, we find the fitting enclosing circle method is more efficient and more accurate. A testing image usually contains several digit balls for detection and recognition, therefore, the detection method used in the training part is no longer applicable. To address this problem, we develop a novel method that is adapted to multi-ball detection. Given a novel gray-scale image   with size   such as the illustrative one in recognition part of Fig.&amp;nbsp; , similar to the enclosing circle fitting method in the training part, we first run some preprocessing steps on the image. Concretely, we use Canny operator to detect the edges, perform binarization on the derived image, and refine the result by omitting the isolated points. The derived result is stored in   with the same size of  , as demonstrated by Fig.&amp;nbsp; a. Then, we create a blank image with all zero-value pixels, or more concisely, a matrix   for the sequential intermediary storage. After these preprocessing steps, by treating each white pixel of  as a center, we draw an -radius solid round and add it to  with the appropriate position defined by the center (the same position in ). Here,  is the radius of the projected ball on the image, and is easy to derive in terms of the spherical radius  and the distance between the diffuse reflection dark plane and the camera. Note any filled value of the solid round will produce the same result, owing to the sequential normalization process. Finally, we normalize the values of  to form a 2D histogram image, proportionally limiting all the values in a specific range, say 0–255, as demonstrated in Fig.&amp;nbsp;b. This normalization process is necessary, because we need a threshold  to locate the detected centers of the ball. Specially, we can detect the local peak values in  that are greater than , and classify them into the centers of the detected balls. It is worth noting that the detected peak-value elements can cluster together, in other words, we will not necessarily obtain the unique center of one ball, but several pseudo ones around the real center. For better understanding of this, we project the 2D histogram onto -plane (the Cartesian Coordinate System), as demonstrated in Fig.&amp;nbsp;c. From this figure, we can see there are always a cluster peak values around the real center of each ball, thus it is not advisable to treat any one as the detected center. But fortunately, it is not a serious situation, and we can merely use the average location of these aggregated elements as the center of each detected ball. Empirically, we find this simple trick is very effective and efficient. The overall procedure of our proposed multi-ball detection method is summarized in Algorithm&amp;nbsp;1. Running the proposed approach, we obtain the final detected balls demonstrated in Fig.&amp;nbsp; d. Concretely, running step 1–4, the algorithm generates the refined binary image   as illuminated by Fig.&amp;nbsp; a and a blank image , both of  and  are the same in size with the original image ; running step 5–14, we can derive the 2D histogram image , as demonstrated by Fig.&amp;nbsp; b and c; after step 15, the algorithm determines the centers of the detected balls, and outputs them as shown in Fig.&amp;nbsp;d). &amp;nbsp;    3 Digit region detection and regularization  As demonstrated previously, the digit projected on the image is badly deformed in shape, because it is captured from various viewpoints with arbitrary rotations of the 3D ball, as illustrated in Fig.&amp;nbsp;. Thus, we cannot directly implement recognition process on these detected balls. For this reason, we choose to regularize the digit to a standard position first, i.e. transforming the digit to a frontal pose to “look at the camera”. Before that we have to detect the informative region where the digit appears, i.e. the digit region. This step is of crucial significance to the ultimate target, and can directly determine the recognition performance. In this section, we present the digit region detection and regularization process. A convenience brought by the real-world digit ball is that there is a circumcircle that encapsulates the digit on the spherical surface, and always at least one circular region is fully visible from top. Such a circumcircle is projected into an ellipse on the image. Therefore, we can exploit this kind of valuable information by detecting the small ellipse first. Our system adopts Hough transform to detect ellipse-like regions, and then judge whether the detected ellipses are acceptable according to their estimated size. In detail, given the detected ellipse-like region with its center, we can well estimate its size (the length of semi-major axis and semi-minor axis) and eccentricity in terms of some geometrical property of ellipse. Then, we compare these estimated results with the real value of these attributes, which are the prior knowledge, and finally determine whether the detected ellipse-like region is the desirable one. As illustrated by Fig.&amp;nbsp;  for better understanding this process, we can easily see that, no matter where the small circle is located on the sphere, the radius of the small circle on the sphere is just equal to the semi-major axis of the projected ellipse. Through some simple derivations, we have the following relation:       where   and   are the radius of the small circle on the sphere and the spherical radius, respectively, both of which are pre-given.   is the distance of the center of the sphere and the center of the ellipse, and   is the length of the semi-minor axis of the ellipse. Equation&amp;nbsp; 1 provides us with the metric to judge whether the detected region is a desirable elliptic region. Figure&amp;nbsp; a displays some ellipse detection results. From this figure, we can see there will be several reasonable ellipses and probably all of them are qualified for the sequential application. But one detected ellipse is enough, and we ought to select the “best” one. Concretely, we simply select the one with the smallest eccentricity for the least deformation of the digit. To summarize, the overall procedure of digit region detection is listed as below:        use Canny operator to detect the edges followed by an adaptive binarization process;         &amp;nbsp;           detect all the contours and refine the result by discarding or merging the noisy contours;         &amp;nbsp;           fit any contours to an ellipse and derive its location and size;         &amp;nbsp;           assess the detected ellipses to judge whether it is a useful digit region;         &amp;nbsp;           select the one with smallest eccentricity as the final digit region to represent the detected ball.         &amp;nbsp; use Canny operator to detect the edges followed by an adaptive binarization process; detect all the contours and refine the result by discarding or merging the noisy contours; fit any contours to an ellipse and derive its location and size; assess the detected ellipses to judge whether it is a useful digit region; select the one with smallest eccentricity as the final digit region to represent the detected ball. To obtain decent recognition performance, we should consider how to transform the elliptical digit region into a standard scale. That is to say, when we obtain the best ellipse region, the next step is to regularize this region via unwarping the ellipse to a circle. Therefore, the digit will be posed on a desired position or on the obverse side. In this way, the digit can be reconstructed to a better position with the least shape distortion. Figure&amp;nbsp;b illustrates such results of the detected elliptical digit regions depicted in Fig.&amp;nbsp;a. Furthermore, we relocate the digit region and use Canny edge detector with binarization process to derive a more accurate digit region, as demonstrated in Fig.&amp;nbsp;c, d. Note that even though the distinct illumination reflections are anticipated to have a negative impact on the recognition performance as displayed by Fig.&amp;nbsp;c, these illumination reflections disappear in the binary image as illustrated in Fig.&amp;nbsp;d. This demonstrates the robustness of our system to illumination reflection changes.    4 Digit ball representation 
Springer.tar//Springer//Springer\10.1007-s00138-012-0464-y.xml:Mind reading with regularized multinomial logistic regression:Elastic net regularization Decoding Logistic regression Classification Natural stimulus Magnetoencephalography:  1 Introduction  Functional neuroimaging relies on statistical inference for explaining relations between measured brain activity and an experimental paradigm. During the recent years, supervised classification has become increasingly important methodology in analyzing functional neuroimaging data [49] within functional magnetic resonance imaging (fMRI) [41] as well as in electroencephalography (EEG) and magnetoencephalography (MEG) (for reviews, see, e.g., [32, 37]) with earliest papers tracing back to early 90’s [24, 27, 31]. The significance of the topic is witnessed, for example, by special issues dedicated to brain decoding in technical [49] and more applied journals [16]. In brain research, pattern classifiers can be used to answer the questions  information about a variable of interest (pattern discrimination),  is the information (pattern localization) and  is that information encoded (pattern characterization) as explained in more detail in [37] and [35]. A major benefit of this pattern classification approach over the more traditional analysis methods is that it, in principle, allows the identification of the set of data patterns, which are diagnostic for engagement of a particular task [39]. Important results regarding, e.g., face processing [15] and early visual processing [17, 22] in human brain have been obtained using the technique. The uses of pattern classifiers on EEG and MEG data have concentrated around applications in the brain computer interfaces (BCIs) for which there is a large body of literature [3, 4]. The majority of studies in BCIs have focused on EEG with relatively few channels. For example, Van Gerven et al. [50] used regularized logistic regression to classify imagined movements of right or left hand based on EEG data from 16 channels. Perhaps more relevant to the present work is [48], where regularized logistic regression was applied for two different problems with 64-channel EEG data:(1) 2-category self-paced finger tapping task from the BCI Competition 2003 and (2) a P300 speller system task from the BCI competition III, which is a 36-category classification problem. In the BCI-IV competition,1 one task was the classification of the direction of wrist movements based on 10-channel MEG data [46]. Surprisingly, only one of four entries to the competition clearly exceeded the chance level in classification accuracy. Other studies focusing on the decoding of MEG data include Zhdanov et al. [53], who applied regularized linear discriminant analysis to MEG signals recorded while subjects were presented with images from two different categories (faces and houses). Chan et al. [6] applied a support vector machine (SVM) classifier to decode the data that was recorded using simultaneous scalp EEG and MEG while the subjects were performing auditory and visual versions of a language task. Rieger et al. [43] applied an SVM to test whether it is possible to predict the recognition of briefly presented natural scenes from single trial MEG-recordings of brain activity and to investigate the properties of the brain activity that is predictive of later recognition. Besserve et al. [2] applied an SVM to classify between MEG data recorded during a visuomotor task and resting condition. In this paper, we propose a method for supervised classification of MEG data. Our method is based on multinomial logistic regression with elastic net penalty [9, 54] and it was the most accurate in the “Mind Reading from MEG” challenge organized in conjunction with the International Conference on Artificial Neural Networks (ICANN 2011) in June 2011 [19]. The task in the competition was to train a classifier for predicting the type of a movie-clip being shown to the test subject based on MEG recordings. In more detail, the subject was viewing video stimuli from five different categories (football match, two different feature films, recording of natural scenery, and artificial stimulus) while MEG signal was recorded. The MEG signal recorded from 204 channels was cut into non-overlapping one-second epochs by the competition organizers. These epochs along with the corresponding category labels were released to the participants as training samples, and the task was to build a classifier that can predict categories of unseen samples. The classification performance was assessed by the competition organizers based on the independent test set, which was hidden from participants during the competition. Our method achieved 68&amp;nbsp;% accuracy on the test samples and was a clear winner among ten methods participating to the competition. In addition to ICANN MEG data, we highlight the good performance of our method with MEG data from the BCI-IV competition where the experimental paradigm is much simpler than with the ICANN MEG data. There are certain key differences between typical BCI and our “Mind reading from MEG” decoding applications as laid out by Zhdanov et al. [53]. Perhaps most importantly, the dimension of input data is much higher in MEG than that of EEG typically used in BCI applications, the number of samples is much smaller, and the behavioral paradigm is much more complex here. Moreover, all the above cited uses of supervised classifiers in MEG [2, 6, 43, 53] differ from the prediction task in the ICANN competition in that they were based on strictly controlled behavioral paradigm and the knowledge about the paradigm was often applied in the feature extraction. Moreover, all except Chan et al. [6] considered only a binary (two-category) classification problem. The elastic net has been used in neuroimaging with fMRI data sets in the context of classification [10, 41] and regression [5] problems, but not with MEG data and, in the classification setting, not in a naturalistic behavioral paradigm like movie watching studied in this paper. As our main contribution, we propose using a linear classifier for classification of MEG data and illustrate its efficiency using simple features such as the mean and standard deviation of the measurement signals. Despite the simplicity of our approach, the experimental results confirm an excellent performance in cases with complex behavioral paradigms (which movie is shown) as well as in simple setups (which direction the hand is moving). The rest of the paper is organized as follows: After introducing the data and the acquisition setup, we will describe the details of the proposed method in Sect.&amp;nbsp;2. In Sect.&amp;nbsp;3 we present results of applying the method for the ICANN data set with basic set of mean and standard deviation features (Sect.&amp;nbsp;3.1), with a larger set of statistical quantities as features (Sect.&amp;nbsp;3.2), and with frequency band energy features (Sect.&amp;nbsp;3.3). Moreover, we consider the classification performance for a modified version of the ICANN challenge problem in Sect.&amp;nbsp;3.4, and experiment with data from an earlier BCI-IV MEG decoding challenge [46] in Sect.&amp;nbsp;3.5. Finally, Sect.&amp;nbsp;4 discusses the results and concludes the paper.    2 Methods  In the results section we study the efficiency of our method using the data released in the mind reading competition. 2 The data set consists of within-subject MEG signals recorded from a single test person while watching five different video stimuli without audio:          Animated shapes or text         &amp;nbsp;             Nature documentary clips         &amp;nbsp;             Soccer match clips         &amp;nbsp;             Part from the comedy series “Mr. Bean”         &amp;nbsp;             Part from a Chaplin movie         &amp;nbsp;      The provided measurements consist of 204 gradiometer channels, and the length of each individual epoch is one second and the sampling rate is 200&amp;nbsp;Hz. Moreover, the five band-pass filtered versions of the signal are also included in the measurement data, with bands centered on the frequencies of 2, 5, 10, 20, and 35&amp;nbsp;Hz. 3 Animated shapes or text Nature documentary clips Soccer match clips Part from the comedy series “Mr. Bean” Part from a Chaplin movie The MEG measurements were recorded on two separate days such that the same set of video stimuli was shown to a test person on both days. Stimuli labeled as either Artificial, Nature, or Football (short clips) were presented as randomly ordered sequences of length 6–26&amp;nbsp;s with a 5&amp;nbsp;s rest period between the clips, while Bean and Chaplin (movies) were presented in two consecutive clips of approximately 10&amp;nbsp;min. In the competition data, the measurements are cut into 1-s epochs that are further divided into training and testing such that the training data with known class labels contains 677 epochs of first-day data and 50 epochs of second-day data while the secret test data contains 653 epochs of second-day data only. Note that the ground truth class labels for the test recordings have been released after the end of the competition. The data is provided in a randomized order, and the complete signal cannot be reconstructed based on the individual signal epochs. During the competition, the competitors were told that the secret test data comes from the second-day measurements only and that—similar to the training data—it is approximately class-balanced. The division between training and test data was elaborate. In particular, 33&amp;nbsp;% of the test samples consist of recording during stimuli not seen in the training phase in order to test the ability of the classifiers to generalize to new stimuli. A more detailed description of the data can be found in the challenge report by Klami et al. [25]. Our method follows the strategy of feature extraction followed by a multinomial linear logistic regression classifier. We use elastic net penalization in estimating the model parameters, which results in a sparse model and works as an embedded feature selector. The structure of our approach is illustrated in the block diagram of Fig.&amp;nbsp; . More specifically, the training and error estimation procedures consist of two nested cross-validation (CV) loops as illustrated in Algorithm&amp;nbsp;1. The outer loop is used for estimating the performance for the unlabeled test data using, e.g.,  splits of the training data, while the inner (e.g., -fold) CV loop is used for selection of classifier parameter  (see Sect.&amp;nbsp;2.4). The high computational complexity of simultaneous error estimation and parameter selection can be clearly seen from the pseudo code. In order to speed up the development, our method uses parallel validation spread over numerous processors as also described in Sect.&amp;nbsp; 2.5. A Matlab implementation of our method is available for download. 4 There were 204 (channels)&amp;nbsp; &amp;nbsp;200 (time points,  )&amp;nbsp;=&amp;nbsp;40,800 measurements per one exemplar and, thus, the possible number of features is much larger than the number of training samples. We approach this problem by first deriving a pool of simple summary features, and then feed these to the joint feature selection and classification. The full set of features consists of 11 simple statistical quantities listed in Table&amp;nbsp; 1. Some of the features are proposed earlier in the literature (e.g., the mean [ 29]), but most were chosen due to their simplicity and widespread use in statistics. The ICANN MEG challenge data includes a nonzero DC component, and thus many epochs exhibit either increasing or decreasing linear trend. As the significance of these random fluctuations for predicting the brain activity is unclear, we decided to calculate the features using also a detrended version of the time series in addition to the raw measurement. Moreover, this is in coherence with the usual preprocessing of several MEG studies, which use a bandpass filter to remove low frequency components, e.g., below 5&amp;nbsp;Hz. However, detrending does not have boundary problems and is thus favorable especially with short segments. For example, a frequency selective finite impulse response (FIR) filter with  taps requires  past samples in the delay line, which are not available in the beginning of the signal. Detrending simply fits a slope into the time series and calculates the residual. More specifically, denote one epoch of the MEG signal from the  th channel by  , for  . Then the linearly detrended signal   is defined by:       where the slope   and intercept   are obtained by a least squares fit minimizing  . Note that the value 100.5 subtracted from   is selected as the midpoint of time indices  . With this particular value the intercept   becomes equal to the sample mean. With the above notation, we can define our pool of features consisting of the following set of statistics , where each element  contains the specific feature values calculated from the measurement signals of all the channels. The extracted features are listed in Table&amp;nbsp;1. With this selection, the total number of features extracted from the ICANN MEG challenge data becomes  if using only the raw measurements; or  if using also the five bandpass filtered channels. With only a few hundred training samples, the problem is clearly ill-posed with a large set of highly correlated predictors. Thus, a natural direction is to seek for an efficient feature selection method, which we will consider next. After extracting multiple candidate features for each of the 204 channels, we still have a large number of features compared to the number of training samples making the prediction problem ill-posed. Further, we are not sure, which features work the best or even turn out useful in our case. To cope with the ambiguity, we use a  model (also known as the ) with  regularization [54]. In addition to designing a classifier, the elastic net includes a sparsity enforcing regularization term and thus works as an embedded feature selector that automatically selects the set of relevant features and channels from the pool of candidates. More specifically, the symmetric multinomial logistic regression models the conditional probability of class   given the  -dimensional feature vector   as       where   and   are the coefficients of the model [ 14]. For this model to be valid we have to assume mixture or  -conditional sampling [ 1] or—in a more relaxed form—that the class frequencies are (approximately) the same in the training and test data. Despite of the apparent nonlinearity of Eq.&amp;nbsp;( 2), the resulting classifier is linear and the class   of a test sample   is selected as  . In the elastic net framework, the training of the logistic regression model consists of estimating the unknown parameters   by maximizing the penalized log-likelihood       where   denotes the true class of the  th training sample   ( ). The regularization term is a combination of the   and   norms of the coefficient vectors  , and the weights for both types of norms are determined by the mixing parameter  . The extent of regularization is controlled by the second regularization parameter  . The role of parameter  is to determine the type of regularization. When , the  norm vanishes and the purely  regularized result can be expected to work well in cases, where there are several noisy and mutually correlating features. This is because penalizing the  norm brings the coefficients of the correlating features closer to each other resulting in noise reduction in form of averaging. On the other hand, when , the  norm disappears, which produces a generalized version of the  () [47]. The LASSO is widely used in regression problems and known for its ability to produce sparse solutions where only a few of the coefficients are non-zero, and this property carries over to the elastic net (except for the case ). Thus, both the LASSO and the elastic net can be efficiently used as an implicit feature selectors. The role of the parameter  is to control the strength of the regularization effect: the larger the value of , the heavier the regularization. For small values of , the solution is close to the maximum likelihood solution, while large values of  allow only restricted solutions and push the coefficients towards zero. In practice, the values of both regularization parameters  and  are determined by CV, i.e., all combinations over a fixed grid are tested and the CV errors are compared. Note that this CV round is separate from that of Sect.&amp;nbsp;2.5, nested inside the error estimation loop of the entire solution. In the case of Eq.&amp;nbsp;(2), the logit model is symmetric unlike the traditional multinomial logistic regression model, where one category is selected as the base category against which all the other categories are compared (see, e.g., Kleinbaum and Klein [26]). Note that while the coefficients of the model (2) are not identifiable without constraints, the penalty term in Eq.&amp;nbsp;(3) solves the ambiguity in a natural way [9]. This symmetry is useful here because regression coefficients are easier to interpret in the classification context. The linear discriminant functions of the classifier have the same parametric form  for every class . Thus, the larger the , the more important the feature  is for the discrimination assuming the features are normalized to an equal variance (note that this is different from normalizing the original data). The traditional asymmetric model would lead to discriminant functions  for  and , i.e., parametric form of the discriminant functions would differ between the base class  and the other classes [52, Page 161]. Elastic net regularized generalized linear models including logistic regression models can be efficiently fit using a coordinate descent algorithm proposed by Friedman et al. [9]. There is also a MATLAB implementation available.5 An important aspect for the classifier design is the error assessment. This was challenging in the mind reading competition, because only a small amount (50 samples) of the test data set was released with the ground truth. There is reason to believe that the characteristics of the data from the 2&amp;nbsp;days can be different. Additionally, we obviously wanted to exploit the second-day data also for training the model. Since we wanted to maximize our accuracy on the secret test data, we concentrated our error estimation to the 50 second-day samples with annotation. A natural cross-validation error estimation technique would be the leave-one-out error estimator for the second-day data. More specifically, we would train with all the first-day data and 49 samples of the second-day data and test with the remaining second-day sample. This way there would be 50 test cases whose mean would be the leave-one-out error estimate. However, we were concerned about the small number of test cases and the high variance of the CV error estimator (see, e.g., [8] and references thereof), and decided to consider alternative divisions of the second-day data to training and testing. As a result, we randomly divided the 50 test day samples into two parts of 25 samples. The first set of 25 samples was used for training, and the other for performance assessment.6 Since the division can be done in \({50\atopwithdelims ()25}&amp;gt;10^{14}\) ways, we have more than enough test cases for estimating the error distribution. This approach gives slightly too pessimistic error estimates because only half of the second-day data is used for training (as opposed to 98&amp;nbsp;% with leave-one-out), but has smaller variance due to larger number of test cases. Moreover, the pessimistic bias is not a problem, because we are primarily interested in comparing feature sets during method development rather than actually assessing the absolute prediction error. The imbalance in the number of samples between the first- and second-day data is quite significant, because with the above division the training set contains more than 25 times more first-day data than second-day data. Since we wanted to emphasize the role of the second-day data, we assigned a higher cost to their misclassification. After experimentation, the misclassification cost for all second-day samples was set threefold the cost of first-day samples. The remaining problem in estimating the error distribution is the computational load. One run of training the classifier with CV of the parameters takes typically 10–30&amp;nbsp;min. If, for example, we want to test with 100 test set splits, we would be finished after a day or two. For method development and for testing different features this is definitely too slow. However, the error estimation can be easily parallelized; simply by testing each division of the test data on a different processor. For example, in our case we had access to a grid computing environment with approximately 1,000 processors, and we were able to obtain an accurate error estimate in a matter of minutes instead of hours or days. The elastic net classifier has two parameters affecting the solution:   and   of Eq.&amp;nbsp;( 3), and they can be selected using cross-validation. However, it turns out that the obvious method of selecting both   and   together in a two-dimensional grid (i.e., inside the outer CV loop of Algorithm&amp;nbsp;1) results in a worse validation and test performance than Algorithm&amp;nbsp;1, where the parameter   is selected outside both CV loops. More specifically, different values are tested for the regularization parameter   while   is automatically selected by fivefold CV. The performance surface for the training data (using cross-validation) and the secret test data is shown in Fig.&amp;nbsp; a, b, respectively. Figure&amp;nbsp;a, b also describe the robustness of the model to the choice of parameters. It can be seen that the plots are very similar in shape, and the top is flat for a wide range of  and . In particular, the algorithm is insensitive to the selection of : For any choice of , there exists a value of , which is within 1.7&amp;nbsp;% points from the absolute optimum for the secret test data. Thus, the performance is insensitive to slightly erroneous parameters settings. In Sect.&amp;nbsp; 3 we cross-validated also the parameter   over the grid   for the cases listed in Table&amp;nbsp; 2. In all, the optimal values tend to be in the upper end (close to   penalty) and the value used in our original submission ( ) coincides with the median of Table&amp;nbsp; 2 and can be used as a good compromise. Different CV rounds give slightly different error surfaces, and improper selection of   is often masked by random variation of the CV folds. Thus, there is a lot of variation on the optimal value of   suggested by the CV, and the full optimization may not be worth the extra computation. A separate selection of   parameter can also be interpreted as a separate selection of model family (i.e.,   penalized LR,   penalized LR, etc...), which is typically done less frequently than the model parameter optimization.    3 Results
Springer.tar//Springer//Springer\10.1007-s00138-012-0465-x.xml:An IR and visible image sequence automatic registration method based on optical flow:IR image sequence Optical flow Visible image sequence Image sequences registration:  1 Introduction  As sensor designs improve and the cost of image sensors decreases, multi-camera applications attract more and more attention. IR and visible cameras are two important examples of imaging sensors. They are complementary and can make 24-h visual surveillance possible. Registration between IR and visible images is required for subsequent fusion and cooperative processing. This paper describes a new algorithm for image registration based on optical flow estimated at the pixel level. It is difficult to detect and match common features in the image sequences captured by two different sensors. Depending on the features used, the existing IR and visible registration methods can be classified into two kinds: methods based on static image features and methods based on motion features. The static image features include grey level [1, 2], grey level change features (edge [3, 4], gradient [5], edge orientation [6]), and geometrical features (corners [7], line, triangle, rectangle). The registration of static image features can be very difficult because of the great difference between IR images and visible light images. The effectiveness of registration based on static image features depends heavily on the content of images. For methods based on grey level changes or geometrical features, both images should have abundant and well-defined edge or gradient features. For methods based on grey levels, the intensity mapping relationship between IR and visible should be global [8]. Image sequences can provide more information than single images. In particular, it is possible to obtain motion features, which can be used for sequence registration [9, 10]. Suitable motion features include tracks and the silhouettes of moving objects [11, 12]. Some methods used both of these motion features [13, 14]. There are many factors which influence the silhouette computation, including noise, shadows, moving background objects, occlusions and object partial appearance caused by limited field of view. The contrast between an object and the background may be very different for the two sensors, and a moving object may be visible in one sensor but not in the other. For these reasons, it is difficult to detect accurately the silhouette of a moving object in two different image sequences. In the case of object tracking, the centroids and the top points of moving objects are often used as the matching points. However, the positions of the centroid and the top point are strongly influenced by any inaccuracies in the estimated silhouette. The computation of the motion information at a single pixel does not require any segmentation of a moving object from the background. All that is required are the changes in value of the given pixel and its neighbors. So in this paper we use optical flow at the image pixel to carry out IR–visible registration. Just as for other registration methods based on motion information, moving objects observed by both IR and visible light videos are required. The rest of this paper is organized as follows: Sect. 2 introduces the algorithm. The proposed normalized optical flow sequence (NOFS) feature is described in Sect. 3. The similarity measurement of optical flow sequence feature and correspondence selection is discussed in Sect. 4. Outlier removal and estimation of a global image transformation are described in Sect. 5. Section 6 includes the experimental results on several real and simulated datasets. Experimental comparison with other method and related discussion is included in Sect. 7. Finally, we present our conclusion and suggest future work in Sect. 8.    2 Algorithm overview  The proposed approach, as depicted by Fig.&amp;nbsp;  , contains three main processing steps. A NOFS is constructed for each pixel in the two image sequences. Then corresponding pixels are found using a similarity measurement for pairs of NOFS. Finally cascaded random sample consensus (RANSAC) is adopted to remove outliers; least-square method (LSM) and Levenberg–Marquardt (LM) [ 15,  16] are used to estimate the global image transformation, which is modeled using an affine transformation. The details of the three steps are described as follows.    3 Normalized optical flow sequence extraction and description  Assume   is the orientation and   is the speed for the optical flow vector at pixel   in the  th frame. Then two time sequences, the motion orientation sequence   and the speed sequence  , are constructed for pixel  , described by Eqs.&amp;nbsp;( 1) and ( 2), where   is the label of the image sequence,   and   is the length of the time sequence.                         As orientation does not have rotation invariance and speed does not have scale invariance, these two sequences cannot be used directly for correspondence selection. It is necessary to normalize them. In addition, the filtering of inaccurate entries from both sequences can increase the accuracy of the image correspondences and speedup the algorithm. Optical flow calculation and filtration, motion sequence filtration, and the normalization of the two sequences (1) and (2) are presented below. The iterative Lucas-Kanade pyramid optical flow algorithm [ 17] is adopted to compute optical flow. By using a multi-resolution pyramid, this algorithm yields accurate results, even for fast moving objects, for which other methods fail. However, the multi-resolution algorithm has many problems that all optical flow methods meet with, including noise sensitivity, imprecise motion edge extraction, optical flow vector shifts in low contrast regions, and occlusions. A filtering procedure is used to remove noise and inaccurate optical flow.    To reduce the influence of noise, the Gaussian filter is used to smooth the input images before optical flow calculation. A  mask is used.   The vector shift problem in low contrast regions and the problem of imprecise motion edge extraction are overcome using a motion detection method based on a GMM background model. Optical flow vectors are removed if they are in region classed as background by the motion detection algorithm. Any optical flow vectors with small magnitudes (\({&amp;lt;}0.1\)&amp;nbsp;pixels) are set to zero.   The reprojection grey level difference is used to remove inaccurate optical flow vectors arising from occlusions. Assume  and  are successive input images, and the optical flow vector between them is . Let  be pixels in image , let  be its grey level, and let  be the optical flow vector at . Then we can get a new pixel  with grey level . For all pixels in image , a new image  can be calculated. If the pixel  does not have integral coordinates, the thrice spline interpolation method is used to estimate the grey level values at the pixels in . Comparing the grey level values of image  with those of , if the average grey level error in a pixel neighborhood is too large, the corresponding optical flow vector is regarded as inaccurate and its speed is set to zero. The grey level is normalized such that the maximum grey level is 1, and the threshold here for comparing pixel values in  and  is 0.1.   Optical flow vectors near the image boundary are apt to be inaccurate. To reduce this effect, optical flow vectors are calculated inside a one-pixel width image boundary. To reduce the influence of noise, the Gaussian filter is used to smooth the input images before optical flow calculation. A  mask is used. The vector shift problem in low contrast regions and the problem of imprecise motion edge extraction are overcome using a motion detection method based on a GMM background model. Optical flow vectors are removed if they are in region classed as background by the motion detection algorithm. Any optical flow vectors with small magnitudes (\({&amp;lt;}0.1\)&amp;nbsp;pixels) are set to zero. The reprojection grey level difference is used to remove inaccurate optical flow vectors arising from occlusions. Assume  and  are successive input images, and the optical flow vector between them is . Let  be pixels in image , let  be its grey level, and let  be the optical flow vector at . Then we can get a new pixel  with grey level . For all pixels in image , a new image  can be calculated. If the pixel  does not have integral coordinates, the thrice spline interpolation method is used to estimate the grey level values at the pixels in . Comparing the grey level values of image  with those of , if the average grey level error in a pixel neighborhood is too large, the corresponding optical flow vector is regarded as inaccurate and its speed is set to zero. The grey level is normalized such that the maximum grey level is 1, and the threshold here for comparing pixel values in  and  is 0.1. Optical flow vectors near the image boundary are apt to be inaccurate. To reduce this effect, optical flow vectors are calculated inside a one-pixel width image boundary. Not all optical flow time sequences are useful for finding corresponding pixels. Sequences for which the optical flow is always close to zero or which frequently have a non-zero optical flow vector should not be used. These optical flow time sequences can be found using Eq.&amp;nbsp;( 4), in which 0 means the optical flow sequence is discarded and 1 means that is retained.   is the accumulated time for which a non-zero optical flow vector was detected at pixel   in image sequence  , as described by Eq.&amp;nbsp;( 3).   is the maximum accumulated time of image pixel in sequence  . The thresholds   and   are set to 0.05 and 0.95, respectively. Orientation has scale and translation invariance, but it does not have rotational invariance. For this reason, the orientation cannot be used for image registration when one image is rotated relative to the other. To obtain rotational invariance, each orientation sequence is subject to the following transformation. Eight quantification codes 1–8 are used to describe the motion orientations with non-zero speeds. Each code denotes a  interval. The coding procedure is described by Eq.&amp;nbsp;(5). Code 0 (zero motion status) is used to describe the motion orientation when the magnitude of velocity is zero. The quantification and coding of  yield the sequence  as shown in Eq.&amp;nbsp;(6). There are still other orientation quantization ways, which will be discussed in Sect. 7.2. The main motion direction of  , marked as  , is selected by the maximum element in its 1–8 code histogram. Then   is turned to the code 1 direction by a clockwise rotation, and other seven directions are rotated by the same amount, as shown in Eq.&amp;nbsp;( 7). The normalized motion orientation sequences   described as Eq.&amp;nbsp;( 8) all have the same main direction, and so they can be compared with the orientation sequences obtained from other image sequences. The speed has translation and rotation invariance, but it does not have scale invariance. The sum of all the magnitudes in each sequence   is used as a normalization factor. The normalized speed sequence is  , as defined by Eq.&amp;nbsp;( 9).    4 Similarity measurement and correspondence selection  The proposed fast similarity measurement and correspondence selection method is presented in this section. It is assumed that  has a validated sequence of optical flow vectors, as defined by Eq.&amp;nbsp;(4). The problem is to find the pixel corresponding to  in a second image sequence. The corresponding pixel is defined as the pixel in the second image sequence, whose orientation sequence and speed sequence are most similar to those of pixel ’s. However, comparing orientation and speed sequences with all the sequences obtained from the second image is time-consuming. Here three steps based on three kinds of information, namely motion accumulated time, motion orientation histogram distance and optical flow sequence distance, are designed to eliminate unlikely candidates from corresponding pixels, as described below. For corresponding point pairs, their total motion accumulated time should be similar.  is calculated as described in Sect.3.2. The pixels in the second image sequence, whose motion accumulated time is in the interval , are selected as the candidate corresponding pixels to , and form a candidate set . For corresponding point pairs, their motion orientation statistic should be similar. Therefore an orientation histogram distance is adopted to further select the candidate corresponding pixels to  from , and the selected candidate corresponding pixels form a new candidate set . The motion orientation histogram distance is defined as the Manhattan distance between histogram vectors of two normalized motion orientation sequences. The pixels in , with the less than  histogram distance with the pixel ’s orientation histogram, are selected as the candidates for corresponding pixels to .  is the maximum limit distance threshold. It should not be too small to uncover the right correspondences, so the value of  is set as the bigger one of  and  is the least histogram distance from . Because the maximum histogram distance is 2, too large maximum limit threshold is unreasonable, the value of  is further set as the smaller one of  and 0.9. Different time sequences can have the same orientation histogram. Therefore the optical flow sequence distances are further used to select correspondences. There are two kinds of optical flow sequence distances, orientation sequence distance and speed sequence distance. The speed sequence distance is defined as the Manhattan distance between normalized speed sequences  . For normalized motion orientation, there are nine motion statuses for  , code 1–8 and a zero motion status 0. The code distance should have the following properties: (1) the distances between the zero motion status 0 and other codes 1–8 should all be the same. (2) The maximum distance between any two of the codes 1–8 should be 4, i.e. the distance between a direction such as 1 and its inverse direction, 5. To meet these two requirements, the distance between motion orientation codes is calculated using Eq.&amp;nbsp;( 10), in which the distance between 0 and the codes 1–8 is set at a fixed value of 4. The orientation sequence distance between two   is defined as the summation of the individual code distances, as described by Eq.&amp;nbsp;( 11).                         The least orientation sequence distance is used to choose the matching pixels to   from  . Sometimes more than one pixels are chosen which form the candidate set  . This situation may be caused by a large scale change or the orientation sequence does not have obvious difference with others. Then the least speed distance is adopted to further select the correspondences from   for this situation. From several experiments we found that the motion speed distance of all correct corresponding pairs of pixels is  \({&amp;lt;}1.6\). So here the distance threshold 1.6 is used to remove mismatches.    5 Outlier removal and transformation evaluation  For many registration methods based on motion feature (track, motion silhouette) [11–14], it is assumed that the objects and scene are far enough away such that they can be regarded as being coplanar. With this assumption, a global transformation model is used. In this paper, we also make this assumption and the global transformation used is affine. There are still some outliers in the calculated pairs of corresponding pixels. RANSAC [18] method proposed by Fischler and Bolls is a robust outlier removal method. It is effective even when more than 50&amp;nbsp;% of the data consists of outliers. Here cascaded RANSAC is adopted to remove outliers, in which the tolerance value gradually decreases, to find the inliers faster. Then least-squares estimation is used to calculate the initial value of model parameters based on inliers. Finally the model parameters are optimized by LM [15, 16] algorithm.   
Springer.tar//Springer//Springer\10.1007-s00138-012-0466-9.xml:The unified extreme learning machines and discriminative random fields for automatic knee cartilage and meniscus segmentation from multi-contrast MR images:Segmentation Meniscus Magnetic resonance imaging (MRI) Discriminative random field (DRF) Extreme learning machine (ELM) Knee cartilage:  1 Introduction  The knee joint is commonly affected by acute injury or osteoarthritis (OA), a prevalent disease mainly characterized by the articular cartilage degradation and the meniscal pathology [1, 8]. The assessment of symptomatic OA raises the problem of knee cartilage and meniscus segmentation, which has gained considerable attention in recent years. Magnetic resonance (MR) imaging has emerged as the most effective imaging modality to detect anatomical changes in knee joints for its excellent soft tissue contrast. The anatomy of a knee joint is illustrated in Fig.&amp;nbsp; . MR imaging has commonly been used for quantitative measurement of knee cartilage and menisci [ 2,  7]. In this work, we develop an automatic knee cartilage and meniscus segmentation system, which utilizes machine learning techniques and works on multiple sets of MR images taken with different sequences (referred to as  ). Quite a few semi-automatic cartilage segmentation methods, [13, 14, 20–22, 27, 31, 32, 39–41, 43, 47] and some semi-automatic meniscal segmentation approaches [15, 35, 42] were reported. However, semi-automatic or fully manual segmentations for large-scale clinical use are typically labour intensive and prone to problematic user variability. It is thus preferred to automate the segmentation of knee cartilages and menisci. There have been some approaches to automatic cartilage and meniscus segmentation in the literature. Tamez-Pena et al. [44] segmented femur, tibia and menisci using a region splitting- and merging-based algorithm. Glocker et al. [12] utilized a nonrigid registration scheme to segment the patellar cartilage by fitting a statistical atlas to MR images. Fripp et al. [11] used a hierarchical segmentation scheme based on a hybrid deformable model to first segment bones and cartilages, then extract menisci by fitting lateral and medial shape models to the MR images which are modelled as a Gaussian mixture model within the volume of interest (VOI) determined from prior segmented tibial and femoral cartilages [10]. Dodin et al. [6] developed a hierarchical automatic segmentation algorithm for OA knee cartilage volume quantification. The work of Yin et al. [49] is based on a LOGISMOS (Logismos-layered optimal graph image segmentation of multiple objects and surfaces) framework for the automatic segmentation of knee bone and cartilage surfaces from volumetric MR images, and subsequently meniscus segmentation was obtained using a pattern recognition technique based on the segmented cartilages [48]. At the 2010 MICCAI conference, a competition for knee segmentation was held at a workshop “Medical Image Analysis for the Clinic—A Grand Challenge” [16], where model-based automatic segmentation methods such as those proposed in [37, 46] were ranked the highest in terms of accuracy. In addition to the above methods, image segmentation can also be considered as a classification task in which each voxel is mapped to a specific class label based on a classifier. Folkesson et al. [9] employed a two step -nearest-neighbour (-NN) voxel classifier to automatically separate cartilages from non-cartilages. While all the aforementioned works are based on a single MR sequence, multi-contrast MR images provide different contrast mechanisms between tissues and help separate different tissues. Koo et al. [23] presented an automatic cartilage segmentation scheme with multi-contrast MR images using support vector machines (SVM). However, these classification-based works [9, 23] assumed that data instances were independent and identically distributed (i.i.d). This assumption may be valid for many data mining applications but is not appropriate for the spatial image segmentation task. As pointed out by [24], class labels are not independent in most real-world spatial classification problems where correlations in labels of neighbouring instances exist in data with multi-dimensional structure, such as images and volumes. This motivates us to incorporate contextual information in the form of spatial dependencies in the classification for the problem of knee segmentation, which has shown to yield more reliable results. Extreme learning machine (ELM) [18, 19] as the training algorithm for the generalized single-hidden layer feedforward networks (SLFNs) can be used in various classification applications. Works on ELM have shown that ELM not only tends to achieve good generalization performance, but also is easy to be implemented, since ELM requires less human intervention and can get direct least-square solution. Unfortunately, ELM does not consider spatial information which is necessary and beneficial to the task of image segmentation. Probabilistic graphical models such as discriminative random fields (DRF) have been used to model spatial contextual constraints in many applications [24]. To improve upon the cartilage and meniscus segmentation-based solely on i.i.d classification, we utilize a DRF model to incorporate spatial dependencies between neighbouring voxels. Inspired by the joint algorithm of SVM and DRF for brain tumor segmentation in [26], we propose a new classification model involving an ELM-based association potential and a DRF-based interaction potential for automatic knee segmentation with multi-contrast MR images. Both potentials are incorporated into an inference graphical model such that the segmentation is cast into an optimal labelling problem which can be efficiently solved by loopy belief propagation. The proposed model not only enjoys the good generalization performance of the classification based on ELM but also incorporates the spatial dependencies in the classification. Benefiting from multi-contrast MR images, we employ a feature set encoding diverse forms of image and anatomical structure information, including both local image structures and (global) geometrical information, all of which are crucial to distinguish cartilages and menisci from the surroundings. A brief version of the proposed approach with some preliminary cartilage segmentation results have been presented in the Second International Workshop on Machine Learning in Medical Imaging (MLMI) [50]. Experimental results show that the proposed segmentation system obtains promising segmentations of the cartilage, and that the unified ELM and DRF model outperforms the classification models based solely on DRF, ELM or SVM in terms of segmentation accuracy when the same features are used. The rest of this paper is organized as follows. Section 2 illustrates multi-contrast MR imaging of the knee, and presents the details of the multi-contrast MR datasets used in this study. Section 3 describes the segmentation system which involves extracting features from multi-contrast MR images, introducing the backgrounds of DRF and ELM, and elaborating the proposed unified ELM and DRF classification model. Subsequently Sect.&amp;nbsp;4 evaluates the proposed segmentation system, and compares the unified ELM and DRF model to other classification techniques. This paper is concluded in Sect.&amp;nbsp;5.    2 Dataset  Tissue structures around knee joints possess different soft-tissue properties, such as T1, T2-relaxation time constants at a given magnetic strength. MR imaging is able to utilize these tissue properties to increase the contrast between the cartilage and its surrounding soft tissues. The common contrast mechanisms used in MR imaging are proton density, T1-weighted, and T2-weighted imaging with or without fat suppression (see Fig.&amp;nbsp; ). Unfortunately, it is difficult to distinguish between cartilages and surrounding tissues based on single contrast mechanism. For example, considering the T1 contrast alone in Fig.&amp;nbsp; , there are severe overlappings between the cartilage, the muscle and the tendons/meniscus, which can also be observed in both the T2 contrast and T2/T1 contrast. There have been many MR sequences utilizing different contrast mechanisms which can provide different contrasts between tissues in the knee joint. Figure   illustrates examples of various contrast human knee MR sequences including fat suppressed (FS) spoiled gradient recall (SPGR) [ 5], fast imaging employing steady-state acquisition (FIESTA) [ 36], and iterative decomposition of water and fat with echo asymmetry and least-squares estimation (IDEAL) gradient echo (GRE) water and fat images [ 34]. These multi-contrast MR images can help distinguish cartilages from other tissues. An illustration of the usefulness of multi-contrast MR images in separating cartilage and other tissues is also shown in Fig.&amp;nbsp; . For instance, the cartilage in the IDEAL GRE water image (see arrow in Fig.&amp;nbsp; (c) can hardly be distinguished from its surroundings, but the successful separation becomes possible by the presence of other MR images as illustrated in Fig.&amp;nbsp;  a, b and d. In this work, we use multi-contrast MR datasets acquired from 11 volunteers (8 males and 3 females, aged 24–41 years), with unknown health conditions. One knee joint (left knee or right knee) of each subject was imaged in the sagittal plane on a GE Signal MRI scanner (GE Healthcare, Waukesha, WI, USA) using 8-channel knee coil at 3.0&amp;nbsp;T. Four three-dimensional (3D) MR sequences including the FS SPGR, FIESTA, IDEAL GRE water and IDEAL GRE fat images (see Fig.&amp;nbsp;) were obtained within 30&amp;nbsp;min. The knees of all subjects were scanned for each MR sequence acquisition in such a way that larger slice numbers are right, increasing rows are inferior, and increasing columns are posterior. The acquisitions were accelerated with a factor of 2 using ARC (Auto-calibrating Reconstruction for Cartesian Imaging) [3]. Different MR sequences were acquired using different sets of parameters. The FS SPGR sequences were obtained with echo time (TE) 4ms, repetition time (TR) 28ms, and flip angle (FA) ; IDEAL-GRE (water &amp;amp; fat) with TE 3.4ms, TR 9.4ms and FA ; FIESTA with TE 3.5ms, TR 7.2ms and FA . In addition, several parameters were common: all images acquired with slice thickness 1.5mm, in-plane spacing 0.625mm0.625mm and matrix size 256 256. Cartilages and menisci were manually labelled by a radiologist with over 10 years’ experiences using the software ITK-SNAP1 on the FS SPGR MR sequence and FIESTA sequence, respectively. The manual labellings produced binary mask images of the cartilages and menisci, respectively, which are later used as the ground truth or gold standard in our experiments.    3 Segmentation methods  Throughout this paper, let  represent the observed MR images, where the feature vector of the th voxel is typically represented by -dimensional feature vector , and  is the set of voxels. Let the corresponding labels at the image voxels be given by , where  is the class label of the th voxel. The aim of the classification-based segmentation is to infer the most likely joint class labels  based on the observed data  by a classification model. Local image structures can capture local object appearance and shape information. The local structures of an image up to second order can be described by the intensity, the gradient, and the Hessian. In this study, we adopt local image features including (normalized) intensity values, the first- and second-order derivatives, and the eigenvalues of the Hessian images. The details on the calculation of the local image features are provided in . Nevertheless, local features are insufficient to extract knee cartilage and menisci from other tissues [9] due to overlapping intensity distributions and ambiguous boundaries. We thus need to exploit global features which are typically crucial for object-oriented medical image segmentation. Next, we exploit the geometrical information of anatomical structures in the knee joint with the help of multi-contrast MR images, which can be considered as a type of global shape feature. There are three steps for the extraction of geometrical features. . Benefiting from the multi-contrast MR sequences, 3D automatic bone segmentation can be robustly done by using a simple but reliable technique working on the FS SPGR and IDEAL-GRE water &amp;amp; fat images. We detect the bones using a threshold-based method followed by a 3D connected component labelling [ 38] and 3D distance transform [ 28]. In more detail, intensity values of the FS SPGR, IDEAL-GRE fat and IDEAL-GRE water sequences are first normalized to [0, 1]. The difference sequence of the IDEAL-GRE fat and IDEAL-GRE water is then calculated, and thresholding with a reasonable choice of a hard threshold (i.e., 0.1) is used to divide both of the difference sequence and the FS SPGR sequence into two classes, respectively, where high-intensity class in the difference sequence is considered as the foreground while low-intensity class in the FS SPGR sequence is considered as the foreground. A logical operator ‘AND’ is subsequently performed to combine the resulting two sequences as a new 3D binary image BMRI in which the foreground mainly contains the voxels belonging to the bones and adipose while the background mainly contains the voxels belonging to the remaining tissues. The new 3D binary image BMRI usually includes some small holes which involve voxels inside the bone zones being erroneously identified as the background, and also includes several small patterns which involve voxels belonging to other tissues being erroneously detected as the foreground. A connected component labelling algorithm [ 38] is thus performed on both the 3D binary image BMRI and then its inverse image to remove the small patterns and to fill the small holes, respectively. Here the small patterns and holes are the connected zones whose total number of voxels below 1/100 of the maximum total number of voxels for the largest connected zone. Nevertheless, the image BMRI still contains several unwanted foreground contents which mainly belong to the adipose and are typically thinner and smaller than the bones. Hence, thinning the foreground voxels to a predefined threshold (3.5 mm) on the distance transformation map of the binary image BMRI can remove all these unwanted contents and leave behind the desired bones only although the surface voxels of the bones are partly eroded. We then recover these surface voxels by dilating the left foreground contents to the same threshold (3.5 mm) as before on the new distance transformation map. To this end, the bones are extracted from MR images of the knee joint. Figure   shows an example segmentation result of the three main bones, i.e., femur, tibia and patella. Note that we used the same thresholds as above during the phase of bone segmentation for all subjects. Although the thresholds are selected in a heuristic and empirical way, they are robust to the multi-contrast MR data of all subjects in this study. . We detect the centers in both of the distal femoral condyles and the femoral center by exploiting the coordinate information of voxels belonging to the segmented femur. First, the index of the central slice belonging to the femur is detected as the mean of the indexes of the maximum and minimum slice. Using the index of the central slice as the break line, the femur can be divided into two parts: part 1 which consists of the slices before the central slice, and part 2 which consists of the slices after the central slice. As mentioned in Sect.&amp;nbsp;2.2, the MR sequences used in this study are acquired by scanning the right or left knees of all subjects with such an orientation that larger slice numbers are right, increasing rows are inferior, and increasing columns are posterior. Therefore, part 1 contains the lateral femoral condyle for a left knee (the medial femoral condyle for a right knee) while part 2 contains the medial femoral condyle for a left knee (the lateral femoral condyle for a right knee). Here we take the left knee as an example. For the lateral part, by finding the index of the minimum column and the average of the corresponding row indexes and slice indexes, respectively, we can get the anterior center (the black box in Fig.&amp;nbsp;) of the lateral femoral condyle. Correspondingly, the posterior center(the grey box) of the lateral femoral condyle can be detected by finding the index of the maximum column and the average of the corresponding row indexes and slice indexes, respectively. In a similar way, we can also get the anterior center (the black circle) and posterior center (the grey circle) of the medial femoral condyle. Subsequently, we easily locate both the lateral femoral center (the white box) which is the mean of posterior center and anterior center of the lateral femoral condyle, and medial femoral center (the white circle) which is the mean of posterior center and anterior center of the medial femoral condyle. Finally, the femoral center (the triangle) which is the mean of the lateral and medial femoral centers is determined. It should be noted that all the centers in Fig.&amp;nbsp; are referred to the 3D centers. . Based on the segmented bones and detected femur features, we extract several geometrical features. Specifically, for each voxel, the Euclidean distance from the closest bone surface is calculated by using 3D distance transform, so is the relative location (distance) along the medial femoral center and lateral femoral center, and radian measurement of the angle between the main magnetic direction and the line that connects the voxel to the femoral center is also obtained by using four-quadrant inverse tangent. The distance values are then normalized to [0, 1] by dividing the maximum value, and the angle values are normalized to [1, 1] by dividing . As mentioned before, the extracted geometrical features play an important role in the knee segmentation task. When other structures in the knee joint have very similar appearance to the cartilage and menisci, local image features would have very limited effect, but the geometrical features are able to remove spurious voxels and increase the segmentation accuracy. For example, the distance from the closest bone surface is helpful in separating cartilage and meniscus from muscle since cartilages are always adjacent to the end of the bones, menisci away, and muscle further away. The angle between the main magnetic direction of the MR imaging and the line that connects the voxel and the femoral center is also useful in distinguish different tissues since they have different locations and directions in the knee joint resulting in different angle values. Thus, the feature set adopted in this study consists of normalized intensity values, local image features, and geometrical features. This yields a 35-dimensional feature vector   for each voxel  ,       where   defines the 32-dimensional (8 4) local image features of four MR sequences and   defines the 3D geometrical features. A summary of the full feature set can be found in the  . We now briefly describe the basics of DRF and ELM, and then elaborate the proposed classification model which is based on the combination of ELM and DRF. Conditional random field (CRF) [25] is a discriminative approach which directly models the posterior distribution  without building the joint distribution. CRF relaxes the conditional independence assumption of the observations and allows the modelling of complex dependencies (i) between the label of an instance and its features, (ii) between the labels of adjacent instances, and (iii) between the labels of adjacent instances and their features. CRF was designed for labelling and segmenting sequential data, which assume 1D chain-structure. Accordingly, DRF [ 24] was proposed as a multi-dimensional extension of CRF for lattice-structured data. In DRF model, the joint distribution over the labels   given the observations   can be written as,       where    is the so-called partition function,   is the association potential modelling dependencies between   and  , and   is the interaction potential modelling dependencies between   and  , and   stands for a neighbourhood of pixel  . DRF is a powerful method for modelling dependencies in spatial data, but the logistic regression for the association potential [ 26] often cannot estimate appropriate parameters for the problems with unbalanced class labels, high-dimensional feature spaces, or highly correlated features. Due to this, in some tasks DRF is not able to produce results as accurate as powerful classification models such as SVM and ELM. Feedforward neural networks are ideal classifiers for non-linear mappings, but traditional feedforward network learning algorithms like Back-Propagation may face several issues, such as converging to local minima, overfitting and inferior generalization performance, slow learning speed, etc. To overcome innate shortcomings of traditional learning techniques, Huang et al. [19] proposed ELM as the training algorithm for the generalized SLFNs, which can be used in various regression and classification applications. The main feature of ELM is that the hidden layer parameters are randomly generated instead of iterative tuning as in the traditional training algorithms, and can be independent of the training data. After randomly generating   hidden nodes, the outputs of the   hidden nodes with respect to the input   are presented as  , which is a row vector. According to [ 17], almost any type of nonlinear piecewise continuous function such as the sigmoid functions, the radial basis, exponential functions, can be used as the hidden node output functions   with  . Note that   actually maps the data from the  -dimensional input space to the  -dimensional hidden layer feature space. Let   be the vector of the output weights between the   hidden nodes and the output nodes. The output function of ELM for generalized SLFNs is       And the minimal norm least-square solution can be obtained by       where   is the label matrix, and   is the Moore–Penrose generalized inverse of the hidden layer output matrix   with respect to   training samples. One of the methods to calculate Moore-Penrose generalized inverse of a matrix is the orthogonal projection method:  . According to the ridge regression theory, one can add a user-specified positive value   to the diagonal of   and the resultant solution       is more stable and tends to have better generalization performance. The study in [18] shows that ELM can achieve good generalization performance as long as the number of hidden nodes  is large enough, that is, the performance of ELM is not sensitive to  and  thus needs not to be tuned. Therefore, only one parameter  in (5) needs to be chosen in ELM instead of two or more user-specified parameters in other classifiers like SVM so that less human intervention is required. Furthermore, ELM can directly get a least-square solution, which makes the implementation of ELM quite easier than that of SVM where an optimization using quadratic programming is performed [45]. Although ELM tends to achieve good generalization performance and can be easy to implement in many classification applications, ELM assumes that data instances are i.i.d, which does not consider interactions on the labels of neighboring data instances and may produce undesirable results for the knee segmentation task. Conversely, DRF considers these interactions but does not have the appealing generalization ability as ELM. To incorporate spatial dependencies and model contextual interactions in the classification process of ELM, we present a unified formulation of ELM and DRF. The posterior distribution   for the unified ELM and DRF is defined as follows            where we employ the ELM to learn the association potential  , and in the interaction potential   we use a DRF model to incorporate the interactions in adjacent labels   and make it data-adaptive. Inspired by the modelling of support vector random field [26], to convert the distance value produced by the ELM output function to a posterior probability, we also adopt a similar method in [33] to map the output of  into probability using a sigmoid function2 in the formulation of . Different from the work in [26], the association potential parameters  and the interaction potential parameters  are simultaneously learned in the training phase, avoiding to use a  method to separately estimate the parameters  in advance. In addition, a four-neighbourhood system, i.e., , is used in this work, which defines a local dependency structure. We denote the feature vector for a pair of neighbouring voxels  and  as  which is computed from the observations . We set  by taking the absolute difference of  and , that is,  which penalizes for high absolute differences between the features of neighbouring voxels. For the proposed classification model of Eq. ( 6), we use a sequential learning strategy for parameter estimation. First, we compute the ELM decision function by solving a least-square problem Eq. ( 5) as in [ 19]. With the resulting ELM decision function  , the parameters   are then simultaneously estimated with   training images using pseudolikelihood       where   is the observed label for  th node in the  th training image, and    with    and    To prevent over-fitting, we utilize the  -regularization, which involves adding a penalty term in the form of a sum of squares of all the parameters in order to discourage the parameters from reaching large values. We thus have the following penalized negative logarithm pseudolikelihood            where   and   are nonnegative regularizing constants determined by cross-validation. Similar to [ 24], the penalized logarithm pseudo-likelihood in Eq. ( 8) is jointly convex with respect to the parameters   and can be easily minimized using gradient descent. We herein adopt a quasi-Newton strategy, where limited-memory BFGS updates are used in computing the step direction, and a bracketing line-search is used to find a step length satisfying the strong Wolfe conditions [ 29]. Given a new test knee MR image , the inference problem is to find an optimal label configuration  based on the learned model parameters . Unlike the work in [26] where the iterated conditional modes used for inference, we perform the sum-product loopy belief propagation (LBP) inference algorithm [30] to efficiently solve this problem. LBP is a message passing algorithm which performs approximate inference on general graphical models. It calculates the marginal distribution for each unobserved node (pixel), conditional on the observed nodes. Starting from some initial set of belief propagation messages, we iterate through all the pixels and repeatedly apply the belief propagation updates to the messages. The optimal label configuration is obtained by computing the maximum of the marginals. A preprocessing pipeline is required before segmenting the cartilages and menisci from the knee MR data described in Sect.&amp;nbsp;2.2. First, the multiple MR sequences should be registered, and we spatially aligned them with the help of mutual information maximization-based multi-modal 3D image registration algorithm in Insight Segmentation and Registration Toolkit (ITK), i.e., the application ‘MultiResMIRegistration’ in ITK.3 The FS SPGR sequence was used as the target, to which other sequences were co-registered. The application ‘MultiResMIRegistration’ performs rigid multi-modal registration using mutual information metric and a gradient descent method for optimization. We set a quite large iteration number, i.e., 500, to try to ensure good registration results. Next, intensity values of all sets of MR images were normalized to [0, 1], which brings them within the same dynamic range, improving the stability of the classifier. In addition, image histogram matching is performed across all knees for each MR sequence separately to allow for inter-subject classification. We first calculated the histogram of each MR sequence for one subject, where 100 bins were used. Then each corresponding MR sequence from other subjects was transformed using histogram equalization so that their histograms were approximately matched. Since menisci only locate at some certain regions in knee MR images, besides the above preprocessing steps, we restrict the meniscus searching region into a volume of interest (VOI). The size of VOI is much smaller than the original MR image size, which thus dramatically reduces the computational load. The VOI can be automatically detected based on the segmented bones in Sect.&amp;nbsp; 3.1. For each knee, we generate a 3D Euclidean distance map from the segmented femur and tibia bone, and determine the possible meniscal region by thresholding the distance map at 18mm. The candidate meniscal locations are a bounding box VOI. An illustration on an example 2D MR slice is shown in Fig.  .   
Springer.tar//Springer//Springer\10.1007-s00138-012-0467-8.xml:Geometrical approach for rectification of single-lens stereovision system with a triprism:Uncalibrated Rectification Projection matrix virtual camera Triprism:  1 Introduction  Conventional stereovision systems employ two or more cameras to capture two or more different views of the same scene. The differences of the correspondence points among these views are known as the disparities. Using the computed disparity values and also the geometric relations between each camera, the depth recovery and 3-D reconstruction of the captured scene become possible [8]. Single-lens stereovision (the stereovision system that employs only a single camera) has some significant advantages over typical two or multiple-camera stereovision systems, normally including compactness, lower cost, fewer system parameters, easier setup, etc. These advantages provide this kind of system with a good application potential and attract great interest from many researchers. In this paper, we create a single-lens stereovision system with a triprism. The image plane of this camera will capture three different views of the same scene behind the filter in one shot. These three sub-images can be taken as the images captured by three virtual cameras which are generated by the three Face (3F) filter. This paper presents a geometrical approach for rectification of this system. Rectification problem is a very important topic in stereovision area. Given a pair of stereo images, rectification determines a transformation (or warping) of each image such that pairs of conjugate epipolar lines become collinear and parallel to one of the image axes, usually the horizontal one [1]. The importance of rectification is that the correspondence problem, which involves 2-D search in general, is reduced to a 1-D search on a scanline identified trivially. In brief, the main contribution of our rectification approach provides four advantages. Initially, this paper proposed a geometry-based method for rectification of single-lens stereovision system with a triprism. This system is also called virtual stereovision system as the image captured can be divided into three, which is equivalent to three images captured using three cameras system with different perspective. Furthermore, the parallelogram rule and refraction rule were employed to determine sketch ray functions which can easily obtain desired rays and also reduce the computational error. Thirdly, computing the projection transformation matrix of three virtual cameras based on a unique geometrical ray sketching, it can accurately obtain the extrinsic parameters. Finally, computing the rectification transformation matrix which applied on the images captured using the system. As the geometrical analysis eliminated the complex calibration process and rectification reduces the correspondence searching to one dimensional, this method provides a complete and simple stereo matching technique for stereovision system. This paper is organized as follows. In Sect.&amp;nbsp;2, the single-lens stereovision system and the relative works about rectification are introduced. In Sect.&amp;nbsp;3, the background of multi-view stereo images rectification is stated. We provide a more detailed overview of our method in Sect.&amp;nbsp;4. In Sect.&amp;nbsp;5, the experiment results are shown. Finally in Sect.&amp;nbsp;6, the conclusions are given.    2 Literature review  Single-lens stereovision system with optical devices was proposed by Nishimoto and Shirai [2]. They proposed using a glass plate which is positioned in front of a camera and the glass plate is free for rotation. The rotation of the glass plate causes deviation of the camera’s optical axis due to reflection which produces a pair of stereo images. The main disadvantage of this method is the disparities between the image pairs are small. Teoh and Zhang [3] further improved the idea of the single-lens stereovision camera with the aid of three mirrors. Two of the mirrors are fixed at  at the top and bottom, while the third mirror is rotated freely at the middle. Two shots will be taken with the third mirror aligned to be parallel to the fixed mirror. Francois et al. [4] further refined the concepts of stereovision from a single perspective to a mirror symmetric scene and concluded that a mirror symmetric scene is equivalent to observing scene with two cameras and all traditional analysis tools of binocular stereovision can be applied. The main problem of mirror-based single-lens system is that its application is only limited to a static scene as the stereo image pairs are obtained with two separate shots. This problem was further overcome by Gosthasby and Gruver [5] whose system captured image pairs by the reflectance from mirrors. Lee and Kweon [6] proposed a single-lens stereovision system using a biprism placed in front of a camera. Stereo image pairs are captured on the left and right halves on the image plane of the camera due to refraction of light rays from the prism. They assumed that , which is the corner angle of the biprism, is very small and ignored. However, in our paper, we thought that  is very important to virtual cameras formulation. So we considered different the value of  and employed the rectification method to rectify the virtual cameras. Furthermore, Lim and Xiao [7, 8] improved the system and extended the study of multi faces prism. They proposed the idea of calibrating the virtual cameras. is a process used to facilitate the analysis of a stereo pair of images by making it simple to enforce the two view geometric constraint. The rectified images can be thought of as acquired by a new stereo rig, obtained by rotating the original cameras. The important advantage of rectification is that computing stereo correspondences [ 9] is made simpler, because search is done along the horizontal lines of the rectified images (see Fig.&amp;nbsp; ). In other words, to find the point corresponding to   of the left image, we just look along the scanline   in the right image. The stereo matching algorithm hence executes faster and, when the assumption is valid, gives more accurate results [ 10,  11]. Rectification is a classical and important problem of stereo vision; many methods are available in the computer vision literature, to our knowledge. Ayache and Lustman [12] introduced a rectification algorithm, in which a matrix satisfying a number of constraints is handcrafted. The distinction between necessary and arbitrary constraints is unclear. Some authors report rectification under restrictive assumptions; for instance, Papadimitriou and Dennis [13] presented an algorithm for rectifying stereo images when the images are taken with convergence geometry (coplanar X and Z axes and parallel Y axes). Hartley and Gupta [14], Robert et al. [15] and Hartley [16] have introduced algorithms which perform rectification given a weakly calibrated stereo rig, i.e., a rig for which only points correspondences between images are given. Robert et al. [17] in their latest work also proposed a distortion criterion based on simple geometric heuristics along the same line. Loop and Zhang [18] decompose each collineation into similarity, shearing and projective factors and attempt to make the projective component “as affine as possible”. Isgr‘o and Trucco [19] build upon [16] and propose a method that avoids computation of the fundamental matrix, using the same distortion criterion as in [16]. The practice has shown that the rectification produced by these methods is not always satisfactory, if compared to results obtained in the calibrated case. On the contrary, in the case of uncalibrated cameras, there are more degrees of freedom in choosing the rectifying transformation [16] and a few competing methods are present in the literature [18, 19]. Each aims at producing a “good” rectification by minimizing a measure of distortion, but none is clearly superior to the others, not to mention the fact that there is no agreement on what the distortion criterion should be. Fusiello and Irsara [20] refer to as quasi-Euclidean epipolar rectification to achieve a good approximation of the Euclidean epipolar rectification. Geometrically, in the Euclidean frame, rectification is achieved by a suitable rotation of both image planes. The correspondent image transformation is the collineation induced by the plane at infinity. As a result, the plane at infinity is the locus of zero-disparity in the rectified stereo pair. This is signified by saying that Euclidean rectification is done with respect to the plane at infinity. In [21] the transformation that best preserve the sampling of the original images is selected, by penalizing minification and magnification effects. Hartley gave a mathematical basis and a practical algorithm for the rectification of stereo images from widely different viewpoints [9]. Al-Shalfan et al. [22] presented a direct algorithm for rectifying pairs of uncalibrated images. Pollefeys et al. [23] proposed a simple and efficient rectification method for general two view stereo images. Above all algorithms are developed for two view rectification situations. Ayache and Hansen [24] presented a technique for calibrating and rectifying pairs or triplet images. In their case, a camera matrix needs to be estimated. Therefore the algorithm works for calibrated cameras. Shao and Fraser [25] also developed a rectification method for calibrated trinocular cameras. The Digiclops [26] developed at Point Grey Research Inc used three calibrated cameras for stereo vision after rectification. These algorithms about triplet images or trinocular rectification only work in calibrated cameras.    3 Background  A pinhole camera is modeled by its optical center  and its image plane. A 3D point  is projected into an image point  given by the intersection of image plane with the line containing  and. The line containing  and orthogonal to the image plane is called the optical axis and its intersection with image plane is the principal point . The distance between  and the image plane is the focal length. Let   in the world reference frame and   in the image plane (pixels). The mapping from 3D coordinates to 2D coordinates is the perspective projection, which is represented by a linear transformation in homogenous coordinates. Let   and   be the homogeneous coordinates of   and  , respectively; then, the perspective transformation is given by the matrix  :       where   is a scale factor. The camera is therefore modeled by its   (henceforth called PPM), which can be decomposed into the product:       The matrix   is the intrinsic parameters which has the following form:        where   is the focal length,   and   are the effective size of the pixels (in millimeter) in the horizontal and vertical directions, respectively. The camera position and orientation (extrinsic parameters) are encoded by the   rotation matrix   and the translation vector  , which represent the rigid transformation that relates the camera reference frame to the world reference frame. Let \(Q=\left[ {{\begin{array}{l@{\quad }l@{\quad }l@{\quad }l} {q_{11} }&amp;amp;{q_{12} }&amp;amp;{q_{13} }&amp;amp;{q_{14} } \\ {q_{21} }&amp;amp;{q_{22} }&amp;amp;{q_{23} }&amp;amp;{q_{24} } \\ {q_{31} }&amp;amp;{q_{32} }&amp;amp;{q_{33} }&amp;amp;{q_{34} } \\ \end{array} }} \right]\), where , , , be the PPM which is obtained by camera calibration. The relationship between point   in the world coordinator and its projection image point   in the image plane coordinator is expressed as following from Eq.&amp;nbsp;( 1)       Equation ( 3) can be rewritten as       From Eq. ( 4), we find this equation is composed of two planar equations. All of the points on the line, which is intersection of two planar equations and named by  , are decided by the coordinator of   and  . The normal vectors of the two planes are  and , so the direction of  is . Therefore,  can be defined with , where  is a real number correlated with . For rectification step, the new PPM can be written as  \(O=\left[ {{\begin{array}{ll} {m_1^T }&amp;amp;{m_{14} } \\ {m_2^T }&amp;amp;{m_{24} } \\ {m_3^T }&amp;amp;{m_{34} } \\ \end{array} }} \right].\) According to Eq. ( 1), we acquire       where   is the pixel coordinates of the rectified image corresponding to  , and   is a new scale factor in the new camera coordinate system. Since  , Eq. ( 5) can be written as thus            It is clear that the degrees of freedom in rectification are only in  . It can also be written as  , which claims that rectification is relevant to the product of the intrinsic parameters matrix and cameras pose matrix. Now, assume an arbitrary geometry of the 3D world. Also, do not “special” geometry between camera locations. So, we have to establish the relationship amongst the projected points in this generic situation. Three views can be thought of as three stereo pairs. Hence, the three stereo pairs are (Cam1, Cam2), (Cam2, Cam3), and (Cam3, Cam1). We can generate some constraints using the epipolar constraint (Fig.&amp;nbsp;). Assume that we have matched eight points over the three views. Using the eight-point algorithm, we can now compute the fundamental matrices , respectively, where  corresponds to the (Cam , Cam ) pair. Next, for any point  in Cam 1’s image, we can compute the epipolar line in the Cam 3 image, using the matrix . Call it line . If we have already computed a dense stereo match between the images of Cam 1 and Cam 2, then we would know the location of  in Cam 2 (call it . For the point  in Cam 2, use the fundamental matrix  to generate the epipolar line in Cam 3. Call it line . The intersection of  and  gives us the estimation of the location of the corresponding point of  and  in third image (called it . When C is in the focal plane of the right camera, the right epipole is at infinity, and the epipolar lines form a bundle of parallel lines in the right image. A very special case is when both epipoles are at infinity, that happens when the line  (the baseline) is contained in both focal planes, i.e., the retinal planes are parallel to the baseline. Epipolar lines, then, form a bundle of parallel lines in both images. Any pair of images can be transformed so that epipolar lines are parallel and horizontal in each image. This procedure is called . &amp;nbsp;    4 Our proposal method  We present a novel design for stereovision—a 3F filter (triprism)-based single-lens trinocular stereovision system. It is firstly designed by our senior Xiao [ 8], we redesign it based on original stereovision system (Fig.&amp;nbsp; ). An image captured by this system is divided into three sub-images, called image triplet, and these three sub-images can be taken as the images captured by three virtual cameras which are created by the 3F filter. The image triplet is captured simultaneously by this system, and hence a dynamic scene will be handled by this system without any problem. However, we only use still scene in our experiment. The key issue in modeling the single-lens trinocular stereovision system is to determine the extrinsic parameters of the virtual cameras. If a 3F filter is vertically positioned in front of a CCD camera as shown in Fig.&amp;nbsp;, in which the shape of the 3F filter is also illustrated, the image plane of this camera will capture three different views of the same scene behind the filter in one shot. These three sub-images are captured by three virtual cameras which are generated by the 3F filter. Two sample images captured by this system are shown in Figs.&amp;nbsp; and  in Sect.&amp;nbsp;5. There are significant differences among the three sub-images as they are captured from different view angles and view scopes of the virtual cameras. Each virtual camera consists of one unique optical center and one “planar” image plane. In our ensuing analysis, the conditions below are assumed to hold:        the real image plane of the CCD camera has consistent properties, such as pixels size, distortion and focal length;         &amp;nbsp;           the 3F filter is exactly symmetrical with respect to the three apex edges and its center axis, which passes through the prism vertex and is normal to its back plane;         &amp;nbsp;           the back plane of the 3F filter is positioned in parallel with the real camera image plane, and;         &amp;nbsp;           the projection of the 3F filter vertex on the camera image plane is located at the camera principle point (or the centre of the real camera image plane) and the projection of the three apex edges of the filter on the real camera image plane divides it into three sub-images equally.         &amp;nbsp;      With the aforesaid assumptions satisfied, the camera optical axis will pass through the 3F filter apex; the three virtual cameras will have identical properties and will be symmetrically located with respect to the real camera optical axis. Thus, the analysis of any one of the virtual camera would be sufficient as the results can be transposed to the other two virtual cameras. The three sub-regions of the image plane (and also the three corresponding virtual cameras) can be differentiated by using labels   and   which stand for left, right and bottom, as shown in Fig.&amp;nbsp; . the real image plane of the CCD camera has consistent properties, such as pixels size, distortion and focal length; the 3F filter is exactly symmetrical with respect to the three apex edges and its center axis, which passes through the prism vertex and is normal to its back plane; the back plane of the 3F filter is positioned in parallel with the real camera image plane, and; the projection of the 3F filter vertex on the camera image plane is located at the camera principle point (or the centre of the real camera image plane) and the projection of the three apex edges of the filter on the real camera image plane divides it into three sub-images equally. In this section, the use of the geometrical knowledge to analyze the ray sketching that links the real camera and the 3F filter is described, from which the properties of the virtual cameras can be determined. And we also obtain the rotation and translation relative to real image plane. In our experiment, the camera lens distortions are assumed to be negligible. &amp;nbsp;
Springer.tar//Springer//Springer\10.1007-s00138-012-0468-7.xml:Object recognition by spectral feature derived from canonical shape representation:Polygonal approximation Computer-assisted intervention Content-based image retrieval Spectral feature Shape representation Content-based object recognition:  1 Introduction  In our recent work in [16], we have developed an integrated computer-aided recognition (CAR) system for lung nodule classification using content-based image features. At the heart of the system, we proposed a new hierarchical modular learning (HML) scheme that was implemented by a hierarchy of Artificial Neural Networks (ANN). In our CAR system, we utilized two classes of features: geometric (shape) features and photometric features. The geometric feature class was composed of two types of features, namely, circularity and spectral features. In this paper, we first explain how to represent content of an image object by our canonical string representation which is obtained from the polygonal approximation of the object’s boundary. Next, we describe how to obtain a new spectral shape feature from this canonical representation. Following the spectral feature derivation, we briefly explain our hierarchical learning scheme and its conceptual model. In Sect.&amp;nbsp;3, we present assessment of classification performance using the spectral feature proposed by our HML decision engine. In the Appendix, we provide the proof of the theorem regarding the computation complexity of the canonical representation algorithm, the robustness conditions of the canonical representation and related theorems, and images of two sample ROIs together with their shape functions and spectral features computed by the proposed algorithm in this paper. Numerous models have been defined for shape representation. Each representation has its own primitives employed to find the best match. Shape primitives can be grouped based on utilizing radial distance from an origin, the edge segment lengths, and the angle between connected segments [3–5, 7, 12, 19, 20], and [14]. Representing shapes can be achieved using vertex-based or edge-based methods. According to [4], the former has less computation time while the latter is more accurate. Among the edge-based methods, polygonal representation of shapes has been studied extensively [4]. Matching of polygons has been achieved by string representation using the three primitives mentioned above [20] and [12]. These three primitives can be organized in different combinations to obtain a unique representation. An orientation invariant representation has been obtained by shifting [1], and [7, 12] or using max/min distance [2] and [14]. We proposed a new polygonal shape representation which is called ‘canonical polygonal representation’ (CPR) and an efficient algorithm to compute a unique representation of a polygon. The representation can be applied under rigid transformation as well as affine deformation with some additional processes [18]. Although we showed that the proposed shape representation can be used for object recognition, it also can be used for similarity matching purpose in image retrieval. The computational complexity for matching of two polygons with  vertices has been reported as either O( [15] or O( log  [1] and [11] to the best of our knowledge. Our proposed method has O( log  computational complexity to obtain the canonical representation as proved at the Appendix and  for matching polygons with  vertices. Hence, if shapes are stored with the proposed representation, matching can be done in , which is the optimum, and online matching that is performed without storing representations can be achieved in O( log  after obtaining vertices. Our new shape representation exploits two primitives which are radial distance and its corresponding edge distance for a given polygon . For each vertex of the polygon, a distance vector  is obtained using these two distances and then all vectors are cyclically ordered in a specific direction to get a canonical string representation  of the polygon. To handle the starting point problem, we proposed a normalized canonical string representation  which is obtained from the . The  provides a unique representation of the polygonal shape. The normalization procedure is described in the subsequent sections.    2 Canonical representation  Let   denotes a polygon of   vertices ordered in a specified direction and   denotes the centroid of   given by ( 1). Let   be a distance vector where the edge   [from a vertex (  to a vertex ( ] and the radial   [from (  to  ] distances defined in Eqs.&amp;nbsp;( 2) and ( 3) below. We define the ‘canonical polygonal representation’ (CPR) of   as       where   defines a sequence and the ‘normalized canonical polygonal representation’ (NCPR) of   as       which is obtained from   as explained below, where the anchor index   defines the starting vertex which is unique for the polygon; note that 1   = mod(  for 1  . The module symbol   is used to locate indices in a cyclic way; accordingly, mod(  returns the first remainder from ‘  divided by  ’. The Fig.&amp;nbsp;  shows components of a distance vector for a vertex. Our normalization procedure  makes canonical string representation a unique identifier of a polygonal boundary. Construction of the tree starts at the terminal node and ends at the root producing the normalized string. The tree is obtained by cyclic divide-and-conquer strategy; that is, the distance vector after the last vector is the first vector of the string (sequence), the second vector after the last vector is the second vector, and so on. We divide the sequence into non-overlapping subsequences (segments) which are composed by a ‘head’ vector and a ‘tail’. All heads are equal and used to separate segments. A tail is a sequence of vectors between two consecutive heads. Now, the goal is to find the max segment by the iterative comparison-and-merging operations. At the initial step, all vectors of  are considered as a subsequence which has only one vector element as in . Then, the set  of the max-vectors are obtained; in case of having only single vector in the , that vector will be the anchor which defines the unique starting vector for the string. However, if the  has more than one max-vector, then it is said that the  has more than one segment. Now, the objective is to find the segment which satisfies ‘max-segment’ definition to be given below. The algorithm initiates an index sequence  that holds the index of max-vectors among}. This  is placed at the terminal layer of the tree.  enables an efficient computation of  by performing merge operation without moving vector elements. After the second step, the max-segment is found by comparing each segment with another one such that if \(\text{ segment}_{i}&amp;gt; \text{ segment}_{j}\), then  is dropped from further comparisons. Once the comparison produces a single segment, the head of the segment is identified as the anchor vector. If more than one segment is found as max-segment, then the comparison-and-merging continues until a single max-segment is obtained. The comparison of two segments is performed by first checking their lengths, then, if the lengths are equal, a vector-wise comparison is conducted. The vector-wise comparison starts from the first elements, and then continues with the second one in case of having identical first vectors and so on until finding an un-identical vector. We introduce the tolerance value  which is used in defining family of identical distances. Based on this , the distances  and  are said to be identical ( if . We used this definition in comparison of two distances belonging to two vertices which would be imprecisely located. We present the formal definitions, the algorithm, and its complexity in the sequel. Given two distance vectors   and   and a tolerance value  \(\tau &amp;gt;0\). These vectors are first compared by their first elements; if they are identical, then the second element of the vectors is compared. Formally, the inequality  \(\delta _1 &amp;gt;\delta _2 \) is true if one of the conditions in ( 6) is satisfied:            Clearly, given two distance vectors   and   we have either  \(\delta _1 &amp;gt;\delta _2 \), or  \(\delta _2 &amp;gt;\delta _1 \), or   (meaning   and  . In the last case, we say   and   are essentially identical. The comparison of two sequences is performed compared vector by vector starting from the first vector to the last. If the first vector of each sequence are identical ( , then the second vectors are compared. This comparison continues until finding an unidentical vector pair if exists. Formally, given two sequences   and  , the inequality  \(T_1 &amp;gt;T_2 \) is true if one of the conditions in ( 7) is satisfied:            Finding the max-segment: Let     be the sequence of all maximal distance vectors for the polygonal representation  . Let   be an element in  .   denotes a ‘head’ and   denotes a ‘tail’, which is the segment between two successive heads   and   for  , and let   denotes a segment of  . All heads are essentially the same; hence, two segments are compared by their tails. The max of two segments   and   is computed using ( 7) and ( 8):          Let   denote the sequence of segments obtained at iteration   using the head index sequence   from the previous iteration and the initial set  . The   holds the parent segments whose child was from the previous iteration; hence,  . After computing   the indices of the heads of the maximal segments in   is obtained, which form the set   The set   always keeps track of heads referencing to the original indices in  . The set  is computed by the algorithm   as                                                   for          where  . Algorithm Normalize String Input: A cyclic string . Output: The normalized form   of string  .              [Initialization]: Sequence of initial segments:                                  &amp;nbsp;           [Terminal layer construction]: Find all heads of  and store their indices in  and compute . Make  the terminal layer of the tree.         &amp;nbsp;                 [Parent layer construction:       ]: Repeat until        or a symmetrical case        will be decided. In the Eq.&amp;nbsp;(      19) the        is computed as       .                       Obtain the set  using the  and  by equation (9).                   &amp;nbsp;                                     Find the max-segments            , and store their index in             by equation (           13) using the following operations:                                      First compare the segments in length;                             &amp;nbsp;                                         If more than one max-segment in length exists, then compare these segments pair-wise using Eq. (8).                             &amp;nbsp;                                                        &amp;nbsp;                               &amp;nbsp;            .         &amp;nbsp;      As an example, let a cyclic string be  . Here we use   is cyclic in the sense that  , and so on. The first step is to find initial heads in the sequence:    In our example the head is   = (5, 1) located at the third and sixth index of the sequence  ; hence,   and  . [Initialization]: Sequence of initial segments: [Terminal layer construction]: Find all heads of  and store their indices in  and compute . Make  the terminal layer of the tree. [Parent layer construction:  ]: Repeat until   or a symmetrical case   will be decided. In the Eq.&amp;nbsp;( 19) the   is computed as  .        Obtain the set  using the  and  by equation (9).         &amp;nbsp;                 Find the max-segments       , and store their index in        by equation (      13) using the following operations:                       First compare the segments in length;                   &amp;nbsp;                          If more than one max-segment in length exists, then compare these segments pair-wise using Eq. (8).                   &amp;nbsp;                               &amp;nbsp; Obtain the set  using the  and  by equation (9). Find the max-segments  , and store their index in   by equation ( 13) using the following operations:        First compare the segments in length;         &amp;nbsp;           If more than one max-segment in length exists, then compare these segments pair-wise using Eq. (8).         &amp;nbsp; First compare the segments in length; If more than one max-segment in length exists, then compare these segments pair-wise using Eq. (8). . Next, at   = 1, applying the step (3.a) of the algorithm we obtain        Then, we find the max-segments of  . Since   and   have the same length, they will be compared in pair-wise. This comparison yields that  \(S_2^{(1)} &amp;gt;S_1^{(1)} \), hence   and since   the iteration stops. At the final step, we obtain   and    where segments are   and   with   and   and  . As a symmetrical example, let  , so   will be any of head   or  . An illustration of the algorithm with a different example is given in Fig.&amp;nbsp; . In this illustration, the string is given as  . The indices array of   of heads   are obtained at the second step; notice that   and  . At the next step (Parent layer construction), the index of heads   for iterations   are obtained as  . Finally, the anchor   and the normalized string   are obtained. Notice that the tree is upside down (having the terminal nodes at the top and the root at the bottom) in the illustration. The spectral shape feature proposed is a vector of Fourier coefficients derived from shape function of . We utilized the Fourier coefficients that are obtained by discrete Fourier transform, because they can serve as a spectral descriptor of a function. In addition, the performance of the Fourier descriptor for content-based representation is acknowledged in [21]. The shape function of a normalized canonical representation can be derived by three methods. An illustration of these methods is seen in Fig.&amp;nbsp;  and they are:              The values for the dependent variable        are obtained from the set of        and the values for the independent variable        are obtained from the set of       , formally:                             where        for       , and c       \(&amp;gt;\)              &amp;nbsp;                   is a combination of the sequence        followed by the sequence       , and        is obtained from indices, formally:                                            where       , for       , and c &amp;gt; 0.              &amp;nbsp;                 The odd-indexed        values are obtained from        and the even-indexed        values are obtained from       , formally:                                            where       , for       , and       , and c       \(&amp;gt;\)              &amp;nbsp;         and   denote the radial distance and the edge distance, respectively. After obtaining the shape function, an interpolation is necessary to obtain a normalized axis with equal intervals for the discrete Fourier transform. The values for the dependent variable   are obtained from the set of   and the values for the independent variable   are obtained from the set of  , formally:         where   for  , and c  \(&amp;gt;\) 0. is a combination of the sequence   followed by the sequence  , and   is obtained from indices, formally:              where  , for  , and c &amp;gt; 0. The odd-indexed   values are obtained from   and the even-indexed   values are obtained from  , formally:              where  , for  , and  , and c  \(&amp;gt;\) 0. Figure&amp;nbsp;  shows such an operation to the shape function obtained in Fig.&amp;nbsp; b. In this paper, the Fourier coefficients are used as a feature vector, which is an abstract representation in a vector form, due to its discriminative property. The feature vector   is defined as       where   is the discrete Fourier transform of   and  . Input: NCPR . Output: The shape function  .        Obtain the shape function ŷ[x] which is obtained from  or by changing sign of even indexed  values, e.g., , and so on, or from combination of derivative and sign change.         &amp;nbsp;           Apply linear interpolation to ŷ to obtain .         &amp;nbsp;      In this paper, considering the results in [ 17], we applied sign change and then derivative to the function defined in ( 15). The coefficient   is chosen as   so that the shape function will be defined in the  -axis interval [0, 1]. Interpolation interval is chosen as 0.01 which yields 100  -values for all polygons whose shape feature to be extracted. A linear interpolation is applied and first 50 Fourier coefficients are used as the feature vector. Obtain the shape function ŷ[x] which is obtained from  or by changing sign of even indexed  values, e.g., , and so on, or from combination of derivative and sign change. Apply linear interpolation to ŷ to obtain . In this section, we briefly explain the heart of the CAR system which is used for classification of nodules and non-nodules in a volume of interest (VOI) from lung CT images. The classification is performed by the hierarchical learning scheme (HLS) proposed in [ 16]. We implemented HLS in the inter-slice inter-plane (ISIP) model; Fig.&amp;nbsp;  illustrates the conceptual model structure of this model. The ISIP model first synthesizes predictions from each slice of a perspective and then obtains intermediate predictions for each perspective before reaching the final classification for the whole VOI.
Springer.tar//Springer//Springer\10.1007-s00138-012-0469-6.xml:Pi-Tag: a fast image-space marker design based on projective invariants:Projective invariants Camera calibration Pose estimation Augmented reality Fiducial markers:  1 Introduction  A visual marker is an artificial object consistent with a known model that is placed into a scene to supply a reference frame. Currently, such artefacts are unavoidable whenever a high level of precision and repeatability in image-based measurement is required, as in the case of vision-driven dimensional assessment task such as robot navigation and SLAM&amp;nbsp;[5, 8, 36], motion capture&amp;nbsp;[2, 38], pose estimation&amp;nbsp;[37, 39], camera calibration&amp;nbsp;[7, 14] and of course in field of augmented reality&amp;nbsp;[6, 40]. While in some scenarios, approaches based on naturally occurring features have been shown to yield satisfactory results, they still suffer from shortcomings that severely limit their usability in uncontrolled environments. Specifically, the lack of a well-known model limits their use in pose estimation. In fact, while using techniques like bundle adjustment can recover part of the pose, the estimation can be only up to an unknown scale parameter; further, the accuracy of the estimation heavily depends on the correctness of localization and matching steps. Moreover, the availability and distinctiveness of natural features are not guaranteed at all. Indeed the smooth surfaces found in most man-made objects can easily lead to scenes that are very poor in features. Finally, photometric inconsistencies due to reflective or translucent materials severely affect the repeatability of the point descriptors, jeopardizing the correct matching of the detected points. For this reasons, it is not surprising that artificial fiducial tags continue to be widely used and are still an active research topic. Markers are generally designed to be easily detected and recognized in images produced by a pinhole camera. In this sense, they make heavy use of the projective invariance properties of geometrical entities such as lines, planes and conics. One of the earliest invariance used is probably the closure of the class of ellipses to projective transformations. This implies that ellipses (and thus circles) in any pose in the 3D world appear as ellipses in the image plane. This allows both for an easy detection and a quite straightforward rectification of the plane containing any circle. With their seminal work, Gatrell et al.&amp;nbsp;[10] propose to use a set of highly contrasted concentric circles and validate a candidate marker by analyzing the compatibility between the centroids of the detected ellipses. By alternating white and black circles, a few bits of information can be encoded in the marker itself. In the work proposed in&amp;nbsp;[3], the concentric circle approach is enhanced by adding colors and multiple scales. Later, in&amp;nbsp;[18] and&amp;nbsp;[24], dedicated “data rings” are added to the marker design. A set of four circles located at the corner of a square is adopted in&amp;nbsp;[4]: in this case, an identification pattern is placed in the middle of the four dots to distinguish between different targets. This ability to recognize all the viewed markers is really important for complex scenes where more than a single fiducial is required; furthermore, the availability of a coding scheme allows for an additional validation step and thus lowers the number of false positives. Circular features are also adopted in&amp;nbsp;[31], where a set of randomly placed dots are used to define distinguishable markers that can be detected and recognized without the need for a frame. In this case, to attain robustness and to avoid wrong classification, a large number of dots are required for each marker, thus leading to a likely high number of RANSAC iterations. Collinearity, that is the property of points that lie on a straight line of remaining aligned after any projective transformation, is another frequently used invariant. Almost invariably this property is exploited by detecting the border edges of a highly contrasted quadrilateral block. This happens, for instance, with the very well-known ARToolkit&amp;nbsp;[ 16] system which is freely available and has been adopted in countless virtual reality applications. Thanks to the ease of detection and the high accuracy provided in pose recovery [ 21], this solution is adopted also in many recent marker systems, such as ARTag&amp;nbsp;[ 9] and ARToolkitPlus&amp;nbsp;[ 35]. The latter two methods replace the recognition technique of ARToolkit, which is based on image correlation, with a binary-coded pattern (see Fig.&amp;nbsp; ). Finally, many papers suggest the use of the cross-ratio among detected points&amp;nbsp;[20, 28, 30, 33], or lines&amp;nbsp;[32] as invariant properties around which to build marker systems. A clear advantage of the cross-ratio is that, being a projective invariant, the recognition can be made without the need of any rectification of the image. Unfortunately, the ease of detection offered by the use of the cross-ratio often comes at the price of a high sensitivity to occlusions or misdetection. In fact, spurious or missing detection completely destroy the invariant structure. Further, cross-ratios exhibit a strongly non-uniform distribution&amp;nbsp;[13], which in several situation limits the overall number of distinctively recognizable patterns. In this paper, we introduce a novel visual marker system that uses the cross-ratio and other projective invariants to perform both detection and recognition in the image plane, without requiring the estimation of an homography or any other technique of perspective correction. Further, our approach introduces some redundancy by replicating the same pattern on different sides, which can be exploited to obtain a moderated robustness to occlusion or to lower the false positive rate. In addition, the detection and recognition algorithms are both efficient and very simple to implement. In the experimental section, we validate the proposed approach by comparing its performance with two widely used marker systems under a wide range of noise sources applied to synthetically generated scenes. Finally, we also tested the effectiveness of the novel marker when dealing with real images using it to solve a number of different real-world measurement tasks and applications.    2 Image-space fiducial markers  The proposed marker, which we named  (), exhibits a very simple design. It is made up of 12 dots placed on the sides of a square: four dots per side, with the corner dots shared. There are two distinct configurations of the dots and each is repeated in two adjacent sides. See, for example, the marker in Fig.&amp;nbsp;e: The top and left sides show the same configuration, and so do the bottom and right ones. The two different configurations are not random. In fact they are created in such a way that the cross-ratio of the two patterns is proportional via a fixed constant . The interplay between the detection of these cross-ratios in the image plane and other invariants such as straight lines and conics projections allows for a simple and effective detection and recognition approach for the Pi-Tag. Our approach relies on four types of projective invariants, namely the invariance of the class of ellipses, collinearity, angular ordering (on planes facing the view direction) and cross-ratio. The invariance of the class of ellipses has been extensively exploited in literature. Circular dots are easy to produce and, since they appear as ellipses under any projective transformation, they are also easy to detect by fitting on them a conic model with a low number of parameters. In addition, while the center of the detected ellipses is not preserved under perspective, if the original dots are small enough, the localization error has been shown to be negligible for most practical purposes&amp;nbsp;[22]. Other advantages of the elliptical fitting include the ability of using the residual error to filter out false detections and to perform gradient-based refinements. For this and other reasons, dots are widely adopted also for accurate tasks such as lens distortion correction, and stereo calibration. Given a set of points, projective geometry preserves neither distances nor the ratios between them. Fortunately, there are some interesting properties that remain invariant and can be put to use. One is the angular ordering of coplanar points. That is, if we take three points defining a triangle, once we have established an ordering on them (either clockwise or anti-clockwise), such ordering is maintained under any projective transformations that looks down to the same side of the plane. The second invariant is collinearity and derives from the fact that straight lines remain straight under perspective transformations. Almost all rectangular fiducial markers rely on this property in the detection stage by finding lines in a scene using a wide range of different techniques. Finally, we use the cross-ratio of four collinear points  , ,  and  , a projective invariant defined as:       where   denotes the Euclidean distance between points   and   (see Fig.&amp;nbsp; ). The cross-ratio does not depend on the direction of the line , but depends on the order and the relative positions between the points. The four points can be arranged in  different orderings which yield six different cross-ratios. Due to this fact, the cross-ratio is unlikely to be used directly to match a candidate set of points against a specific model, unless some information is available to assign an unique ordering to such points. Many fiducial marker systems use projective and permutation -invariants&amp;nbsp;[23] to eliminate the ambiguities of the different orderings. For example, this invariants are used to track markers or interaction devices for augmented reality in [33] and [19]. It has to be noted, however, that permutation invariance results in the inability to establish correspondences between the detected features and points in the reference model, making it impossible to fully estimate the camera pose without relying to stereo image pairs or other features in the markers. The main idea behind the design of the proposed Pi-Tags is to combine all the afore-mentioned invariants to identify each dot without ambiguities, even in the presence of moderate occlusions, thus allowing fast and accurate pose estimation. To this end, it should be noted that we assume the imaging process to be projective. While this holds to a reasonable approximation with many computer vision devices with good lens and moderate focal length, wide angle cameras could hinder our assumption due to lens distortion. In this case, a proper distortion-correcting calibration step&amp;nbsp;[29] should be performed before processing. In our design, each marker is characterized by properties that are common to all tags. Specifically, each side of the marker must be made up of exactly four dots, with the corner dots being shared and labeled as in Fig.  a. For a given constant  , a set of Pi-Tags is generated by varying the dots position constrained by the following property:       All these properties allow to decouple the detection and recognition pipeline into two separate steps. In the detection process a set of possible marker candidates are localized in the image by exploiting the projective invariants described in the previous section. First,the dots are located by searching for the ellipses present in the image (projective invariance of conics). To this end, we use the ellipse detector supplied by the OpenCV library [1] applied to a thresholded image. To be resilient to variations in illumination, a locally adaptive threshold is applied by [27]. Some of the ellipses found at this stage may belong to a marker in the scene (if any), others could be possibly generated by noise or clutter. Next, we group the detected ellipses into potential marker candidates. This is done considering only the centroids of the ellipses (which are a very good approximation for original circle points). The first step to gather all the points belonging to a tag is to find a viable marker side, which can be done by exploiting the straight line invariance (collinearity). For this purpose, we iterate over all the unordered pairs of dots and then, for each pair considered, we check if they are likely to be two corner points (see Fig.&amp;nbsp;a). This check is satisfied if exactly two other dots can be found lying within a fixed distance to the line connecting the first two candidate corners. The distance parameter is expressed in pixels and, since the accuracy of the estimated ellipse center is expected to be subpixel, a threshold of one or two pixels is usually enough to avoid false negatives without the risk of including misdetected ellipses. To obtain a better performance, this step can be accelerated using a spatial index, such as a quad-tree, rather than by testing all the ellipses found. At this point, we have identified a candidate side of the marker. Next, we validate the candidate by finding a third corner of the marker. Again, this is done by iterating over all the dots left and, for each one, by testing if it forms a candidate side with one of the current corner points (i.e. by checking that the line connecting them passes through exactly two ellipses). If a pair of sides is found, then it is possible to test if they belong to a known marker and give a label to each corner. The test is carried on by verifying that the proportion between the cross-ratios of the sides is approximately 1 (in this case we are dealing with  or  adjacent sides) or  (in this case we are dealing with  or ). The labeling happens by observing the ordering of the sides, which is conserved since always the same face of the tag is seen (see Fig.&amp;nbsp;b). With two sides detected and labeled, we can recognize the marker by comparing the measured cross-ratio with the database of current markers. However, to be more robust, we search for the fourth corner with the same line-based technique. Depending on the application requirements, the search for the fourth point can be mandatory (to reduce the number of false positives and get a more accurate pose) or optional (to allow for the occlusion of at most two sides of the marker). Once the points are detected and labeled, it is possible to test if they belong to an expected marker. This final step is done by computing the average between the two or four obtained cross-ratios (divided by  if needed) and by comparing it with all the values in the database of the tags to be searched. If the distance is below a fixed threshold, the marker is then finally recognized. Note that to avoid any ambiguity between tags, the proportion between the cross-ratios of  sides of any pair should be different from . Regarding the computation complexity of the approach, it is easy to see that finding a starting side is  with the number of ellipses, while the two subsequent steps are both . This means that if each detected point triggers the full chain the total complexity of the algorithm could be theoretically as high as . However, in practice, given the relatively low probability of getting four ellipses in line with the correct cross ratio, most of the starting side found lead to a correct detection. In addition, even when the starting side is not correct, it is highly probable that the cross-ratio check will stop the false matching at the second step. While a full probabilistic study would give a more formal insight, in the experimental section we will show that even with a large number of false ellipses the recognition is accurate and it is fast enough for real-time applications. Having detected and labeled the ellipses, it is now possible to estimate the camera pose. Since the geometry of the original marker is known, any algorithm that solves the PnP problem can be used. In our tests, we used the  function available in OpenCV. However, it should be noted that, while the estimated ellipse centers can be good enough for the detection step, it is reasonable to refine them to recover a more accurate pose. Since this is done only when a marker is found and recognized, the computational cost is limited. In our experiments, we opted for the robust ellipse refinement approach presented in [25]. In addition, to obtain a more accurate localization, one might be tempted to correct the projective displacement of the ellipses centers. However, according to our tests, such correction in general gives little advantage and sometimes leads to a slightly reduction in accuracy. Finally, we also tried the direct method outlined in [15], but we obtained very unstable results, especially with small and skewed ellipses.    3 Experimental validation  In this section, we evaluate the accuracy and speed of the Pi-Tag fiducial markers and compare them with ARToolkit and ARToolkitPlus. A first batch of tests is performed with synthetically generated images under different condition of viewing direction, noise, and blur. This allows us to compare the different techniques with a perfect ground truth for the camera pose, so that even slight differences in precision can be detected. The accuracy of the recovered pose is measured as the angular difference between the ground truth camera orientation and the obtained pose. While this is a subset of the whole information related to the pose, this is an important parameter in many applications and allows for a concise analysis. A second set of experiments is aimed at characterizing the behaviour of Pi-Tags with respect to its resilience to occlusion, the presence of false positives, and the sensitivity to the threshold parameters, as well analyze computational time required by the approach.
Springer.tar//Springer//Springer\10.1007-s00138-012-0470-0.xml:Segmentation of Rumex obtusifolius using Gaussian Markov random fields:Broad-leaved dock Weed control Rumex obtusifolius Segmentation GMRF texture Image analysis:  1 Introduction  is one of the most common weeds in the Netherlands. Its rapid growth is a major concern because it competes with grass for natural resources like water, nutrients, and light thereby reducing crop yield. A widely used method of controlling weeds is by application of herbicides; however, the use of herbicides is being questioned due to their adverse affects on the environment. For instance, herbicides are found to cause ground water pollution [22] and force selective bias towards herbicide-resistant weeds. There are several non-chemical methods for controlling weeds, like crop rotation, manual removal, thermal and biological control. However these methods are labour intensive, non-scalable, and expensive both in time and costs [6, 31]. As a result, robotic systems are being considered for mechanical treatment of weed [29, 31]. These systems use vision-based methods for detection of the weed. Our focus is on detection of  in grasslands. Most of the available vision-based robotic systems for detecting weeds are not suitable for real-time detection of  in grasslands for several reasons. For instance, many methods focus on detection in soil background using colour contrast between soil and weed [3, 17]. These methods cannot be applied for detecting  in grassland because they are characterized with similar spectral reflectance in visible range. Another method [2] uses uniformity-based texture measure to detect weeds in lawn field. This technique is not applicable in grasslands, because unlike in a lawn, the grass does not have a consistent texture. In [27] local variance was used as the texture measure to detect broad-leaf weeds in grassland. Local variance is another measure of uniformity of texture, and our preliminary analysis showed that it is unreliable because of the inconsistency of grass texture. A real-time vision system which uses spectral power as a texture measure was developed in [31] for the detection of  in grassland. In this method, the image is partitioned in to square tiles. Each tile is classified as weed or grass based on its spectral power. They report satisfactory performance; however, they also mention that it is sensitive to illumination condition. Local spectral power is the same as local variance [21] and thus suffers from the same drawbacks as mentioned above. The authors of [33] exploited the difference in density of edges between grass and leaf regions and developed a texture measure based on edge strength. The veins and ribs of  forms strong edges due to which differentiating it from grass based on edge strength alone is difficult. In the study of detecting weed in lawn fields [1], the authors use texture features based on Grey-level co-occurrence matrix (GLCM). GLCM features are one of the most widely used texture features [19], but are computationally expensive and thus cannot be applied in real-time. The aim of this study was to develop a robust, vision-based, real-time segmentation method for detection of   in grassland under natural lighting conditions. Segmentation of   in grassland entails uncertainty in terms of shape, size and illumination. Figure&amp;nbsp;  shows a typical image of   in grassland taken by the robot. As we can see, not only do the spatial (spectral and textural) properties of grass and Rumex pixels vary but so also do the spatial properties of grass pixels in different parts of the image. Further, this pattern varies from image to image. A robust segmentation algorithm should account for these uncertainties. Markov random field (MRF) theory helps us to deal with uncertainties in an image by means of explicitly defining the spatial context and describing the spatial interactions between pixels as a probability distribution [ 26]. The suitability of MRFs for modelling natural images have been demonstrated in several studies [ 15,  34]. We assume that the image has only two classes, Rumex and grass and model it as a Gaussian MRF (GMRF) [12]. GMRF is one of the simplest MRF for encoding spatial interactions between pixels, and its parameters can easily be estimated using Least Squares [11]. We regard the model parameters as texture features and formulate the segmentation of Rumex from grass as a  (MAP) inference problem within the MAP–MRF framework [26]. This allows us to seamlessly integrate the GMRF model of the image with a prior model (as detailed in the next section). Finally, the optimization problem is solved via Graph Cuts [10]. The choice of Graph cuts for the optimization was based on real-time constraints. Several studies have shown that they are robust and capable of operating in real-time [8, 30]; they are among the most widely used optimization algorithms for real-time object segmentation [7, 20, 28, 32]. We demonstrate the real-time applicability of the proposed algorithm—referred to as A1—by comparing it with the one developed by [31]—referred to as A2. The rest of the paper is organized as follows. Section&amp;nbsp;2 gives the basic definitions and notations of MRF theory and Graph Cuts. The image model and the segmentation method is described in Sects.&amp;nbsp;3 and 4, respectively. Section&amp;nbsp;5 contains experiments and results followed with discussions and conclusion.    2 Markov random fields  In this section, we introduce the standard definitions and terminology of MRF theory. Although this material has been reviewed numerous times, we include it here to indicate the link between MAP–MRF framework and Graph Cut as well as to make clear how they are implemented for this application. Markov random field (MRF) theory is a probability-based theory that provides mathematically rigorous modelling tools to model interactions between spatially varying entities. In 1984, Geman and Geman popularized the use of MRFs in image analysis [16]. Since then, MRFs have found application in almost all areas of computer vision and image analysis [5, 9, 13, 35]. A review of MRF-based image analysis can be found in [26]. MRF theory considers texture as a 2D stochastic process consisting of random variables in a plane, referred to as random field. The random field is characterized in terms of a conditional distribution of the constituting random variables. Consider a 2D rectangular lattice of pixels    where   is the total number of pixels. Let   be a random variable associated to pixel  . It can take values from the discrete labelled set  . A random variable having a specific value is said to be an instantiated random variable. A set of all instantiated random variables corresponding to all the pixels is denoted by  . That is,  . Note that we use   to denote both the random variable at pixel   and its value (label). The distinction between them is made explicit if the context demands. A neighbourhood is a subset of pixels   corresponding to pixel   given by    where,   is an integer that specifies the radius of the neighbourhood,   is the Euclidean distance between pixels   and  . The radius   and the isotropy of the neighbourhood determine the number of pixels it contains and the parameters associated with them, respectively. Figure&amp;nbsp;  illustrates different neighbourhood systems. In the figure, the members of the neighbourhood set   are marked by the corresponding parameter   We use the boldface notation   to represent a vector of all model parameters.   and   are first-order isotropic, second-order isotropic, first-order anisotropic and second-order anisotropic neighbourhood systems, respectively. Note that the pixels along the boundary of the image do not have the same number of neighbours as the interior pixels. To specify an MRF, we have to specify the conditional probability  . This is done by specifying the Gibbs distribution [ 26], given by       where   is called the potential function and   is the normalizing constant known as the partition function. To define  , we first need to define cliques associated to a neighbourhood system and the corresponding clique potentials. A clique  , is a set of pixels such that every distinct pair of pixels in the set are neighbours. Figure&amp;nbsp;  shows single-site and pair-site cliques of a second order neighbourhood system.   is the set of all cliques where   denotes a clique with   pixels. A clique potential,  , is a potential function associated with a clique type  , whose value is dependent on the structure of  . For example, the clique potential   of a single-site clique  , is a function of the label at pixel   Similarly, for a pair-site clique  , the potential function  , is a function of the labels of the pixel   and  . The potential function  , is defined as       where summation is done over all possible cliques  . Thus, the Gibbs distribution is specified once the clique potentials are determined. In this section, we describe how we can use MRFs in a Bayesian context for a MAP-MRF formulation of the segmentation problem where the solution is given by finding the  estimate, also known as the MAP estimate. The image   is assumed to be a noisy realization of an underlying MRF  . The aim is to recover   using the observed&amp;nbsp; . That is, we want to find the posterior distribution,  . The maximum of the posterior distribution, the MAP estimate, is obtained by minimizing the negative log likelihood&amp;nbsp;[ 14]    where the posterior potential   is given by            Here,   is called the region parameter and determines the importance of the prior potential   over the likelihood potential   is the single-site clique potential and   is the pair-site clique potential, higher order clique potentials are ignored. In graph cut literature (and other graph-based algorithms in general), the potential function  (also referred to as energy function) specifies a graph , where  consists of a set of nodes connected by edges . There are two special nodes namely source and sink. Two regular nodes are connected by  and  connect each regular node to the source and sink. All the edges have an associated cost. The cost of  is analogous to , and the cost of  to . Graph cut partitions the graph into two disjoint subsets by removing edges such that the source and sink nodes are not in the same set, and the total cost of the partition is minimal. This solution minimizes the energy&amp;nbsp;(3).    3 Image model  In this section, we give a detailed description of the image model that we use in the study. We assume that the image contains two classes: Rumex and grass, each with a different texture. It is modelled as a GMRF given by       where the subset   is the asymmetric neighbourhood of  , such that if   then   and  ,   is the model parameter associated with the members of  , the set   is the symmetric neighbour pair and   is the Gaussian noise with correlation structure            The model parameters   and   depend on the label  . They form the features that describe the textures within a region. The model parameters are estimated by means of least squares as       and       where   is a column vector given by    4 Supervised segmentation  A supervised approach consisting of two phases is emp- loyed for segmentation. The first phase—called the training phase—consists of feature selection in which we learn optimal GMRF model parameters   along with the region parameter   using a set of training images. The second phase consists of testing the selected features by using them in the segmentation algorithm. The accuracy of the segmentation algorithm is measured by evaluating the segmentation error: the error between the algorithmically determined centre of the plant from the reference centre obtained from the corresponding hand-segmented image. It is given by       where   is the centre of the plant determined by the segmentation algorithm and   is its centre in the corresponding reference image. The unit of the error measure,  , is in millimeters. The motivation for using the above error measure is based on the application objective which is to identify the centre of   so that it can be uprooted by the robot. Note that the error measure is only applicable if Rumex is detected in the image. Figure&amp;nbsp;  shows the flow chart of the segmentation algorithm. The images used in the experiments were taken using a commercial camera (Cybershot DSC-60; Sony, Tokyo, Japan), in automatic white balance mode, at a resolution of  pixels with the corresponding ground resolution of  per pixel. For experiments, however, the images were downscaled to  to be consistent with the settings used in [31]. Otherwise, the comparative results will be incorrect as we will be comparing algorithms operating on different textures. The images were divided into two sets: a training set and a test set. The training set consisted of 16 images with no —referred to as grass-image—and 24 images consisting of a single cluster of —referred to as rumex-image, while the test set consisted of 43 grass-images and 49 rumex-images. Every rumex-image consists of a corresponding hand-segmented reference-image, where the contour of the plant cluster is determined manually indicating the desired segmentation. The features that accurately discern the label of a pixel as Rumex or grass would be called the optimal texture features. Since the texture features are the GMRF model parameters   and  , several model properties like the neighbourhood system and the scale of the texture (explained below) influence the quality of the texture features. We compare features corresponding to four different neighbourhood system at ten different scales listed in Fig.&amp;nbsp; . The scale of the texture in our case is the degree of smoothness of the texture which is determined by using various Gaussian filters. Through an empirical study we narrowed, down the range of Gaussian filters to the ten shown in Table&amp;nbsp; 1. A feature set corresponding to a specific neighbourhood system   and a scale   is computed as follows: Homogeneous texture patches of a given class (see Fig.&amp;nbsp; ) are smoothed using a Gaussian filter,  . Subsequently, the smoothed texture is modelled as GMRF with the neighbourhood system  , and the estimates   and   are obtained according to ( 6) and ( 7), respectively. The feature selection method employed in the paper is categorized as filter method [ 24] in the machine learning literature. We use Fisher’s criterion [ 14] to evaluate the quality of the texture features for their ability to differentiate between the two classes. Figure&amp;nbsp;  is a plot of the Fisher score of each feature set. Each curve in the plot corresponds to a particular neighbourhood system. The  -axis indicates the scale of the texture. As we can see from the graph, features corresponding to neighbourhood system   consistently outperform other feature sets at all scales. The best features are obtained with neighbourhood system   and scale  . The selection process was repeated for various colour channels: red, green, blue and grey, and we found that the red channel yielded the best parameters. A filter-based method for selecting features provides generic features independent of the segmentation algorithm [ 18]. To ensure that the features are also specific to the application, they were evaluated based on an application-specific criterion: the difference in the log-likelihood of the correct and the incorrect class of a given texture patch given by    where,   is the texture patch of a particular class. The feature set that best describes the texture corresponds to the maximum  . Figure&amp;nbsp; b validates the Fisher Score graph by showing that the neighbourhood system,  , provides the most suitable set of texture features across various scales. The region parameter   plays a critical role in segmentation. An incorrect   value results in unbalanced prior and likelihood terms leading to an incorrect segmentation. Thus, the optimal value of   is determined to yield the best segmentation performance according to the error measure ( 8) using the training images. The choice of the error measure is motivated by the objective of the application—detection of the center of the   in the image. Neighbourhood system   was used in the experiments. As the earlier experiments could not establish the operating scale of the texture unambiguously, several different scales were used in this experiment. The plot of   at different values of   is shown in Fig.&amp;nbsp; . A minimum error of   pixels (33.24 mm) was obtained at  . Other model parameters used in this experiment are as follows: The texture parameter values corresponding to filter   and neighbourhood system   are  ,    ,  ,   and  .    5 Segmentation performance  Based on the feature selection process, the best texture features were obtained at the scale of sz = 4 and   along with the region parameter   and the neighbourhood system  . The performance of the segmentation algorithm was tested with 92 test images—43 grass-images and 49 rumex-images, and the results were compared with the algorithms A2. Table&amp;nbsp; 2 shows the confusion matrix of A1 and A2. The detection rate of A1 is 97.8&amp;nbsp;%, while that of A2 is 87&amp;nbsp;%. It is important to note that A1 also has better false negative and false positive rates than A2 showing that A1 is able to handle a wider variety of rumex-images. Finally, the segmentation error  , averaged over 49 rumex-images, of A1 and A2 are 56 and 308&amp;nbsp;mm, respectively. Figure&amp;nbsp;  shows some examples of the segmentation results of A1. The algorithm A1 takes 0.18&amp;nbsp;s to process a single image of size  pixels. Its computational speed, although slower than A2—which can process a single image at 0.05&amp;nbsp;s—is still fast enough for real-time application. The time comparisons were carried out on an Intel Core i5, 2.4&amp;nbsp;GHz processor with 4 GB RAM.
Springer.tar//Springer//Springer\10.1007-s00138-012-0471-z.xml:Vision system for tracking handball players using fuzzy color processing:Fuzzy Logic Player tracking Player statistics Image processing Game analysis Video processing Player detection:  1 Introduction  Sports play an important role on nowadays society and there is an increasing interest by the sports’ community on having mechanisms that allow them to better understand the dynamics of players and teams. Often, this information is manually extracted by operators that, after the game, visualize game recordings (frequently TV footage) and perform hand annotation, which is a time consuming and error prone task. The effort and time involved in such tasks are huge and the results are subjective to the person performing it, as mentioned in&amp;nbsp;[6, 31, 32]. From these works it is possible to notice that recording a video of the entire field is already very useful for sports’ experts since they are able to visualize all players and ball movements. This allows them to identify weaknesses, define tactics and new training directions to improve the teams’ global behavior, reduce the teams’ weaknesses and explore the opponents’ weak points. However, the next step, automatic detection and tracking enables a systematic, objective, accurate and consistent analysis. The process will also be much more time efficient. In this paper, we present a non-invasive, automatic visual system for detecting and tracking handball players based on a Fuzzy inspired methodology [35]. The document structure is as follows: the next section presents relevant information on image segmentation methodologies and automatic visual systems for detecting and tracking players. Section 3 discusses the proposed architecture principles, providing an overview of the methodology used. Section 4 presents the results achieved, including a detailed sensitivity analysis and, finally, Sect. 5 concludes this paper with the main conclusions and further investigation directions.    2 Fundamentals and related research  The field of automatic player/team detection and tracking represents a very challenging problem due to the complexity of sports analysis itself, as there are several similar fast moving targets that are frequently changing direction and contacting with each other. Although there are two main categories of technologies used for automatic detection and tracking: intrusive (where special tags or sensors are placed on the targets) and non-intrusive (where there are no extra objects in the game environment). This paper only addresses the second category because one of the major problems of intrusive systems is that usually regulations do not allow their usage on official games, where the most interesting information for game analysis is provided. Non-intrusive systems have vision as the main sensory source and, therefore, devote a great effort on developing image processing methodologies that allow a good video/image segmentation. The two following subsections provide a good insight into the existing video/image segmentation methodologies as well as into systems devoted to the player detection and tracking problem. Video segmentation is the first step, and probably the most critical, in any video (image) processing system because the quality of the final result is highly dependent on a good segmentation. There are two main categories of video segmentation:    Temporal segmentation: segments the video into meaningful temporal sequences. It is usually used as the first step of video annotation and segments the video taking into account similarities/dissimilarities between successive frames [19].   Spatial segmentation: aims to divide the content of each frame into homogeneous regions that correspond to independent objects.  The focus of this work is on spatial segmentation; hence, for a detailed survey on temporal video segmentation please refer to [ 19]. Temporal segmentation: segments the video into meaningful temporal sequences. It is usually used as the first step of video annotation and segments the video taking into account similarities/dissimilarities between successive frames [19]. Spatial segmentation: aims to divide the content of each frame into homogeneous regions that correspond to independent objects. Spatial video segmentation inherits many of the methodologies used for image segmentation and can also make use of the temporal characteristics inherent to video. Image segmentation methodologies can be subdivided into the following categories [ 8]:      by determining the peaks or modes of the uni/multi-dimensional histogram of the image.     by grouping the image feature space into a set of meaningful groups or classes based on intensity, color or texture characteristics of pixels.     which includes region growing, Watershed transform and region split and merge. These methods attempt to divide the image domain based on the fact that adjacent pixels in a same region have similar visual features (color, intensity, texture or motion).     that segments the image by finding the edges of each region using an edge detector.     allow classes and regions to have a slight uncertainty and ambiguity, which is generally the case of image processing.     allow parallel processing and adding non-linearities. They can be used either to pattern recognition, classification or clustering.  Nowadays, the tendency is to aggregate techniques from different categories to achieve better results. A typical example of this is the JSEG algorithm [ 10] that initially clusters colors into several representative classes, afterwards replaces each pixel with its corresponding color class label and only then applies a region growing process directly to the class map. by determining the peaks or modes of the uni/multi-dimensional histogram of the image. by grouping the image feature space into a set of meaningful groups or classes based on intensity, color or texture characteristics of pixels. which includes region growing, Watershed transform and region split and merge. These methods attempt to divide the image domain based on the fact that adjacent pixels in a same region have similar visual features (color, intensity, texture or motion). that segments the image by finding the edges of each region using an edge detector. allow classes and regions to have a slight uncertainty and ambiguity, which is generally the case of image processing. allow parallel processing and adding non-linearities. They can be used either to pattern recognition, classification or clustering. As stated previously, videos can also be segmented based on temporal properties, namely on motion along time. In order to perform this task there are two main approaches: Background Subtraction and Optical Flow. Background Subtraction methods model background as a simple static image without objects; more complex variations propose methods such as moving average [14], median filters or mixture of Gaussians [13]. Optical Flow strategies [2] are based on the motion of brightness/color of parts of the image associated with objects—the method assumes a linearization of the objects’ trajectories and, therefore, primarily applies to small displacements. Following the tendency, the proposed method for the video segmentation step is a methodology that combines temporal information through dynamic background subtraction with physical and color information through a Fuzzy color calibration based on region growing and pixel labelling. There are two main streams of research in this area depending on how video footage is obtained: television broadcasting or dedicated camera systems. In both cases, advanced image and video processing techniques must be used due to the complexity of the problem. Usually, these systems involve three steps: image segmentation, player detection, and finally, player tracking. Nowadays, most games (especially soccer) are filmed by television networks and are available for everyone; therefore, this information is not hard to obtain and to create instruments to perform player detection and tracking. However, most of the systems use a single broadcast camera that does not provide an entire view of the playing area and only gives useful information on wide angle images. The first step (image segmentation), in the majority of the literature, consists of modelling the background which can be based on color histograms [33], color intensities [26], clustering [18] or more robust and complex methodologies, such as Mixture of Gaussians (MOG) [4, 16, 36]. The usage of dynamic methodologies such as dynamic clustering and MOG allows having a more robust solution that can better adapt to colour and luminance changes which is crucial in outdoor applications. Nevertheless, MOG methodologies may not be effective in highly static matches, because players that stay still for too long may be absorbed into the background model. Afterwards, the remaining regions are filtered to identify players, using simple clues such as physical constraints [18, 26], boost cascade detector of Haar features [33], clustering algorithms [16], colour template matching [12] or support vector classification [36]. The work [22] adopted a slightly different methodology and performs player detection without background subtraction using two classifiers that are previously trained with hand selected samples. It is important to notice that the usage of classifiers, template matching and boost cascade detectors, requires prior training. Therefore, before analyzing each game, a set of representative samples must be collected. The usage of background subtraction methodologies usually speeds up the processing time, since only a few regions need to be analyzed with more complex algorithms. Player tracking is achieved with weighted graphs [26], Kalman Filters [22], fast level contours [18], CamShift [16] or probabilistic models such as Markov Chain Monte Carlo [33] and Particle Filters [15, 36]. The choice of the tracking methodology plays an important role when dealing with occlusion and merging situations. For example, CamShift cannot deal well with merging, even if it is for short periods of time. On the other hand, probabilistic models are well suited for the non-linear movement often made by the players on the field. However, the computational effort is high. Once the players’ trajectories have been detected, some authors still convert the players’ coordinates into real-world coordinates [5, 16, 33], which is extremely important if ball tracking [5, 18, 36] or high-level game analysis [5, 16, 36] is to be performed. Despite these efforts, it is possible to verify that the detection and tracking accuracy is never 100&amp;nbsp;% because, although temporary occlusion may be handled, more persistent situations, like overcrowded scenes, serious video blur or abrupt camera motion, may lead to miss-tracked players. Dedicated camera systems are mostly used in indoor environments because the smaller playing area makes it possible to use a single camera [25]. However, authors tend to use two [3, 24, 27] or even more cameras [1, 9, 11, 28] to cover the entire field. Benefits of using multiple cameras include higher resolutions, overlapped regions, which allow minimizing occlusion problems and open the possibility of 3D localization [1]. Dedicated systems follow the same processing flow as broadcast camera-based systems. So, the first stage usually consists of eliminating the play field area (that has no useful information), either by static background modelling [25] or dynamic methods, with more or less complex methodologies (ranging from median filters to MOG, or mixture of both) that take into account light variations [3, 9, 11, 24, 27, 28]. The results of the previous step usually contain noisy regions, but only some of them really correspond to players. These regions are detected using morphological filtering [11] aided, more recently, by an Adaboost classifier [3], color histograms [27, 28], templates [24] that are updated along the game or occupancy maps [1, 9] generated from multiple views of the field. Tracking is performed via weighted graphs [3, 11], probabilistic methodologies [25, 27], Kalman filters [28] or more simply methodologies based on velocity constraints [1] or fixed area around the players [9]. An interesting aspect—“Closed world assumptions” —used by [20] and followed by [24] defines several heuristics that can be used on semi-controlled environments and include the partition of the world into Voronoi cells that may be occupied by a single player and, hence, improve the tracking. Dedicated systems place the cameras at specific locations and, therefore, most authors compute the cameras homographies and translate the players’ positions into world coordinates [1, 3, 11, 24, 25, 27, 28]. Additionally, [9] scans the player’s regions for digits to determine the player’s number, [28] is able to track the 3D position of the ball and [27] performs high-level game analysis.    3 Proposed vision and processing system  The challenges of defining a vision system able to identify and track the game elements in an invasion team game are huge due to the dynamic and spatial characteristics of the game itself. In fact, handball is played in an area of 20 by 40 meters and it is quite a dynamic game, with high physical contact among players and rapid movements (a player can achieve velocities higher than 5&amp;nbsp;m/s). These characteristics impose a careful choice on the system’s architecture, which includes choosing not only the cameras and their disposition but also defining the software design. The system must cover the entire handball field including the extra border, the image should have enough resolution to correctly detect all players and capture images with a frame rate adequate to the involved speeds. The characteristics of the problem and of the sports hall place the ceiling as the best spot to set the camera system, since there is no interference from the crowd, a single player never fills the entire field of view of the camera and a bird’s-eye perspective usually means less occlusion and/or merging situations (this solution was also adopted by [21, 24]).
Springer.tar//Springer//Springer\10.1007-s00138-012-0472-y.xml:Estimating 3D human shapes from measurements:Three-dimensional reconstruction Human models Statistical prior:  1 Introduction  Realistic 3D human shapes are required in various applications, such as the design of products that fit a target population. Typically, these shapes need to have certain characteristics or be samples from a population. Although it is possible to digitize humans using 3D imaging technologies, it is impractical to find and scan suitable human subjects for each individual application. On the other hand, there is a long history of using anthropometric measurements to describe the human shape. These measurements are linear and curvilinear distances between anatomical landmarks or circumferences at predefined locations. In spite of providing limited shape information, they are readily available, and therefore widely used. Intuitively, the linear measurements should encode information about the body shape. If an accurate 3D human shape can be inferred from a simple set of measurements, traditional anthropometric data can be leveraged to create 3D datasets without resorting to expensive scanning equipment. The early attempts at solving this problem deform a generic human model to fit the measurement data. However, the models thus obtained are not necessarily human forms because generating a human shape from a sparse set of measurements is an under-constrained problem. More sophisticated methods use knowledge of the human shape learned from a database of 3D scans. These methods build a statistical model from a set of registered 3D scans and establish a relationship between the measurements and the shape space, which allows to predict detailed 3D shapes from sparse and partial shape data. Apart from predicting 3D shapes from measurements [1, 21], other partial shape information, such as marker positions&amp;nbsp;[3] and 2D images [8, 15], has also been used to reconstruct 3D shapes. Existing methods that predict the 3D shapes using a statistical model limit the generated shapes to the space spanned by the shape model and local variations of human shapes that are outside this space cannot be predicted accurately. This means that these methods require a 3D database that accurately represents a target population. For instance, human subjects of different ethnicities and different weight classes must be present in the database. To apply these techniques, one must carefully choose a target population and acquire a representative database. In reality, the acquisition of a 3D anthropometry data that represent a target population well is complicated and expensive. For instance, the acquisition of the Civilian American and European Surface Anthropometry Resource took 4&amp;nbsp;years and cost $6 million.1 In this paper, we address the problem of estimating 3D human body and face shapes given a set of anthropometric measurements. Our main goal, in contrast to previous work, is to predict shapes that are inside the space of human shapes, but still account for local variations not captured by the training data. This extrapolation allows us to create human shapes based on relatively small 3D databases. As most of the previous approaches, our method does not take posture or expression changes into account. That is, we assume that all of the training data are in a standard pose. This scenario is commonly assumed since in a typical 3D anthropometry survey, the human subjects are asked to maintain a standard posture. The rest of the paper is organized as follows. Section&amp;nbsp;2 reviews previous work on estimating 3D body or face shapes from partial data. Section&amp;nbsp;3 gives an overview of our approach, and Sect.&amp;nbsp;4 outlines the details of a shape refinement step, which constitutes the main contribution of this paper. We aim for design applications in engineering where accuracy is essential. We show in Sect.&amp;nbsp;5, through experiments, that straightforward application of existing techniques does not produce satisfactory results. Finally, Sect.&amp;nbsp;6 concludes the work.    2 Related work  This section reviews existing work that aims to estimate 3D human shapes based on partial input information. We categorize the related literature by the type of input data. DeCarlo et al.&amp;nbsp;[10] presented an early attempt at using anthropometric measurements to compute human face shapes. The method generates a random set of measurements based on a set of rules that capture, for instance, typical facial proportions. To create a 3D face shape, the method deforms a template model to fit these measurements using a free-form deformation technique called variational modeling. Variational modeling allows to give the measurements as constraints when deforming the template. To ensure that the estimated shape remains in the shape space of human shapes, much care is taken when creating the measurements that are used as constraints. While this method generates human-like face shapes using a statistical model, the accuracy of the shapes created by this method is limited by the sparse set of measurements used to represent the training data. That is, the results are overly smooth and do not contain realistic details. The following approaches estimate human body shapes from a discrete set of measurements, all proceeding by learning a linear or near-linear mapping between the training set of human shapes and the space of measurements and then using this mapping to predict a shape based on a new set of measurements. While these approaches work well when the predicted shape is inside the shape space spanned by the training set, they do not allow for extrapolations from this shape space. Wang&amp;nbsp;[25] uses a parameterized database of human shapes consisting of feature patches to find a new human shape based on a given set of measurements. Feature patches are initialized to smooth patches that are subsequently refined to capture geometric details of the body shape. While this refinement does not maintain the smoothness of the patches, the final results are visually smooth and do not contain realistic localized shape details. The approach finds models in the database with measurements similar to the given set of measurements and computes the new human shape as a linear combination of these models. Wei et al.&amp;nbsp;[26] use a similar approach, but the models are represented using a layer-based representation. Allen et al.&amp;nbsp;[1, 2] start from a parameterized database of human bodies in similar poses and perform principal component analysis (PCA) of the data. The training database is used to learn a linear mapping called feature analysis from the set of measurements of the training data to the PCA space. Feature analysis can be used to compute a new PCA weight based on a new set of measurements, and the learned PCA model allows to compute a new body shape from this PCA weight. Chu et al.&amp;nbsp;[9] perform feature analysis on a database of human shapes consisting of feature patches. Seo and Megnenat-Thalmann&amp;nbsp;[21] represent the human body as a triangular mesh with an associated skeleton model. As with the approach of Allen et al., this approach reduces the dimensionality of the data using PCA. The approach learns a mapping from the set of measurements of the training data to the PCA space using an interpolating radial basis function with Gaussian kernel&amp;nbsp;[12]. Hasler et al.&amp;nbsp;[17] apply the two previously reviewed approaches to a new representation of human body shapes that is posture invariant. Their method simultaneously models body pose and shape. Recently, Baek and Lee&amp;nbsp;[4] presented a technique that uses hierarchical clustering to build a statistical model of the training data. The approach proceeds by clustering the training database and by performing a multi-cluster analysis. To predict a body shape based on a set of input measurements, the approach finds the shape within the learned shape space that best describes the measurements using an optimization of shape parameters. It is shown experimentally that accurate and visually pleasing body shapes are estimated when the input body sizes are inside the shape space spanned by the training data. This approach is conceptually similar to the first optimization step of our algorithm. Hence, we expect that this approach does not allow to model shape variations that are outside the shape space spanned by the training data. Anguelov et al.&amp;nbsp;[3] aim to estimate a 3D human body shape based on a sparse set of marker positions. This technique is useful when motion capture data is available. Anguelov et al.’s SCAPE model represents the human body as a triangular mesh with an associated skeleton model. As with the approach of Allen et al. this approach reduces the dimensionality of the data using PCA. Anguelov et al. compute a new triangular mesh based on a set of marker positions by adjusting the PCA weights to solve a non-linear optimization problem. Finally, we review approaches that aim to estimate human body shapes based on a set of input images. The following approaches proceed by learning a correlation between a training set of 3D face or body shapes and a set of derived 2D images and by using this learned correlation to predict a shape based on a new set of 2D images. These approaches work well when the predicted shape is inside the shape space spanned by the training set. However, they do not handle optimizations outside the learned shape space of 3D models. Blanz and Vetter&amp;nbsp;[6] estimate a 3D face shape from a single input image in neutral expression. They start by building a parameterized database of textured 3D faces and by performing PCA on the shape and texture data. Given an input image, the learned PCA space is searched to find the textured shape that best explains the input image. Seo et al.&amp;nbsp;[22] estimate a body shape of a human in standard posture from two images. Starting from a parameterized database of human meshes in similar poses, the approach performs PCA of the 3D data. Given the two images, the learned PCA space is searched to find a set of PCA weights that corresponds to a 3D shape that matches the input images well. Boisvert et al.&amp;nbsp;[7] solve the same problem using an optimization that aims to find the optimal set of PCA weights. Chen and Cipolla&amp;nbsp;[8] aim to estimate the human body shape in a fixed pose based on a given silhouette. Starting from a parameterized database of human meshes in similar poses and a set of corresponding silhouettes, the approach performs PCA of the 3D and 2D data separately. The approach then computes a mapping from the PCA space of the silhouette data to the PCA space of the 3D data using a shared Gaussian process latent variable model (SGPLVM)&amp;nbsp;[23]. Given a new silhouette, the approach maps the silhouette into silhouette PCA space and uses the SGPLVM to map to the PCA space of the 3D meshes. Ek et al.&amp;nbsp;[13] use a similar approach to estimate the pose of a human body based on a given silhouette. Recently, statistical prior models allow to track the pose of a human body even when occlusions and self-occlusions are present&amp;nbsp;[20]. Guan et al.&amp;nbsp;[15] estimate both the shape and pose of a human body from a single photograph based on the SCAPE model. When adjusting the PCA weights, the shape is deformed to best match the image in terms of a shape-from-shading energy. Weiss et al.&amp;nbsp;[27] propose to use the SCAPE model to compute a 3D body scan from noisy Kinect data. Hasler et al.&amp;nbsp;[16] estimate both the shape and pose of a human body from a photograph of a dressed person.    3 Approach  This section outlines the proposed approach. As input to the method, we are given a database of triangular manifold meshes  of human bodies or faces with similar posture or expression and a set of measurements . Let  in  denote the measurements corresponding to . Furthermore, we are given a set of distances . Our aim is to estimate a shape  that interpolates the distances . As previous methods, the approach proceeds by learning a correlation between the shapes and the measurements. When predicting a new shape, our approach finds an initial solution based on the learned correlation. Unlike previous approaches, however, our approach refines this solution to fit the measurements using two steps of non-linear optimization. First, we optimize the shape of the model with respect to the learned shape space. That is, we aim to find the point in the learned shape space that best describes the measurements. This gives a realistic human shape drawn from the distribution fitted to the training data, which can only contain local shape variations present in the training data. To account for other shape variations, we perform a second mesh-based optimization. This optimization deforms the shape to fit the measurements as close as possible while satisfying a smoothness constraint without using prior knowledge of the body shape, and therefore predicts shapes with local variations not present in the training database. This second optimization step is the main reason why our approach, unlike feature analysis, can model shape variations not present in the training database. When using a mesh-based deformation that optimizes the sought measurements, we can only expect to obtain a realistic human body shape if we start with a body shape that is close to the solution. For this reason, we start from the shape in the learned shape space that best describes the sought measurements as opposed to starting directly from the point in shape space predicted by the learned correlation. Anthropometric measurement represents a valuable source of human shape information, from which we can infer 3D shapes for design applications. To learn a relationship between anthropometric measurements and a set of 3D models, the measurements computed from the models should be equivalent to the measurements conducted by a human operator. The measurements can be one of three types: Euclidean, geodesic, and circumference. A Euclidean or geodesic distance can be easily computed from two vertices on the model. A circumference can be computed by intersecting (part of) the model with a plane, finding the (possibly closed) polygonal chain of the intersection that contains a specified vertex, and measuring the circumference of the convex hull of this chain. This type of measurement is shown for the hip and waist circumferences in Fig.&amp;nbsp;. The reason we measure the length of the convex hull instead of the length of the chain itself is that many anthropometric measurements, such as the chest circumference, measure the length of a convex hull. We specify the intersecting plane  using a vertex  and a normal direction . Furthermore, we specify a part of the model that is to be intersected with the plane . This is necessary for some measurements, such as for the chest or hip circumference, where we wish to only intersect the torso (and not the arms) of the model with . Note that the intersection of  with a subset of  consists of a set of polygonal chains, where every vertex of the chain is either a vertex of  or the intersection of an edge of  with . We now consider learning a correlation between the set of shapes  and the corresponding measurements . To do this, the database of the human shapes needs to be parameterized, a process that computes point-to-point correspondences among the shapes. This is in general a difficult problem&amp;nbsp;[24]. In practice, anthropometric markers identifying salient anatomic positions are often used to guide the correspondence process. Such markers are provided in 3D anthropometric surveys, for example, the Civilian American and European Surface Anthropometry Resource (CAESAR) database&amp;nbsp;[19]. In this work, the known marker positions are used to deform a template shape to each subject of the database&amp;nbsp;[1, 28, 29]. The Euclidean and geodesic measurements are specified by their two endpoints and the type of distance to be considered and the circumference measurements are given by one plane (specified by a vertex and a normal vector) and the part of the body to be intersected. Since the database is parameterized, the vertices and body parts can be given in terms of their vertex and triangle numbers on the mesh. With this information, a set of distances  can be computed for . We learn a mapping between  and the space of human body shapes using feature analysis. We choose this model because it describes both the shape space of  and the mapping from the space of measurements to the learned shape space using linear functions. This allows us to compute the gradients of the energy functions used in Sect.&amp;nbsp;4 explicitly, and hence, to use an efficient numerical solver to optimize the energy functions. To apply feature analysis, we first perform PCA on the meshes . In PCA space, each shape  is represented by a vector  of PCA weights. PCA yields a mean shape  and a matrix  that can be used to compute a new shape  based on a new vector of PCA weights  as . Recall that the aim is to create a shape based on a point  in . To achieve this goal, feature analysis learns a linear mapping from  to . This yields a matrix  that can be used to compute a vector of PCA weights  based on  as , which generates a shape  based on  as . To give all PCA coordinates the same weight, we normalize each entry of the PCA weights by its corresponding PCA eigenvalue before performing feature analysis. The mapping between the measurements and the PCA weights of the 3D shapes learned in the previous section allows us to find an initial shape  given . However, straightforward application of feature analysis has the disadvantage that the shapes obtained for atypical  are not contained in the space of human shapes (e.g. see Fig.&amp;nbsp;). Since the subsequent refinement process depends on this initialization, it is important to restrict  to be within the range of human shapes. Hence, we restrict  such that each dimension  is at most  times the standard deviation of the PCA space along dimension . This choice is based on the assumption that the shape space is modeled as independent Gaussian distributions and most of the shapes are located within  standard deviations of the mean for a suitable parameter . This step restricts the reconstructed shape to stay within the space of human shapes that was learned using PCA. In our implementation, we use the normalized weight vector to find an initial shape . The shape generated from the PCA weights can only serve as an initial estimation, because the shape space spanned by PCA is limited by the sample size and may not account for all human shape variations. To find a shape that respects the required measurements while staying in the learned shape space, we need to refine . The following section presents our novel method for shape refinement in detail.    4 Shape refinement  This section outlines a novel approach to refine the initial estimate  to respect the required measurements while staying in the learned shape space. We formulate the refinement problem as an energy minimization problem. Let  denote the vertices of  and let  denote their position vectors. Furthermore, let  be the vector of PCA weights corresponding to , where  is the pseudo-inverse of .
Springer.tar//Springer//Springer\10.1007-s00138-012-0473-x.xml:Informative patches sampling for image classification by utilizing bottom-up and top-down information:Patch sampling Image classification Bottom-up Saliency Top-down:  1 Introduction  Generic object categorization has been one of the challenging problems in the field of computer vision. Early works on object categorization utilized global features [1, 2], such as colour and texture extracted from the whole image, to represent image contents. However, due to large intra-class variation of object categories as well as difficulties caused by viewpoint, lighting and occlusion, performance of global features is limited. On the other hand, object categorization based on discriminative local features [3–5] demonstrated superior performance as compared to global features. Especially, an approach called bag of visual words [6] has given state-of-the-art results [6–8]. In bag of visual words, first, a set of patches are extracted from training images, each of which is described by descriptors such as SIFT [9]. Then, a codebook of visual words is constructed by applying vector quantization to the obtained patch features. Centres of quantized features are taken as visual words, all of which are combined together to create the visual word codebook. In image representation stage, an input image is represented by assigning its patch features to the nearest visual words in the codebook. Consequently, an image is represented as a histogram indicating frequencies of visual words appearing in it. This procedure has shown to be able to create robust and characteristic representations for image categorization. As an essential part of the bag of visual words framework, image patches used to create image representations affect the final image classification performance significantly. At first, patches are extracted from interest points detected by detectors like Harris-Laplace [10] and Difference of Gaussian [9]. However, in these approaches, patches are extracted through processing low-level image information in local regions. Although detected interest points are salient in local image regions, it does not necessarily guarantee that they are representative for the image and the category from which it comes. Later, the work in [11] showed that performance of randomly sampled patches can outperform interest point detectors in image categorization, when the number of sampled patches is large enough. However, with the number of extracted patches increased, the computation and memory cost will also increase accordingly, in both stages of patch extraction and image representation creation. In fact, it is reasonable to believe that for image classification based on local features, patches extracted from certain regions will be more useful than patches extracted from other regions. Given an input image, if we can find its informative regions and extract patches from them, a small number of patches will enable the creation of a discriminative image representation. Therefore, procedures to evaluate regions based on their representativeness for the image and the category from which they come are needed. In this paper, we propose to employ bottom-up and top-down information in patch sampling. In the proposed method, each input image is first divided into regular grids. Then, each grid is evaluated to see how informative it is for the image and the category from which it comes. Afterwards, based on the evaluation result, a saliency value indicating its informativeness is assigned to the grid. So that, for the input image a saliency map is obtained. Finally, patch sampling from the input image is performed in accordance with the created saliency map. Specifically, the bottom-up branch is a data-driven process, it explores the low-level information of a given image to find its interest points. Then, instead of extracting patches from each individual interest point, the modes of interest point distribution in the image are found and used to evaluate image grids. Therefore, bottom-up saliency values of image grids are determined not only by low-level information from local image regions but also its importance within the image. This procedure will extract informative image patches more effectively than just extracting patches from each individual interest point. On the other hand, as for the top-down branch, the statistical property of the whole training image set is used as global information, which can also be deemed as a kind of prior knowledge for the given image, to evaluate image grids in respect of their informativeness of categories. Through this manner, grids useful for discriminating categories can be assigned high saliency values. We call this procedure top-down guidance for patch sampling, which can guide the patch sampling towards image regions discriminative with regard to image categories. In the following parts, we first introduce the calculation of bottom-up and top-down information respectively, then a strategy for fusing them are also proposed. Figure   illustrates the framework of the proposed method. The proposed bottom-up information and top-down information are expected to be used for evaluating image regions for local feature based image classification. So that local image patches can be extracted based on the evaluation results. In image classification, two common tasks are scene classification and generic object classification. Some public datasets have been provided by the previous works [12–15]. The properties of the scene images and the object images are a little different, as a result the proposed methods have different performance for them. For scene images, images from different Scene classes often share similar components, for example both the forest class and the coast class contain the component sky and the component trees. But, in different classes, the ratio is different. There are more tree component in the forest class, and there are more sky component in the coast class. Therefore, in this case, utilizing the statistical properties of the training scene images will be helpful for us to find informative regions in the images. On the other hand, for the object categories, some of the images are taken by amateurs, the variation of each object instance is large. At the same time, object instances often appear in unrelated backgrounds. Consequently, bottom-up information from processing low-level information of each single image will be more useful than the statistical property of the whole training sets.    2 Related works  Currently, in image categorization based on bag of visual words framework, patches for creating image representations are extracted mainly based on bottom-up process, either by processing low-level image information to detect salient points [9, 10] or by sampling randomly or regularly [11]. These approaches are not able to guarantee extracted image patches are informative for creating discriminative image representations. Since in the process of patch extraction, relation between extracted image patches and the image as well as the category from which they come is not taken into consideration. There are works using only informative features for creating image representations by applying feature selection to extracted image patches. In these approaches, high level information is incorporated for the selection of an informative subset. For example, in [16], point-wise mutual information between extracted patches and image categories is calculated to evaluate how informative a patch is for the category from which it comes. In [17], the authors propose to use co-occurrence and spatial information among image patches to construct a contextual saliency measure. This measure is able to evaluate how predicative a patch is for other patches in the same image. When a patch is highly predictive of other patches, a high saliency value is assigned to it and vice versa. As to methods mentioned above, a large number of raw patches need to be sampled and described in the first step. After that, each extracted patch is assigned a saliency value based on some selection criterion, then weighted re-sampling is performed over them. So that patches with higher saliency values are more likely to be selected for creating image representations. These methods are not able to increase efficiency of computation and memory usage, since patch selection is performed after they are extracted and described. Considerable amount of resources has been spent for obtaining patch features and evaluating them. Feature selection is also applied to select discriminative prototype features from a pool of candidates. Huang et al. [18] used a learning based method for prototype patch selection. They combined SVM and boosting to select one hundred informative prototype patches from ten thousand ones in the training stage. Then, based on the selected patches, input images are represented and classified. In [19], a vocabulary of prototype parts are constructed by selecting the most discriminative ones from a pool of training image parts using AdaBoost. Subsequently, in the image representation stage, features from an image are compared to entries of the vocabulary to determine whether to preserve them or not. Another work [20] used informative image fragments for classification. The authors selected class specific fragments conveying the maximal amount of information about classes. In this method, fragments are selected from a large pool by using a greedy search algorithm along with an information measure. In addition, there are works for selecting only a small subset of discriminative words from an initial large codebook [21] for creating image representations. However, experiment results showed that codebooks with reduced number of visual words tend to demonstrate inferior performance than the original codebook. Approaches which are more closely related to our work are [22, 23]. These methods are designed to locate image regions that are informative for image categorization, beforehand. Subsequently, in the patch extraction stage, patch sampling is biased towards these regions, so that more image patches are extracted from more informative regions. Specifically, in [22], loose top-down prior information of each object category is learnt from labelled segments of training images. Then, for an input image, the obtained top-down information is explored to generate a probabilistic map, which indicates the probabilities of objects of interest to appear in the image. The approach in [23] proposed to learn object categories against backgrounds and use the prior knowledge about where the classifier can detect discriminative features to create a saliency map. These approaches are able to build probability map or saliency map using learned prior information about categories of interest from training images. However, learning the prior knowledge of every specific object class increases human labour and the complexity of these approaches, and makes them sensitive to image contents. Furthermore, these approaches are not able to be applied to scene image classification. In this paper, we propose a method to extract informative image patches for creating image representations without extracting a large number of raw patches first. At the same time prior information for specific categories is not needed. Through the proposed method, representative image patches are able to be extracted and computation efficiency can be improved.    3 Image patch extraction based on bottom-up information and top-down information  In our method, input images are divided into regular grids. Then, each grid is evaluated based on bottom-up information and/or top-down information, so that a saliency map for the input image is able to be created. In the following part, we introduce bottom-up information and top-down information used for evaluating image grids and give the approaches for calculating saliency maps based on these two kinds of information. There exists considerable amount of literature on local interest point detectors [24–26]. Interest points detected by detectors are invariant of viewpoints and geometric transformations to some extent [24]. Therefore, patches at these interest points are robust for image contents. To evaluate image grids based on bottom-up information, we adopt interest point detector to process low-level image information. However, instead of extracting patches from individual detected interest points, we extract image patches with all interest points in the image considered, so that image patches are extracted based on global information of interest points in the whole&amp;nbsp;image. Through exploring global information of all detected interest points in an image, image patches that are representative of the image contents are more likely to be extracted. To utilize bottom-up information, We first detect interest points in an image by using Harris-Laplace detector [ 10]. Harris-Laplace detector is used because of its robust performance for interest point detection. In fact, other interest point detectors can also be used. After that, distribution of interest points within the image is modelled as Gaussian Mixture Models. Finally, saliency values of grids of the regularly divided input image are determined based on the obtained  . The algorithm for this process is described in Algorithm 1. Specifically, after we obtained interest points from an image in step 1, we apply the mean-shift to the spatial coordinate vectors of detected interest points, so that regions with dense interest points are able to be localized. Figure   gives sample images in which detected interest points as well as localized interest point modes are displayed. In step 3, distribution of interest points in the image is modelled as Gaussian Mixture Models based on the result of step 2. The number of Gaussian models in   is set to be the same as the number of modes of interest point distribution resulted from the mean-shift process. Interest points belonging to modes in the mean-shift procedure are assumed to be generated by the corresponding Gaussian models in  . Hereafter, parameters   of the   Gaussian model in   are calculated as follows:                                      where   is the spatial coordinate vector of the   interest point belonging to Gaussian model  .   is the number of interest points belonging to Gaussian model  . After we obtained the  , each grid of the regularly divided input image is assigned a saliency value on the basis of it. The saliency value assigned to a grid is calculated by using the equation below:       where   is the spatial coordinate vector of the centre of the   image grid.   is the number of modes in  . After all grids of the input image have been assigned a saliency value. Saliency values of grids within the image are normalized as:       So that these saliency values are converted into the range  . Figure   displayed bottom-up information saliency maps of sample images. Instead of evaluating image grids by processing low-level image information, top-down information is incorporated by exploring statistical properties of grids of all regularly divided training images. The informativeness of a grid for the category from which it comes is measured with all images in the training set taken into consideration. In this subsection, the approach to create image saliency map based on top-down information is introduced. Given a set of training images, every image is regularly divided. Then, all grids are described by using SIFT feature [ 9]. From these grid features, a set of clusters are obtained by applying  -means to them. After that, statistical property of each input cluster is calculated. In the saliency calculation stage, for a regularly divided input image, each grid of it is evaluated based on the set of clusters and their statistical information. Finally, the saliency value assigned to the grid is determined in accordance with the evaluation result. The framework of the proposed method is given in Fig.  . To create a discriminative representation for a regularly divided image, grids that are representative for the category from which it comes but difficult to be distinguished from other categories need to be investigated in more detail than others. For example, both cat and dog have head, which are representative for the corresponding category, but it is also difficult to distinguish these two parts, because they have similar appearance and structure. It is reasonable to pay more attention to these kinds of parts during image categorization. In information theory, entropy is used to measure the uncertainty associated with a random variable. The more uncertain a variable, the more informative it is. Therefore, in the proposed method, entropy is adopted to measure informativeness of image grids, so that uncertain grids can be assigned high saliency values. In the following part, we give how to utilize entropy to determine informativeness of image grids. After grids from regularly divided training images are extracted and described using SIFT feature, we apply k-means to these obtained SIFT features to create   clusters. Then, the statistical information of each cluster, including its mean vector, covariance matrix and a modified version of entropy, is calculated. The modified version of entropy of a cluster is calculated over all images of all categories by using the equation below:                                      where   is the modified version of entropy calculated for cluster   is the number of categories used for classification, and   denotes the number of images in category   is the ratio between   (the number of features from image   of category   in cluster  ) and   (the number of features from category   in cluster  ).   is the ratio between   and  , which is the number of features in cluster  .   is a constant value. The first term of the modified entropy equation represents the distribution of cluster  over all images of all categories. When a cluster distributes uniformly over all images of all categories, it obtains its highest value. The second term represents the distribution of cluster  over all categories. It is used for penalizing clusters that distribute uniformly across many categories. Since if a cluster distributes across many categories, it may represent common background information which is of no use for image categorization. Consequently, clusters distributing uniformly over many images of a small number of categories obtain higher  value. Clusters in this case contain representative information of a small number of categories, which are similar in appearance and need to be checked in detail to be distinguished. At the same time, the mean vector and covariance matrix of a cluster are also calculated as follows:                         where   is the mean vector of cluster  , and   is the descriptor of the   grid in cluster  , while   is the covariance matrix of grid features in cluster  . All calculated statistical properties are recorded together with their corresponding clusters. For an input image divided into regular grids, each grid of this image is described by SIFT feature as in the previous subsection. To measure how informative a grid is based on the proposed top-down information, we first calculate the Mahalanobis distance between the given grid feature and the obtained clusters as follows:       where   is the Mahalanobis distance between the   grid feature   extracted from the input image and cluster  .   and   are the mean vector and covariance matrix of cluster  . Then,   nearest clusters of the given image grid feature are preserved to calculate its informativeness. The reason to preserve   nearest clusters is to make the result more robust against ambiguity in the grid feature assignment to clusters. Using the   preserved nearest clusters, we calculate a saliency value for the grid  ,       where   is the Mahalanobis distance between grid feature   and its corresponding nearest cluster   ,   and   are constant values.   is the modified version of entropy of cluster  . By utilizing this equation, the closer a grid feature is to a cluster in Mahalanobis distance, and the higher entropy value this cluster has, the larger saliency value is assigned to this grid. Finally, saliency values of an input image are normalized to the range  , in the same way as in the previous section,       Figure   gives top-down information saliency maps of sample images in Fig.  . In order to extract image patches by utilizing both bottom-up information and top-down information, an information fusion strategy is designed to fuse bottom-up saliency map and top-down saliency map. After we obtain the two separate saliency maps, we fuse them through the following equation:       where   is the fused saliency value of grid  ,   is saliency value calculated based on bottom-up information,   is saliency value calculated based on top-down information, and   is a constant value in the range  . For an input image, patch sampling from it is performed based on the obtained saliency map. Assume   patches are extracted from the given image, then the number of image patches extracted from a regular gird   is given as       where   is the saliency value of grid  , the saliency value can be bottom-up saliency value, top-down saliency value or fused saliency value. By following saliency map of the input image, more patches are extracted from more informative regions, while fewer patches are extracted from less informative regions. The calculated number of patches are extracted from each grid with random position and random scale.    4 Experiments  In this section, experiments are designed to assess the proposed patch sampling strategies. Bottom-up information and top-down information as well as their fusion are tested using object category dataset and scene category dataset. Experiments are conducted under the bag of visual words framework. That is the traditional bag of visual words procedure is adopted for creating image representations, after image patches are extracted from images and described by SIFT descriptor. A codebook of 1,000 visual words is used, which is constructed by applying k-means to obtained image patch features from training images. We set , since it is reported to be a good trade-off between accuracy and speed [6]. In image representation creation stage, image patch features are assigned to their nearest visual words in the codebook. Finally, created image representations are classified by using SVM classifier with RBF kernel. Parameters for the SVM classifier are determined through a 5 cross-validation process over the training sets. In our experiments, we used LIBSVM [27]. The effectiveness of the proposed image patch sampling strategies is tested over both object category dataset and scene category dataset. Datasets Caltech 256 [ 12] and Scene class 13 [ 13] are employed, respectively. For Caltech 256, 10 categories are selected randomly from the dataset, and for Scene class 13, all the categories are used. Sample images from the used datasets are given in Fig.  . In each category, 100 images are used, where half of the images are taken as training set, while the left half are used for testing. In each experiment, we implemented it three times with different training and testing set divisions and take the average as the result.
Springer.tar//Springer//Springer\10.1007-s00138-012-0474-9.xml:Algorithmic methodologies for FPGA-based vision:FPGA Embedded systems Parallel processing Hardware:  1 Introduction  Motivated by the demand for high-speed performance, alternative architectures and hardware have been used in computer vision as accelerators. These include Graphics Processing Units (GPUs), Application Specific Integrated Circuits (ASICs), and Field Programmable Gate Arrays (FPGAs). Another motivating factor is power-efficiency for embedded applications. ASICs are especially attractive in this regard, as the lower required clock frequency also leads to overall lower power consumption. A major advantage offered by these architectures is parallelism, which often enables a significant speed-up. However, many state-of-the-art algorithms have been designed for serial processing, where speed is achieved by minimizing the number of serial operations [12, 15, 25, 27, 30]. While it is possible to realize a serial algorithm on a parallel architecture, the parallelism available may end up being wasted or not fully exploited. In addition, the lower maximum clock frequencies available in devices such as FPGAs would also mean that a serial implementation on an FPGA would often be slower than similar implementations on CPUs, which typically run at clock frequencies in the gigahertz range. Furthermore, given that work on finding a set of efficient software algorithms has resulted in notable breakthroughs [12, 15, 25, 27, 30], it is reasonable to expect that it is possible to find a set of similarly important hardware-efficient algorithms. Thus, there is motivation to examine hardware-specific issues in more detail. We have chosen to analyze FPGA-based implementations (rather than a GPU). The reason for choosing FPGAs is that they offer more flexibility in terms of architecture, often with a broader range of options in processing. For example, in a CPU variables are stored in memory, while on an FPGA, there is a choice of retaining variables either in memory (either off-chip or on-chip) or in logic. While GPUs are also parallel, GPU programming is based on writing threads, which are in themselves more similar to a CPU or a software approach compared to an FPGA. For the ultimate efficiency in power consumption, it is also possible to export an FPGA design to an ASIC implementation, through services such as Altera’s Stratix to HardCopy conversion&amp;nbsp;[2]. Designing computer vision for FPGAs presents unique challenges&amp;nbsp;[23]. The designer has to work with limited silicon area and memory, while at the same time also satisfying timing constraints and requirements. Due to its flexibility, there are often multiple ways to implement a given task, and it is not always easy to find the implementation with the best performance. FPGAs are also constrained to use primarily fixed-point arithmetic, where in many cases thorough analysis of the algorithm is needed for the required precision. Furthermore, the need to carefully manage low-level resources means that the design process is longer and more laborious compared to software, where such issues are hidden from the programmer.    2 Contributions  Our main contribution is the proposal of a strategy for the design of FPGA-based computer vision. We advocate the idea of designing these algorithms in a way that properly reflects the goal of running on FPGAs, fully exploiting the advantages and parallelism available. This contrasts with modifying or copying an existing software implementation to make an algorithm more suitable for hardware implementation. We present detailed analyses of two case studies to justify our approach. In the first case study, we present an analysis of FPGA-based corner detection in which two corner detectors are studied. In this case study, we examine aspects in which FPGA implementations are different from their software equivalents. Specifically, we discuss properties that can be exploited to create a fast algorithm. We make a number of observations that point to the advantages of an algorithm tailored for an FPGA. We also formulate a set of principles and guidelines for such an algorithm from the observations in this case study. In the second case study, we use the principles from the previous analysis to design an FPGA-tailored face detection algorithm. We show how these properties enable the design to run at a considerably high frame rate on a low-cost, low-end FPGA device, although a software implementation of the same algorithm would have been infeasible.    3 Related work  While there have been many successful attempts at hardware implementations of computer vision algorithms, there has not been comparable published research in the methodological aspects of this practice. The work of MacLean&amp;nbsp;[23] evaluates the suitability of FPGAs for embedded vision by listing its advantages and disadvantages in specific contexts, and prescribes future research directions for this problem. The more comprehensive work by Bailey et al.&amp;nbsp;[11, 18, 19] aims to find a systematic way of adapting image processing algorithms for hardware. Bailey prescribes a set of recommendations that can be applied effectively to many low-level and intermediate-level tasks, such as connected components analysis. These recommendations include mapping a data flow diagram of the algorithm, using design patterns, and transforming the algorithm by replacing and rearranging operations to better suit hardware. One potentially useful aspect not explored in the works mentioned is the idea of deliberately designing a new algorithm to make it more suitable for hardware implementation, as well as the strategies for doing so. This is the approach we propose in this paper. We have a different focus from the work of Bailey&amp;nbsp;[11] where the focus is on adapting existing methods to hardware. In other words, instead of finding a way to modify an existing method to better suit hardware implementation, we propose a strategy to look at what advantages are available in hardware and to design an algorithm with the goal of exploiting these advantages. The most similar study to our own work is by Kim et&amp;nbsp;al. [21], but with a focus on GPUs. Kim also argues for designing algorithms targeted at the underlying architecture, and arrives at some of the same conclusions as we do. This supports our own work on FPGAs, but FPGA design is different enough from GPUs&amp;nbsp;[16] to warrant its own study. Particularly, there is a range of elements and structures that give FPGAs flexibility not offered by GPUs.    4 Case study in FPGA-based corner detection  In this section, we present an analysis of FPGA implementations of two corner detectors, namely the FAST&amp;nbsp;[25] and the Harris&amp;nbsp;[20] corner detectors. FPGA implementations of these corner detectors have been published previously by other authors [13, 22]. Our contribution is an analysis of our own similar implementations. The non-maximal suppression implementations of FAST are our original work. We discuss some observations of these designs, and from this we form a set of principles to use in our design of a face detector. The Harris corner detector&amp;nbsp;[ 20] aims to find points that contain changes in intensity along all directions by computing the gradients in the   and   directions. Harris found that the “cornerness” of a point can be characterized by comparing the eigenvalues of the auto-correlation matrix  , defined as            where   and   are the gradients in the   and   directions at a point in the image, and   is a weighted window (usually Gaussian) around the point. The operator   refers to convolution. If both eigenvalues of   are large, it indicates that the change in intensity along all directions is large, and the point can be classified as a corner. Harris and Stephens originally avoided the computationally expensive calculation of eigenvalues by proposing the following measure that uses the determinant and trace:       where   and   are the eigenvalues of the auto-correlation matrix  , and   is a constant that is normally between 0.04 and 0.15. Shi and Tomasi&amp;nbsp;[28] later proposed directly computing the smaller eigenvalue, , which under certain assumptions results in more stable corners for tracking. Benedetti et al. [13] have previously published an implementation that uses a variant of the Shi–Tomasi measure for corner score. Benedetti’s measure would result in the same features as Shi–Tomasi (albeit quantitatively different), but avoids the square root operation, which is difficult to implement in hardware. In this section, we present a similar design (with a non-maximal suppression step added), which we will then analyze for hardware insights, and compare with the FAST corner detector. For this implementation, we use the Terasic DE2-115 board&amp;nbsp;[ 7], which contains a low-end Altera Cyclone IV FPGA&amp;nbsp;[ 1] (Fig.&amp;nbsp; ). The TRDB-D5M camera&amp;nbsp;[ 8] is used for video input, which generates a single RGB pixel source. The modules have been written in the hardware description language, Verilog. Figure&amp;nbsp;  shows a block diagram describing the functionality of our design. We use a sliding window to compute   and  . Instead of first saving the video frame to memory and reading from it, in hardware we can take advantage of the fact that pixels can be accessed as they come in from the camera. To apply   Sobel masks for   and  , FIFO row buffers are used to store 2 rows of pixels. We use Altera’s RAM-based Shift Register Megacore&amp;nbsp;[ 10], which allows for intermediary taps at the beginning of each row. With this, we can access points in the two previous rows of pixels, as well as the current one coming in from the camera (3 rows). We store the pixels we need in our computation (the sliding window) in logic-based shift registers. Figure&amp;nbsp;  shows a diagram illustrating how this can be done. As each pixel comes in from the camera, the window is shifted by one pixel. With the pixels in the window accessible immediately, the values for , and  can be computed using a combination of addition, subtraction, multiplication (using hardware multipliers) and bitwise shift operations. Additional row buffers are needed to perform convolution for , and  as in Eq.&amp;nbsp;1. The coefficients of the Gaussian window were chosen to take values that are powers of 2, so that convolution can be done with bitwise shift operations. The corner score in Eq.&amp;nbsp;2 can thus be computed, also using multipliers and subtraction. We chose the value of  to be  so that this is implementable using shift operations, but we could in theory use any value by using hardware multipliers and dividers. The preceding result produces a corner score that is used for the non-maximal suppression stage. For this, we use additional row buffers for the corner score. A  window is used for this purpose. Non-maximal suppression can be performed by comparing the center pixel with its surrounding pixels using comparator logic. A point will be retained as a corner if it is the point with the largest score compared with its neighbors. Where there are multiple points with the same score within a neighborhood, only one of them will be selected. This can be done by using “greater than” comparator logic for points above and to the right of the center pixel, and “greater or equal to” for points to the right and below the center pixel. FAST&amp;nbsp;[ 25] works on the principle that a corner or interest point is dissimilar to their surrounding pixels, specifically the 16 pixels in a Bresenham circle around the point (Fig.&amp;nbsp; ). These pixels are compared to the center pixel in terms of intensity value, and are classified as one of the following: brighter, darker or similar in intensity. The amount by which a pixel is classified as “brighter” or “darker” is set by a user-selected threshold, . The point is accepted as a corner if there is a segment of  contiguous pixels that are classified as brighter or darker where  is a number between 9 and 16 (edges are detected for ). Rosten et al.&amp;nbsp;[26] showed that  produces the most stable points. Thus, this problem can be classified as the detection of specific patterns in a 16 element ternary vector (every element in the vector can have three possible states). In this case, the pattern of interest is a sequence of 9 consecutive bright or dark elements in the vector. The number of possible patterns is . The software implementation in&amp;nbsp;[25] avoids performing a full search by using a decision tree generated using the ID3 algorithm&amp;nbsp;[24] to select an ordering of tests such that a minimal number of comparisons is used to classify a corner. With this decision tree, non-corners are rejected quickly. In C code, this decision tree translates to 4,000 lines of nested if-then-else statements. For the non-maximal suppression stage, a measure of corner score is required. There are a few measures that have been used previously. The first published measure in&amp;nbsp;[25] uses the sum of absolute differences between the pixels in the ring and the center pixel. The measure used in&amp;nbsp;[26] is the maximum  for which a point can still be considered a corner. For this, a binary search is used to find this score. Some authors (notably&amp;nbsp;[27]) have also proposed setting a lower threshold for the FAST detector and using the Harris corner measure (Eq.&amp;nbsp;2) for each detected corner as the corner score, which empirically gives more stable results. Figure&amp;nbsp;  shows our implementation displayed on a VGA monitor. We use the same development platform as in Sect.&amp;nbsp; 4.1.1. Figure&amp;nbsp;  shows a block diagram of the design. Similar to the Harris corner example, we use row buffers for the   window. This requires 6 rows to be buffered in the FIFO. The 16 pixels together with the center pixel are passed to the corner classification module. Instead of using a ternary vector, we form two binary vectors (Fig.&amp;nbsp; ). In the first binary vector, “brighter” elements are assigned ‘1’, while “similar” pixels are assigned ‘0’. The second binary vector assigns ‘1’ to “darker” pixels, and ‘0’ to “similar” pixels. These vectors are computed using subtraction and comparators. By using two binary vectors, the classification of corners is simplified to a search for 9 consecutive 1’s (in either vector), and this is done using look-up tables (LUTs). The LUTs perform a function equivalent to AND operations on every possible segment of 9 consecutive pixels. There are only 16 patterns that correspond to a corner. An OR operation is applied to their outputs to see if such a pattern is found. For the non-maximal suppression stage, we require a measure of corner score. As discussed earlier, there are a few ways of computing the corner score. The first measure, detailed in&amp;nbsp;[25], is not presented here, as an FPGA implementation has been previously published in the literature by Kraft et al.&amp;nbsp;[22]. In Sects.&amp;nbsp;4.2.3 and&amp;nbsp;4.2.4, we discuss the implementations for the two other corner measures.
Springer.tar//Springer//Springer\10.1007-s00138-012-0475-8.xml:Change detection for moving object segmentation with robust background construction under Wronskian framework:Gaussian mixture models Temporal analysis Object detection Background subtraction:  1 Introduction  An important problem in computer vision is the detection of moving objects from a video sequence [7]. Proper detection of moving objects is crucial for many computer vision and artificial intelligent systems. Moving object detection has been widely applied in the fields like visual surveillance [23, 27], face and gait-based human recognition [29, 37], activity recognition [17], robotics [28], etc. Background subtraction (BGS) method [22] is very popular for video object detection. It needs to handle different situations like illumination variation, noise, non-static background, and shadow of the moving objects in the scene. The effects of noise and illumination variation are very common in daily life video scenes [4]. In this regard a robust BGS scheme using running Gaussian average was proposed by Wren et al. [36]. Here, the authors have modeled the background independently at each pixel location by fitting a Gaussian probability distribution function (pdf) over a sequence of previous/past image frames. The parameters of the Gaussian pdf (i.e., the mean and the variance) at each pixel location is stored in a buffer. The locations of the moving objects in the target frame are obtained by comparing the intensity value of each pixel location in the target image frame against the corresponding parameter values of the Gaussian pdf. If they match then the parameters of the model are adjusted. Lo and Velastin [18] have proposed a moving object detection scheme, where median value at each pixel location from the sequence of previous image frames are used to model the background of the considered scene. It is observed that instead of considering the mean value of the Gaussian pdf, use of median value shows better stability to the background model [21]. However, all the above-mentioned techniques are not robust against changes due to non-static background. It is natural that at a particular location of the image frame, different background objects appear. Hence, a multi valued background model is needed to model the background properly. Stauffer and Grimson [31, 32] proposed a multi-valued background modeling scheme where the pixel at a particular location of the video image frame is modeled with multiple Gaussian pdfs. Such an approach is found to be robust and gives better results against non-static background. Recently, a modification of the above model incorporating dynamic texture is considered for detecting moving objects from non-static background motion [9]. It is also concluded that the above-mentioned techniques are pixel based and do not take into account the spatio-contextual information for change detection. Spagnolo et al. [30] have proposed a new BGS scheme for moving object detection. Here, the authors have used radiometric similarity of a pixel (influenced by its neighbors) at a particular location of the background modeled frame and the target frame to solve problems of non-static background, gradual variations of light conditions, and ghost elimination in the scene. However, radiometric similarity measure is very sensitive to noise and illumination variation and produces holes in object segments. A spatio-contextual BGS technique that takes the advantage of invariant local textural property is investigated in [15]. Here the authors have explored the concept of local binary pattern histogram at each pixel location for construction of background model and detection of moving objects. However, such an approach is limited by its use in monochrome video sequence. A novel BGS technique using a mixture of Gaussian models is proposed by Poppe et al. [24]. Here, the authors have introduced an edge-based image segmentation to improve the object detection accuracy. Elgammal et al. [12] have proposed a kernelized GMM model to model object background of the scene and have efficiently detected moving objects from the scene with less misclassification error. A non-Gaussian modeling-based background subtraction scheme is proposed by Kim et al. [16], where the concept of codebook is used to reconstruct the background scene and hence does not need an estimation or fixing of any parameters. Recently, Mandellos et al. [19] have used the histogram filtering technique to model the multi-valued background information from a complex video scene and have successfully detected the moving objects with an improved accuracy. A  modulation-based motion detection filter is also studied by Manzanera et al. [20], where a simple nonlinear recursive approximation of the background image is considered. This is found to be a simple method for visual surveillance. A realtime visual surveillance system, popularly known as  was proposed by Haritaoglu et al. [14], where three parameters, maximum, minimum, and maximum difference in intensity values are considered to track the moving objects in the video scene. However, such an approach is limited by its use to monochrome image sequences. ViBe [3], a computationally efficient visual surveillance tool is designed based on the non-parametric pixel-based classification strategy. A random sampling strategy is used here and the neighborhood pixel statistics is considered to build the background model. Comprehensive studies of different linear algebra-based change detection schemes for moving object detection is presented by Durucan and Ebrahimi [11]. Here the authors have shown that the Wronskian change detection model [11] is robust against noise and can detect moving objects in the scene by suppressing illumination changes successfully. The objects in a scene were detected by defining a Wronskian function over the target image frame (in which an object is required to be detected) and the reference image frame. Authors have selected the reference image by picking (manually) a visually significant image frame that contains only background scene. It is also observed that the Wronskian change detection model can handle some other changing environmental conditions like changing cloud cover or moving trees in the background. However, a choice and updating of the background model (reference frame) is not discussed in detail. It is found that their approach completely relies on proper selection of background model (or the reference frame). From the above analysis it is evident that several techniques using background subtraction were proposed by different researchers in the field of computer vision for moving object detection. Different algorithms have their own drawbacks and robustness. However, it is found that no method is good to solve all problems in a video and all methods are also not good for any particular problem in a video. It is also true that any background subtraction scheme is considered to be robust if it gives better performance against different problems in a video like noise, illumination variation, non-static background, etc. Hence research in this field is still active. It is also concluded that for designing a good background subtraction scheme, it needs the construction and updating of a stable background model that can efficiently characterize the background of the video scene. It also can discriminate the effects of noise and non-static background, etc. It is also to be noted that many existing techniques do not discuss the construction of the background model (consider a randomly picked background image as background model) or may consider an accumulation of pixel level information to represent the background model of the considered scene. Such an approach can be considered as intrinsic type background model [3]. However construction of a robust background model is a prime factor since it affects the accuracy of object detection. We can summarize here that the use of Gaussian model in background construction and object detection is very popular [36]. It uses different modes which arise over time to capture the underlying richness of background. Hence it can be considered as an approach for background building by considering temporal modes. However, the Wronskian change detection model uses a spatial region of support in this regard [11]. To the best of the authors’ knowledge, in the literature there is no method available that simultaneously takes the advantages of Gaussian averaging-based temporal mode and spatial region of support-based Wronskian model for background modeling. In the present work, we have tried to integrate/exploit the advantages of both the schemes for background construction and object detection from video scenes. Hence the proposed model is a spatio-temporal mode dependent model. In this article, we propose a novel BGS scheme to detect moving objects from static background scenes. The novelty of the proposed scheme lies in constructing a stable (robust) spatio-temporal background model from a given video sequence that efficiently characterizes the property of the background of the scene. The constructed background model is further compared with different image frames of the same sequence to detect different moving objects. In the proposed scheme the background model is constructed by analyzing a sequence of linearly dependent past image frames in Wronskian framework. It uses Wronskian function to detect changes between the target frame and the constructed background model. An experimental verification is performed over four benchmark public test video sequences to confirm the effectiveness of the proposed scheme. It is observed that the proposed BGS scheme provides robust results against gradual variations of light conditions, atmospheric noise, non-static background. and ghost in the scene. The effectiveness of the proposed scheme is verified by comparing it with manual thresholding-based BGS scheme [7], BGS scheme using GMM [31], radiometric similarity-based BGS scheme [30], Wronskian change detection scheme [11], and codebook-based BGS scheme [16]. It is found that the proposed BGS scheme provides better results as compared with the considered schemes. The proposed scheme is evaluated by three performance evaluation measures: precision, recall, and F-measure [8]. The organization of the remaining portion of this article is as follows: Section 2 represents a brief description of BGS scheme. A description of the Wronskian change detection model is provided in Sect.&amp;nbsp;3. In Sect.&amp;nbsp;4, the proposed Wronskian change detector-based BGS scheme is presented. Section 5 provides experimental results and analysis. Conclusions are drawn in Sect.&amp;nbsp;6.    2 Details on background subtraction scheme  Motion detection scheme uses BGS technique to detect moving objects from a video scene. Let us consider the intensity value of a pixel at location   in the background image to be   and the intensity value of the corresponding pixel in the target image frame at time   to be  . The conventional BGS scheme [ 7] suggests that the objects in the target frame can be detected by considering the absolute difference of the background and the target frames followed by thresholding. This method produces an image that can be greatly corrupted by spot noise if threshold selection is not proper. The accuracy of this approach severely degrades as the scene is affected by noise and non-static background. In literature, Gaussian averaging [ 36] and multi level Gaussian averaging [ 31] schemes are used to detect moving objects from such scenarios. In such approaches, initially a background model is developed by analyzing a sequence of previously available frames. This modeling is done by considering the average and variance of the intensity value at a particular location of the image frame from the sequence of previously observed image frames of that video. If a single Gaussian pdf is considered, then a single set of parameters are required. Similarly, for a multi-valued Gaussian pdf multiple sets of parameters are required. Hence, the developed background model at a particular instant of time   can be obtained as follows:       and       A pixel in the foreground mask   at   in the target image frame (at  th instant of time) can be classified as either the object pixel (1) or the background pixel (0) by            where   is a constant. At each time instant  , the background model is updated.    3 Wronskian change detection model  Wronskian change detector is a vector image model to detect changes from one frame to another. In such an image model, each pixel in a frame is assumed to be associated with its neighboring pixels (of that frame) termed as . Hence, each pixel of the image frame is represented as a vector, and the components of this vector are the central pixel and its neighboring pixels (in that frame). This represents the components of the vector corresponding to the illumination values stored in each pixel of the image frame. To detect the changes at a pixel location in two image frames, a linear independence test can be conducted on the corresponding region of supports. If a pixel is found to be linearly independent with the corresponding pixel in the previous frame, then it is a changed pixel. A simple test for determining the linear dependence or independence of vectors is the Wronskian function (also termed as  change detection Model). Wronskian change detection model for detecting changes between two consecutive image frames is initially suggested by Durucan and Ebrahimi [ 11]. They derived the expression for Wronskian change detection model by two mathematical definitions. For any two linearly dependent gray-level functions   and   (as gray value is a function of amount of incident light and amount of reflected light),       where   and   are two real constants. Determinant of the Wronskian matrix   vanishes for two linearly dependent functions. Thus            This can be reduced to the form       for linearly dependent functions. For detailed derivation we refer the readers to [ 11]. We can incorporate the spatio-contextual property of a pixel in Wronskian change detection model by including the region of support in Eq. ( 6) [ 11]. Figure   illustrates the region of support of pixel at location   in an image frame with a window size  . At a particular location  , the Wronskian function is computed for detecting changes between the  th and  th frames as follows:       where   represents the number of pixels in the region of support for pixel location  . This is equivalent to checking the linear independency for all pixels in a region. A point   in the  th frame can be detected as a changed pixel if   deviates from 0.    4 Proposed background subtraction scheme  Here we propose a novel BGS scheme which is able to detect moving objects in a scene by efficiently minimizing the effects of noise, illumination variation, and non-static background in the scene. In the proposed scheme the background scene is constructed by analyzing a sequence of linearly dependent past image frames using GMM in the Wronskian framework. The changes between the constructed background scene and the considered target frame are obtained by Wronskian change detection model. For any foreground segmentation scheme, the initial step is the background construction. Accuracy of the foreground segmentation depends on the stability of the constructed background scene. The foreground segmentation accuracy degrades if the constructed background is corrupted by noise. It degrades more if the background is affected by non-static background. Hence, an important task of any foreground segmentation scheme is to construct a robust background model. In the proposed scheme, we build the background model by considering a sequence of previously available image frames. We assume that linearly dependent background pixels at a particular location of the image frames follow Gaussian distribution in temporal direction. The background model is developed by considering the average of the intensity values at a particular location of the previously observed linearly dependent pixels. Wronskian function is used to check the linear dependency for change detection [11]. Hence in the proposed scheme we have used Wronskian function to check whether a pixel at a particular location of the constructed background model (we have considered first frame as the background model for initialization of the algorithm, i.e., ) is linearly dependent on the pixel at the corresponding location in the subsequent frames. We test the linear dependency at pixel location   using the following function:       where   is the target image frame and   is the constructed background model. If the determinant of the Wronskian matrix   does not deviate much from 0, the pixel is considered as a background pixel. If a pixel at location   is found to be linearly dependent with the background model  , we update the background model by considering the Gaussian averaging criterion as       where   represents the number of already occurred linearly dependent pixels at location   until time  . If it is found to be linearly independent then the background model at location   will remain unaltered as       and The important part of each motion detection system is BGS which is required to extract the accurate shape of moving objects in the scene. In the proposed scheme, for detection of moving objects initially we consider a region of support at every pixel location of the constructed background model () and target image frame (). The pixel at location  in the target frame can be detected as a changed pixel if the value of the Wronskian function (Eq. (8)) deviates from . Thus a pixel can be classified as either object (1) or background (0), as follows:            Here,   represents the absolute value. Sometimes, it is observed that the detected changes using the proposed technique gives disjoint regions rather than a connected one. A connected component analysis [ 13] may be applied to obtain a complete region corresponding to the moving object. The next stage of the BGS is background updating. In the proposed scheme at each time instant  , we updated the background model as follows:            and            where   ( ) is a user-defined constant and varies from scene to scene. For better illustration of the proposed technique, let us consider an example of ‘train’ video sequence [ 26]. The original video sequence is noisy and was captured in a rainy environment. The scene also has non-static background. Figures  a, b represent the background constructed by the proposed scheme and the considered target image frame of the ‘train’ video sequence, respectively. Figures  c, d show the moving object location and its video object plane (VOP) obtained with manual thresholding (Th&amp;nbsp;=&amp;nbsp;25)-based BGS. It is found from the results that due to noise, illumination variation, and non-static background a lot of irrelevant points are detected as changed pixels. It is also observed that to eliminate the effect of noise, if the threshold value is increased, many parts of the objects are merged with the background. The results obtained using GMM-based BGS scheme [ 31] for these image frames are shown in Fig.&amp;nbsp; e, f, which also fail to give good impression in this regard. Figures  g, h show the moving object location and corresponding VOP obtained by the scheme described in [ 30] where the object in the scene is detected in a better way. However, some parts (like hand, bag, etc.) of the object are merged into background. A few background noise is also present. The moving object location and corresponding VOP obtained by Wronskian function [ 11] are shown in Fig.  i, j, where the object in the scene is not properly detected. Similarly, the moving object location and corresponding VOP obtained by codebook [ 16] scheme are shown in Fig.  k, l, where the object boundary are not properly detected and hence there is an effect of silhouette. The results obtained by the proposed scheme are shown in Fig.  m, n. It is found from these results that the object detected by the proposed scheme has less effect of object-background misclassification as compared with other four methods considered for BGS schemes.    5 Experimental results and analysis  The proposed technique is implemented in C/C++ and is run on a Pentium D, 3.2 GHz PC with 16 GB RAM and  operating system. In order to establish the effectiveness of the proposed technique, it is tested on three complex video and one public available long video sequences. The considered complex video sequences are namely ‘Water Surface’, ‘Curtain’ and ‘Lobby’ [1] and PETS2006 video sequence [2]. To validate the proposed scheme, results obtained by it are compared with those of manual thresholding-based change detection scheme [7], BGS scheme using GMM [31], radiometric similarity-based BGS scheme [30], Wronskian change detection scheme [11], and codebook-based BGS scheme [16]. Steps of different considered techniques used for comparison are as follows: 
Springer.tar//Springer//Springer\10.1007-s00138-012-0476-7.xml:A novel particle filter with implicit dynamic model for irregular motion tracking:Gauss–Newton optimization Dynamical model Object tracking Particle filter Color histogram:  1 Introduction  Particle filtering is a successful approach in visual tracking since it provides a general framework for estimating and propagating probability density functions for nonlinear and non-Gaussian dynamic systems. Since particle filtering is able to handle multi-modal problems by maintaining multiple hypotheses, it is more robust than the single hypothesis tracking approach such as mean shift [1]. However, its performance largely depends on the discriminative power of the observer model and the accurate prediction of the dynamic model. The inaccuracy of either of these models will lead to poor tracking results. Although many works have been done to improve the accuracy of the observation model, the motion prediction improvement is almost not considered. We propose to formulate the motion by an implicit model which can improve the accuracy of the prediction in irregular motion tracking. For the convenience of description, we use color histogram to represent the object’s appearance. In addition, the proposed algorithm is a general method which can adopt different other appearance representations and observation models. In early-stage visual object tracking, the appearance of the object is commonly represented by the extracted different features like colors, image silhouettes, and image edges. This single feature is susceptible to be affected by noises. To improve the performance, some researchers proposed to combine difference features [2, 3], discover the salient object [4, 5], or the sparse representation [6, 7] to enhance its robustness. However, in practice, these prior-established observation models are unsuitable due to changes of illuminations, scales, cluttered backgrounds, and occlusions. To deal with this problem, online learning algorithms such as Random Forests [8], Boosting [9], multiple instance learning [10, 11], and subspace update[12, 13] have been proposed to enhance the trackers’ adaptivity. By embedding these online learning approaches, the observe model used in the particle filter is more discriminative which can present desired tracking results [14–16]. Mei et al. [17–19] apply the sparse representation to the visual tracking and deal with occlusions via trivial templates. They adopt the holistic representation of the object as the appearance model and then track the object by solving the  minimization problem. In [20], Wu et al. extend the proposed sparse representation to track blurred targets by introducing the blur template subspace and integrated it with the sparse tracking framework. However, in the irregular motion tracking, due to the uncertainty of object motions, the traditional dynamic model always assumes the temporal continuity. In practice, we often encounter irregular motions in many real-world scenarios, such as camera switching, low frame rate videos, and uncertain object’s dynamics. Therefore, these traditional dynamic models bring poor predictions which cause the inaccurate tracking results. Worst of all, such accumulated error may lead to potential drifting. The recently presented tracking by detection approaches [21] or methods integrating a tracker and a detector [22–24] are attractive to handle the above-mentioned problem because the resulting trackers re-initialize themselves in every frame. In [24], a new tracker is designed for long-term tracking of arbitrary objects in unconstrained environments. The object is tracked and simultaneously learned to build a detector that supports the tracker once it fails. Such approaches involve the continuous application of a detection algorithm in individual frames and the association of detections across frames. These characteristics make the approaches robust to drift and are able to self-start. However, these approaches require templates that reliably detect objects. This requirement is non-trivial. The main deficiency of these approaches is that the resulting output is unreliable and sparse. Some researchers combine the particle filter and the local optimization to improve the sample efficiency [25, 26]. In the implementations of these approaches, each evolved particle is independently moved to a new position by the optimization to compensate the poor prediction of the dynamic model. These shifted particles may be trapped in local minimum due to the inherent deficiency of the local optimization. Therefore, the tracker might fail catastrophically while tracking encounter occlusions and background clutters. Recently, some approaches have been proposed to track abrupt motions by adopting different sample methods to improve the efficiency of sampling [27, 28]. In particle filter, the importance function is crucial in achieving an efficient and robust tracking. In practice, the importance function is usually simply fixed to the evolution law . This constitutes a crude model which is counterbalanced by a systematic selection of the particles with large weights. Consequently, the goal of more recent approaches is to design efficient importance functions approximating as closely as possible the optimal ones and to guide the particles in high likelihood areas [29, 30]. In [29], the paper proposes the use of partial linear Gaussian models in the particle filter estimation, and the authors demonstrate the effectiveness of such models in point tracking. In [30], the paper proposes an optimal importance function derived from an analytical appearance measurement function for sampling particles on the affine group. We proposed a novel particle filter with an implicit dynamic model to effectively deal with the poor prediction of the dynamic model in the irregular motion tracking. Rather than moving each particle to a new position by mean shift, we use a Gauss–Newton or simulated annealing optimization to move all particles to new positions to obtain the maxima of summed particle likelihood. This improvement is based on an observation that the sum of all particle’s likelihoods which are evolved by the correct dynamic model almost reach the summit over the sample space. This particle shift strategy provides better performance in dealing with the local-trap problem. Figure   illustrates an overview of the proposed algorithm. The work in [ 31] is the most related work to ours in that both use the optimization to move particles to higher likelihood regions. In comparison, our method is different in several aspects: (a) our method is strictly Bayesian-based recursive solution; (b) In [ 31], the particle is evolved by a random propagation, our method employs an implicit dynamic model to evolve the new sample set, and (c) We use Gauss–Newton and simulated annealing optimization while particle swarm optimization is used in [ 31].    2 Particle filter with an implicit dynamic model  In the conventional particle filter tracking framework, a dynamic system can be modeled with two equations: evolution equation and measurement equation. Evolution equation is also referred to as the state transition or the dynamic model, which is used to predict the current state by a nonlinear transform function based on the previous state. Measurement equation is also referred to as the observation model, which is used to measure the similarity between a given state and the current observation. These two equations can be given by Eqs.&amp;nbsp;( 1) and ( 2):       where   is the nonlinear system transition function and   is an i.i.d zero mean, white noise.       where   is a measurement function and   is another i.i.d zero mean, white-noise. From a Bayesian perspective, the tracking problem is to recursively construct the posterior probability density function (pdf)   of the state   given all measurements  . In particle filter scheme, the required posterior pdf is approximated by a weighted sample set  : each sample   represents one hypothetical state of the object, with a corresponding discrete sampling probability   which is determined by the observation model ( 2), where  . The state of an object at each time   is finally estimated by       The sample set is re-sampled by drawing a particular sample with probability with replacement. The new sample set is then evolved by propagating each sample according to the system model ( 1). From the above-mentioned analysis, we know that the dynamic model play an important role in particle filter. Therefore, accurate prediction is critical for reducing the cost of the observation and improving estimation accuracies. However, it is generally hard to find a suitable nonlinear dynamic model to admit all plausible motions. Given the pdf of the observation model is a convex function, the particle with high likelihood may be close to the ground truth of the state. In particle filter scheme, the new particle set is re-sampled by the dynamic model according to the weight of each particle; therefore, the particle with higher weight can get more chance to be selected as a new particle. Finally, with an accurate dynamic model, most particles in the re-sampled particle set have high likelihood, and thus the state with the biggest value of the sum of all particles’ likelihoods will reach or be close to the ground truth. This conclusion is verified by the experiments on the experimental video sequences. Table  1 gives the statistical result. Based on the above knowledge, instead of modeling the object motion as an explicit function, we obtain the optimal dynamic model by maximize the following objective function:            Next, we will discuss how to maximize the goal.    3 Particle set shift by Gauss–Newton optimization  We use color histogram to model tracking object appearances; the color information of the neighborhood in the object will be described by the discrete color density distribution  :       where   is a sampling of the employed color space. We used uniform sampling with   quantization steps per color coordinate, and thus, if the source image is a color image, then,   has   4,096 samples; otherwise,   has only 16 samples because gray image has only one color channel. The distribution   is derived from the data using kernel density estimation,       where   represents the color of pixel  , and   represents the color associated with the  th bin in  .   is the Epanechnikov kernel which is defined as       Taking the kernel bandwidth equal to the quantization step   means that the pixel   contributes only to the bins within unit distance. The normalization constant   is derived by imposing the condition  , from where       To eliminate the affection caused by occlusions (clutter) or the background, we employ a convex and monotonic decreasing kernel profile which assigns a smaller weight to the locations that are farther from the center of the target. Therefore, ( 6) becomes       where   is the coordinates of  th pixel in the target,   is the coordinates of the center pixel in the target, and   is the size of the spatial neighborhood. The similarity between the template and the candidate determines the candidate’s likelihood. We use the sum of squared error between two normalized histograms to measure the error of two components of the color distribution. Therefore,   is defined as       where   and   are the  th component in the template histogram and the candidate histogram, respectively. The rigid motion of an object point   between time   and   is       where   and   are the coordinates of  th pixel in the target at time   and  , respectively, and   is the unknown warp parameter which should be optimized. Substituting ( 10) and ( 11) into ( 4), we obtain the optimal dynamical model by       where   represents the  th component in the  th candidate histogram, and the  th candidate is obtained by warping the template according to Eq. ( 11) given the parameter  . It is equivalent to Minimizing the expression in Eq. ( 13) is a non-linear optimization task, thus, we use Gauss–Newton method to obtain the optimal parameters. We assume that a current estimate of   is known and then iteratively solves for increments to the parameters  ;   the following expression is (approximately) minimized:       with respect to  , and then the parameters are updated       These two steps are iterated until the estimates of the parameters converge. The non-linear expression   is linearized by performing a first-order Taylor expansion:       where  J is the Jacobian. This is a matrix containing the first partial derivatives of the function components. According to the chain rule,   is obtained as            where   is the derivative of  ,          is the gradient of the current image evaluate at  .   is the Jacobian of  . For example,   is defined as a 2D affine motion model with parameters   where   and   denote   translation, scale and rotation.            where   are the coordinates of pixel   in frame  . Then,   is obtained:            Substituting ( 16) to ( 14), we get       The partial derivative of the expression in Eq. ( 21) with respect to   is       Setting this expression equal to zero and solving it gives the closed-form solution for the minimum of the expression in Eq. ( 22): In particle filter, the prior established dynamic model fails to deal with the irregular motion for it is based on the smooth motion assumption. In our work, after particle propagation, we use the proposed optimization method to move all particles to new positions to obtain the local maxima of the sum of all particles’ likelihoods. This particle shift strategy can improve the prediction accuracy; as a result, a better tracking performance can be achieved. Since the histogram is not sensitive to rotations, we do not take rotations into account, and so the state   is parameterized by a 3D vector:   where  , denote translation in   direction, translation in   direction and scaling. In our optimization scheme, the iteration start from an initial point, a carefully chosen start point can avoid local-trap. Therefore, in our work, we use a second-order autoregressive dynamic model to predict the start point:       where   and   define a constant acceleration model, and   is an i.i.d zero mean, white-noise. The object is represented by a rectangle, and the appearance of the object is model by a color histogram. The location of the target object in an image frame can be represented by an image warping. This warping transforms the image coordinate system centering at the target within a box. Since the irregular motion always occur in drastic shift, in the implicit motion model, we only take displacements into account.  , the implicit motion model defined as            where   are obtained by ( 24), and            Eq.&amp;nbsp;( 1) is modified as       Algorithm 1 summarizes the whole process of theproposed tracking method.    4 Particle set shift by simulated annealing  The presented Gauss–Newton optimization in Sect.&amp;nbsp;3 can move the particle set to the optimal state accurately while the predicted state is in the vicinity of the ground truth, and the efficiency can be improved by a pyramidal implementation [32, 33]. However, when the predicted state is far from the ground truth and the state space is not unimodal, the Gauss–Newton optimization will be trapped at a “local optimum”. To avoid the local minima, we employ simulated annealing to find the desired global optimum. It is started by initially sampling with a reduced sensitivity to the underlying modes (on a flattened cost function surface) and then by progressively increasing the sensitivity to drive samples towards peaked cost regions. The convergence of simulated annealing to an optimal estimation is proved in [34]. In drastic motion tracking, the particle set shift strategy presented as step 3 in the algorithm 1 is replaced by simulated annealing. The simulated annealing optimization is presented in pseudo code C as Algorithm 2. In algorithm 2, each move in the iterative process is accepted on the probability , , which decreases with time , and also as  increases, where  is a “temperature” parameter that gradually decreases. The simulated annealing accepts all moves while  is a high temperature, and it works as Stochastic Hill-Climbing while  is a low temperature. This may make the search fall out of mediocre local minima and into better local maxima.    5 Experiment  To verify the effectiveness of the proposed approach, we have carried out a lot of experiments on kinds of irregular motion in various scenarios. We compare our method with Gauss–Newton optimization to other four tracking methods: standard particle filter (SPF) [35], mean shift embedded particle filter (MSEPF) [25], mean shift (MS) [1], and optical flow (OF) [36]. For the sake of fairness, all of the tracking methods adopt the same dynamical model and likelihood model. The algorithm is implemented in C++ and runs on a PC with a Pentium 1.58-GHz CPU. The test video sequences were captured by a Samsung digital camera (S600) with a temporal sampling rate of &amp;nbsp;s and the resolution of . We show how accurate the proposed particle shift approach can improve the prediction of the dynamic model. In the test video sequence, a man performs rapid whole body motion. The number of iterations necessary for each frame in the Man sequence is shown in Fig.&amp;nbsp; . We observe that most of the procedures can quickly converge in 50 iterations; those who reach the peak are due to the large displacements of the object. Figure&amp;nbsp;  demonstrates one instance of the iterative process and the corresponding visual output is shown in Fig.&amp;nbsp; . From Fig.&amp;nbsp; , we find that the proposed approach successfully converges the sum of all particles’ likelihoods to the peak of sample space. The predicted particles by the dynamic model are shifted by the optimization approach to new positions, and the tracking result is improved, according to Fig.&amp;nbsp; . To quantitatively evaluate the tracking performance of the proposed algorithm, Fig.&amp;nbsp;  shows the results as compared with other four tracking algorithms. The ground truth for comparison is manually labeled, and we define the correct pixel rate as       where   is the pixel set within the tracked object region obtained by the tracking approach,   is the pixel set within the ground truth region. The max of CPR is one which means that the tracking approach perfectly tracks the object; in other words, the tracking result overlay the ground truth. The less the CPR , the poorer the tracking performance. According to the statistics, our method shows much higher accuracy than other methods while tracking rapid object motions. Figure&amp;nbsp;  gives the visual comparative result. The  MS cannot guarantee global optimality and is susceptible to fall into local maxima (#46, #68). The performance of the  OF is the worst, and it fails to track from frame 45 to 85 due to brightness constancy constraint adopted. Actually, in this experiment, the object performs large body motion, the appearance of the object changes drastically. Although the  MSEPF combines global and local optimization, the  MSEPF is still trapped in the local maxima due to the inside visual ambiguities which can be observed from frame 124. The  SPF is more robust because it can represent uncertainty through a rigorous Bayesian paradigm, but the poor prediction reduces the tracking accuracy in the irregular motion. Compared with other four algorithms, the proposed approach is more robust and has higher accuracy.
Springer.tar//Springer//Springer\10.1007-s00138-012-0477-6.xml:A combined topological and statistical approach for interactive segmentation of 3D images:Segmentation Interactive operations Graph 3D images:  1 Introduction  Image segmentation is an important topic in image analysis and computer vision and is used in many application domains. The purpose of segmentation is to partition an image into homogeneous regions or to isolate objects from the background. The results of segmentation can be used to compute characteristics corresponding to isolated objects, to produce visualisations, or to track an object in a video, for example. Numerous automatic methods have been proposed in the literature [1–4]. However, automatic processes do not always obtain the desired results. Poor image acquisition quality or abnormalities in a scene can lead to some variations which can make building a robust static segmentation method difficult. Moreover, automatic methods are often dedicated to specific problems and are not generally applicable. Interactive segmentation algorithms provide a solution to these problems. Interaction allows the operator to drive and improve the segmentation computation according to the type of image being processed. A large number of interactive methods have been developed for 2D images [5, 6]. Among the interactive volume segmentation methods (volume segmentation systems), we can find the systems proposed in [7–13] that are usually dedicated to medical image segmentation. This paper describes a formal procedure to design a powerful and generic segmentation system by structuring the information extracted from the images using two data structures: (1) a region adjacency graph (RAG) which provides a topological representation of the regions inside an image and (2) a hierarchical classification tree (HCT), which gives statistical information of the similarities between classes of voxels according to user selected features. By combining a RAG and a HCT, knowledge extracted during the incremental analysis of a 3D image can be easily managed and exploited to progressively construct a segmentation using interactive operations. We presented an initial approach in [14]. In this paper, this architecture has been modified and improved notably using statistical information and adding several interactive operations. Moreover, additional experiments are presented for the evaluation of the proposed method. The first contribution of our work is the combination of a RAG and a HCT. At the best of our knowledge it is the first time that such a powerful data structure is proposed to exploit interactively and simultaneously information about the spatial and topological organization of the extracted regions (RAG) and information about the visual similarities between the extracted regions (HCT). The second contribution is the formal definition of the operators we can use to transform these data structures allowing the framework to be used in batch mode if a predefined processing chain has been defined, in an adaptive manner if a scheduler is available, and in an interactive way if the users want to choose the operators to be used after each step. To give the user the ability to incrementally improve the segmentation, powerful visualization and manipulation interfaces of these data structures have been proposed. The framework has been validated by a user study and tested by specialists in sonography to segment 3D ultrasound images of the skin. In Sect.&amp;nbsp;2, interactive methods that use a graph representation to segment an image are briefly described. In Sect.&amp;nbsp;3, we describe the content and construction of the proposed data structures (the RAG and the HCT). Section 4 presents the operations defined to improve the segmentation through incremental transformations of the RAG and the HCT. In Sect.&amp;nbsp;5, an interactive system for volumetric texture segmentation is constructed using our model. The benefits of the interactivity of the system are evaluated in a user study. We also present segmentations of 3D ultrasound images using the same textural features and our interactive proposition. To conclude, we provide a discussion of our work and future prospects.    2 Related works  Currently, the main techniques using graphs for interactive segmentation are based on watershed, graph cuts, shortest paths and the random walker [15]. These methods often interact with the user at the beginning of the segmentation process. The user marks, or scribbles areas of the image which is not very convenient in 3D. Algorithms use this input to produce a segmentation. Through a series of interactions, the user can refine the segmentation. In [16], Boykov and Jolly proposed the first interactive graph cut framework for image segmentation. In this approach, nodes correspond to the pixels/voxels. In addition, the graph contains special nodes called terminals, which are defined using the area marked by the user. If two terminals are present, they are called the “source” and the “sink”. The graph cut methods use the terminals to cut the graph, attempting to resolve the Min-Cut/Max-Flow problem:       where   and   are two connected components of the graph   is the complement of   and   is the weight between the vertices   and  . The GrabCut method [17] is a variant of the interactive graph cut with a simplified user interaction. In [18], Grady proposed using random walks for interactive image segmentation. Starting from a scribble, each pixel is labelled based on the maximal probability that a random walker reaches the pixel. In [19], the authors proposed the binary partition tree algorithm. This method uses an automatic segmentation technique such as the watershed, to produce a hierarchical output in the form of a binary partition tree. The root node of the tree is the entire image, the leaf nodes are the pixels and the intermediate nodes represent regions inside the image. To find the segmentation, leaf nodes are labelled using pixels that were marked by the user. The labels are propagated upward towards the root of the tree. This is repeated for every marked leaf in the tree. Finally, labels are propagated to any unlabelled child nodes in the subtree. Several additional interactive approaches are described in [15]. In each method, the interactivity with the user is limited to scribble definitions. For 2D images, it is straightforward for the user to mark and to visualise the different areas of the image. In 3D, it is more difficult to have a good spatial and topological representation of the regions inside the image. If a user have a good perception of the 3D image to process, scribble-based methods seems to be well adapted since it is a simple task for the user to tag the different image parts. Nevertheless, when processing complex images, tagging the image becomes challenging. In Sect.&amp;nbsp;5.2, we show an example of possible difficulties that a user can encounter to process 3D ultrasound images using a scribble-based method. In our work, the RAG and HCT representations are not used to provide a graph based minimisation technique. We want an operator to be able to interactively construct a segmentation within a user friendly environment that operates on these structures. The intermediate segmentations and the graph structure are useful information to understand the image content that facilitate the user task. Some approaches have been developed to provide structural information of 2D and 3D images [20, 21]. Nevertheless, their purpose is different from the proposed approach here. It is not our objective to setup a complete geometrical and topological representation of an image but to provide a generic and robust segmentation system that use interactive operations.    3 Framework description  Figure   presents the structure of the proposed general interactive system for image segmentation. As explained in [ 5], such a system contains several main components including computational and interactive units. The computational component corresponds to a set of methods used to generate a segmentation. Specifically, numerical features   computed by feature extractors are exploited by segmentation processes to generate a segmentation. In a 3D segmentation system, a set of features   is computed for each voxel in the image (or in the region of interest). Generally, all of these methods use parameters determined by prior knowledge or by interaction with the user. In Fig.&amp;nbsp; , the Feature Configuration and the Segmentation Parameters components allow the user to tune the feature extraction and the segmentation process, respectively. The user is brought into the procedure by proposing an incremental segmentation process. Two problems have to be solved during the development of the system. First, how does the user define the criteria to be used during the segmentation computation? Second, how are results provided and what interactive tools or operators are proposed to the user to make the representation evolve in the desired direction? To answer the first question, a region based approach (clustering method) that uses criteria (features) defined by the user is a good choice. The second problem is solved by structuring the large amount of data to be processed. Our system uses two data structures: a RAG and a HCT. These data structures are constructed in the Computational component. The user interaction is defined thereafter. By coupling these components with the Interactive Operations module and manipulating the visual information of the segmentation given by the RAG (feedback), the user is able to select actions to incrementally drive the segmentation process. In other words, the interactive operations allow the user to drive the RAG and the HCT evolution. We define   to be the set of all possible image feature extractors. Each is able to provide one or more image features,  . We can write the following membership:       where   is the extractor  , that provides the feature   is the number of possible image features and   is the number of extractors that provide  . The image features are then obtained using the appropriate extractors as follows:       where   is the set of voxels to process,   are the parameters of   that should be easily tunable through the interactive component. User can choose the adequate features  using the interactive component. These features are computed for each voxel of the image by running the corresponding algorithms in the computational component . The choice of these features (texture, colour, etc.) depends directly on the purpose of the application. To provide a truly interactive system, the feature should be easily understandable by a human even if he is not a specialist in computer vision. In our framework, the concept of “Region” is very important and has to be distinguished from the concept of “Class of voxels”. We propose applying clustering methods to the voxels described by a feature vector to realise the successive segmentations. The application of the clustering algorithm provides a set of Classes of voxels. If the voxels are scatter all over the image, a class of statistically similar voxels can correspond to several different regions, not just a single region. We have implemented a specific algorithm (RegId) to construct the list of regions corresponding to a list of classes provided by a clustering method. These two data structures are denoted as “Labels of Classes”   and “Labels of Regions”   in Fig.&amp;nbsp; . The image of   is obtained by tagging all of the voxels (on which we have applied a clustering) with the label of its corresponding class. A connected component extraction method can be applied to this new image   to obtain  . Several algorithms have been proposed to extract the connected component from 2D binary images. The main algorithms are presented by Chassery and Montanvert [ 22]. For 3D grey level images, we have adapted an algorithm (that corresponds to RegId function) that requires only two scans to process an image. With this algorithm, the complexity depends on the size of the image. For a sequential algorithm, the number of iterations depends on the complexity of the objects. To create the HCT  , we consider the image   that represents the classes   that have been identified using features   (Fig.&amp;nbsp; ). Each class in   is described using average features   (class centroids) obtained in the computational component. The centroid information of each class is used in our application, but it is possible to define other measures. A binary tree (the HCT  ) of the classes is constructed using an ascendant hierarchical classification method on the labels of classes in  . The classes of the image   correspond to the first level of  , i.e., the leaf nodes of  . For each level of the HCT, the two merged classes are the two classes   and   with the minimum distance between   and  .   and   are the average features of the class   and the class   respectively. If we consider the euclidian distance, then the classes   and   that satisfy       are merged.   is the number of classes. Figure  shows an example of a simple HCT. In a statistical point of view, the classes  and  are the closest, that is why they are merged in the second level of the HCT. The tree   provides information about the statistical similarities between classes of voxels according to the selected features,   (by using the centroid). By selecting a level in  , the user can increase or decrease the number of classes. In other words, the user chooses the level of the tree that is the most significant for his specific segmentation objective. The HCT construction depends directly on the distance used to compare the classes. This distance is related to the application and should be adapted according to the computed features. We define  to be the RAG where  is the set of vertices and  is the set of edges. To create the RAG , we consider the image  that represents the different regions  identified in the  representation (label of classes). Each vertex  of the graph represents a region  of the image . Let  be the adjacent regions of . Two vertices  and  are connected if  or if . For each vertex we associate the following information: the average features  of the region that corresponds to the centroid of  in the region, the centre of gravity  of the region that corresponds to the position of the vertex in the 3D image, the volume of the region Vol and a name Id which can be set interactively by the user. To connect the vertices (edge constructions), the adjacent regions of each vertex must be identified by computing the intersection (a surface) between each region. If the surface is not null, the two vertices are connected. Considering two regions   and  , the   intersection surface with   corresponds to the number of   voxels with at least one neighbour belonging to  . Each edge in   contains the information of the two linked vertices and the intersection surface (Surf) of the two regions. Figure   illustrates the structure and the attributes of the RAG. In Sect.&amp;nbsp;4, we will see the operations that have been associated to the graph to incrementally evolve the segmentation by increasing or decreasing the number of vertices, and thus the number of regions.    4 Interactive segmentation scheme  Figure  shows the steps of our proposed method and describes how the data structures (the HCT and the RAG) are constructed. The user chooses the relevant initial features needed to process the image (Sect.&amp;nbsp;3.1). These features have to be understandable to the user, even if he is not a specialist in image analysis or computer vision. Taking the example of texture analysis, the significance of a power spectrum is not obvious. Words such as contrast, roughness, and directionality are more useful [23, 24]. These attributes can be exploited by a segmentation function () that generates a clustering representation (a classification of voxels). In Fig.&amp;nbsp;, the image  represents the different classes that have been identified in the processed image. The system uses the image  to construct or update the HCT (see Sect.&amp;nbsp;3.2.1). In the image , two regions can have the same label. Two regions can be similar from the point of view of their features, and thus belong to the same class. The purpose of the region identification module is to extract the regions of the image  and to allocate to each of them a label. At the beginning of the segmentation process, the image is considered as a single region. From a data structure point of view, it is represented by a node in the RAG and by a single branch in the HCT. The first step of the segmentation process is a splitting operation (Sect.&amp;nbsp; 4.1.1) to identify different regions inside the image according to the specified features. In the following expressions, the operator can define parameters, or actions that can evolve the segmentation. The segmentation progression can be written as follows:       where   and   are respectively the parameter set of the RAG and the HCT. Especially, these two sets contains the parameters   and   where the interactive operations are specified (see Fig.&amp;nbsp; ). For each iteration, the user has the choice to define  or . The function  generates a new clustering representation  using  or , the graph  or the tree , features  and the segmentation parameters . The  function uses  to create a new segmentation . Finally, the function  constructs a new HCT  using the new clustering representation . The function  adds or removes nodes to  exploiting the information inside  to create . According to the chosen actions in  and  and  transform the HCT and RAG representations using formal operators described in Sects.&amp;nbsp;4.1.1, 4.1.2, 4.1.3 and 4.2. The user can stop the incremental process when the desired segmentation is obtained.
Springer.tar//Springer//Springer\10.1007-s00138-012-0478-5.xml:Active contours methods with respect to Vickers indentations:Vickers Active contours Focus Shape Prior:  1 Introduction  Hardness, which is an important characteristic of solid materials, can be determined with the Vickers hardness test. A pyramidal indenter is pressed into the material with a defined force and causes an indentation. An important issue is to measure the size (diagonal length) of the approximately square indentation to determine the Vickers hardness [1] (depends on the applied force and the diagonal length). As the manual measurement of the indentation images not only is expensive but also interpretive and subjective, a robust and accurate automatized measurement method is highly beneficial. Segmentation algorithms are used to get the positions of the four vertices of the indentation. Having the four vertices, the diagonal lengths can be calculated easily. There are several proposals for image segmentation of Vickers indentations. One group of algorithms rely on wavelet analysis [2, 3]. These methods assume that object borders are perfectly straight lines, which is not always true. Another approach [4] is based on edge detection followed by Hough transform and least squares approximation of lines. Edge finding techniques are based on the assumption that high differences between neighboring pixels imply that these pixels are part of the border. Especially in case of noisy images, this assumption is not right at all. The method introduced in [5] applies thresholding followed by a Hough transform. If thresholding is applied to images, it is necessary to calculate a threshold value which depends on the current image, because a fixed threshold surely does lead to good results. Often a separation of the object from the background with thresholding is not possible, since the assumption of different gray values between object and background is not valid. Other suggested methods also binarize the image using thresholding [6, 7], followed by morphological closing. Another approach is based on axis projection and Hough transform [8]. This approach is based on the assumption that the objects are perfectly aligned (diagonals vertical/horizontal). Methods relying on template matching [9, 10] are quite robust to noise. This is because big templates suppress noise as large regions are summed up. The template-matching approach introduced in [10] provides robust as well as precise results, but requires an accurate alignment (diagonals horizontal/vertical). A high degree of accuracy is achieved by applying four corner templates instead of one complete square template [9] (square templates in different sizes and different rotations are matched with the image). The results of the approach using complete square templates [9] are robust but only serve as approximations. A refinement strategy [11] adds accuracy. In this work, first we investigate the active contours approaches with reference to Vickers indentation images. The aim of these approaches is that a contour with a defined initialization converges to the real object boundaries. In experiments, different energy functionals, which depend on pixel values of the image on the one hand and homogeneity criteria of the contour on the other hand are utilized. We found out that this technique suffers from poor initializations and we illustrate the reason for this behavior. To achieve appropriate initializations, a flexible Shape Prior approach, which produces highly robust, but not accurate results, is investigated [12]. In the two-stage segmentation approach, these approximative, but robust results deal as initializations for a subsequent active contour. Moreover, active contours as well as the Shape Prior approach are joined with the existing Shape from Focus approach [13], which extracts additional information from a series of images (of the same indentation). This increases the robustness of segmentation, especially of low quality images. Furthermore, a gradual enhancement approach [14] based on different unfocused images is investigated, in order to decrease the overall runtime and maintain the robustness. With new image data, different combinations of the mentioned approaches are evaluated in a uniform way in order to compare the segmentation accuracy. All these different approaches which are explained in the consecutive sections are summarized in Fig.  . The proposed variable two-stage segmentation approach defines different initialization stages (1A–1C) and enhancement stages (2A–2C). One initialization and one enhancement stage must be chosen and lastly, the local Hough transform has to be applied. For high quality images, we propose the methods 1A and 2A. In the case of lower quality images, the segmentation robustness can be increased, if in the first stage, method 1B (Shape from Focus) is utilized. Very low quality images even benefit if the method 2B is used. To decrease the execution runtime, method 1C can be used for all images (followed by 2A). We found out that whereas a certain degree of inaccuracy in the first approximative stage does not affect the overall segmentation accuracy, outliers cannot be refined in the precise stage 2. In the following, robustness means that the outliers ratio is low. Experiments showed, that a vertex position can be refined without a loss of accuracy, until a detection error of about 50 pixels is reached (in stage 1). Consequently, we set the outliers threshold to 50 pixels. In Sect. 2, different active contours approaches (stage 2, especially 2A and 2C) are investigated with reference to Vickers indentation images. In Sect. 3, the new approximative Shape Prior segmentation approach (stage 1, especially 1A) is introduced to achieve good initializations. In Sect. 4, the Shape from Focus approach [15] is incorporated with traditional segmentation methods (stage 1B, 2B), in order to improve robustness. In Sect. 5, a new gradual enhancement approach is introduced (stage 1C). Experiments are shown in Sect. 6. Section 7 concludes this paper. The images to segment approximately fit the following description:            Figure  a shows quite perfect images. Images in Fig.  b show different kinds of noise. Some bright images lack of contrast (Fig.  c), especially the diagonals’ gray scale is the same as the background. Another inconvenience is the fact, that in such images, possible noise is often darker than the imprint. In Fig.  d, the diagonals are not aligned horizontally and vertically. In Fig.  e, a concave and a convex curvature is shown. The images in Fig.  f represent the smallest and largest imprints in our databases. Square geometry of object to segment Dark object, bright background Diagonals are horizontally and vertically aligned Object is situated close to the center.    2 Active contours approaches  The traditional active contours (snake) model has been introduced in [16]. A snake is a closed curve which iteratively converges at the object’s borders, by means of gradient descent of an energy functional. The energy is computed from image pixel values (e.g. gradients) and the homogeneity of the contour. The curve is represented parametrically by a sequence of pairs containing  and  coordinates connected with straight line segments. The level set method introduced in [ 17] is an alternative to the traditional snake model. Apart from other inconveniences, traditional active contours suffer from an explicit parametrization (by frontier points) of the contour. In the level set formulation, the contour is given by its level set  .          is a function which is   inside,   outside of the region and exactly   at the frontier of the evolved shape. Evolution of the frontier happens by moving the level set  in normal direction  with a specified speed . There exist lots of different ways of calculating the speed function , which influences the behavior of the evolving level set. As with the snake approach, edge-based level set approaches [ 18] require the propagation of edges to increase the capture range (region where the contour converges to the real boundary). To bypass this issues, in [ 19], a region-based approach has been introduced, where the force of the contour is not based on image gradients. This method is based on the assumption that the object’s surface as well as the surface outside of the object are homogeneous as far as its gray value is concerned:               is the image,   is the image gray value,   ( ) is the average image value inside (outside) of the contour,   ( ) is the surface inside (outside) of the contour,   is the gradient operator and   is the curvature weighting term. The region based-approach is based on the assumption that images do not necessarily have strong gradients at their boundaries, but the regions inside and outside of the contour have to be homogeneous as far as the gray value is concerned. This assumption usually is quite appropriate, however it is inappropriate for some kinds of images (e.g. the image in Fig.  : background consists of regions which are darker than the object). Consequently, a region-based model would state that such dark noise pixels are more likely to be part of the object than to be part of background (as background average color is brighter). Obviously, this does not match with reality. To overcome that inconvenience, a statistical approach has been introduced [ 20]:            The regularization term   prevents the contour from developing zigzag patterns.   and   are the probabilities of the feature vectors   inside and outside of the contour. Intuitively, the energy is low, if both regions are homogeneous with respect to the feature vectors  . This formulation allows not only to use the gray value as feature but also each feature which could be defined for a specific pixel might be included in an arbitrary feature vector. In our experiments, the following feature vectors are used: First, we discuss the edge-based approaches which are based on image gradients. That means, the contour moves into the direction, where the gradients are increasing. Close to object boundaries, this actually is appropriate. However, we do not already know where the objects are located approximately. In Fig.   (left), the gradient image and a possible initial contour are shown. Surely, a gradient descent of the contour would never be successful as the capture range is too small. Especially, the segmentation of noisy images suffers because the gradient image shows lots of regions with low image energy (Fig.  , right), where the contour potentially converges to. To increase the capture range and decrease the effect of noise, large edge operators instead of small ones (Sobel  ) could be utilized. This causes a propagation of the edge information and thus makes a gradient descent of the contour possible. However, the size of the gradient operator is limited as small objects are blurred too much and moreover, the computational costs become high. The effect of differently sized edge operators is shown in Fig.  . However, although a big gradient operator is used in the right image, the segmentation fails. In [ 21,  22], strategies to increase the capture range are proposed. One big problem of these methods is that not only edge information is propagated but also noise. Noisy edge images affect the active contours as shown in Fig.  . Although region-based approaches (and the statistical approach) which do not rely on gradients are less vulnerable to the initial configuration, starting with a general level set is often not successful as well. On the one hand, even region-based algorithms do not succeed to converge at the desired boundary, if the contour is too far away from the indentation. The contour often does not shrink to converge at the boundaries, but grows instead. This is because the average gray value of the background might be darker than the average gray value within the contour (shown in Fig.  ). On the other hand, if such long distances must be managed, computational costs are tremendous.    3 An approximative Shape Prior method  The methods mentioned so far suffer from converging to local minima if the initialization is inappropriate. In existing Shape Prior level set approaches [23, 24], a weighted shape term is added to the energy function. This increases the robustness, but for a standalone segmentation even these methods are inappropriate. The same problem as shown in Fig.  arises. We propose a quite different way for robust segmentation of shapes which are known a priori [12]. Our approach requires a parametric description of the prior shape. The object that will be segmented will have exactly the prior shape, as not a contour (parametrized by points or level set), but the object description parameters are directly evolved by gradient means of descent. Whereas the traditional active contours as well as level set algorithms allow arbitrary deformation of the initial contour, our approach only allows the evaluation of the following four parameters (the effect on the contour is shown in Fig.  ):         : scaling         &amp;nbsp;            : translation  axis         &amp;nbsp;            : translation  axis         &amp;nbsp;            : rotation         &amp;nbsp; : scaling : translation  axis : translation  axis : rotation The contour of the square is given by the points   with the distance   to a center  .   is calculated in the following way to ensure that the evolved contour has a square shape:            Of course, this algorithm will not be able to segment Vickers images perfectly, as Vickers’ shape often cannot be described by a perfect square. Though this is not our objective, we aim at a pretty good approximative segmentation of a very high rate of images. These approximations deal as the initializations for a precise segmentation method. In order to reduce the computational effort, we previously downscale the indentation images by factor 10. Although this causes a further loss of accuracy, the results after the precise segmentation stage are not affected. The regions in- and outside of the square are given by   and  :                         As with the level set approach, we define an energy criterion which is minimized by gradient descent. We investigated different energy functions (edge based, region based). Tests showed that the following statistical criterion, which is derived from the statistical level set approach proposed in [ 20], is the best choice:          is an arbitrary feature of the point  . We investigate the feature vectors in Eqs. ( 4) and ( 5). The evolved parameters are collected in the vectors  . The vector   is the initialization.   is defined recursively:          is defined to be signum function:                         e.g. the partial derivative of the   dimension is calculated as:            Although the introduced approach is already able to deal with local minima caused by noise, we still have not achieved a total invariance to the initialization  . Local minima still prevent from a proper localization of the indentation in several cases. The balloon approach [ 25] introduced for active contours deals with this problem by adding an energy term, forcing the contour to become smaller or larger. Our approach allows to apply a kind of balloon force in an easy but effective way. Instead of calculating the radius   by gradient descent,   is simply decreased by one in each iteration of the gradient descent. If the contour starts at the image boundaries (  is large), it necessarily has to cross the object’s boundaries, when getting smaller and smaller. Unlike unforced gradient descent, the proposed balloon method does not stop before   becomes zero (or a defined minimum). In a second step, the history of the gradient descent has to be analyzed to get the best fitting vector   from a set of several local minima. In our case, the best results are achieved when using the vector   with the highest response (achieved by convolution) of the image information to the template (parametrized by  ) shown in Fig.   with a thickness of 3 pixels.    4 Including Shape from Focus
Springer.tar//Springer//Springer\10.1007-s00138-012-0479-4.xml:Feature matching based on unsupervised manifold alignment:Feature matching Image registration Unsupervised manifold alignment Manifold learning:  1 Introduction  Image registration is a process to establish feature correspondence between two images of the same scene. Registration problems frequently arise in the change detection, medical imaging and computer vision domains. Accuracy of image registration is directly affected by the accuracy of feature matching model. With this reason, many feature matching techniques have been investigated in recent years. Classical approaches to feature matching are usually based on correlation [1]. It is well known that correlation based approaches suffer from viewpoint changes and do not take into account the global structure of the image. In this respect, methods based primarily on the spatial relations among the features are usually applied. The well-known feature point matching includes the iterative closest point (ICP) algorithm, introduced by Besl and Mckay [2]. Nearest neighbors are matched between fixed and moving point sets. The optimal transformation parameters are subsequently generated from the matching process. This iterative procedure cycles until convergence. Chui and Rangarajan [3] formulate an algorithm known as “softassign” which relaxes the exact correspondence problem. When graphs are used to represent the features, feature matching problem is reformulated as that of graph matching [4]. Extensions to this work include that of [5]. Some important feature descriptions have been introduced using spectral representations of input features or graphs. Scott and Longuet-Higgins [6] were among the first to use spectral decomposition methods for correspondence analysis. They show how to recover correspondences via singular value decomposition on the point association matrix between different images. Shapiro and Brady [7] developed an extension of the Scott and Longuet-Higgins method, in which point sets are matched by comparing the eigenvectors of the point proximity matrix. One major weakness of spectral decomposition is that they cannot tolerate outliers and structural variations. The intrinsic structure of data is very important to improve matching under the assumption that the data resides on a manifold [ 8]. Many manifold learning methods have been developed to reveal data structure, such as Locally Linear Embedding (LLE) [ 9], Laplacian Eigenmaps [ 10], Isomap [ 11], Locally Preserving Projection (LPP)[ 12] and Neighbors Preserving Embedding (NPE)[ 13]. In this paper we address the problem of feature matching by manifold alignment. There is a growing body of work on manifold alignment in a semi-supervised way [ 14– 17]. Ham [ 15] map the points of the two data sets to the same space by solving a constrained embedding problem, where the embeddings of the corresponding points from different sets are constrained to be identical. Lafon [ 18] use diffusion maps to embed the nodes of the graphs corresponding to the aligned sets, and then apply affine matching to align the resulting clouds of points. The work of Zhai et al. [ 19] and Wang [ 20] are based on a similar framework as ours. In their approach, they map the two data sets to a lower dimensional space simultaneously preserving the neighborhood relationship within each set. An illustrative example of manifold alignment is in Fig.&amp;nbsp; , where two 2D manifolds   and   are seemingly different in their original space, and the goal is to align them in a new space. By doing this, we can compare the embeddings of the two manifold instead of their original representations. From this figure, we can see that the local geometries within   and   have been preserved in embedding space. However, all above alignment methods assume that some pairwise correspondences of points between data sets are known, and then use those information to guide the alignment. In many real world applications it might be difficult to obtain and use such information [ 21]. In general, images to be registered come from the same scene or the same object, and the topological structure has not been changed under small geometrical structural variations. In this case it seems to be a reasonable assumption that the images lie on or close to a manifold. Based on these, the object in each registration image can be thought of obeying the same manifold distribution, and each feature set extracted from images is a sample from the distribution. So, to cope with the problem of image registration with outliers and geometrical structural variations, we formulate feature matching as a manifold alignment problem, and propose a new unsupervised manifold alignment method. The rest of this paper is organized as follows: In Sect.&amp;nbsp;2, we describe the energy function of our unsupervised manifold alignment method. In Sect.&amp;nbsp;3, we give the optimization algorithm along with the proof of convergence of the proposed algorithm. Experimental results that verify our approach are depicted in Sect.&amp;nbsp;4. Finally, conclusions are drawn in Sect.&amp;nbsp;5.    2 Energy function definition  Assuming that we have two images in which significant points have been extracted, our aim is to find the correct correspondences between these two sets of points. First, let us represent two data sets in matrix forms as  and  , where each column vector denotes a data point in the input observation space. All manifold alignment approaches need to balance two goals:        Preserving correspondences relationship in embedding space. It penalizes the discrepancies between  and  on the mapping results of the corresponding points.         &amp;nbsp;           Preserving local geometrical relationship. Local geometry preservation is extremely important to achieve the smooth of the local neighborhood matching.         &amp;nbsp;      We now motivate the energy function used for feature matching. We would like to match the point sets as closely as possible. The correspondence problem is cast as a linear assignment problem. Our feature matching task is to learn non-negative mapping matrices   and correspondence matrix   simultaneously. We propose to minimize the following energy function:            Subject to the following constraints:       There are three terms in this energy function. The first term   is correspondence preserving energy. And the other two terms,   and  are the manifold regularization terms which are used to preserve the spatial relationship among data points. The regularization parameters   and  control the smoothness of the new representations. Preserving correspondences relationship in embedding space. It penalizes the discrepancies between  and  on the mapping results of the corresponding points. Preserving local geometrical relationship. Local geometry preservation is extremely important to achieve the smooth of the local neighborhood matching. In the energy function in Eq. ( 1), the correspondence preserving energy penalizes the differences between   and   on the matched local patterns in the new space. It is defined in the sense of Euclidean distance as:       This term characterizes the role of geometric spatial distance in finding correspondence. If a point   corresponds to a point , , otherwise  . The row and column summation constraints guarantee that the correspondence in one-to-one. For explicitly controlling the uniformity or uncertainty of the correspondence matrix  , an entropy regularization term   is added to the objective function. When performing a feature matching, smoothness regularization is necessary to restrict the mapping matrix. In the energy function in Eq. ( 1), the second and third manifold regularization terms guarantee that the neighborhood relationship with   and   will be preserved. Inspired by LLE and LPP methods, the two manifold regularization terms are defined as:            and            where,   and   are sets of  -nearest neighbors of   and  , respectively. And  can be computed by minimizing the following objective function,       with constraints       Please see [ 9,  23] for the details about how to solve the above minimization problem. And   could be got in a similar manner. As for  , it could be presented as       The matrix   is defined in a similar way on . The spatial relationships among data points are preserved by minimizing   and   in the mapped space. If two data point and  (or   and  are close,   and  (or   and   are also close to each other. Our approach has three fundamental differences compared to semi-supervised alignment. First, since we do not have correspondence information, we introduce correspondence indicator matrix  . Second, we seek for linear mapping functions   and   rather than direct embeddings, so that the mapping is defined everywhere [ 20]. Third, an entropy regularization term is incorporated into the objective function for controlling the uniformity level of the correspondence. Ultimately, we formulate the energy function as the following form:            Subject to the following constraints:       We detail the proposed model and clarify its differences from semi-supervised manifold alignment [ 19] in the following:        The main gain with the proposed model is that no a priori knowledge is needed. Our model regards the correspondence matrix  as a new optimization variable, while in semi-supervised model it is artificially predefined and fixed in subsequent procedure.         &amp;nbsp;           Our model will impose new constraints for solving correspondence matrix, which not only avoids  degenerating, but also gives a probability interpretation.         &amp;nbsp;           For controlling the uniformity or uncertainty of the correspondence matrix , our model introduces an entropy regularization term , which is indispensable. Without it, all the correspondence elementswill degenerate to trivial solutions [22].         &amp;nbsp;      &amp;nbsp; The main gain with the proposed model is that no a priori knowledge is needed. Our model regards the correspondence matrix  as a new optimization variable, while in semi-supervised model it is artificially predefined and fixed in subsequent procedure. Our model will impose new constraints for solving correspondence matrix, which not only avoids  degenerating, but also gives a probability interpretation. For controlling the uniformity or uncertainty of the correspondence matrix , our model introduces an entropy regularization term , which is indispensable. Without it, all the correspondence elementswill degenerate to trivial solutions [22].    3 Iterative optimization  The initial one-to-one matching   between point sets can be done by computing the distance between their characteristic vectors. Given  , after dropping the terms independent of   and  , we need to solve the following optimization problem:                         The matrices   and   are defined based on the matrix   as   and  , and the off diagonal entries in the two matrices are all zero. Let  and   be the Lagrange multiplier for constraint   and   respectively, and  , the Lagrange function is            The partial derivatives of   with respect to   and   are:               Using the KKT conditions  , we get the following equations for  :                         These equations lead to the following updating formula: The objective functionin Eq. (10) is non-increasing under the updating formulas in Eqs. (19) and (20), hence it converges. Theorem 1 guarantees that the update rules of  and  in Eqs. (19) and (20) converge and the final solution will be a local optimum. Please see the Appendix for a detailed proof. Given   and  , after dropping the terms independent of , we need to solve the following optimization problem:                    To optimize  , we need to solve a discrete linear assignment problem [ 24]. Some continuous methods have been proposed to solve this discrete problem in the literature. The most well-known methods for the matching problem employ softassign technique [ 3]. We adopt the softassign framework for our algorithm. The basic idea of softassign is to relax the binary correspondence matrix   to take values from interval  . So the objective function subjects to the following constraints:       To solve the  , we construct the Lagrangian function as follows [ 22]:            Let  , we have       since , we get       So, we can obtain Our approach incorporates manifold alignment into the general feature matching framework. The algorithm of unsupervised manifold alignment is summarized in Table&amp;nbsp; 1. The element  indicates the extent of pairing between features   and  . If  is both the greatest element in row   and column   then we regard those two features as being in   correspondence with one another; otherwise, it means that feature   competes unsuccessfully with other feature for partnership with   [ 6]. Each element of the matrix   measures the similarity between a pair of points, which is assumed to be non-negative in this work. Here, we use Gaussian kernel with Euclidean distance to initialize  . Here, we use Gaussian kernel with Euclidean distance to initialize  .       In Gaussian kernel, the value of the parameter   is often chosen manually, in this paper, we compute the   by       automatically, which are used in [ 25].    4 Experimental results  In this section, we provide some experimental evaluation of the unsupervised manifold alignment for feature matching. To show the performance of the proposed approach, we did extensive tests on both synthetic and real images. And we also compare our approach with some state-of-the-art feature matching methods. In the following subsections, we first use a bioinformatics example to illustrate how our manifold alignment algorithms work, and then we test the effectiveness of the proposed method. After that the robustness of the algorithm against outliers is analyzed using synthetic images. At last, we test the method on real-world data. In this test, we directly align two manifolds to illustrate how our algorithm works. The two manifolds are from bioinformatics domain. Protein 3D structure reconstruction is an important step in Nuclear Magnetic Resonance (NMR) protein structure determination [20]. With the information available to us, NMR techniques might find multiple estimations (models). Models related to the same protein are similar to each other but not exactly the same, so they can be used to test manifold alignment approaches. To align such two manifold   and  , used roughly 25&amp;nbsp;% uniformly selected amino acids in the protein as correspondences. Our approach does not require correspondence and can be directly applied to the data. For the purpose of comparison, we plot both manifolds on the same figure (Fig.&amp;nbsp; a). To show how our approach works, we plot 3D (Fig.&amp;nbsp; b), 2D (Fig.&amp;nbsp; c) and 1D (Fig.&amp;nbsp; d) alignment results in Fig.&amp;nbsp; . These figures show that the alignment of two different manifolds in achieved by projecting the data onto a new space using our generated linear mapping functions   and  . Here, the reference point set  was randomly generated between 0 and 1. Then, reference point set was transformed by affine transformation  to get the sensed data set , where  is a scaling parameter,  is the translation vector, and  is the 2D rotation matrix. In our experiments, the rotation angle ,  and . And Gaussian noise is added to the data set to test the robustness of the algorithm. Synthetic point position jitter is synthesized by generating a matrix , the point jitter is added to the matrix of feature point set positions for the second point set  using the equation . Here,. At first, we test the efficiency of our method on random point sets under translational, rotational, and position jitter. In Fig.&amp;nbsp; a–d, the transforms between reference points and sensed points respectively are translation, rotation, scale and similar transformation; Fig.&amp;nbsp; e the transform is mixture of similar transformation and position jitter. As the results shown, our algorithm can deal with these transformations effectively. To simulate outliers, we have tested our procedure on a known benchmark, known as the “CMU House” sequence ( http://​vasc.​ri.​cmu.​edu/​idb/​html/​motion/​index.​html). We have extracted a set of feature points from six images by applying the Harris detector [ 26] which produces errors and hence the point sets are of different sizes. We compare the performance of our new matching method with three alternatives. These are the semi-supervised manifold alignment [ 19], Scott and Longuet-Higgins method (Scott method) [ 6] and Shapiro and Brady method (Shapiro method) [ 7]. One pair images used in our study are shown in Fig.&amp;nbsp; . Superimposed on the images are the detected feature points. The two images are taken from different viewpoints. There is rotation, scaling, and perspective distortion present. Figure&amp;nbsp;  shows the correspondences between the feature points as lines between the first frame and the sixth frame. After checking by hand, the number of correct correspondences is summarized in Table&amp;nbsp; 2. From these experiments, it is clear that our unsupervised manifold alignment approach gives encouraging results when compared with the approaches of Shapiro and Brady, Scott and Longuet-Higgins. But our method is poor than semi-supervised manifold alignment. That is because partial alignments of the data sets obtained from prior knowledge of their manifold structure are completed, and then use that information to guide the alignment. However, in practice it might be difficult to obtain and use such information, and bad information may lead to poor results. Real images often include different geometric and photometric transformation, such as rotation, scale variance, viewpoint change and illumination contrast. In this part, we commence our real-world evaluation of the feature matching method on remote sensed images. Experiments have been carried out on two remote sensing image pairs comparison with the following four state-of-the-art feature matching methods: Shapiro and Brady method, Scott and Longuet-Higgins method, ICP method [ 27] and Scale Invariant Feature Transform (SIFT) [ 28]. The two images in Fig.&amp;nbsp;  are Landsat TM (Lake Casitas, America, 600 600) images but acquired at different times. Figure&amp;nbsp; a was obtained on 1984, and Fig.&amp;nbsp; b was obtained on 1986, which are used as the reference image and the sensed image, respectively. The matching results for the two images are shown in Fig.&amp;nbsp; c, and the registration result is shown in Fig.&amp;nbsp; d. Figure&amp;nbsp;  illustrates feature matching process for ALOS PLASAR images of Baishuihe, which was one of the most severely affected earthquake regions in the 12 May 2008 Wenchuan earthquake. Figure&amp;nbsp; a is used as reference image and Fig.&amp;nbsp; b as sensed image. The matching results for the two images are shown in Fig.&amp;nbsp; c, and the registration results are shown in Fig.&amp;nbsp; d. Figure&amp;nbsp;  shows registration results of two remote sensing image pairs with affine transformation. The two images in the first pairs are optical remote sensing images (Tokyo Bay, Japan) but acquired by different sensors. Figure&amp;nbsp; a was obtained by Landsat TM, and Fig.&amp;nbsp; b was obtained by ASTER sensor, which are used as the reference image and the sensed image, respectively. The registration result is shown in Fig.&amp;nbsp; c. The second image pair is X-band airborne SAR images (Shaanxi, China) but acquired from different views. We use Fig.&amp;nbsp; d as the reference image, Fig.&amp;nbsp; e as the sensed image and the registration results for them are shown in Fig.&amp;nbsp; f. To evaluate the performance of all five procedures, we consider the root mean square error (RMSE) between the corresponding point pairs. RMSE is the conventional and most widely used measure. When evaluating an estimator   of the geometrical transformation  , it is defined to be       where   are the estimated coordinates and   are the coordinates of the   corresponding point pair,   is the number of corresponding point pairs. If the value of RMSE is smaller, then the registration is regarded better. The registration accuracies in terms of the RMSE for Figs.&amp;nbsp; ,  and   by using the five different methods are summarized in Table&amp;nbsp; 3. From the Table&amp;nbsp; 3, it can be seen that the proposed algorithm is better than all four competing algorithms in terms of RMSE and running time, which show our method performs best in the five different methods.   
Springer.tar//Springer//Springer\10.1007-s00138-012-0480-y.xml:Large-scale gaussian process multi-class classification for semantic segmentation and facade recognition:Facade recognition Scene interpretation Gaussian processes Random decision forest Semantic segmentation Large scale classification:  1 Introduction  Semantic segmentation can be regarded as one of the most difficult visual recognition problems, since it requires turning each pixel of an image into a suitable category label. Due to the very general problem description, semantic segmentation approaches can be used in nearly every application that requires a precise labeling. Especially in the context of facade recognition, semantic segmentation has been found to be an useful tool. In contrast to the direct categorization of objects in a street scene&amp;nbsp;[12], the general framework of semantic segmentation can be often augmented with additional information about the special task at hand. For instance, the consecutive nature of images drawn from a sequence can be exploited to enhance classification accuracy&amp;nbsp;[45] or to infer a 3D reconstruction of streets&amp;nbsp;[43]. In the work of&amp;nbsp;[36], prior information regarding the composition of facade parts, such as the relative location of windows and doors, is enforced using shape grammars. Irrespective of the kind and amount of prior information used, the semantic segmentation step remains a crucial part in most facade recognition approaches. Usually, this task is solved in a supervised manner by learning a classifier on local patches with training examples obtained from pixelwise labeled images&amp;nbsp;[9, 11, 30, 31]. To cope with the large amount of training data, previous works use piecewise linear classifiers as classification techniques such as logistic regression&amp;nbsp;[9], random decision forests&amp;nbsp;[12, 30, 36] or boosting&amp;nbsp;[16, 31, 43, 45]. In this area, the use of non-linear and non-parametric learning machines, such as Gaussian process (GP) classifiers&amp;nbsp;[24], is limited due to their computational demands and their need for a large memory capacity. In this paper, we demonstrate how to perform inference for GP classification with tens of thousands of training examples occurring in supervised semantic segmentation scenarios. Our approach is based on pre-clustering the available training set with decision trees and learning a GP classifier for each leaf of the tree (cf. Fig.  ). In contrast to large-margin-based learners, such as support vector machines&amp;nbsp;[ 28], GP classifiers implicitly allow us to calculate the uncertainty of an estimate, which is particularly useful to derive suitable multi-class probabilities or for novelty detection. The resulting combined classifier offers to go beyond the restrictions of piecewise linear classifiers. Due to the large variability of local features belonging to different object categories, the ability to discriminate classes in a non-linear way is especially important for semantic segmentation tasks. Furthermore, our approach is adaptive and allows for handling the trade-off between accuracy and computation time. In comparison to other semantic segmentation methods like conditional Markov random fields (CRF), our proposed method models the image in multiple local features, which are all analyzed in a continuous probabilistic framework. Furthermore, the output of our framework can be used as unary term in different CRF methods. With this, our probabilistic framework is not in a direct competition with CRF methods, but with typical classification methods like support vector machines and logistic regression. With the rich and meaningful representation of a pixelwise labeled image, a whole bunch of applications is directly available. In the following work we concentrate on facade recognition, which, for example, allows for automatically generating facade models used for 3D city modeling [14]. Semantic segmentation is abstract name for all methods trying to label any sort of image pixelwise. The goal is to separate an image into homogeneous areas, where each region represents an instance of one of the trained classes. Due to the high need of computational resources, this research topic got important in the second half of the last decade. Csurka et al. [9] presented a straight forward approach very similar to ours. The main parts are unsupervised segmentation, feature extraction, feature classification and region labeling. Further approaches focused on improving these local results using conditional Markov random fields (CRF)&amp;nbsp;[15, 19, 44]. Yang and Förstner&amp;nbsp;[47] present an approach to label facades using a CRF, in which the unary potentials are computed by applying a random forest classifier. A subsequent work of the same authors&amp;nbsp;[46] improves this method by considering a hierarchical CRF that exploits region segmentations on multiple image scales. Another way to improve the results is by applying model-based approaches with hard coded prior knowledge. Teboul et al. [36] uses the so-called shape grammars. For this, the authors of [36] propose to use a simple random decision forest (RDF) for an initial result which will be improved by optimizing the labels with respect to a given grammar. In [35] the authors advanced the solving of the optimization problem in speed and accuracy. Normally, model-based methods tend to much better results than non-model-based methods like ours, with the precondition that the analyzed images are in a similar scenario as in the model encoded. Instead of that, classical semantic segmentation approaches like ours are much more flexible and tends also to good results on more general images. In contrast to all those works, we focus on the essential part of accurately classifying local patches without any contextual knowledge. In our experiments, we show that we are even able to outperform previous CRF approaches. We expect that adding a CRF model to our approach, which is beyond the scope of this paper, would further improve the recognition performance. In the last years, a large amount of scientific effort has been spent to develop fast inference techniques for GP regression and classification&amp;nbsp;[24]. Most of them usually rely on conditional independence assumptions&amp;nbsp;[4] with respect to a small set of predefined variables which might either be part of the training dataset&amp;nbsp;[37] or learned during training&amp;nbsp;[33]. A separate branch of methods is based on decomposition techniques, where the original large-scale problem is broken down into a collection of smaller problems. Next to simple Bagging strategies&amp;nbsp;[7], unsupervised d-trees neglecting label information during clustering were recently proposed&amp;nbsp;[29] for GP regression. As a supervised alternative, Broderick et al.&amp;nbsp;&amp;nbsp;[3] combined a Bayesian decision tree with GP classifiers. The approach of Urtasun et al.&amp;nbsp;&amp;nbsp;[40] performs GP regression by selecting training examples from a local neighborhood of a test point. The paper also compares the local approach to global ones using a pre-clustering technique. Whereas, their local approach allows reducing boundary effects, our pre-clustering method leads to a logarithmic rather than a linear computation time during learning with respect to the number of training examples. Another important direction for fast inference with Gaussian process models is Bayesian committee machines as introduced by Tresp et al.&amp;nbsp;&amp;nbsp;[38]. The underlying idea is also to decompose the training set into several subsets and to learn a regressor or classifier for each set independently. However, unlike our approach, each partition is used for classifying test examples. Tresp et al.&amp;nbsp;&amp;nbsp;[38] also study fast GP classification with time-consuming approximate inference techniques instead of relying on GP regression as done in this work. Especially in the context of visual classification tasks it has been shown that, despite its improper noise model, GP regression directly applied to the labels is often sufficient [26]. There is also a large number of related papers concerning large-scale learning with support vector machines (SVM). For example, Tsang&amp;nbsp;et al.&amp;nbsp;&amp;nbsp;[39] improves the core vector machine formulation of SVM by considering enclosing balls of fixed radius and presenting corresponding approximation techniques. In contrast to our approach, they do not focus on speeding up the prediction time necessary to classify a new example. An approach highly related to ours is proposed in Chang et al.&amp;nbsp;&amp;nbsp;[5], where SVM are accelerated using a decomposition derived from a decision tree. In their setting, standard SVMs are employed resulting in a classifier which produces hard decisions. In the context of scene recognition, Fröhlich et al.&amp;nbsp;&amp;nbsp;[13] recently proposed a GP-based method relying on a pre-clustering via random decision forests. However, this approach is solely based on a-posteriori estimates of the predictive distribution, neglecting available uncertainty values. The remainder of this paper is organized as follows. First of all, we describe the basic principles of the semantic segmentation approach used. Section&amp;nbsp;3 reviews Gaussian processes for classification tasks and proposes a method to obtain suitable probabilities from the one-vs.-all method of [18]. Our tree-based acceleration technique for inference with Gaussian processes is presented in Sect.&amp;nbsp;4. We perform experiments for facade recognition applications as a special case of semantic segmentation and evaluate them in Sect.&amp;nbsp;5. A summary of our findings and a discussion of future research directions conclude this paper.    2 Semantic segmentation framework  As described above, semantic segmentation is concerned with assigning class labels (or probabilities) to each pixel of a given image. Csurka&amp;nbsp;et al.&amp;nbsp;&amp;nbsp;[9] proposed a simple, but powerful framework for tackling this task. Relying on a bottom-up methodology, their approach combines an initial unsupervised over-segmentation of a given image with pixelwise classification results. It has been recently shown on an empirical basis&amp;nbsp;[12] that a time-consuming feature transformation step from the original framework&amp;nbsp;[9] can be bypassed. Using a random decision forest, training and prediction time is considerably reduced. The whole processing pipeline of this approach is depicted in&amp;nbsp;Fig.&amp;nbsp; . It mainly includes four steps:          an over-segmentation is obtained using an image segmentation algorithm.         &amp;nbsp;             to capture color and texture information in a local neighborhood, feature descriptors are computed on an equally spaced grid for various scales.         &amp;nbsp;             labels are softly assigned to each grid point using a probabilistic classifier. To generate dense probability maps, all grid-based classification results are convolved with a Gaussian filter. The final probability map is generated by averaging all maps obtained for different scales.         &amp;nbsp;             one deterministic class label is assigned to each cluster segment by choosing the category with maximum average probability within that region.         &amp;nbsp;      A detailed description of our experimental setup can be found in Sect.&amp;nbsp; 5.2. an over-segmentation is obtained using an image segmentation algorithm. to capture color and texture information in a local neighborhood, feature descriptors are computed on an equally spaced grid for various scales. labels are softly assigned to each grid point using a probabilistic classifier. To generate dense probability maps, all grid-based classification results are convolved with a Gaussian filter. The final probability map is generated by averaging all maps obtained for different scales. one deterministic class label is assigned to each cluster segment by choosing the category with maximum average probability within that region.    3 Gaussian process classification  In the following, we briefly review Gaussian process (GP) regression and classification. We concentrate on the main model assumptions and the resulting prediction equation. For a presentation of the full Bayesian treatment, we refer to Rasmussen and Williams&amp;nbsp;[24]. Given  training examples  denoting input feature vectors and corresponding binary labels , we need to predict the label  of an unseen example . Therefore, a learner has to find the intrinsic relationship between inputs  and labels . It is often assumed that the desired mapping can be modeled by , where  is a latent function (which is not observed during training) and  denotes a noise term. One common modeling approach is to assume that  belongs to some parametric family and to learn the parameters which best describe the training data. However, the main benefit of the GP framework is its ability to model the underlying function  directly, i.e.&amp;nbsp;without any fixed parameterization, by assuming that  is a sample of a specific distribution. Defining a distribution over functions in a non-parametric manner can be done with a Gaussian process, which is a special stochastic process. To use the modeling ideas described in the previous section, we formalize and correctly specify the two main modeling assumptions for regression and classification with Gaussian processes:              The latent function        is a sample from a GP prior                     with zero mean and covariance or kernel function       :                                  &amp;nbsp;           Labels  are conditionally independent given latent function values  and are described using some noise model .         &amp;nbsp;      The Gaussian process prior enables to model the correlation between labels using the similarity of inputs, which is described by the kernel function. It is, thus, possible to model the assumption of smoothness, i.e.&amp;nbsp;that similar inputs should lead to similar labels. The latent function   is a sample from a GP prior      with zero mean and covariance or kernel function  : Labels  are conditionally independent given latent function values  and are described using some noise model . For classification purposes, sigmoid functions are often employed as noise models&amp;nbsp;[ 24]. In contrast, we follow Kapoor et al.&amp;nbsp;&amp;nbsp;[ 18] and use zero mean Gaussian noise with variance  :       which is the standard assumption for GP regression. The advantage of this label regression approach is that tractable predictions for unseen points   are possible, without using approximate inference methods&amp;nbsp;[ 24]. Let   be the kernel matrix with pair-wise kernel values of the training examples   and   be kernel values   corresponding to test example  . The most likely outcome   given input   and labeled training data can then be predicted analytically using the following equation:       with   being the vector of the binary labels of all training examples. This prediction equation is equivalent to kernel ridge regression, but with a clear probabilistic meaning. For example, the GP framework allows for predicting the standard deviation   of the estimation by:       Please note that standard support vector machines lack this intrinsic probabilistic formulation and that the associated optimization objective does not give rise to an uncertainty estimate. An example of the result of GP regression is given in Fig.&amp;nbsp; . In the previous section, GP classification is restricted to binary tasks. However, by applying the one-vs.-all strategy in combination with a majority voting scheme, multi-class classification problems can be solved without much additional computational effort&amp;nbsp;[ 18]. Let   be the vector of binary labels corresponding to class   derived from the multi-class label vector   by:       where   if and only if   is true. The final predicted category is the one that achieves the highest predictive posterior mean given by the corresponding binary problem: Due to additional smoothing applied to the probability maps, the semantic segmentation framework presented in Sect.&amp;nbsp;2 requires that the classifier predicts benign probabilities for each class. The one-vs.-all approach of [18] only offers a hard classification decision as given in Eq.&amp;nbsp;(6). To derive probability estimates for each class, we could squash the posterior means using a softmax function&amp;nbsp;[24]. However, this strategy completely ignores the uncertainty of the estimate and hides the fact that the one-vs.-all decision is also probabilistic in its nature. We propose to take the whole posterior distribution    of each random variable   into account, so that the probability of class   achieving the maximum score can be expressed by       Unfortunately, it does not seem to be possible to derive a closed-form solution for the probability on the right hand side of Eq.&amp;nbsp;( 7) for a multi-class scenario with  \(M&amp;gt;2\). Therefore, we use a simple Monte-Carlo technique and sample   times, e.g.&amp;nbsp; , from all   Gaussian distributions   and estimate the probability of each class   by       with   denoting the number of times where the draw from   was the maximum value. A large variance  , i.e.&amp;nbsp;a high uncertainty of the estimate, leads to a nearly uniform distribution  , whereas a zero variance results in a distribution which is equal to one for the class which corresponds to the highest posterior mean. An alternative would be to directly use a multi-class classification approach with Gaussian processes, but this has to be paid with time-consuming approximation techniques like Laplace approximation&amp;nbsp;[24].    4 Large-scale GP classification with tree-based models
Springer.tar//Springer//Springer\10.1007-s00138-012-0481-x.xml:Visual spatial-context based wildfire smoke sensor:Visual context Fuzzy evaluation measures Forest fire smoke detection Wildfire smoke detection:  1 Introduction  Wildfires are a constant threat to ecological systems and human safety, especially in sparsely populated rural areas with high fire risk factors. Estimations indicate that the general volume of the world forest stand is rapidly reducing and wildfires represent one of the main causes for such trend [1]. To minimize potential damage caused by wildfires, three types of actions are available: preventive action in pre-fire phase, early fire detection in initial fire ignition phase and quick and efficient fire-fighting in active fire burning phase. The focus of this paper is on fire detection in its initial stage so the prompt reaction in terms of fire-fighting activities could be executed. Traditional way of detecting wildfire is based on human observers situated on fire lookout towers with good visibility covering larger areas around the post. Thanks to advances in visual systems technology, remote cameras can be located on different posts enabling a single observer to cover multiple remote areas. In the past 10–15 years, development of automatic fire detection systems started to take place as an aid to human wildfire observation. There are many different approaches to wildfire detection using color and motion information to detect fire flames [2–9]. However, most of the visual detection systems are mainly designed for smoke detection, since the appearance of smoke is in most cases more visible than the fire itself. Terrain configuration and other obstacles could often occlude the fire flames. Methods dealing with smoke detection are mainly based on fusion of several different approaches, such as texture-based approach [10–12], chromatic-based approach [13–15], neural networks [16], detection of smoke motion [17–21], support vector machines [22, 23], clustering on fractal curve [24], detection based on fractal properties of smoke [25], fuzzy-finite automata [26] and wavelet analysis [16, 27–32]. Smoke detection systems provide surveillance assistance for smoke detection, but cannot be used as a self-sufficient solution. Such systems are capable of detecting smoke in the image with high accuracy but they often need additional human confirmation for final decision. The goal of our research is the increase in detection accuracy, bringing the performance of the detection systems closer to the results that could currently be obtained by human observers. We present a novel method for visual smoke detection. The proposed method is based on motion detection, chromatic analysis and smoke-dynamics analysis followed by alarm verification based on the spatial context information. The method is named spatial context smoke detection method (SCSD). The rest of the paper is organized as follows: the SCSD method is presented in detail in Sect. 2. Evaluation methodology is given in Sect. 3, followed by evaluation results in Sect. 4. Conclusion is given in Sect. 5.    2 Smoke detection  Detection phases for SCSD method are presented in Fig.&amp;nbsp; . Sequence of visible spectrum images is used as an input for the smoke detection system along with real-time meteorological data from the detection site. The smoke detection process is divided into several phases dedicated to the different aspects of detection. In the motion detection phase, moving regions are extracted and forwarded to the following phases with the original image. In the chromatic analysis phase, additional verification of motion regions is performed based on region color characteristics. The wavelet-based analysis is used for change detection in high frequency content of the image. Appearance of smoke over a region should gradually change the energy of the region, and the edges should loose their sharpness without vanishing instantly. Every detected region is iteratively examined for smoke dynamics like growth and lateral or upward motion. Another part of the detection system is the visual context analysis. First, the image is divided into homogenous regions in the segmentation phase. The classifier generates the possibility for a region belonging to a certain class, so only those regions that have high classifying certainty are classified. Regions that are not classified do not play a role in the spatial-context analysis. Certain categories of false alarms could be eliminated based on the spatial arrangement of the detected regions and the classified regions. Real-time meteorological data are obtained from the detection site using a meteorological weather station. Meteorological analysis phase outputs a value that indicates the potentiality of fire in the surroundings given the current meteorological conditions. Finally, all the information regarding color, texture, dynamics, meteorological and spatial context is used as an input to the inference engine. In the case that all the indicators imply that the region in the image could be classified as smoke an alarm is raised and a possibility of smoke for the given image is generated. The first step in most fire and smoke detection systems is motion detection. There are many approaches to motion analysis used for smoke detection like detection based on binning and clustering, motion segmentation using motion history image or clustering motion on a fractal curve [ 33– 35]. Method used in SCSD is adaptation of a general method for motion detection presented in [ 36], modified to take into account smoke characteristic and smoke-detection scenarios. First, a statistical background model is used            where   represents value of pixel   in the   frame,   is the relative deviation threshold and   represents standard deviation for a given pixel in the   frame. Standard deviation is calculated for every pixel using       where   is a parameter defining responsiveness of the model to the changes in the background, and   is the running average for a given pixel, calculated by       Parameters   and   are variable parameters, and we have to determine the values that give the best results on image testing database. Testing algorithm performance for every parameter combination is computationally expensive, since the quality of selected parameters has to be evaluated using several different detection sequences and scenarios, so a heuristic approach is adopted using genetic algorithms. Genetic algorithm fitness function evaluates the motion detection error for given set of parameters. Errors from all the test sequences are accumulated and represent the fitness value. The aim of the optimization is to find a global minimum of the fitness function in continuous search space. Calculating fitness value for every individual in every generation could be computationally expensive if the test database contains a large number of images. So the population size is set to 20 to achieve a compromise between search space coverage and computational requirements. Diversity is defined by limiting the initial range of the input variables to   and  . Two of the best-scored individuals are labeled   and are guaranteed to survive to the next generation. The rest of the parents create new children through crossover and mutation. In this case, 80&amp;nbsp;% of children are generated using crossover, and 20&amp;nbsp;% through mutation. Best fitness function values are obtained for parameter values   and  . Chromatic analysis is performed after the candidate regions are detected. The analysis is based on results obtained from database of ground truth segmentation containing 1,000 images. Ground truth images are analyzed taking into account smoke regions mean value, pixel color space distribution and first, second and third region moments. Based on experimental results, certain rules are made concerning the characteristics for the detected phenomenon that define the range of chromatic properties specific to smoke. Since smoke is semi-transparent phenomenon, in specific scenarios it is difficult to distinguish it from the background based only on color information. Figure   shows RGB values for neighboring smoke and background pixels taken for 5 images from different scenes. However, color space covered by smoke pixels is relatively limited, and diagonally positioned due to the fact that smoke is always colored in light-to-dark gray color interval. So by adopting color-spread rules and limiting region intensity moment values, a certain number of non-smoke regions could be eliminated. Smoke is visible because of the effect of light scattering from smoke molecules, and such effect is not the same in every color channel. The light exhibits scattering effect when it comes in contact with atmospheric molecules, known as Rayleigh scattering, where the intensity of the scattered light is inversely proportional to wavelength. This implies that the scattering at  &amp;nbsp;nm is 9.4 times as great as that at  &amp;nbsp;nm for equal incident intensity, so in performing chromatic analysis the blue channel is more extensively used than other channels. Besides the color information, texture of the region is also used as an indicator about smoke characteristics. After the initial change detection, region candidates are checked for texture consistency. The appearance of smoke affects the high frequency content of the image by gradually smoothing the edges. Edges in the image represent local extrema in the wavelet domain and the smoothing effect of the edges results in a decrease in values in these extrema. Since the edges and texture contribute to the high frequency information of the image, energies of the wavelet subimages drop due to smoke in the image sequence [ 31]. The possibility of frequency analysis with different band-pass filters of different sizes is the reason why wavelet analysis was chosen over other image analysis tools. Deviation in texture content is analyzed using spatial discrete wavelet transformation (DWT). The analysis is performed in the first four decomposition levels, since the further decomposition does not contribute any significant information regarding the possibility of smoke in the analyzed region [ 27]. After testing several wavelet-basis functions, Daubechies 10 wavelet was selected giving equal or better results over other basis functions. The results of the transformation in particular decomposition levels are approximation coefficients matrix (cA) and detail coefficient matrices in horizontal (cH), vertical (cV) and diagonal direction (cD). The total energy of the region is calculated using standard wavelet energy equation:       The energies of the detected region are normalized for every decomposition level; components of all the directions are taken into account and compared to the referent background values. Significant deviation from the referent values is an important indicator considered in further detection phases. Regions detected using motion detection are checked for smoke-like behavior. Smoke regions should exhibit gradual growth without intensive variations in size. Size of the detected region is measured in every iteration. Since the distance between the camera and the real position of the detected region is unknown, certain aberrations are allowed, but the size of the region should gradually increase. Smoke regions should also exhibit upward and lateral motion. The upward shift of the center of gravity of the detected region should be persistent in the first phase of the detection. These motion characteristics are compared against referent values obtained from the test. The correlation value between the referent and current motion dynamics values is used as an output from this phase. The meteorological context includes information about wind speed, wind direction, air pressure, humidity and temperature measured by appropriate meteorological stations. Meteorological information is used to determine the weather conditions to automatically adjust the sensitivity of the detection process, for example excessive humidity implies rainy weather or after-rain period. Meteorological analysis gives as its output a value that indicates the potentiality of fire in the surroundings given the current meteorological conditions. Wind speed and direction are used in combination with motion dynamics analysis. The lateral motion of the detected regions should not greatly deviate from the wind direction readings of the station. However, a certain extent of deviation is allowed due the possible difference of the meteorological conditions between the detection site and the actual position of the region. In case the lateral motion of the region is the opposite of the wind direction readings from the station the region should not be classified as smoke. It is important to emphasize that although wildfires can affect the direction of the wind, for early fire detection that is not the case. We are trying to detect the fire in its starting, incipient phase when the fire is small, so fire influence to the wind direction could be neglected. There are many approaches to image segmentation such as clustering methods, compression based methods, graph partitioning methods, histogram-based methods and region merging methods [ 37]. SCSD uses statistical region merging [ 38,  39] for image segmentation. Statistical region merging used in SCSD is adapted for segmentation that is more sensitive to visual smoke characteristics, giving priority to correct segmentation of smoke regions over the segmentation of other categories. In the start of the segmentation process, every pixel is considered as a separate region, and the merging is iteratively performed based on merging predicate   that decides whether two regions   and   belong to the same statistical region. Merging predicate is defined as:            where   and   represent average values of different color channels from RGB color model,   represents blue color channel,   and   represent blue and general threshold, respectively, where  \({T_\mathrm{b}} &amp;lt; {T_\mathrm{G}},\) and   is defined as:       where   represents the cardinality,   is the maximum allowed value for used color channel (usually 255),   is defined as  , where   represents the number of pixels in the image and   is a tunable parameter used for varying the general number of the segmented regions. Predicate   defines three conditions to be satisfied in order for merging to occur. First condition is a statistical region merging criterion defined by   [ 38]. Using this criterion, visually homogenous regions could be extracted based on their chromatic characteristics. This enables the method to perform under variable lighting conditions, since the segmentation is based on statistical region homogeneity, rather than intensity differentiation constraints. Regions satisfying the region homogeneity condition for every color channel are further analyzed. To make the segmentation procedure more sensitive to smoke regions, additional conditions are introduced in the predicate. Appearance of smoke in the image region results in change in the chromatic characteristics that is most noticeable in the blue channel. To make the segmentation procedure more rigorous when dealing with smoke regions, conditions concerning the aberration in the blue channel are limited using a threshold  . Another characteristic of smoke is its neutral coloring, spanning from light to dark gray. Average values for the regions should be similar in every color channel, so a general threshold   is introduced. These conditions are introduced to reduce partial blending of smoke regions with the background regions in the process of segmentation. However, the process of segmentation is not the replacement for smoke detection, it is rather used to improve segmentation of regions adjacent to smoke for false alarms reduction using spatial context analysis in the following phases. The experimental testing performed with test database images and ground-truth segmentation shows that the segmentation gives the best results in the interval  ,   and   for various scenarios, but there is not a single optimal value for every situation. For evaluation purposes, values  ,   and   are used giving the best average segmentation results on segmentation test database. Fire detection systems are placed in nature surroundings, and for the task of general categorization six main categories are defined as:                The phase of categorization is not used for detection itself. It is used in validation process for false alarms elimination, so it is not necessary to categorize the whole image. Categorization of a certain region should be performed only if the classifier decision with high degree of confidence could be made. Regions that are not classified do not play a role in the alarm validation process. In this phase, regions are not categorized into   category as well as   category, because these phenomena are of our prime interest. Method used for general categorization is Naïve Bayes (NB) classification based on kernel density estimation [ 40]. NB is selected for the classification process for several reasons. NB classifiers are effective, efficient, robust and support incremental training. Furthermore, NB classifiers can deal with a large number of variables and large data sets, and they handle both discrete and continuous attribute variables. Features such as average RGB and HSI values, first and second region moments, and the size of the region are selected for the classification process. Let   be the possible semantic class from one of the defined six categories. The probability that a region having a feature vector   belongs to a class   is given by the Bayes’ theorem       where   represents the prior probability of region belonging to the class  , that can be obtained from the ground truth segmentation, and   represents the probability of the given feature vector from the prior feature distribution. Probability of an observed value could be easily obtained using kernel density estimation. Gaussian kernels are used with estimated density averaged over a large set of kernels                         where   ranges over the training points of feature vector  . The training of the classifier is performed using a ground truth image database containing over 6,000 regions belonging to one of the previously defined classes. Smoke Low clouds and fog Sun and light effects Water surfaces (sea, lakes, rivers, etc.) and sky Distant landscapes Vegetation. Primary causes for false alarms are natural phenomena visually similar to smoke that could be in certain conditions misinterpreted as smoke even by human observer such as fog, clouds low to the ground particularly when video cameras are located on mountain tops, dust from the ground, water evaporation, etc. On the other side, there are also other types of false alarms caused by natural phenomena but human observer could easily dismiss these alarms. Examples are rain drops on camera (Fig.&amp;nbsp; a), or sun effects (Fig.&amp;nbsp; b). These occurrences can easily adduce the system to trigger a false alarm. Such scenarios can be avoided by introducing specific methods for false alarms elimination using alarm shape characteristics. Smoke has irregular convex shape that is not compact, while raindrops on the other hand have high compactness factor. Measure of compactness and curvature can be calculated using Eqs. ( 10) and ( 11), respectively, taking into account the perimeter and area of the object:                         where   represents compactness,   is the perimeter,   is the area,   is curvature,   is the index of border point of the region and   is the angle between two lines intersecting at point&amp;nbsp; . Each line is passing through a different boundary point with distance of three neighboring boundary points in opposite circular directions from  . The method used for reducing alarms generated due to sunlight effects is based on specific characteristic for sunrays that have a rather elongated shape very distinguishable from smoke. The goal of the method is to calculate the elongation factor for the detected region and determine if it could be rejected as sunray using this property. The axis of the least moment of inertia is calculated by       where   is the angle of this axis with the positive   axis, and   represents the moment of the   order. When the axis is determined a bounding box for the region can be found, and the elongation factor obtained. From experimental experiences, the data show that the smoke regions have elongation less than factor 3, while the sunrays have elongation factor greater than 5. Presented methods are used for two particular types of false alarms, and by using the spatial context information additional scenarios resulting in false alarms could be dismissed. Most of the cameras used for smoke detection have pan-tilt ability and cover a   area around the mounting point. In some situations, large water surfaces appear in part of the scenes, where water motion and spray in windy weather could be a trigger for false alarms. It is also possible that clouds, which by shape and chromatic characteristic resemble smoke, could trigger a false detection, as well as partial capture of sun contours on the edge of the image. All potential detections are compared with categorization results. In most cases, categorized regions do not cover the whole area of the image because only those regions that are classified with high certainty factor are used in this phase. Relative positions of the candidate alarm regions and categorized regions in their vicinity are analyzed. Figure   shows a possible spatial relation between two regions. These relative relations could be divided into directional and topological. Directional relations describe the relative positions of the regions to each other, e.g. left, above, completely below, mostly right, etc. Topological relations describe the non-directional relations such as near, far, surrounds, excludes, connects, borders, etc. [ 41]. Absolute positions of the regions are also used, such as left corner, middle of the image, bottom, etc. Relative relation between two regions can be acquired using regions bounding box, center of gravity and relative angle between two regions as Fig.&amp;nbsp;  shows. Depending on the relative position of the candidate regions for detection and classified categories in the vicinity, certain rules can be introduced for elimination of specific scenarios. Examples of spatial-context based elimination rules are The set of elimination rules contains all the constraints regarding spatial arrangement of specific region classes. Based on this information, certain categories of false alarms could be dismissed in this phase of the post-detection false alarm reduction. There are possible scenarios where smoke could occur beyond direct vision of the camera and gradually appear in the scene (e.g. behind a mountain). In such cases, the smoke candidate regions would be detected after emerging behind the occluding terrain. Such scenarios are not eliminated as false alarms due to the base-line contact of the emerging region with regions categorized as  or . In case of the base-line contact of the region with the mentioned categories, the rejection is not performed. This prevents the elimination of valid alarms that appear out of the context of their origin. The results from all detection phases are used as inputs to the inference engine. Every input value represents the extent of deviation from the referent values for the given input type. All the values are mapped to the interval [0,1], where 1 represents significant deviation from the referent values, and 0 represents no deviation. The mapping function is specific to every input type, e.g. the deviation for the shape elongation parameter is mapped to 0 if the calculated elongation is 2.5, which is the actual referent value, and in case the calculated elongation is greater than 7 the parameter is mapped to 1. Similarly, mapping functions are defined for every input type based on the referent values from the training set and the allowed range. The mapping is performed for every input type except for meteorological information. Meteorological input is used for the adjustment of the detection sensitivity. In the case of weather conditions that imply low fire risk (e.g. excessive humidity implies rainy weather or after-rain period), the sensitivity of the system is adjusted accordingly. Every region detected by the motion detection phase is recorded in the system, and the analysis methods are performed in several iterations. This ensures the consistency of smoke-like behavior. The system uses elimination-based detection, where every input is used as a basis for potential elimination. Significant deviation from the referent value in any of the input parameters results in elimination of the region from the detection process. One example would be the deviation in chromatic characteristics. If the chromatic values of the candidate region fall out of the allowed range predefined in the testing phase of the system, the deviation values would be significant and would act as a trigger for the region elimination. The same rule applies for motion analysis, dynamics analysis and texture analysis. The deviation tolerance is additionally adjusted based on the current sensitivity of the system. In the case of weather conditions where fire occurrence is highly unlikely, the tolerance to deviation is decreased for 10&amp;nbsp;% in all the detection aspects. If the candidate region persists through the confirmation period an alarm is raised along with the actual calculated possibility of smoke. The possibility of smoke is calculated as a complement of average deviation values from all the inputs. Examples of detection images are shown in Figs.&amp;nbsp; and . The computational complexity could be obtained through analysis of the computationally significant phases of the algorithm. The most computationally demanding phases are motion detection, wavelet analysis and segmentation based on region merging. Other detection phases could be omitted from computational complexity analysis, since they are not computationally expensive in relation to the mentioned phases. The motion detection phase can be shown to have complexity of , where  denotes the number of pixels in the image, and  is the number of foreground pixels. The complexity of the wavelet analysis phase is  for four decomposition levels, where only pixels from the candidate regions are taken into account. The segmentation based on region merging can be shown to have complexity of , where  is a constant defined in Sect.&amp;nbsp;2.4, so the complexity is linear in . The overall complexity of the algorithm is cumulative complexity of all the phases  which is linear in  and as such suitable for real-time operation.    3 Evaluation methodology  To evaluate and compare different smoke detection algorithms certain evaluation measures have to be defined. A novel approach to smoke detection quality assessment is introduced through the definition of fuzzy measures. They have been used in evaluation process in combination with the state of the art measures that are best suited for smoke detection. Quality measures for such systems can be divided in two categories: evaluation on global and local scale. Evaluation on global scale is performed using classifier results from multiple consecutive images or multiple image sequences. Classifiers are evaluated based on true–false detections on the image level. When evaluating on local scale, focus is on detection quality on a single image [ 42]. A pixel is considered to be the smallest detection unit when evaluating on local scale, and a single image is considered to be the smallest detection unit on the global scale. In detection theory, various detector evaluation measures could be defined [ 43,  44] and for wildfire smoke sensor evaluation seven measures for general detection quality assessment have been used: cd—sensitivity measure or true positive rate defined by ( 13), cr—specificity measure or true negative rate defined by (), md—false negative rate (complement to sensitivity) defined by ( 15), fd—false positive rate (complement to specificity) defined by ( 16), acc—accuracy measure defined by ( 17), ppv—precision or positive predictive value defined by ( 18), and mcc—Matthews correlation coefficient [ 45] defined by ( 19).                                                                                          where TP is the number of correctly classified positive detections, FP is the number of falsely classified positive detections, TN is the number of correctly classified negative detections and FN is the number of falsely classified negative detections. In ideal case FP and FN are zero, so ideal classifier has cd   1, fd   0, cr 1, md   0, acc   1, ppv   1 and mcc   1. Individual pixels could be classified as partially smoke or background, based on degree of membership to each of these classes. Using fuzzy evaluation, the classification error is determined based on membership difference of referent (ground-truth) and observer decision regarding individual pixels for each class [ 42]. Error   of an observer for a pixel   is calculated by            where   is the referent fuzzy value for the pixel  ,   is the value given by the classifier, and   is the parameter defining the cost of the error. This error measure takes into account the type of error as well as the extent of the error. The case where the assessed value for the smoke membership is grater than the referent ground-truth ( \(O&amp;gt;R\)) fuzzy-segmentation is called fuzzy false detection. The real cost or impact on the environment of such scenario is significantly less than in the case of fuzzy missed detection when. The error cost for fuzzy missed detection increases with referent fuzzy value for that pixel and with the difference between observer and referent values. When both values are equal ( ), the error is zero. The parameter   is defined experimentally, and value   gives the best cost proportion between the two types of errors. This parameter takes into account the impact of the error on the environment as well as on the practicality of the detection system.    4 Results  The proposed SCSD method is compared with two other existing methods: method denoted as   presented in [ 32], and method denoted as   presented in [ 46]. Figure   shows comparison flowcharts for all three methods.   consists of several detection steps. First, a motion detection algorithm is used to obtain moving regions from the image. Motion detection is based on background estimation and recursive thresholding. The following phase is the detection of decrease in high frequency content using spatial wavelet transform. Image is divided into blocks of size 8 by 8 pixels and wavelet coefficients are calculated for each block. Single-level discrete wavelet transform is used in the implementation. The following phase is the detection of decrease in   and   color channels caused by the appearance of smoke in the region. In the next step, the flicker on the smoke boundaries is used as additional information for detection. Finally, shape analysis is performed determining the convexity of the detected region. The algorithm gives the best results at close range ( \({&amp;lt;}100\)&amp;nbsp;m), but is also very reliable at greater distances ( \(&amp;lt;\!2\)&amp;nbsp;km). The authors have also designed algorithms that deal primarily with long range smoke detection such as [ 47,  48]. is based on detection of changes in the blue channel of the image and dynamics analysis of the detected regions. The first step of the method is the binning of the image, based on calculations of the average variation in the number of smoke pixels. Mean value for every bin is compared with referent background value. Background bin values are periodically updated to avoid significant change in scene lighting that could trigger false alarms. In case that the difference in the blue channel between current and the referent bin is greater than the calculated threshold, the detected bins are declared as candidate bins and the confirmation phase is initiated. During the confirmation phase, a predefined minimum number of clustered candidate regions should exist in the scene, or the confirmation phase is interrupted and the process reverts to pre-detection state. Also, the detected regions should exhibit gradual growth, so the regions that are exceeding the growth speed specific to smoke are dismissed. Regions satisfying all the detection conditions for a sufficient validation time period are declared as smoke regions and a smoke alarm is raised. Evaluation is performed on the set of 2,870 test images from 10 different video-sequences [ 10]. Meteorological data were not used in the evaluation process. Figure   shows samples of detection outputs for the same frame in one testing sequence. Basic type of evaluation is based on the most important global measures: sensitivity measure (cd) and specificity measure (cr) shown in Table  1. The results show that none of the methods performs flawlessly since for all methods   and  . Results for measures   and   are not presented, since these measures are complementary to   and  , respectively. Local measures could be used for more detail detectors quality comparison in the case when compared methods have similar results concerning global measures. Various graphical representations of detection quality measures have been proposed like ROC curves [ 32] or DET curves [ 44], but they are suitable for analysis when discrimination threshold is varied. For detection algorithm quality comparison   [ 42] are more appropriate (Fig.&amp;nbsp; ). Quality graphs have image rank on   axis and increasingly sorted appropriate quality measure values on   axis. It is important to emphasize that image rank is not the same for all quality measures. There are specific situations where certain measures could not be calculated, for example, when   measure could not be calculated due to division by zero, hence in Fig.&amp;nbsp;  the number of sample images was not 2,870 (number of images in test sequences), but 185.
Springer.tar//Springer//Springer\10.1007-s00138-013-0482-4.xml:A novel plane extraction approach using supervised learning:UAV navigation Planar surfaces Autonomous navigation Point cloud:  1 Introduction  An accurate estimate of the landmarks provides a crucial input measurement for solving a simultaneous localization and mapping (SLAM) problem. In Manhattan environments such as indoor scenes, most ubiquitous and consistent landmarks that can be exploited are planes. It may seem trivial to extract accurate planes in such an environment; however, it becomes a challenging problem due to the presence of multi-level planar surfaces within the three-dimensional (3D) point cloud obtained through reconstruction of the scene. Conventional region growing methods for extracting geometrical primitives provide stable output but are often avoided due to high computational cost incurred even in 2D space. In addition to being computationally expensive, such methods also have high dependence on good accumulator design which merges the various group of points based on a high probability of being on a plane. In contrast to these region growing-based methods, model-based approaches using supervised learning can learn a model from a given set of examples of planes and can predict accurately the class of the new set of points by checking their closeness to learned model. Such methods can not only provide better retrieval of planar surfaces but also learn the associations between the planes if the features are carefully selected. In this work, a novel PLAnar Surface Extractor (PLASE) method is being proposed along with a feature selection mechanism. As a first step, features are extracted for each point in the point cloud of planar surfaces. Then, the extracted features are trained on a set of known planes and a model is learned based on estimation of a multivariate probability density function. The learned model is then used to classify the unknown instances of planes in the segmented point cloud. The extraction of primitive shapes and using them as landmarks can provide simplified yet meaningful abstractions of an environment that can be used for both building abstract maps and localizing a robot [1]. In the case of visual navigation, estimation of the ground plane is often made to track the vehicle and to detect the on-road obstacles [2–4]. Planar features play an important role in estimating the position of landmarks especially when the environment being explored is Manhattan. Most work in the past decade addresses this problem by taking it as a first step in solving the SLAM problem. In Ref. [2], ground plane estimation is performed by texture segmentation applied on the reconstructed depth map generated from a single image. The extracted ground plane then provides an efficient basis for localization of the robot. Similarly, in the case of given 3D range data, planar features are often used to build a simplified and efficient map of the environment that is useful for robotic navigation in a non-cluttered environment [5, 6]. In order to construct a meaningful map from multiple plane extractions, the extracted planes need to be merged. This is usually done in one of the two ways: (a) registering the planes on a predefined global coordinate system which is tracked either by observing ego-motion or using an external reference [2] and (b) applying a region growing strategy that accumulates nearby planes [7]. The problem of retrieving known geometrical primitives can be broadly classified into two main classes: region-based extraction and model-based extraction. In region-based shape retrieval, and particularly in the case of 2D images, the first step involves extraction of a raw skeleton of the shape, for example, that can be accomplished by applying morphological operators on binary images [8]. These raw skeletons are then transformed into a compressed and invariant form such that loss of information is minimized [9, 10]. The region containing the candidate shape is then expanded to include points that are closest in representation space. In model-based shape retrieval methods, a predefined shape model is constructed from a set of points and is evaluated against a fitness criterion which determines whether a point satisfies the model or not. The best model is then selected based on majority voting. A variation to model-based approaches is the use of features for the construction of a model. Given a set of features extracted from points lying on the surface of a shape, a model is learned which can then be used for extraction of the targeted shape in subsequent reoccurrences. A featureless model-based approach is proposed in Ref. [11] that employs a modified version of efficient RANdom SAmple Consensus (RANSAC) implementation for the detection of the planes. A multi-level surface map is constructed by dividing the data into grid-based cells where each cell encapsulates the intrinsic information of the cell by keeping mean, variance, and height at the position of the cell. A candidate plane is selected only if probability of not overlooking the better candidate is high. Once a candidate plane is accepted, each cell that matches the plane is assigned to that plane. Model-based primitive detectors that utilize a region growing strategy to merge the detected planes often outperform other methods; however, the selection of seeds to grow and limit of growth greatly affect their performance. In Ref. [7], an efficient strategy employing a grid-based growth with better seed selection criteria is proposed. The use of Hough transform in a region growing-based technique for robotic navigation is often avoided due to its high computational complexity and lack of general accumulator design. In this regard, many variations of the original Hough transform (HT) have been proposed namely, randomized Hough transform (RHT), probabilistic Hough transform (PHT), and PPHT (progressive probabilistic Hough transform) [12–14]. The adaptation of the aforementioned methods for their use in 3D data processing and particularly for plane detection has also been cited such as in Refs. [15, 16]. All of these methods share the basic idea that is a transformation of each point into Hough space although the variation is in the point selection criteria that helps to improve efficiency. In this paper, planar structures are extracted from the 3D point cloud data of indoor scenes using a feature-based learning approach. A set of planar features are extracted from an example point cloud and a model is learned. The learned model is used for the classification of a set of points in a new unknown instance of the scene which contains both planar and non-planar objects. The point cloud of the scene is segmented into a set of clusters by a graph partitioning approach (i.e., normalized cuts [17]) using better association and similarity function. The NCut (Normalized Cut) is chosen as a segmentation technique because of its broad applicability in the related context and its robustness in non-linear segmentations. A maximum likelihood estimation of the features facilitates in retaining only those set of points which satisfy the planarity constraint imposed by the learned model. The complete detail of the algorithm is described in the next section. This paper is organized as follows: Sect.&amp;nbsp;2 provides details of the proposed algorithm. In Sect.&amp;nbsp;3, the architecture of the target unmanned-air vehicle (UAV) platform is described. Experimental results are presented in Sects.&amp;nbsp;4 and 5 presents our conclusions.    2 Methodology  In this section, processing steps of the proposed method (PLASE) is described. Raw sensor data in the form of point clouds are resampled by removing points in the neighborhood. These resampled data form an input to the NCut algorithm, which decomposes the data into a given number of clusters. Such resampling can improve processing efficiency by reducing the amount of data that will be available to subsequent stages of processing, preserving enough structural detail needed to detect planes. These clusters are scanned for planar surfaces by applying a pre-trained model on each scene resulting in a set of planar points. A graphical depiction of the process is given in Fig.&amp;nbsp; . The segmentation of a point cloud can be modeled as a graph partitioning problem. Each point in 3D space is modeled as a node   and an edge   which is formed by connecting two nodes in a weighted undirected graph  . The weight for each edge is a function of the distance of one point to another point in a pair with small distance resulting in larger weights. This represents a similarity function between the nodes/points because it encapsulates the local neighborhood of a point, by making closer points weighted more than those farther from it. The task is to segment the point cloud into disjoint set of points or clusters   such that the similarity of nodes in   is maximized and similarity in between the nodes of   is minimized. According to the NC (Normalized Cut) algorithm, the optimal bipartition of a graph into two sub-groups A and B is obtained by minimizing the NCut value as follows:       where cut  represents the dissimilarity between   and   and   is the similarity between the node   and  . assoc  is the total connection from node   to all the other nodes and assoc  is the total connection from node   to all other nodes. Let   be an   dimensional vector for a partition that divides graph   into two sets   and   such that   if   is in   and   otherwise. Let   be the total connections from node   to all other nodes. According to Ref. [ 15], an approximate discrete solution to minimize NCut  can be obtained by solving the following cost function:       where  , and  \({y} = {(1+x)} - {(1-x)}^{\sum _{x_i &amp;gt;0} d_i }/{\sum _{x_i &amp;lt;0} d_i }\). Using  , the Eq. ( 2) can be solved by the general Eigen value system: The core of NCut is the representation of the association among the points. This is done using a weighting function describing the similarity/dissimilarity of the data. As mentioned in Ref. [ 17], this function is application specific; hence, it needs to be adapted to work on the target point cloud data. In order to enhance dissimilarity among the point so as to facilitate the construction of disjoint graphs, a linear combination of multiple distance metrics is used. The distance function   along with updated association function   of point ‘ ’ with median   taken as the center of gravity of each node is given by:                         where   are two points in a neighborhood, alpha is joining factor and   with ‘ ’ is used in this study. This makes  , a function of the  , thus making it adaptive for each node, which contributes in dynamically reshaping of weighting function according to the input data.The improvement of the weight function lies in its enhanced discrimination ability due to a combination of dissimilarity metrics and making it scale invariant.The weighting function itself is also a scalable function which is adaptively scaled depending on the center of gravity. This results in an overall segmentation that is more sensitive to the variation in the small changes by enhancing the effect of minimization in the NCut which is dependent on the individual weights. In this section, the process of estimating the planarity of each cluster is described. For a set of ‘ ’clusters obtained by NCut those clusters that have a planar tendency are identified. In order to achieve this, an approximated model of the planes occurring in the scene which truly represent the planar features in the scene are identified. Given a planar point cloud, each point ‘ ’ is transformed into feature space   by building a complete graph   in the point’s neighborhood. If   is the normal vector of the plane spanned by point ‘ ’ with ‘ ’ and ‘ ’ in its close neighborhood in   then following features are extracted:            where   is the edge formed by point ‘ ’ and ‘ ’ while ‘ ’ is the feature number. A pictorial depiction of the graph is given in Fig.&amp;nbsp; . The features for a point are obtained using Eq. ( 6) recursively by traversing a tree in the local neighborhood thus representing a point in a ‘ ’ dimensional feature space where ‘ ’ is the depth of the traversed tree. The next step in planar detection is to learn a model from the extracted features. Given a set of feature vectors  , best estimates of mean   and covariance  , a probability density   is to be estimated as given below:       The probability density of first two features can be seen in the Fig.&amp;nbsp; . While training the system, model parameters are learned from planar point clouds. At test time, the learned model is applied on   cluster by first extracting the features from each cluster and then removing the points which do not satisfy  \(P(f)&amp;lt;\epsilon \) where best   is learned from a separate validation dataset by selecting a value that minimizes the overall classification error. The planar clusters can have a small number of isolated points considered as noise that passes through the plane extraction threshold  and are misclassified as planes. An example of such noisy scattered points as visible in Fig.&amp;nbsp;c, which should be removed to prevent their adverse affect on later processing especially when the next step is a RANSAC-based plane fitting. In order to remove these noisy points, a local variance-based pruning method is implied. Each point is classified as data or noise based on its distance from the ‘’ nearest neighbors. The result of this pruning method is shown in the Fig.&amp;nbsp;e. The RANSAC [ 18] algorithm works by extracting the shapes of objects by randomly selecting a minimal set of point data and constructing a shape model from the sampled points. The size of the sampled point data depends on the geometric model. Candidate shapes are tested against the constructed model to form a consensus among the points in the data. The model is reconstructed by resampling the random points if there is a low consensus score. The candidate shape with the highest consensus score is considered to be the closest and is extracted from the data after a given number of iterations. RANSAC is widely used because of its unique and distinctive properties:        Tolerance: it exhibits high robustness even in the presence of more than 50&amp;nbsp;% outliers in the data (which can occur in vision data).  The randomness in the RANSAC is favorable, although it can be inefficient if it is applied without optimization. In our algorithm, application of RANSAC is an optional step which can be used in addition to planar estimation framework described in Sect.&amp;nbsp; 2.3. The application of RANSAC might be favorable in situation where few points are left after maximum likelihood estimation and best fitting set of planes are required. Simplicity: simple concept to adapt. It can be easily implemented, even in the case of small embedded systems. Generality: it can be adapted to a wide range of application contexts. Tolerance: it exhibits high robustness even in the presence of more than 50&amp;nbsp;% outliers in the data (which can occur in vision data). In this section, algorithmic detail of the planar surface extractor is presented. In order to construct a planar surface model, a number of features are extracted from a set of example point clouds which serves as a training set. The detail of feature representation is described in Sect.&amp;nbsp; 2.3. A new test example is segmented using Normalized Cuts [ 17] with better association and weighting functions. Given a model ( 7) learned from the training and a set of clusters obtained as a result of segmentation step, each cluster is processed for identification of its planarity nature by estimating its closeness to the learned model. The list of steps of PLASE algorithm is given in Table  1. The classified planar data are filtered for possible noise, which then provides the input to an optional RANSAC step that fits a planar model within the classified set of points.    3 Platform and equipment  The proposed algorithm is designed to be used on a UAV platform that has been customized as part of the study and is used for data collection for experimentations. The UAV system consists of a 6-rotor copter with obstacle avoidance and vision processing capabilities. The architecture of the system is depicted in Fig.&amp;nbsp; . A 3D camera has been mounted on the UAV to provide point cloud data. An on-board CPU with 1.3&amp;nbsp;GHz processing speed is responsible for the vision processing tasks. Lower level obstacle avoidance is performed by a microcontroller that is responsible for generation of control signals to the flight controller. The microcontroller uses input from six ultrasonic sensors chained together to generate appropriate control (Yaw, Pitch, Roll) commands to change the state of the UAV. The reactive control layer is a PID controller which is responsible for the low-level decision making for the absolute obstacle avoidance. It also has a higher priority over perceptual control layer. The auto pilot commands are generated as the result of visual algorithm and are sent to the reactive layer which makes decision of forwarding these commands to the flight controller by observing the signals received from the ultrasonic distance sensors. In case of near obstacle, reactive control generates its own pilot commands to keep the vehicle at a safe margin.    4 Experiments setup and analysis of results  The proposed PLASE algorithm is applied on a set of point cloud data obtained from a RGBD camera and the results are validated. In order to collect the point cloud datasets, the camera mounted on a UAV is moved in five different indoor environments and data are saved for each scene. A total of 50 point clouds each containing 2,000 points are labeled into two classes: planes and non-planes. These labeled examples are used as ground truths to validate algorithm performance. The results of clustering with original and improved weighing functions and planar surface extraction from an indoor scene can be visualized in Fig.&amp;nbsp; . The clustering result has been improved by the use of   (Eq.&amp;nbsp; 5) which enhances the cuts in the graph of the nodes. Also the   calculation in Eq.&amp;nbsp; 5 is adapted to input data which helps to represent the locality of each node more accurately and thereby ensures the relative spread of the cluster. This also makes the method invariant to changes in data due to variation of viewpoint or sensory noise. However, it has been observed that the association function is not invariant across different scales of the data. In this study, each point cloud is down-sampled to the same degree, and hence every point cloud has the same data size. The algorithm has also been validated using synthetic 3D scenes containing a set of planes and non-planar objects. These point cloud model data are injected with Gaussian noise and some random structures to mimic the natural scene, as shown in Fig.&amp;nbsp; . This point cloud model is used to generate various indoor scenes containing different numbers of planes and noisy objects, and the performance of the algorithm is computed. The examples of scenes created from the model along with their respective results are visualized in Fig.&amp;nbsp; . For each   cluster that results from the application of the method, an error rate   is calculated which reflects the clustering performance of the proposed method over multiple scenes. Each cluster of the ground truth is assigned to a class that is most frequent in the resultant cluster. Then, the error rate of this assignment is measured by counting the number of correctly assigned points:       where   is the set of clusters and   is the set of classes. The error for every scene is calculated 10 times by taking a different noise density each time and the average error for each scene is taken. The evaluation results for the real and synthetic datasets with different weighting functions are depicted in the Tables&amp;nbsp; 2 and  3, respectively. This reflects the average error rate over multiple runs of the same scene with varying noise   density of the scene. For the evaluation of the proposed method, a number of detected planes are to be evaluated. In a multiclass evaluation problem, the error caused by all other planes is considered while calculating the precision or recall of a plane. In order to evaluate the algorithm’s ability for correctly classifying the planes, a confusion matrix is calculated. True precision and recall of the system is the average precision and recall of all the participating planes. So, the precision and recall of a plane ‘ ’ can be written as follows: where   is the number of points of target plane ‘ ’ that are correctly classified as target plane and   is the number of erroneous points with ground truth class ‘ ’ and predicted class ‘ ’ while ‘ ’ is the set of all planes. The harmonic mean or the  -measure is a performance metric that combines precision and recall. The  -measure is used as a combined measure to calculate the plane extraction performance of the algorithm. The general   measure can be given as following:       where   is the weight of precision to recall.   is the balanced  -score. The results, as can be seen in Fig.&amp;nbsp; , indicate the tendency of the algorithm for successfully finding planes in the environment even in the presence of noise and other non-planar objects. The proposed plane extraction method is able to classify the scenes consisting of planar surfaces with good precision, as reflected in the combined performance measure (i.e.,  -score). The performance of the method is compared with variations of Hough Transform. It is important that extraction must be efficient as well as accurate so, performance metric is devised by taking linear combination of effectiveness and efficiency. The performance measure that is used for evaluation is as given below:       where   are metric weights, detected number of planes, total number of planes, and time, respectively. An example result is shown in Figs.&amp;nbsp;  and  . The performance of RHT is comparable to PLASE due to its better selection criteria for selection of points while PLASE outperforms all other methods.   
Springer.tar//Springer//Springer\10.1007-s00138-013-0483-3.xml:A 3-degree of freedom binary search pose estimation technique:Markers Robot vision Pose estimation POSIT Fiducial:  1 Introduction  Pose estimation refers to the computation of the position and orientation of a calibrated camera with respect to the known reference points [1] and is a frequently performed task within machine vision applications. In the related field of photogrammetry, pose estimation is known as space resection and has algebraic solutions dating back to 1841 [2]. In contrast to blind channel estimation [3], where coefficients of a communication channel are estimated with an unknown input signal, the system input in pose estimation is known. It is widely known that a total of 6 degrees of freedom (DOF) are required to fully describe the pose of a rigid object in free space [1, 4]. This dimensionality is significantly reduced as the motion of the object becomes constrained in multiple dimensions, leading to applications like self-calibration for printed circuit board (PCB) fabricating and populating machines which have a surface constrained to 2-DOF [5]. Pose estimation algorithms typically operate using sets of correspondence points, which are fiducial points in the image each with a known 3D reference point [1, 6]. The number of correspondence points required is dependent on the DOF that the object can be positioned and revolved in. For the simple 2-DOF PCB drilling example which is constrained to having translation in two axis, only one correspondence point is required to fully estimate the pose. For the more complex example of a 6-DOF object in free space, a total of four correspondence points are required to generate a unique pose estimate [1]. To facilitate pose estimation, fiduciary markers (coplanar 2D patterns within a scene) are typically used to provide a point of reference [7]. Besides machine calibration and virtual reality applications, these fiducials are often used in robotic navigation and for object recognition tasks [8]. Once fiducial points have been identified (and linked to their correspondence points), a pose estimation technique can be applied. Traditionally pose estimation problems in computer science and machine vision have been solved using algorithmic techniques [1, 4, 9] or iterative techniques [2], with some newer approaches like POSIT [10] which use a hybrid of the two methods. Algorithmic pose estimations algorithms, like those proposed by Quan et al., perform pose estimation by numerically solving a group of quadratic equations. For the minimum of four correspondence points (required for a unique 6-DOF pose estimation), three fourth degree polynomials are generated and solved using singular value decomposition (SVD). Additional redundant points have been shown to significantly reduce both the estimated translation and rotation error as noise is introduced into the system [1]. Lowe [11] describes iterative approaches which uses Newton’s method as a basis to solve a least-squares minimisation problem. Although performance is based on the initial guess pose of the algorithm, Lowe reports fast convergence towards a solution over a small number of iterations with stabilization resulting in guaranteed convergence using Levenberg–Marquardt methods for the reported rare case of divergence resulting in a poor estimation. DeMenthon et al. have developed pose from orthography and scaling with iterations (POSIT) as a hybrid algorithm [10]. POSIT performs pose estimation without the requirement of an initial guess pose and uses an order of magnitude less operations than the afore-mentioned techniques—avoiding computationally intensive operations like matrix inversions. Convergence towards accurate results is reported over 4–5 iterations. The application this paper is directed towards involves using a mobile robot to navigate underneath a vehicle to perform undercarriage imaging. To perform this search functionality, the mobile robot must first determine its starting pose from the 3-DOF environment which it operates in. The remainder of this paper is organised as follows: Sect. 2 describes the binary search pose estimation (BSPE) technique along with a deterministic algorithm for path planning for a mobile robot beneath a motor vehicle. Section 3 then provides both simulation and an experimental validations along with a discussion of the results, which is then concluded in Sect.&amp;nbsp;4.    2 Methods  This section describes a two-step process used to estimate the pose of a camera (mounted to a mobile robot) with respect to a target vehicle. Firstly fiducials, in this case the car tyres, are identified and quantified by their position in the image frame. Secondly, the BSPE technique is applied to the fiducials to estimate the current pose of the mobile robot which then may facilitate path planning exercises to enable the mobile robot to inspect the vehicle. As with other techniques, before pose estimation can occur, fiducials within the image need to be identified and parameterised (location of fiducial on 2D camera sensor). For the application described previously, that of pose estimation with respect to a motor vehicle, the centre of each tyre is parameterised as a fiducial point using techniques discussed in [13]. The technique uses a visual approach, as opposed to using a traditional 3D laser scanner as commonly used in mobile robot navigation, as the tyres on the car were found to absorb the majority of the laser light during initial testing and so were poorly detected by the scanner. Fundamentally, the approach described in [13] seeks to identify tyres as fiducial points by first morphologically selecting the largest dark region in the image (which almost always encompasses the undercarriage and tyres). From there, a series of thresholding steps along with edge detection are used to find the inner and outer contours of each of the tyres along with shadow removal to exclude the shadow region cast by the vehicle. Since the vast majority of motor vehicles have four wheels, a fourth, redundant fiducial is easily parameterised which can later be used as a self-validation of the technique. This self-validation additionally ensures that all the fiducials have been correctly identified. A sample image with tyre fiducials identified is shown in Fig.  . One possible alternate solution would be to affix a traditional 2D fiducial marker to the vehicle rather than attempting to extract the tyres as fiducial points. The problem with using a traditional 2D fiducial marker is that if this marker became unreadable (due either dirt or deliberate tampering) pose estimation would not be possible. Additionally, the use of tyre positions as fiducials enables this method to be used across a range of cars, requiring only track width and wheelbase measurements with no specific calibration as would be required when a new 2D fiducial is installed. Once correspondence points have been established, pose estimation can be performed. Proposed here is a BSPE algorithm which is guaranteed to converge to the correct solution, as the   variable decreases each cycle by a binary order of magnitude, effectively halving the search space. Figure   provides an outline of three iterations demonstrating how the value of   (the camera orientation with respect to the vehicle) converges towards a solution as a binary pattern. The algorithm is supplied three non co-linear points of a two-dimensional object  and , where  and the corresponding sensor vectors . The algorithm is initialised by assuming that the image sensor is the  line, where  is the focal distance and the focal point is at the axis intercepts. The sensor vectors  and  are the sensor space co-ordinates of the sensor projections of  and , respectively. The pose of the camera is calculated and described as the rotation matrix   and translation vector   which would transpose the sensor vectors from sensor space to the correct object space. Therefore, on the   iteration, the BSPE best estimate of the true sensor vectors is given by       where   and            and initially   and  . Assuming that   is correct, the translation vector is then chosen so that two of the sensor vectors   and   point directly at   and  . Therefore,   must be chosen to so satisfy the following parametric linear equations.                         where   and   are the parametric variables. Expanding these variables into their   and   components (and omitting the iteration superscript for brevity) gives:                                                   Equating ( 5)–( 8) for   gives:                            can then be eliminated by subtracting (Eq.  10   from (Eq.  9  ) to give:       Solving for   gives,       which can then be substituted back into Eqs. ( 5 and  6) to solve for  . At this point of the algorithm, the translation vector   causes the lines spanned from   and   to the focal point to pass through   and   given the current rotation matrix. To update the rotation matrix, the direction of rotation for the camera needs to be computed. This is performed figuratively by drawing a line from the third point   to the focal point   and recording where this line intersects the image sensor in  ,       If   is to the right of   then   should be updated in the clockwise direction, otherwise it should be updated in the counter-clockwise direction. Since only the direction of rotation is known and not the exact rotation angle, upper and lower bounds of the rotation angle with initial values of  and  are ascribed. The rotation angle is updated by averaging the current angle   with the upper or lower bounds, depending on the direction of rotations as follows:               and   are the upper and lower limits of the domain in which the correct angle   resides and   is the mid point for each iteration. At each iteration,   and   are compared to determine the direction in which half of the domain the true angle   is located. Therefore, at each iteration, the upper or lower bounds of the domain are updated. That is, if  \(s_{2,x}^{(n)}&amp;gt;r_x^{(n)}\), then   otherwise, if  \(s_{2,x}^{(n)}&amp;lt;r_x^{(n)}\), then  . From Eq. 14, it is clear that after every iteration the range of possible rotation angles is reduced by a factor of two, thus the iterative procedure converges rapidly to the required solution. The previous section detailed mathematically the BSPE technique demonstrating the convergence towards a pose over a number of binary periods. This section briefly details the algorithmic steps required to implement the binary search pose estimation scheme. To provide validation of the proposed BSPE technique, the following section describes a closed form geometric solution which solves for the fiducial sensor points in terms of the computed camera pose. This section details a simple algebraic geometric approach to solving the 3DOF pose estimation problem given three correspondence points. Equations  15– 18 describe a closed form solution for the computation of the point of the fiducial on the image sensor ( – ) in terms of the pose variables   and   shown in Fig.&amp;nbsp; . These equations allow the computation of the fiducial locations for a given pose and hence allow quantification of the measurement noise.                                                   where   are the respective positions of the fiducial point on the image frame.   describes the empirically determined fixed distance between the camera and the image frame with   and   describing the pose of the camera with respect to the left near tyre of the target vehicle.   and   then, respectively, denote the wheelbase and trackwidth of the vehicle as marked in Fig.&amp;nbsp; . This section has discussed the BSPE implementation, both mathematically and computationally, along with a closed form solution which can be used to validate the technique. The following section tests and evaluates BSPE using both synthetic and real-world data.    3 Experimental validation and discussion  The following sections explain and detail the results of two experiments used to validate and evaluate the described pose estimation approach. Firstly, a simulation was constructed to perform pose estimation using both the BSPE and the POSIT techniques based on synthetic image data. Secondly, the pose estimation algorithm was applied to a series of real-world images with identified fiducials. The pose estimates for these images were compared against measured values for ,  and . The real-world image results are validated using the geometric solution (as described in Sect.&amp;nbsp;2.4) which can further be used to provide a measure of the effects of error in the fiducial placement. The first experiment is designed to validate the described BSPE technique against a recognised pose estimation technique, namely POSIT, as well as to provide some comparison of the computational complexity for different pose estimation algorithms. A series of 100 synthetic images were generated, each with four correspondence points based on the layout shown in Fig.&amp;nbsp; . These correspondence points were stored as an 4-Tuple and were applied both to implementations of DeMenthon’s et al. POSIT algorithm [ 14,  15] and the BSPE algorithm described in this paper. For each of the different cases, the real world coordinates of the fiducials are kept constant and their corresponding image sensor points are changed (simulating a change in camera pose). When using synthetic data, the pose estimation results of the BSPE technique average within 0.3 of those achieved when using the POSIT algorithm. The worst errors observed correlated to very large values of  (where abs(\(\theta ) &amp;gt; 15^{\circ }\)). The largest recorded error of 1.84 occurred with  = 21.8. As the fiducial identification algorithm requires a viewpoint where the far tyres are constrained between the near tyres, values with (abs\((\theta ) &amp;gt; 15^{\circ }\)) are not expected as they would not describe a valid pose. When only results (abs\((\theta ) &amp;lt; 15^{\circ }\)) are considered the pose estimation disparity drops to 0.077, a significantly low value comparable to POSITs performance when compared to a closed form SVD solution, therefore, validating the BSPE technique for pose estimation. One of the powerful advantages of using a binary search approach is the rapid convergence towards a solution. Figure&amp;nbsp;  shows that the algorithm converges towards a pose within 10 iterations (  convergence to better than one tenth of a degree), whilst avoiding computationally expensive SVD and square root operations. For the synthetic data used in this experiment with four correspondence points, POSIT would require a total of 384 simple arithmetic operations and a total of 8 square-root operations over 4 iterations [ 10]. In contrast, Lowe’s method [ 11] would require a total of 1,358 arithmetic operations over 4 iterations. The BSPE method uses a total of 420 arithmetic operations for three correspondence points or 843 arithmetic operations for 4 correspondence points over 10 iterations calculated using Eq.  19:       where   is the number of iterations,   is the number of arithmetic operations and   is the number of sets of points used. The BSPE technique, designed specifically for pose estimation with 3-DOF performs well compared with the alternate algorithms operating in a 6-DOF space (if POSIT was constrained to 3-DOF it would be expected to be significantly more efficient than the BPSE technique). With low algorithmic complexity and neglecting expensive operations present in other techniques (such as SVD or square roots), the BSPE technique makes a good candidate for an embedded system implementation. The complexity of the BSPE algorithm is a function of the number of fiducial groups used. If we make use of all of the possible combinations of the  fiducial markers, then the number of fiducial groups is given by  resulting in a complexity of . In contrast POSIT has been empirically measured as  [12], exhibiting significantly better performance in terms of scaling with the number of image points. The proposed technique was, however, primarily designed for use with only 3–4 fiducial points in a 3-DOF environment and so was not optimised to scale with fiducial points or dimensionality. Having validated the BSPE technique against POSIT using synthetic data, the next experiment applies the BSPE to a practical real-world problem of pose estimation with respect to a motor vehicle. Verification of the results is in comparison of the calculated pose with respect to the measured pose and using the back-propagated fiducial sensor points using the closed form solution.
Springer.tar//Springer//Springer\10.1007-s00138-013-0484-2.xml:Fast spatiotemporal MACH filter for action recognition:Action recognition Spatiotemporal MACH filter 3D normalized correlation Integral video:  1 Introduction  A human action can be represented by a very short video clip showing a single cycle of the corresponding motion of the body. Action recognition is one of the most challenging problems in computer vision. Effective solutions to recognize motion patterns in uncontrolled environments can lend themselves to a host of important applications, such as analysis of sports videos, video indexing, surveillance, and the development of intelligent environments. Polana and Nelson [13] developed methods for recognizing human motions by obtaining spatiotemporal templates of motion and periodicity features from a set of optical flow frames. They used these templates to match the test samples with the reference motion templates of known activities. Essa and Pentland [7] proposed spatiotemporal templates based on optical flow energy functions to recognize facial action units. Bobick and Davis [3] computed Hu moments of motion-energy images and motion-history images to generate action templates based on a set of training examples which were represented by the mean and covariance matrix of the moments. They performed the recognition by determining the Mahalanobis distance between the moment description of the input and every known action. Efros et al. [6] proposed to recognize human actions at low resolutions which consisted of a motion descriptor based on smoothed and aggregated optical flow measurements over a spatiotemporal volume centered on a moving figure. They treated the spatial arrangement of blurred channels of optical flow vectors as a template to be matched using spatiotemporal cross-correlation against a database of labeled example actions. In order to avoid explicit computation of optical flow, a number of template-based methods attempt to capture the underlying motion similarity among examples of a specific action class in a non-explicit manner. Shechtman and Irani [18] avoid explicit flow computations by employing a rank-based constraint directly on the intensity information of spatiotemporal cuboids to enforce consistency between a template and a target. Given one example of an action, they correlated spatiotemporal patches with a test video and considered the detections as those locations in space-time which produced the most motion-consistent alignments. Given a collection of labeled action videos, a disadvantage of these methods is their inability to generalize from a collection of examples and create a single template which captures the intra-class variability of an action [16]. Effective solutions are required to be able to capture the variability associated with different styles of performing the action, execution rates, skin color, and clothes of the individual actors. Recent popular methods, which employ machine learning techniques such as SVMs and AdaBoost, provide one possibility for incorporating the information contained in a set of training examples. The research that is most relevant to our work is that done by Rodriguez et al. [ 16], who proposed a spatiotemporal MACH filter—an extension of traditional 2D MACH filter [ 12,  15]. It is also a template-based method for action recognition, but it is capable of capturing intra-class variability. They match the filter (designed from various examples of a specific action) with the test video by performing 3D normalized correlation between them. Then, they consider the action detected at the spatiotemporal locations in the test video where they find peaks higher than a threshold in the filter response. In order to asses the effectiveness of their approach, they have performed an extensive set of experiments on publicly available datasets: the KTH action dataset [ 17], the Weizmann action dataset [ 2,  10], the Cohn-Kanade facial expression database [ 4,  20], and two new homegrown datasets [ 16]. These include dancing, diving, walking, running, jumping, bending, sports-related actions as featured on broadcast television channels, actions found in feature films, and so on. Some of the actions are shown for instance in Fig.&amp;nbsp; . By computing the standard cross-correlation response of the filter in the frequency domain and then normalizing it in the spatiotemporal domain, Rodriguez et al. have tried to avoid the high computational cost of 3D normalized correlation to some extent. Their method is significantly faster than any other action recognition method. However, the normalization procedure that they use in the spatiotemporal domain is still very computation intensive especially for high-resolution long-duration videos or large filters. Therefore, we propose an integral video based method to efficiently normalize the response of the spatiotemporal MACH filter. We compare our method with the relevant traditional methods and show that our approach tremendously outperforms all of them in speed. The organization of the paper is as follows: Sect.&amp;nbsp;2 describes the theory behind the spatiotemporal MACH filter for action recognition and the proposed speedy algorithm for its implementation in detail. Section&amp;nbsp;3 compares the computational complexity of the proposed algorithm with that of the relevant traditional methods. Section&amp;nbsp;4 presents the experimental results. Finally, Sect.&amp;nbsp;5 concludes the paper.    2 Fast spatiotemporal MACH filter  A spatiotemporal MACH filter [16] can be simply considered as a generalized 3D template representing an action or event. It is an extension of the MACH filter [12, 15], which is a generalized 2D template representing an object. When it is applied on a video, its response contains a very high peak at the spatiotemporal coordinates  where and when the action appears in the video. Before the proposed fast algorithm for its implementation is discussed, it is important to know how the action examples are formed, how a spatiotemporal MACH filter is synthesized, and how the filter is applied on a test video to detect the corresponding action in it. The spatiotemporal MACH filter is synthesized (designed) from various examples of a specific action performed by different persons having different skin colors, physical structures, motion speed, and clothes. An example of an action is prepared by extracting a portion of the video frames which capture a single cycle of the motion involved in the action. The size and duration of the portion is set constant in such a way that it encompasses various body structures and speeds of performing the action. In order to have a well-discriminating filter, the mid-point of all the examples of the action in space and time is synchronized. The extracted portion is then stored in a video clip which is normally small in size and duration. For example, some frames of the clipped videos of the bending action performed by three different persons from a publicly available Weizmann action database [ 2,  10] are illustrated in Fig.&amp;nbsp; . In order to make the filter brightness- and color-invariant, each frame of the small video clip is edge-enhanced by Gaussian smoothing, edge detection, thresholding, and normalization as explained in [ 1]. The edge-enhanced frames are then concatenated to form a spatiotemporal cuboid (3D matrix) that serves as an action example. Such examples are obtained for all the available training videos for a particular action. The spatiotemporal MACH filter is synthesized in the frequency domain. Therefore, each action example is transformed into the frequency domain by performing a 3D FFT (fast Fourier transform) operation, defined as            where   is the action example cuboid in the spatiotemporal domain.   is the number of columns,   the number of rows, and   the number of slices (or frames) in the action example. Finally,   is the resulting action example cuboid in the frequency domain. The efficiency of this step is increased by exploiting the separability property of the Fourier transform [ 9], i.e., the 3D FFT is computed by obtaining 1D FFT of each row (i.e., along  -axis) in each slice of the cuboid,  , as       then obtaining the 1D FFT of each column (i.e., along  -axis) in each slice of the resulting 3D matrix,  , as       and finally obtaining the 1D FFT of each line in the depth (i.e., along  -axis) in the resulting 3D matrix,  , to have fully transformed cuboid,  , as       The transformed cuboid is then reshaped into a pattern (i.e., column-vector) by concatenating all the columns in it. Using the same procedure, the patterns for every training example of a particular action are obtained. Let the patterns be denoted by  , where  , and   is the number of available training   with which the filter is to be synthesized for the specific action. Once all the training patterns representing the action are prepared, the spatiotemporal MACH filter is synthesized as follows [ 12,  15]:       where   is the mean of all the patterns, and   is the filter in vector form in the frequency domain.   is the diagonal   of size  , where   is the total number of elements in a pattern. If the noise model is not available,   can be set as  , where   is the standard deviation parameter and   is a   identity matrix.   is also a   diagonal matrix representing the   of the training videos and is defined as       where   is a   diagonal matrix in which the diagonal elements are the same as the elements of the   pattern, and the asterisk symbol   in the superscript represents the complex conjugate operation.   in Eq. ( 5) is the diagonal   defined as       where   is a diagonal matrix, of which diagonal elements are the same as those in  . Finally,  , and   in Eq. ( 5) are the parameters that can be set to obtain the trade-off among the performance measures [ 12,  15]. After designing the filter   in vector form in the frequency domain, it is reshaped back into the form of a cuboid by applying the reverse of the process that was used to reshape a cuboid of an action example into a pattern. Subsequently, the 3D inverse FFT of the cuboid is computed efficiently using three consecutive 1D inverse FFT operations along  -axis, then  -axis, and finally  -axis as explained earlier for the case of forward 3D FFT implementation. The resulting 3D matrix constitutes the edge-enhanced spatiotemporal MACH filter,  , for the particular action. Some slices of an edge-enhanced spatiotemporal MACH filter for a bending action designed from the examples of six different persons in the Weizmann action database [ 2,  10] are shown in Fig.&amp;nbsp;  (three of those six persons performing the bending action have been already shown in Fig.&amp;nbsp; ). It may be noted in Fig.&amp;nbsp;  that the filter has captured all the variations in the six examples of the bending action in space and time to serve as a generalized spatiotemporal template of the action in the application phase. Once the edge-enhanced spatiotemporal MACH filter,   (an   matrix), for a particular action has been synthesized, it is applied on a test video,   (an   matrix), to detect similar action occurring in it. In order to have brightness- and color-invariant action recognition, each frame in   is also edge-enhanced by following the same procedure that was used while preparing the action examples before the synthesis of the filter. The action recognition process is accomplished using a 3D correlation operation, defined as       where   and   are the number of columns, rows, and slices of  , respectively. Furthermore,  , and  . The resulting filter response,  , is of size   instead of  . It is called   correlation which is faster to compute than   correlation. Valid correlation assumes that, if the action to be detected is present in the test video, it is almost   inside the video considering both space and time. It is a valid assumption in most of the cases. The 3D position of the highest peak   in the filter response corresponds to the starting spatiotemporal position of the action detected in the video and the position is denoted by  . The 3D correlation defined by Eq. ( 8) can also be computed efficiently in the frequency domain by extending the 2D correlation theorem of the 2D discrete Fourier transform [ 1,  9] up to three dimensions as       where   is the 3D FFT operation as defined in Eq. ( 1), the asterisk ( ) is the complex conjugate operation, the dot ( ) is the element-by-element multiplication operation,   extracts the real-part of the complex numbers in the 3D matrix in its argument,   is the resulting correlation response in the form of a 3D matrix, and   is the 3D inverse FFT operation defined as            The values in the 3D   correlation response obtained by Eqs. ( 8) or ( 9) are unconstrained and they depend on the contents of the video and the filter, so it becomes difficult to decide whether its peak value is due to the action to be detected or just the background clutter with higher values. This problem can be solved using the 3D   correlation defined as       where   in the numerator is the standard 3D correlation described by Eq. ( 8).   is the energy of   and   the energy of the filter-size cuboid at   position in  . These energies are determined as       and       Due to the division by the square roots of the energies, each element in the 3D normalized correlation response is restricted to the range [0.0, 1.0]. Thus, it can be exploited as a level of confidence in a probabilistic manner. The highest value   in the filter response is compared with a threshold  . If it is greater than the threshold, it can be confidently inferred that the action is occurring at the corresponding location in the video. A good value of the threshold can be determined automatically for each action class just after the synthesis of the filter as  , where   is the peak value in the correlation response obtained when  th   example of the particular action is correlated with the filter.  , as explained earlier, is the number of all the   examples of an action used while synthesizing the filter. We use   as a tolerance constant which can be set to a value slightly less than 1.0, so that the action recognition process can be flexible enough to recognize the action correctly even in the worst case in an unseen (i.e., test) video and strict enough to eliminate or significantly discourage false alarms. Figure&amp;nbsp;  shows that the edge-enhanced spatiotemporal MACH filter (Fig.&amp;nbsp; ) designed from   action examples (Fig.&amp;nbsp; ) successfully recognizes the specific action in a video obtained from the Weizmann action database [ 2,  10], even when the video was not used during the synthesis of the filter. The 3D  correlation has solved the problem that was with the  3D correlation, but it has significantly increased the computational complexity since it necessitates the computation of the local energies of the filter-size cuboids at all the  locations in . Therefore, in the next sub-section, we propose a very efficient method to compute the 3D normalized correlation. If we look at the direct formula of the 3D normalized correlation in Eq. (11) closely, we notice that it has mainly three parts: (1) the standard 3D correlation at its numerator, (2) the square root of the energy of the filter, , and (3) the square root of the local energy of the filter-size cuboid of  at . In order to reduce the computational complexity of the 3D normalized correlation, we compute each part of the formula separately as follows. We obtain the standard 3D correlation at the numerator efficiently in the frequency domain using Eq. (9) instead of the spatiotemporal domain using Eq. (8). Then, we compute the energy of the filter only once, since it is constant throughout the whole correlation process. Finally, we calculate the local energies of all the filter-size cuboids in  very efficiently using  [19], which is the extension of the concept of  (or ) [5, 11, 14, 21]. An integral video is simply an integration (or cumulative sum) of a video along  , and   axes. If the size of a video,  , is  , that of its integral video,  , will be  . In order to generate an integral video (3D matrix), we first initialize it with zeros and then its elements can be set as follows:       where  , and  . It may be noted that   whenever  , or  , or  . That is, all the elements in the   frame, all the elements in the   row in every frame, and all the elements in the   column in every frame are 0. This is automatically accomplished during the initialization of the integral video with 0’s. The higher the indices of an element in the integral video, the larger the value of the element at those indices. Therefore, we must use a proper data type of the elements of the integral video that can easily accommodate a very large value, especially when the original video is of long duration or high resolution. Equation ( 14) suggests that, for obtaining the value of   at   position, we will have to add all the elements of   at the positions from   to  . Thus, it may take a significant amount of computation time just to generate the integral video itself, especially if the original video,  , is of long duration or high resolution. In order to speed up the process, we temporarily have two 3D matrices   and   each having same size as that of  , initialize them with zeros, and finally determine the integral video using the following recurrences:                                      where  , and  . It may be noted that this implementation uses only three addition operations to determine the value of any element of the integral video. Once the integral video is generated, we can efficiently compute the local sum of all the elements in an  cuboid with its initial element at any arbitrary position  in the  video by algebraically adding only eight corner elements of the corresponding cuboid of size  at  position in the  video. It may be noted that the size of the local cuboid in the  video is same as that of the synthesized spatiotemporal MACH filter. If we were not using the integral video and computing the local sum directly from the  video, we would have to add  elements instead of eight elements. Since we are interested in the local   instead of the local  , we first take square of every element in   and denote the resulting 3D matrix by  . Then, we compute the integral video of  , and denote it by  . Finally, we efficiently calculate all the local energies of the   video,  , as       where  , and  . Furthermore, as illustrated in Fig.&amp;nbsp; ,  , and  . Now that we have efficiently computed , and , we substitute them in Eq. (11) to obtain the 3D normalized correlation response, .    3 Computational complexity  Since we apply the spatiotemporal MACH filter on a video for action recognition using , we present  computational complexity analysis when  is determined by the following three approaches: Both the numerator and the denominator are computed  in the spatiotemporal domain, as described by Eq. (11). The denominator is computed in the spatiotemporal domain but the numerator is computed in the frequency domain, as in [16]. The numerator is computed in the frequency domain and the denominator in the integral video domain, as proposed in the previous section. In order to simplify the computational complexity analysis, let us assume for the three approaches that we have a video of size  and a spatiotemporal MACH filter of size . Furthermore, we have experimentally found out that the time taken by single-precision floating-point addition, multiplication, division, or complex conjugate operation is almost equivalent and the time taken by the square-root operation is almost double than that by addition operation. As far as Approaches 2 and 3 (that exploit the FFT) are concerned, the 3D matrix of the test video and the 3D MACH filter have to be zero padded to have the size  to obtain linear correlation instead of circular correlation (as explained in [9] for 2D correlation), where . In order to further simplify the analysis, we assume that  is a power-of-2 integer (e.g., 2, 4, 8, 16, etc.) so that the number of operations required to compute 1D FFT or 1D inverse FFT can be . However, in actual implementation, we do not restrict the size to power-of-2, because we use FFTW algorithm [8] and round the value of  to the next integer which has small prime factors (i.e., 2, 3, 5, and 7). This implementation of FFT works almost as fast as in case when  is power-of-2. The number of addition-equivalent operations involved while performing major functions in Approaches 1, 2, and 3 are listed in Tables&amp;nbsp; 1,  2, and  3, respectively. After adding those numbers of operations and simplifying the resulting expressions, we get the estimated total numbers of operations  , and   required by the three approaches, respectively, as If we look closely at the three equations described above, we observe that the factor which adds the most significant computational burden on any of the three approaches is the one which is multiplied with . Furthermore, the smallest value of the factor is 11 which is in the expression for  representing the computational complexity of the proposed approach.    4 Experimental results  Impressive recognition results of the spatiotemporal MACH filter for extensive set of human actions from various publicly available datasets (including Weizmann action dataset [10]) have been already shown and analyzed in [16]. We have tested that the proposed implementation of the spatiotemporal MACH filter also provides exactly the same numerical and recognition results on those datasets. Therefore, we present only the computational efficiency analysis of the proposed method (Approach 3) and the relevant traditional methods (Approaches 1 and 2) on various sizes of the test videos of the Weizmann action dataset. This dataset is challenging in the sense that the videos in it consist of ten human actions each performed by nine different persons (men as well as women) having different physical structures, different colors of their skin and clothes, and different speeds and styles of performing the action. The ten actions in the dataset are bending, running, jumping, galloping sideways, jumping in place, skipping, jumping jacks, walking, waving with one arm, and waving with both arms. The spatiotemporal MACH filters were synthesized using the videos of only six performers: Daria, Denis, Eli, Ido, Lena, and Shahar. The videos of the remaining three performers (i.e., Ira, Lyova, and Moshe) were reserved for testing the synthesized filters. Thus, 66.7&amp;nbsp;% of the dataset was used for training, and 33.3&amp;nbsp;% of the dataset was used for testing. The experiments were performed using MATLAB on a 2.8 GHz Core2 Duo computer with 4GB RAM and Windows 7 operating system. First, a   spatiotemporal MACH filter representing a generalized bending action (Fig.&amp;nbsp; ) was applied on test videos of different sizes   using the three approaches under discussion. Table&amp;nbsp; 4 shows the time taken by Approaches 1, 2, and 3 (i.e., Time 1, Time 2, and Time 3). It also shows the speed gains of Approaches 2 and 3 with respect to Approach 1 (i.e., Gain 2 and Gain 3). It may be noted that Approach 1 takes 67–503&amp;nbsp;s, Approach 2 takes 27–198&amp;nbsp;s, and Approach 3 takes only about 0.4–1.3&amp;nbsp;s for these sizes of the test videos and the filter. The speed gains show that Approach 2 is only up to 2.8 times faster than Approach&amp;nbsp;1, while Approach 3 (proposed) is up to 372 times faster than Approach 1. Furthermore, the speed gain of the proposed approach increases with the cuboid size of the test video. Second, a   spatiotemporal MACH filter representing a generalized running action (Fig.&amp;nbsp; ) was applied on the same test videos as in the previous case. The figures in Table&amp;nbsp; 5 show that Approach 1 takes 159–803&amp;nbsp;s, Approach 2 takes 61–304&amp;nbsp;s, and Approach 3 takes only 0.5–1.3&amp;nbsp;s. Thus, Approach 2 is only up to 2.7 times faster than Approach 1, while Approach 3 (proposed) is up to 605 times faster than Approach 1. It may be again noted that the speed gain of the proposed approach increases with the cuboid size of the test video. Third, we synthesized a   spatiotemporal MACH filter representing a generalized template for a typical   (Fig.&amp;nbsp; ) and applied the filter on a publicly available   (i.e., relatively long duration) video named Birmingham_Ballet_data.avi [ 10] using the three approaches. We found out that Approaches 1 and 2 took 8,567.2 and 3,103.5&amp;nbsp;s, respectively, while Approach 3 took only 9.496&amp;nbsp;s. Thus, in this case of long-duration video, Approach 2 was only 2.8 times faster than Approach 1, while Approach 3 (proposed) was 902 times faster than Approach 1.
Springer.tar//Springer//Springer\10.1007-s00138-013-0485-1.xml:Framework for developing image-based dirt particle classifiers for dry pulp sheets:Dirt particle classification Papermaking Feature selection Image processing and analysis Pulping Particle segmentation Ground truth generation Machine vision:  1 Introduction  To optimize its production processes, the pulp- and papermaking industry is searching for intelligent solutions to assess and control product quality. Optimization, in this context, can be defined as building resource-efficient and environmentally sound production with known quality, using less raw materials, water, and energy. This optimization is not easy since the pulp- and papermaking process consists of a large number of stages where the treatment of the material can have a significant impact on the properties of the final product. Therefore, it is important to know how the important characteristics of the product are formed at each stage. The relevant properties of pulp and paper are traditionally evaluated in a paper laboratory, where the procedures follow the standards related to quality control and the properties of the raw material or end-products are measured from samples taken from the process [13]. Since traditional quality control is time-consuming and does not allow the direct control of the production process, the industry is interested in transferring the laboratory measurements to the inline process. In this scenario, the measurements will be made in real-time directly of the material in the process to reduce the delay in obtaining quantitative quality information and enable even real-time process control. The amount of the produced pulp and paper at the industrial scale is considerable, which means that the inline solution offers significant benefits for the industry, allowing it to adjust the process according to the information obtained from the process measurement. This would considerably reduce the risk of producing a large amount of products with undesired properties. The detection and classification of dirt is an important part of pulp and paper quality assessment. This is because dirt causes undesired surface properties in subsequent processing, negatively affects the surface appearance, and can impair the printability of paper. Commonly, the detected unwanted matter in pulp or paper is an indicator of less-than-optimal operation or a malfunction of the process. Therefore, knowledge about the dirt type can help to identify the problematic stage and to perform maintenance at that stage. To detect such unwanted matter, methods have been developed to segment and count the dirt particles. The concentration of dirt in pulp can be evaluated from sample sheets prepared from pulp suspension, by screening methods or when the dried pulp sheets are prepared for shipping. In this work the dirt particles are detected and classified in the dried pulp sheets. Dirt detection and counting have been studied earlier, but the problem of dirt particle classification was not addressed. For example, Fastenau et al. [5] present a laser system for dirt counting on the industrial scale. The paper gives the motivation for the automation of the dirt counting process, explaining the difficulties of the manual procedure. Based on the shape of the obtained signal, the system was capable to perform dirt particle categorization by the size of particles. Sutman&amp;nbsp;[23] presents a method for measuring the testing precision. The effect of a sample size on dirt count test precision was not well understood and it was the motivation for the research. Rosenberger&amp;nbsp;[18] showed that the threshold for dirt counting should be selected automatically as well to be able to adapt for different lighting conditions as well as paper and dirt particle properties. Juntunen et al.&amp;nbsp;[10] introduce an automated analysis system for colored ink particles in recycled pulp. The samples were prepared with a known percentage of ink. A microscope with an attached color video-camera was used to image the samples. Thresholding was performed separately for three HSI channels, followed by the connectivity analysis. The system allowed to obtain the dirt counts and to measure the size distribution of the particles. Since the ground truth did not contain the information on the location of the particles, there was no opportunity to judge about the spacial distribution of the dirt. In Ref. [15], the “Pulp Automated Visual Inspection System”, which segments and counts dirt particles, as areas in an image with an intensity lower than a certain threshold. Another example InsPulp, an on-line visual inspection system, is explained in Ref. [1]. The paper is imaged by a CCD line and the dirt is segmented using a local dynamic threshold, which allows the system to segment and detect the impurities in pulp with a low error rate. These methods only count the dirt particles and do not address the more challenging problem of dirt particle classification. The industrial dirt counter system by VERITY IA [25] can divide the particles into a few groups based on their shape, but is still not able to identify the specific dirt types. The accurate classification of particles would be a great benefit. Savings in chemical and energy consumption could be attained by adjusting bleaching and screening, the aim of which is to eliminate the impurities in the material. In a production problem situation, fast and precise information on the type of particles present in the process can reveal the source of the problem, and the process can be adjusted accordingly. One of the major problems in developing an automated dirt particle classification system is the collection of ground truth data essential for training of a supervised system. To obtain the ground truth, the exact location and type of each dirt particle need to be given. The identification of specific dirt particles can be a very difficult task even for experts, and the large amount of data required makes collecting the ground truth a very laborious and time-consuming process. In some systems, the difficulties with the performance evaluation are mentioned. For example, in Ref. [15] there was no opportunity to compare the results with manually segmented particles. In Ref. [1], it is also shown that an inspection by humans may be subjective: the number of dirt particles detected by different inspectors was different. In their previous publications, the authors have presented a solution for generating the ground truth [22] and conducted initial experiments concerning the dirt classification problem [21]. In this work, the previous studies are combined and extended to build a full framework for developing dirt particle detection and classification systems. The framework was designed using the provided laboratory paper sheets, but it can be transferred to the industrial scale. The expertise of the paper laboratory personnel is not used to provide the ground truth manually but for carrying out their most important task: produce dirt and pulp as clean as possible. This paper introduces a method for generating the ground truth and methods for dirt particle classification. The framework was tested both with semisynthetically generated images based on real pulp sheets and with independent original real pulp sheets without any generation. The paper is structured as follows: Sect.&amp;nbsp;2 provides a general scheme of the framework for developing dirt classification systems and demonstrates the initial data. Section&amp;nbsp;3 describes the solution for generating the ground truth. Section&amp;nbsp;4 handles the question of feature extraction, evaluation, and dirt particle classification. Section&amp;nbsp;5 discusses the results of the experiments, and the conclusion is drawn in Sect.&amp;nbsp;6.    2 Framework for developing dirt particle classification  The initial data are represented by scanned images of dry pulp sample sheets. The samples were provided by pulp and papermaking experts from the Fiber Laboratory, a collaborating unit with expertise in pulping and papermaking. The samples used in this study consisted of three different pulp types, and four different types of dirt particles. The pulp types were bleached hardwood, bleached softwood, and softwood pulp after the second chlorine dioxide bleaching stage ( ), the color of which is not completely white (see&amp;nbsp;Fig.&amp;nbsp; ). Thus, more variation for the background was gained. Despite the fact that the number of different pulp and dirt types is low and cannot be considered to represent the full variation of pulp in the industry, the sample set was sufficient to develop the framework. Four common types of dirt particles were selected based on the literature [2, 9] and expert knowledge: shives, bark, plastic, and sand (see Fig. ). The dirt particles were either prepared or separated from the pulp in a paper laboratory. The shives were separated from reject pulp from brown stock screening. The bark particles were created by disintegrating pine bark mixed with water in a disintegrator. A plastic canister was grinded to create excess plastic particles. The natural sand was washed to get rid of extra particles and dust. A small amount of sand was also obtained as reject pulp was washed. Sample sheets of the three different pulp types were prepared according to the ISO 5269-1 standard [17]. The amount of pulp equivalent to 1.63&amp;nbsp;g of dry pulp and an adequate amount of a single type of dirt particles were mixed before sheet forming. Consequently, the prepared sample sheets contained an amount of fibers equivalent to a standard 60&amp;nbsp;g/m sample sheet and an adequate amount of dirt particles. The amount of dirt particles was controlled so that there would be more than 20 particles per sample sheet, but not too many to avoid significant overlapping of the particles. All of the three pulp types, one at a time, were mixed with one of the four types of dirt particles. Five sample sheets per test point and also one sample containing sand separated from industrial pulp were prepared. As a result, the sample set consisted of 61 sample sheets. To image the samples, the pulp sheets were scanned with a professional Microtek ArtixScan 2500f scanner with 1250&amp;nbsp;dpi (A4) resolution, true 42 bit RGB colors, and under reflected light. According to the ISO 5350-1:2006 standard [ 16], the minimum size of particles that are to be detected is 0.04&amp;nbsp;mm . The physical resolution was around 0.0004&amp;nbsp;mm  per pixel, which means that the smallest dirt particle occupies, approximately, 100&amp;nbsp;pixels. The basic idea of the proposed framework is illustrated in Fig.&amp;nbsp; . The initial images with a single dirt type in each are used to produce the semisynthesized images. From each of the images, segmented dirt particles are collected to create a database of dirt particle images which are to be scattered on a generated background. The method to produce the semisynthetic background is discussed in Sect.&amp;nbsp; 3. The actual ground truth is represented by a binary mask, containing the exact location of each particle and its type. After the semisynthesized images are obtained, one can segment and classify dirt, which can be evaluated using the semisynthetic ground truth. The semisynthetic images are processed using the following Algorithm&amp;nbsp;  where a set of features are extracted from the segmented particles and the close-to-optimal feature set is determined to be used in further classification.    3 Semisynthetic ground truth generation  In any classification system, it is important to have reference data to allow the evaluation of the classification result. As already mentioned, the ground truth can be, in several cases, laborious or even impossible to obtain. Therefore, there is a need to produce the reference data synthetically. Manual dirt annotation can be substituted by the semisynthetic procedure that is described in this section. The semisynthetic ground truth generation consists of the following stages: (1) dirt particle segmentation and database generation, (2) background generation to fill the holes left by removed dirt particles, and (3) random scattering of the dirt particle images and creation of the corresponding ground truth image. The first step of semisynthetic ground truth generation is to create a database of dirt particles. To accomplish this, dirt particles need to be segmented from the initial images. According to the survey [19], there exists a number of methods to segment foreground objects (dirt particles) from the background, but none of them can be universally used for segmentation problems. In this study, the Kittler thresholding method [11] is used. The choice is based on previous experimental studies on detecting small particles from fiber-based surfaces [3, 7]. A global thresholding approach should be adequate for segmentation purposes in this case since the image backgrounds are rather homogeneous. Gray-scale images are considered for segmentation, because the segmentation method is based on the intensities. The images are divided into the foreground, which consists of the dirt particles, and the background. The foreground and background are modeled as a mixture of two Gaussians [11]. The threshold can be calculated by optimizing the cost function based on the Bayesian classification rule. The segmented particle images are used to create the database. Table 1 introduces the scheme of the database. An image with a bounding box for each of segmented particle is stored into the database, as well as its area and type. The segmented dirt is removed from the images so that the area occupied by the dirt particles is substituted by white pixels. These holes in the image are then filled by using a Markov Random Field (MRF) method [ 26], which is one of the common approaches for texture synthesis, e.g., in&amp;nbsp;[ 12,  20]. In the concept of MRFs [ 28], a random field is composed as pixel by pixel. The probability of each pixel to have a certain intensity depends on the intensity or color of its neighboring pixels. Before background generation, the user commonly specifies a sample region from the image manually, such that it contains no dirt particles and the background is reasonably uniform. This sample image is used as the ideal background for the area filling. In Algorithm&amp;nbsp; , the probability is not calculated explicitly to reduce the computational load. The number of iterations can be restricted, for example, by a threshold dependent on the standard deviation of color which should be close to the original background. There are several factors to consider in this algorithm: (1) how to determine the neighborhood?, (2) how many iterations should be carried out?, and (3) what the best matching neighborhood is? In the implemented version of the algorithm, the neighborhood is determined by the user, as well as the number of iterations. The best match is found by computing the distance between two neighborhoods and choosing the one with the shortest Euclidean distance. After the dirt database has been created and the holes left by the removed dirt particles are filled, the dirt particles can be embedded into the background images to create semisynthetic images. During this stage, a specified number of dirt particles is spread over the uniform synthesized background. According to Ref. [ 27], the particles should be placed randomly, and the implementation includes an option of random rotation of the particles. To prevent dirt particle overlaps, a binary mask is used to store the information on occupied areas in the image. In Algorithm&amp;nbsp; , the place for a new particle is selected from the unoccupied area. One aspect that should be taken into account is controlling the amount of dirt in an image. It can be controlled by the percentage of the dirt covering the surface. To form the list of dirt particles to be positioned on the image, the user should specify what type of dirt is needed to be added onto the image and its proportion. As its result, Algorithm&amp;nbsp; outputs the image with a uniform background and dirt particles placed on it randomly. In addition, information about the dirt particles is stored in the form of a labeled binary mask (the ground truth). In some cases it might be useful to consider normalization of the colors [22]. In this study, the color normalization was not used.    4 Dirt features and their use in classification  The data used in this study cannot be considered to represent the full variation of pulp nor all characteristics of dirt types. To make the system stable when a new type of dirt is introduced or to adapt the system for completely new dirt types, a tool is required to tune the classification process. This calls for feature evaluation, which aims at estimating the feature importance and makes it possible to determine the set of features that describe the available data in the most efficient way. The dirt features used in this work can be divided into two categories: geometric features and color features. The geometric features include characteristics related to the shape, form, and uniformity of dirt particles. To complement them, the color features include, for example, mean color, variation of color, and intensity. The features are presented in Table  2. For each dirt particle, a bounding box is determined which is the smallest rectangle enclosing the dirt particle. The solidity specifies the proportion of the pixels in the convex hull that belong to the region. Eccentricity specifies the eccentricity of the ellipse that contains the same second-moments as the region. The convex area is the number of pixels in the convex hull of a dirt particle. The extent specifies the ratio of pixels in the region to pixels in the total bounding box. The mean color and mean intensity are calculated as the mean hue value and the mean intensity over a dirt particle area. Std of color describes the standard deviation of color within the area of a dirt particle. The other geometric features are calculated according to the following formulas:                                                                                                       To determine the feature set to be used in classification, one should choose a feature evaluation procedure. The exhaustive search, i.e. evaluating all the possible combinations of features, would be computationally infeasible. This is why the sequential feature selection [14] procedure is used instead. At each step, the method adds (forward selection) or removes (backward selection) a new feature to the feature set and calculates an objective function value, which is aimed to be minimized. Since the method moves only in one direction, adding or deleting features, it eventually evaluates only a subset of all possible feature combinations. Therefore, the optimal feature set cannot be guaranteed but rather a close-to-optimal feature set is produced. In this work, the sequential feature selection was applied using the forward selection and the linear discriminative function [ 4]       was selected as the objective function. In Eq.  9 the weights   in the linear combination of the features   are optimized to minimize the linear discriminative function, taking into account the bias  . This provides the information how well the combinations of features can be distinguished.
Springer.tar//Springer//Springer\10.1007-s00138-013-0486-0.xml:Touch tracking with a particle filter:Optical touch tracking Particle filters:  1 Introduction  Touch-based interactive surfaces can be split into two subcategories: non-optical and optical-based touch. Non-optical techniques include [22, 23]: resistance-based touch, capacitance-based touch and surface acoustic wave touch. Non-optical techniques have become ubiquitous in the consumer market for smartphones and tablet computers. The main disadvantage of these techniques is that they require special industrial fabrication of the touch surface itself and this limits their cost-effectiveness especially for medium to large size displays. Optical techniques, on the other hand, are more cost-effective for larger displays and are consequently the focus of this paper. There are several different styles of optical touch sensors, and several of them are described in the following paragraphs. Most optical touch sensors use variations of infrared reflection and a standard camera to image the touch surface [1, 6, 28]. A traditional optical touch screen involves backlighting a diffusive display surface with infrared light [28]. A touch can then be detected as a decrease in infrared reflection by a backmounted infrared camera. The seminal work using infrared light is that by [6] which uses frustrated total internal reflection (FTIR) of infrared light shining internally along the surface. A touch on the surface causes a change in the optical properties at the touch point allowing the infrared light to leak out at the point and subsequently be detected by the camera. Variations on these two themes include shadow tracking using an overhead camera [5] and using stereo infrared cameras to give some depth information [8]. Other techniques using only visible light include the work of [1] which uses a stereo overhead camera and can be applied to any horizontal surface, but is of limited utility for vertical surfaces. The main limitation of these optical techniques is that they require a camera to sense the touch surface from a high angle of incidence (usually close to orthogonal), and this implies the use of significant space either behind or in front of the touch screen and significant computing power to process the image. The ThinSight technology [7] now used in the Microsoft PixelSense™[11] solves the space problem by using infrared LED, infrared sensor pairs in an array configuration behind the surface and of the same physical size; essentially a light source and camera sensor of the same size as the display itself. This allows for thin touch displays, at the cost of introducing specialised industrial fabrication—a similar problem to capacitive touch systems, albeit without the problems of size restriction. One particular set of methods that has received little academic attention, but has produced successful commercial products (Smart Technology’s Digital Vision Touch) is the work of John Newton of NextWindow Limited [ 12– 15]. These systems use multiple line scan cameras, infrared lights and retroreflective strips all mounted on a narrow bezel surrounding the touch screen surface. The cameras detect a constant amount of infrared light except at touch points on the surface where the signal strength decreases significantly. If multiple cameras are used, simple triangulation will yield the two-dimensional location of the offending touch. The system is very effective for single touch applications, inexpensive to produce, and able to scale to medium sized formats (large LCD displays, but not as yet wall sized displays). Figure   shows the basic idea of how such a system works with actual touches labelled 1–4. Figure   also shows the limitations of the system for multi-touches: touch 4 is occluded by other touches from both sensors, and cannot be detected; unlabelled grey areas indicate “ghost” touches that produce touch ambiguities—actual touches cannot be uniquely determined from the sensor data alone. Adding more cameras helps to solve the problem, but does not completely alleviate it. Recent work purporting to solve the problem of ghost touches is reported in [27] and is the closest work in spirit to that reported here (their prototype actually uses area cameras, presumably because of ease of use and availability compared to line-scan cameras). They solve the problem by calculating a single point in the centre of the occluded area in each camera and triangulating each computed point from every camera. If the centre points are computed accurately, then each touch can be detected as the single point of intersection of all lines projected from each camera. If an intersection point has fewer intersecting lines than cameras, then this can be considered a ghost point. In practice, perfect intersection of all lines does not occur due to aliasing effects and inaccuracies in detecting the centre points, and therefore a thresholded intersection test must be used. Wang et al. [27] do not show any quantitative results for live testing, but do show some results for reconstructing reference points on the boundary of the screen using a lookup table approach. Unfortunately, the lookup table approach uses the reference points as part of the method and therefore the reported error of 0&amp;nbsp;mm for the method is not a fair test. Furthermore, the authors have not given any results indicating the effects of the various thresholds used in their technique. A further issue with multi-touch sensors, and this applies to all types, is the problem of data association. Most of the techniques described above return touch points at each timestep and leave it to the application (or hardware driver) to determine which touch point at time  matches with which point at time . Some recent work makes use of Bayesian methods such as particle filters [19, 20] and Markov random fields [16]. The particle filter methods we use are similar to those discussed in the context of capacitive touch [19, 20], and FTIR [16]. However, the different nature of the input in our case (1D line scan cameras) necessitates different formulations of the problem. The system we propose here is unique as we extend the system introduced in [12–15] to multi-touch using particle filters to simultaneously solve the problem of eliminating ghost points and solving the data association problem. In this context, Kalman filters are inferior because dynamics errors cannot be modelled as Gaussian at all time points due to occasional unpredictable finger motion. We present a basic description of the system in Sect.&amp;nbsp;2. We propose three solutions to the multi-touch problem given the hardware in Sect.&amp;nbsp;3, and evaluate these solutions in some detail in Sect.&amp;nbsp;4.    2 System overview  The main motivation for building touch screens with optical technology is its relative affordability. As the components of the optical sensing system are placed outside the screen area, for larger screens the cost grows with the perimeter, as opposed to the area of the touch screen for active surface technologies. While the exact details of the mechanics, optics and electronics of building an optical touch screen are outside the focus of this paper, a basic system can be described as follows. Multiple inexpensive line-scan sensors with a field of view of around  are placed along the perimeter of the screen. Each camera produces a one-dimensional signal of infrared light hitting the sensor. Lighting can be achieved in two ways: either infrared LEDs are placed around the border at regular intervals; or a single infrared LED is placed near each sensor and a retroreflective border is fitted to the entire screen. When there is no touch, the linescan cameras record a high signal across the whole image (fully illuminated by the infrared LEDs). As touch objects come into view, they obstruct the light sources from reaching the sensor, producing a low signal in parts of the sensor. By comparing the current sensor signal to the known no touch signal, shadows (decrease in the signal) produced by touch objects can be detected. As with many optical imaging systems, calibration plays an important role in the data acquisition process. In particular, in order to make use of the information coming from multiple sensors, they must be put into the same frame of reference. A process commonly referred to as linearisation is used to calibrate the system by establishing the relationship between the pixels in the sensor and the absolute angles on the screen plane. This can be achieved by placing objects at known positions on the screen and observing their shadows on every sensor. A basic workflow for acquiring data for the touch tracking problem looks like the following:        Shutter the sensors to acquire the waveforms of the current signal.         &amp;nbsp;           Compare the signal to known no touch signal pixel by pixel.         &amp;nbsp;           Detect starting and ending edge pixels of any shadows present in the signal.         &amp;nbsp;           Convert the pixel positions to corresponding absolute angles on the screen plane using the linearisation model.         &amp;nbsp;      With the above data acquisition process in mind, the problem of optical touch tracking can then be defined as the following. Shutter the sensors to acquire the waveforms of the current signal. Compare the signal to known no touch signal pixel by pixel. Detect starting and ending edge pixels of any shadows present in the signal. Convert the pixel positions to corresponding absolute angles on the screen plane using the linearisation model. Given a set  of  sensors  with positions  and field of view angles , and a set of shadows  where a shadow  is defined by starting and stopping angles from the point of view of the corresponding sensor, over a duration of  frames, locate the touch objects  in every frame , where a touch object  is defined by its position and radius (see Fig.&amp;nbsp;). Note that the touch objects are considered to be circular, meaning the shadows they cast on the sensors are rotation invariant. This simplification is generally considered acceptable for representing human finger or stylus touches, and while in practice the shape of finger contact area can be quite elliptical, the complexity of considering other shapes in the problem seems to outweigh the benefits that could follow. Back-projection is a process of finding the ‘would-be’ measurements that estimated touch objects produced, were they observed by the sensors. With the circular touch model, finding the  is as simple as finding the touch circle tangent lines from the point of the sensor position (see Fig.&amp;nbsp;). Centre angle of the circle can be found as , and with distance from sensor position to circle centre  can be found as , giving . A solution to the touch tracking problem would typically have the following steps, with some considerations:     Prediction Based on the information from the previous  frames, what are the likely positions of the objects? Generally some sort of a motion model is employed that predicts the motion of each object between the previous and current frames.    Association At this step the correspondence between the predicted object locations and the incoming observations can be established. This can be achieved for example by computing the back-projections , where a  is a range defined again by a starting and a stopping angle, of every predicted  onto every , and comparing the predicted shadows with the observed ones.    Detection   Fitting  A tracking approach following the general description above would address the problem of occlusions and ambiguities by cross-relating the information observed in different sensors, using predicted positions of touch objects. In the frames where occlusions cause ambiguity (Fig.&amp;nbsp; ), if the predicted positions are accurate enough, the association between the existing tracks and the observations will be established correctly, and the tracking will continue in the right direction. Incorrect placement of touch objects due to ambiguity is still possible, but is limited to the new touch detection stage. With the assumption that multiple new touch objects are not very likely to be appearing in the same starting frame, they have a higher chance of being placed correctly one at a time, considering limited new information that is not explained by the prediction of existing tracks. Prediction Based on the information from the previous  frames, what are the likely positions of the objects? Generally some sort of a motion model is employed that predicts the motion of each object between the previous and current frames. Association At this step the correspondence between the predicted object locations and the incoming observations can be established. This can be achieved for example by computing the back-projections , where a  is a range defined again by a starting and a stopping angle, of every predicted  onto every , and comparing the predicted shadows with the observed ones. Detection The association information is commonly used to initialise or to drop the object tracks. For example, if the current frame has new shadows on some sensors that do not correspond to any predicted touches, a new touch has come into view and needs to be located. Similarly, if the predicted position of a previously existing object does not correspond to any shadows in the current observation, the touch object must have lifted off the screen. Fitting Optionally, the association information can also be used to fit the prediction to the observed data by performing some form of local search, for example making the minimal adjustments to the predicted positions of the objects to make their projections better coincide with the observed shadows.    3 Touch location solutions  That is not to say that the alternative of an exhaustive search that locates all the touch objects in every frame without considering the history is entirely impossible. By intersecting the angles of the shadow edges observed by all the sensors simultaneously, one could explicitly compute the set of polygons that represents the exact feasible regions for the touch objects casting the shadows. The touch objects would be guaranteed to lie inside some of these polygons, but not necessarily all of the polygons would have to be occupied by actual touch objects (see Fig.&amp;nbsp;). If two or more touch objects are so close together that none of the sensors can observe a gap between them, they will naturally fall into the same feasible polygon. To constrain the problem, we will assume such close configurations to represent a single touch object, and therefore that at most one touch object can occupy a feasible polygon. The problem of finding the touch object positions in a single frame can then be formulated as follows. Given a set of feasible polygons , find a minimal subset  that maximises the coverage of sensor shadows . A following basic greedy algorithm can be proposed for solving the above problem:        Consider subsets  of sizes .         &amp;nbsp;           Enumerate all subsets  of size .         &amp;nbsp;           For every , fit a circle  in every polygon .         &amp;nbsp;           Project every  onto every sensor  to obtain a back-projection .         &amp;nbsp;           Compute the intersection of back-projections with the observed shadows for every sensor : .         &amp;nbsp;           Score the coverage of subset  by how well the intersection covers the observed shadows: , for .         &amp;nbsp;           Once all  of size  are scored, if  represents an acceptable threshold coverage of the observation (e.g. , where  is a perfect coverage), return  as solution, otherwise consider larger .         &amp;nbsp;      The above algorithm is greedy in a sense that it stops as soon as it finds an acceptable solution with smallest number   of touch objects, and in cases with heavy occlusions, where it is possible for some combination of  \(K &amp;lt; N\) objects to generate shadows that mostly ‘hide’ the remaining  , only the   objects will be output as a solution. Also note that   is monotonically non-decreasing with increasing   since considering more polygons can only improve the best coverage.   has a maximum value of   and therefore any threshold less than   is guaranteed to be satisfied (when  ). In practical tests we found that by selecting a reasonably high coverage score threshold, the algorithm can have a high success rate. Consider subsets  of sizes . Enumerate all subsets  of size . For every , fit a circle  in every polygon . Project every  onto every sensor  to obtain a back-projection . Compute the intersection of back-projections with the observed shadows for every sensor : . Score the coverage of subset  by how well the intersection covers the observed shadows: , for . Once all  of size  are scored, if  represents an acceptable threshold coverage of the observation (e.g. , where  is a perfect coverage), return  as solution, otherwise consider larger . The above touch detection algorithm has two major drawbacks. Firstly, it makes limiting assumptions about touch objects coming into tight occlusion groups (one object per polygon; consider two or more fingers doing a common ‘pinch’ gesture to the point where they touch one another) and will fail due to the greediness. Secondly, it can be computationally expensive to enumerate all the possible polygon subsets. We therefore suggest a tracking solution that utilizes a particle filter and robustly deals with occlusions as well as provides a trade-off parameter between accuracy and computational speed in the form of the number of particles used.
Springer.tar//Springer//Springer\10.1007-s00138-013-0487-z.xml:Infrared system for 3D scanning of metallic surfaces:3D Scanning Prototype Scanning from heating Specular surfaces Infrared:  1 3D digitization of specular surfaces  3D reconstruction of surfaces is an important topic in computer vision and corresponds to a large field of applications: industrial inspection, reverse engineering, object recognition, biometry, archeology... Because of the large varieties of applications, we can find in the literature a lot of approaches which can be classified into two families: passive and active [ 1]. Certainly because of their reliability, active approaches, using imaging system with an additional controlled light source, seem to be the most commonly used in the industrial field. In this domain, the 3D digitization approach based on active 3D triangulation has undergone important developments during the past 10&amp;nbsp;years [ 2] and seems to be mature today if we consider the important number of systems proposed by manufacturers. Unfortunately, the performances of active 3D scanners depend on the optical properties of the surface to digitize. As an example, on Fig.  a, a 3D shape with a diffuse surface has been digitized with Comet V scanner (Steinbichler). The 3D reconstruction is presented on Fig.  b. The same experiment has been done on the same shape presenting a specular surface (Fig.  c, d) where we can observe the influence of specularity on the performance of the digitization. Numerous approaches have been proposed in the past to solve the problem of digitization of non-cooperative surfaces (specular surfaces, including also glossy and transparent...). Irkhe et al. [3] present a consequent state of the art and a classification of the experimental 3D digitization solutions according to the optical particularities of the surface. Among the cited solutions, Park and Kak [4] object to the basis assumption of an active scanner, considering that a projected point corresponds to a single point in the image frame, and they propose to measure all the points that are reflected by the specular object toward the sensor (resulting of inter-reflections). Then, the algorithm iteratively removes false points through a series of tests. Because several images have been acquired from different points of view, constraints can be applied to these tests, but the thresholds may need to be defined a priori. Although the results allow for a complete visualization of the glossy object, the accuracy is not sufficiently good for measurement applications or for surface inspection. Approaches based on “Shape from polarization” have also been the topic of many research studies on specular surfaces (and transparent surfaces). Wolff and Boult [5] proved the relation between the surface normal and the polarization parameters of the reflected ray at the surface of the material. Then, the technique was extended to 3D digitization of specular metallic objects [6], especially by means of active lighting [7]. Even if the measurement accuracy is good, this method requires a confined environment, which limits the size of the objects that can be reconstructed. While in the visible range, specular reflections are usually considered to be noise in the reconstruction algorithms, deflectometry takes advantage of specular reflections to compute the depth and orientation of the surface. Tarini et al. [8] propose an approach called “Shape from Distortion”. They proposed to observe the distortion of several light patterns that were projected from a diffuse source (a computer monitor) on the mirroring surface. Bonfort et al. [9] propose a similar method and show that it is possible to recover the shape of specular surfaces by projecting a light pattern from two unknown positions. For these techniques, the cloud of points that is obtained could be very dense, but the matching constraint is high, and a high curvature shape cannot be fully recovered from one pose. Another method, sometimes classified in the “Shape from specularities” category, was developed by Zheng and Murata [10] and has been widely used. The principle relies on the tracking of a specular reflection that is induced by light sources surrounding the object, when the object is rotating. The continuous acquisition by a fixed camera provides a spatiotemporal collection of images. The study of the apparent motion of specular highlights on the object will help to recover the whole surface. However, the authors noted that the reconstruction accuracy depends strongly on the nature of the specular surface. We note that this remark highlights a general drawback to all of the non-conventional techniques that were mentioned above: the presence of any diffuse part on the object will interfere with the point cloud digitization. In other words, the diffuse surfaces cannot be entirely reconstructed by this type of technique. Other approaches such as multi-camera passive systems [11] and structured-light based systems [12], have also been proposed, but the surface roughness should not exceed a low bound. “Shape from fluorescence” technique [13] is a recent promising approach. The authors use an UV laser to generate a visible pattern on the surface, using fluorescence effect. The accuracy is similar to the one obtained using conventional scanners on a diffuse surface. But the main limitation is that the fluorescence in visible domain does not exist on all the material. In other words, the method is not applicable to all the metals (steel for example). Chromatic confocal microscopy [14] is another way to estimate the 3D coordinates of an opaque surface with good accuracy, but the surface should not be excessively tilted. Finally, we can see that some commercial systems mounted on robotic arms operate on the assumption that the reflected radiation at the surface of an object has a quite diffuse component; thus, scanners can acquire a reflective shape after several successive scans if the incident radiation is sufficiently intense and if the distance of the measurement is relatively low. We can note that the approaches presented previously exploit specularity of the surface to realize 3D reconstruction. That is why each of them is generally not adapted on surfaces whose present different optical properties. A second remark is that all these approaches work in the visible domain and do not explore other spectral domains, such as for example, infrared domain. In this paper, we will present some developments based on using infrared imaging to realize an operational 3D scanner able to digitize metallic objects with low dependence on the optical property of the surface. The method we studied, called “Scanning from Heating”, has been initially proposed to realize 3D scanning of transparent glass. The evolution to digitization of metallic objects will be presented in Sect. 2, with the evaluation of the performances obtained on a first experimental setup. In Sect. 3 we will present an evolution of this setup to realize an operational 3D scanner. After having the system described, the scanning results will be presented and commented. Finally, we will conclude in Sect. 4 and present some future works.    2 Scanning from heating: application to metallic objects  Infrared imaging is known well for its use in realizing non-contact temperature measurement, defect detection, non-destructive control, and so on. But researchers have been also interested in designing 3D acquisition systems in infrared domain, for example to realize invisible structured light 3D scanners. Kinect Microsoft is certainly the most popular one. Other example can be found in [15] where authors propose to use infrared structured light imaging for 3D airbag generation. Another way has been presented in [16] where the authors describe an omnidirectional sensor using infrared structured light. More recently, authors present in [17] an infrared stereoscopic system dedicated to combine temperature measurement, 3D shape and displacement of a heated metallic object. As the authors explain, the matching between the two images is not so easy because of the low contrast in the thermal images. IR polarization imaging system has been experimented and presented in [18]. The choice of the camera (long wave infrared camera) permits to obtain images in a spectral domain where the glass is opaque. The 3D reconstruction is obtained by analyzing the parameters of polarization of the IR light reflected on the surface. These techniques correspond to an application in infrared domain of methods developed in the visible domain (see Sect. 1), based on the analysis of reflected light on the surface and similar limitations to optical properties of the object. A second way, of 3D digitization in IR domain, seems to have been inspired by the active thermography, commonly used in non-destructive testing applications. In [19] Bodnar presents a flying spot laser system, which is able to detect and evaluate the dimensional characterization of wear, cracks that occur in metallic materials. The method used is called “Photothermal radiometry” and is based on scanning the object with a laser excitation. In the same field, Pelletier and Maldague [20] have developed a technique for thermal image analysis called “Shape from heating”, which can extract the orientation and depth of simple surface (a cylinder in the paper) that is heated by a diffuse infrared source. The main goal of this 3D shape extraction is to improve the detection of defects by correcting the temperature variations that result from the non-planarity of the surface. Even though the accuracy of this method has not been demonstrated yet for the 3D digitization of complex shapes, this study was among the first to consider thermography to extract 3D information. The studies presented in this article on the 3D scanning of specular surfaces are related to a non-conventional infrared approach introduced by Eren et al. [21] for the 3D digitization of glass objects. Unlike classical active triangulation approaches, the principle of this technique, called “Scanning from heating” (SFH), is based on the measurement of the infrared radiation that is emitted by the object instead of the reflection of visible radiation. A laser source is used to cause a local elevation of temperature; the 3D coordinates of each point are then extracted from the IR images with a prior geometric calibration of the system. As the emissivity is omnidirectional for most materials, the model of infrared radiation tends to approach the model of the diffuse reflection of visible light for diffuse surfaces and, thus, addresses the issue of specular reflection or transparency in the visible spectrum. Using a CO laser, the feasibility of this concept has been demonstrated on glass objects, and some scanning results have been presented [22]. The evolution of this work is the study of the application of scanning from heating to perform 3D scanning of metals, specular or not, that have a high reflectivity and a high thermal conductivity. In a heat transfer problem, we need to evaluate the energy balance for the three exchange modes: conduction, convection and radiation. In the case of “Scanning from heating” (see Fig.  ), the heat quantity is brought by a laser and has to be adjusted according to the thermophysical properties of the studied material and the thermal sensitivity of the camera. The generated heated point has to be intense enough to be detected by the sensor but it should not exceed a temperature limit, which would damage the surface by melting or marking. Given that convection and radiation are only surface exchanges, we can consider two different elements of the material to write the global equation: the elements on the surface of the material and the elements within the volume. For any volume element   of the material surface, the heat stored causes a temperature rise [ 23] as follows:       where   is the total quantity of heat accumulated in the element   is the density of the material and   is its specific heat capacity and   the temperature. Then, we can demonstrate that the sum of heat transfer components gives            The second member of the equation is, respectively, the sum of:    the conduction term where  is the thermal conductivity of the studied material   the radiation term where  is the total emissivity of the surface   the convection term where  is the ambient temperature and  is the heat transfer coefficient of the air during natural convection   and the heat flux absorbed by the surface  from the laser beam is a Gaussian energetic distribution given by [24]:          where   is the surface absorptivity for the laser wavelength,   is the incident power of the laser and   is the radius of the laser beam at the focal plane. the conduction term where  is the thermal conductivity of the studied material the radiation term where  is the total emissivity of the surface the convection term where  is the ambient temperature and  is the heat transfer coefficient of the air during natural convection and the heat flux absorbed by the surface  from the laser beam is a Gaussian energetic distribution given by [24]: Concerning the elements within the volume (under the surface), the conduction transfer becomes uniform according to the three directions and Eq. ( 2) becomes the classical heat equation:       Using a finite element numerical solver, we have shown that it is possible to solve this equation in any case of three-dimensional problem [ 25]. Assuming that we know or estimate the thermophysical properties of the material, we can predict the best settings for the heating process (incident power, pulse time) to improve the efficiency of the digitization method. As an example in Table  1, the laser power is computed for different pulse durations such that the temperature of the surface is increasing by 3 K from the initial temperature. 3 K is a detectability threshold depending on the intrinsic properties of the thermal sensor. Different common materials are used for the simulation and the diameter of the laser beam (0.5&amp;nbsp;mm) is chosen according to the spatial resolution of the camera. Relying on the simulation results, a first experimental setup has been implemented, based on the SFH technique. Concerning the choice of the incident wavelength, we can rely on the numerous databases of optical constants in the literature ([26, 27]). Indeed, the absorption coefficient of metallic surfaces tends to increase when the wavelength decreases, the reason why the laser wavelength should be chosen in the near-infrared spectral band. The system consists of a diode laser emitting at  &amp;nbsp;nm, and an infrared imaging detector, which is sensitive to mid-wavelength infrared (InSb quantum-based sensor for which the sensitivity spectral band extends from 1.5 to 5&amp;nbsp; m). The system is fixed on a 3-axis moving table, and the studied object is motionless, to have a large scanning window (see Fig.  ). The working distance between the camera and the object is 60&amp;nbsp;cm and the focal length of the laser optics is 15&amp;nbsp;cm. A programmable controller is used to synchronize the laser emission, the camera trigger and the displacement of the table. After setting the parameters of the laser source (pulse of 1 ms and power increasing from 4.1 to 17.5&amp;nbsp;W—see Table 1), the scanning procedure begins. For each intermediate position of the system, an image is acquired before and during the laser pulse to facilitate the position computation of the thermal spot in the images, using background subtraction. A subpixellic method is used to obtain the maximum heat localization in the image frame. An initial calibration procedure has been realized before and gives the relationship between the coordinates of the spot in the image and the depth  in the world coordinates, and knowing the position of the moving platform the 3D coordinates of the measured point are determined. Figure   presents the 3D digitization results that were obtained for several specular objects with the system described above. To efficiently compare the 3D scanning results for different surface conditions, we have used cylinders that were made of steel AISI 316L and have identical dimensions but different average roughnesses (see Fig.  ) obtained with various polishing techniques (mechanical or electrolytic). Figure   presents an example of the deviation map between reference points obtained from the touch-probe scanner on a surface and the points that were digitized by conventional scanner (Fig.  a) and our experimental setup (Fig.  b). Concerning the eight samples, the overall average error is similar: 154 m with the SFH-based scanner and 174  with the conventional scanner. But the standard deviation obtained with the Comet scanner is  (average for the eight measurements) and is larger than the one obtained with SFH . In the example presented in Fig. , the surface points for the SFH are acquired for an incidence angle of up to , with a total average error of 117 m. This value is quite similar for the eight cylinders and is not really dependent on the roughness. In comparison, for the traditional system, the maximum incidence angle is only , with a total average error of 148 m. It can be noted that as the specularity of the surface increases,  decreases to . Of course, the evaluated measurement accuracies we have summarized here (see [28] for a complete description of the evaluation process and results) depend on the quality of the fitting between point clouds obtained by the two scanners and the reference obtained with touch probe. They are also related to the cylindrical shape of the samples. A metrological study would be necessary to improve the evaluation of the global error of this prototype. However, it appears that the quality of the point clouds obtained with this system is fair and that Scanning from heating can give interesting results on metallic surfaces with low dependence to roughness. But the system we used here to realize the experiments presents a great disadvantage. Because of the moving platform used here to realize the scanning process, the time we need to obtain one scan is consequent (about 3&amp;nbsp;s to obtain 1 point). A second point is that all the system cannot be easily moved and this point limits the field of applications. That is why the development of a more compact and portable system has been done. It will be presented in the next section.    3 Scanner prototype based on SFH  According to the previous theoretical and experimental study, a SFH-based prototype has been developed with the aim of avoiding mechanical vibrations, increasing the accuracy of the scanning and speeding up the process. In place of the moving platform, we have chosen to use a galvanometer to scan the surface with the laser; it reduces significantly the dimensions of the system and improves the scanning speed and quality thanks to two mirrors. The prototype can be seen in Fig.  . The laser source is made of ytterbium fiber pumped by diodes, which delivers a 1.07  m laser beam and the power ranges from 0 to 50&amp;nbsp;W. The optical components are three lenses that are assembled through the laser optical path. The two first lenses constitute an afocal system reducing the beam diameter, and the third one focuses the laser to a 0.44&amp;nbsp;mm beam waist at 500&amp;nbsp;mm with a 200&amp;nbsp;mm depth of field. The galvanometer scan head is made up of two mirrors controlled by piezoelectric motors that can deflect the laser beam with 150rad step, in  and  directions. The infrared camera performs the thermal measurement with a bolometer detector, which is sensitive to Long-Wavelength ([8–13&amp;nbsp;m]). A workstation controls the synchronization between camera acquisition, laser emission, and galvanometer scanner and then performs the 3D reconstruction from image analysis. Before the acquisition step, it is necessary to geometrically calibrate the laser-camera system. As the Rayleigh length of the laser (equivalent to the depth of field) is relatively long, we assume that there is no geometrical aberration, so the laser calibration is only based on the calculation of its extrinsic parameters, i.e. two rotations that are given by galvanometer mirrors. The electrical signals that command the mirror motors are linearly related to the deflection angles  and . The experimental determination of this relation is performed to know each position of the laser beam that is used for the scanning process. The world frame is attached to the rotation center of the galvanometer scan head. After laser calibration step, we have to compute the transformation between the world frame and the image frame. The principle of the method we have implemented [ 29] is based on the rules of projective geometry. As illustrated in Fig.  , a calibration plane is translated along   direction perpendicularly to the laser incidence direction. Figure   presents the sum of all the images obtained for 225 orientations of the laser beam, with an angle of   between each of them, for one position of the plane. The plane is translated and   acquisitions are realized. Each point of the grid presented on Fig.  , is moving along a line that corresponds to the projection of the laser beam in the image. An example can be seen in Fig.   with  . Each color of points corresponds to one depth.
Springer.tar//Springer//Springer\10.1007-s00138-013-0488-y.xml:Shadow compensation and illumination normalization of face image:Shadow compensation Face recognition Illumination normalization:  1 Introduction  The pose, face expression and illumination variations of face images, which usually occur in unconstrained nature images, will greatly affect accurate face image analysis such as face recognition, tired face detection, etc. In this study, we focus on eliminating the effects from both shadow and illumination variations. Over time, a variety of approaches have been proposed to solve the problems caused by illumination. These approaches can be broadly divided into three main categories according to [2]: (1) extracting illumination invariant features, (2) modeling illumination variations, (3) restoring the illumination to a canonical form. In the first category of approaches, the early literatures, edge maps, local binary patterns (LBP) [1], etc., have been extracted. From the aspect of time–frequency analysis, the illumination insensitive features are used such as waveletface [9, 15], dctface [11], spectorface [6, 13], etc. Based on the Retinex theory [14], to get features (small intrinsic structures) that are illumination invariant, a variety of methods have been used to decompose the face into two different parts [7, 19–22, 27]. These methods have better performance than those which extract the illumination insensitive feature. However, as denoted in [25, 26], the large intrinsic structures (the low-frequency components of face which contain most of the illumination variations) also contain some useful information for face recognition. The methods in the second category directly use training examples which are under all possible illuminations to learn a global model of the possible light variations. In the image space, the illumination cone of a convex Lambertian object can be approximated by a linear subspace [3, 4, 10]. These approaches need many training examples. Some of them assume that the pose is fixed and face image is a Lambertian surface without cast shadow. The last kind of approaches normalize the illumination to a canonical form. The Histogram Equalization (HE) and Gamma Correlation (GC) are commonly used to get a normal face, which is easily influenced by the direction of the light. In this direction, many approaches have been proposed, such as the method based on low-frequency domain [8], the method based on morphing face [12], the method based on quotient image [17, 22], etc. The latest approach is proposed without assuming of any prior knowledge about the pose and expression [2], which needs the face image’s co-registered 3D point cloud. It is hard to be widely used in the real life. Moreover, illuminating the face from different lighting directions causes significant differences, not only due to the direct changes in illumination, but also because of the shadow cast by nose or other parts of the face. Thus, reducing the effect of shadow is a problem needs to be found. Based on it, an approach is proposed to remove the shadow of face image in [5, 24]. The robust Principal Component Analysis (robust PCA) is used to compensate the shadow. This algorithm recovers the low-rank matrix  which contains the face image without shadow, and sparse matrix  which is treated as the error, from the observation matrix . Unfortunately, in this algorithm, the observation matrix  must be build up on the same person’s face images. For example, if the observation matrix  includes training face images of person  and a face image from another person , the robust PCA will treat the face image from person  as an error. After compensating shadow of the face image from , this face image will be similar to the person . Thus, directly using robust PCA method on face recognition is not suitable because we do not know which subject the query face image belongs to. Motivated by the idea of decomposing the face image into the small- and large-scale features [ 25,  26], in this study, we propose a new framework to compensate shadow and normalize illumination, which is shown in Fig.&amp;nbsp; . Based on the Retinex theory [ 14], we first decompose face image into small- and large-scale features. The Retinex theory suggests that the illumination and shadow lie in the large-scale features. So we only use robust PCA method on the large-scale features. Then, the shadow compensation and illumination normalization are adopted on the large-scale features and smoothing operation is used on the small-scale features. Finally, these two processed parts are combined to get a normal face image without shadow. Compared with the methods in [ 7,  8,  26], our experimental results confirm that the performance is significantly improved by our framework. In our method, we use the idea of decomposing the face into the small-scale and large-scale features, which can avoid the small-scale features to be destroyed by the robust PCA method. To illuminate this point, let us continue the above example. The large- scale features of the face image from  will be similar to the large-scale features of  after compensating shadow. However, the combination of large-scale features and small-scale features will produce a very strange face image. On the other hand, if the query face image belongs to the same subject of training examples, the processed face image will be norm which can help face recognition. This is our key contribution. Thus, the paradigm of face recognition under our framework can be the following. First, the training face images for every subject are decomposed into large-scale features and small-scale features. For a query image, it is first decomposed into large-scale features and small-scale features. Because the small-scale features contain some useful information for face classification, some face recognition method, here eigenface method, is used to determine which subject the query face image belongs to. In our experiment, we choose the top ten similar subjects. Then the large-scale features of query image are combined with the corresponding training images’ large-scale features to do shadow compensation and illumination normalization, respectively. Even though we need processing ten times, it is not a big deal, because robust PCA method is very fast. To make it easy to read, in this study, the small- and large-scale features are also referred to as the small and large intrinsic structures, respectively [26]. The rest of this paper is organized as follows: In Sects.&amp;nbsp;2, 3, 4 and 5, the novel illumination normalization and shadow compensation method is presented in detail. In Sect.&amp;nbsp;6, experimental results are shown. The conclusions are given in Sect.&amp;nbsp;7.    2 Preliminary  The Lambertian reflection theory [ 3] reveals that the intensities of an image   can be represented by:       where   represents the reflectance component and   is the illumination effect.   depends on the material of the object surface. At the same time, in this model, quantization and camera non-linearities are not considered. From ( 1), we know that estimating   directly from   is an ill-posed problem. To solve this problem, an alternative approach [ 7] is proposed as follows:            where   and  . For the face images,   is the albedo of large-scale skin areas and backgrounds. The item   only includes the small intrinsic structure of face image. The term   includes the extrinsic illumination and shadows caused by the nose or other parts of the face. In this study, we will use   and   to represent the small-scale and large-scale features, respectively. Based on the findings by Basri and Jacobs [ 3], illumination variation and shadow mainly lie on the low-frequency components of face image. So we only need to remove the shadow and normalize illumination on the large-scale features. Thus, the invariant features of face can be protected. Based on this observation, we propose a new method for shadow compensation and illumination normalization. Figure&amp;nbsp; shows the procedure of the proposed method. It contains five steps: (i) decomposing the face image, (ii) smoothing on the small-scale features (), (iii) removing shadow on the large-scale features (), (iv) normalizing illumination on the large-scale features after shadow removed (), (v) reconstructing the normalized face image. The details of the proposed approach will be discussed in the following sections.    3 The decomposition of face image  In this section, we will describe how to decompose the face image into the large- and small-scale features and smooth the small-scale features. First of all, by taking logarithm transform on both sides of ( 2): The multiplicative model is turned into an additive one by the logarithm transform. The terms   and   keep similar properties as   and  . Because logarithm transform preserves the structures, to obtain the decomposition in ( 3), we utilize the Logarithmic Total Variation (LTV) model [ 7] rather than using the low-/high-frequency filters to decompose the face image. LTV model can extract the illumination invariant features efficiently as proposed in [ 26]. The   and   are estimated as the following:            where   is the total variation of  , and   is a scalar threshold. In our experiments, for the image resolution size of  , we use the same parameter   in [ 26]. The good performance with this parameter has already been shown in [ 26]. After getting the   and   and   can be estimated as follows:       The result of decomposition by LTV model is shown in Fig.&amp;nbsp; . As shown in Fig.&amp;nbsp; b, the small-scale features preserve the identification information of the original face. From Fig.&amp;nbsp; c, we can observe that the shadow and illumination variations mainly lie in the large-scale features. Estimating the reflectance of the Lambertian object is an ill-posed problem. The decomposition by LTV model also brings some undesirable problems. In Fig.&amp;nbsp;b, we can see after decomposition, some shadow edges and light spots appear in the estimated small-scale features . The influences from shadow edges and light spots should be ignored for face recognition. But they are negative for the visual quality of the reconstructed face image. For this reason, we perform the threshold minimum-and-maximum filter on  to improve the visual quality. The threshold minimum-and-maximum filter is defined as follows. Suppose that  is the center point of a convolution region. The minimum convolution filter works only if . After the minimum filter is performed, the maximum convolution filter will work only if . The parameters,  and , are the thresholds. Both the minimum and maximum filters are the order-statistic filters. The minimum filter chooses the minimum pixel value in the convolution region to replace the center point’s pixel value. On the contrary, the maximum filter replaces the center point’s pixel value by the maximum pixel value in the convolution region. In our experiments, a   mask is used. From Fig.&amp;nbsp; a, we can see that one side of the shadow edge is gray and another side is a narrow and long white region. Also the pixel values on the shadow edge are less than both sides. Because the shadow is usually below the eyes, we just simply count the values of pixels below the eyes. Since the pixel values in the white region are larger than others, we can roughly get the parameters   and  , such that the values of   of pixels are smaller than   and the remaining   are among   and  . At the same time, since the pixel values in the shadow edge are lower than others, we can also get   and  , such that the values of   of pixels are larger than   and the remaining   are between   and  . In other words, this threshold minimum-and-maximum filter replaces the pixel values in the long white region by the local minimum value and the pixel values in the shadow edges by the local maximum value, respectively. Because the pixel value ranges of different people are not the same, thus, the parameters are estimated individually for each  . From Fig.&amp;nbsp; b, it can be observed that the shadow edge is smoothed by the threshold minimum-and-maximum filter.    4 Shadow compensation  The face images in different illuminations lie near an approximately nine-dimensional linear subspace, which satisfies the low-rank model. However, this low-rank model is always violated because of the shadows and specularities caused by different illuminations. At the same time, the shadows and specularities are always in a small region on face image. Thus, the robust PCA method can help us remove the shadow and specularities without any assumption of good distribution of shadow regions and specularities. To protect the small-scale features and avoid changing the query face image after compensating shadow, we only perform robust PCA on the large-scale features . The robust PCA method recovers the low-rank matrix  from the observation matrix . Both of the matrices  and  are unknown, but  is low rank and  is sparse. If we get enough face images, we can recover the low-rank matrix  which contains the faces without shadow. In this study, we stack the large-scale features  as the column of the observation matrix . Then we can get the large-scale features without shadow by seeking the proper low-rank matrix . To seek the low rank  , subject to the constraint that the errors are sparse  , we can immediately reformulate shadow compensation as an optimization problem:       The problem ( 6) is highly nonconvex, which has no efficient solutions. We use a tractable approach in [ 24] to solve this problem. In our experiments, the resolution of each large-scale features is  . First, a total of ten illumination environments per subject constitute the training set. When a new query face image comes, after decomposing, we stack these 11 large-scale features as the columns of our observation matrix  , then set  . The algorithm of robust PCA is almost real time and can remove the shadow and specularities very well. The result of shadow compensation is illustrated in Fig.&amp;nbsp; b. Comparing Fig.  a with Fig.&amp;nbsp; 4b, we can see the shadow around the nose on the right side of face have been removed well. However, as we can see, although the shadow is perfectly compensated, there still exist some illumination variations. Therefore, we use the LOG-DCT to normalize the illumination.    5 Illumination normalization  As we just mentioned above, illumination variations mainly lie in the low-frequency part of face image. Chen et al. [8] prove that the appropriate number of DCT coefficients in the logarithm domain can be used to approximate the illumination variations, and the DCT coefficients can be truncated to normalize the illumination and improve the recognition rate for face recognition. We use the LOG-DCT algorithm to normalize the large-scale features. According to Eq. ( 2), we have            where   and   are the large-scale features under the normal and variant illuminations, respectively. The terms   and   respect to the normal and variant illuminations. Performing the logarithm transform on both sides of ( 7), it follows that            where the term   is the difference between the normal illumination and the variant illumination in the logarithm domain, which is referred to as the error. From Eq. ( 8), we can conclude that   can be obtained from   by truncating the error term in logarithm domain. Assume that the DCT coefficients of   are  . According to the method proposed in [ 8], the inversing of DCT is given by:            where   is the normalization coefficient function. Because the illumination variations mainly lie in the low-frequency components, we can estimate the normal large-scale features by truncating some low-frequency components. This is the same as the case of setting the   low-frequency DCT coefficients to zero:            To obtain the uniform illumination, the DCT coefficients should be set to the same value  . In our experiments,   and the coefficients   around the original coordinates are set to zero. The result of illumination normalization on   is illustrated in Fig.&amp;nbsp; c. Comparing with Fig.&amp;nbsp; a, after shadow compensation and illumination normalization in Fig.&amp;nbsp; c, both the shadow and illumination variations are all eliminated and a better visual quality is gotten. According to Eq. ( 2), the final normalized face image is reconstructed by combining the normalized large-scale features   and the smoothed small-scale features   [ 26],    6 Experiments and discussions
Springer.tar//Springer//Springer\10.1007-s00138-013-0489-x.xml:Nearest neighbor weighted average customization for modeling faces:customization Face modeling Perspective projection Active shape model Procrustes analysis:  1 Introduction  Over 2500 years ago Greek philosopher Plato introduced the “Theory of Forms”. He asserted that non-material information possesses the highest and most fundamental kind of reality. He explained this paradigm with the simple fact that an object  is not object  because there is an ideal form of object  which we employ in identifying all objects of the same kind. Similarly object  has an ideal form as well. He went on to emphasize that these forms are the only true subjects of study that can provide us with genuine information in the notion of a material object. Human mind is very skilled in extracting this non-material information, in other words doing . This skill provides humans the invaluable capability of abstraction to identify and differentiate surrounding objects. The way human mind does this is a highly complicated, astonishing process. Plato did not go as far to elaborate how human mind differentiates between objects of the same kind. He simply stated that there is a basic ‘form’ of any particular object that human mind represents it with, in other words “the model” to identify it. For decades one of the main tasks of computer vision was to model and differentiate objects. One of the most challenging tasks of computer vision is to create an accurate model of the human face. Face, being a non-rigid and deformable structure is more sophisticated compared to many other objects we encounter. Non-rigidity of the face makes face modeling an intricate task, not to mention inter-personal and inter-racial variations. A simple movement of a facial muscle can drastically change the appearance of the face and convey a different yet important message. in computer vision terminology is to create an epitome that can be exploited in simulating any facial behavior or expression. The goal of face modeling research is to develop an automated modeling schema. To be practical, this schema must systematically find facial features and correspondences to an actual face with minimal user intervention. Human facial anatomy is flexible enough to convey thousands of different messages through contraction of facial muscles in varying degrees and combinations. These messages provide clues in our emotional state, short-term feelings and mood. For that reason, building a parameterized face model requires comprehensive knowledge in the face anatomy. A parameterized face model is commonly represented with a wireframe of polygons that represent skin surface and 3D layout of muscles beneath these polygons. Our first contribution in this paper is an anatomically accurate, HIgh-resolution GEneric wireframe face Model (HIGEM). HIGEM consists of 612 vertices and 1,128 polygonal surfaces and it embodies all major muscles of the human face. Throughout this study we present our results for parameterizing HIGEM to fit to the 2D observations of human faces. A completely automated algorithm for parameterization of an anatomically accurate generic face model will have numerous potential applications, one of which is generating animations. Animations can be produced in two dimensions, through interpolation of parameters between frames [1, 2]. When we have 3D models and defined action modes (e.g. smile, frown) of objects (e.g. faces), we have the opportunity to build a scene in a 3D virtual world and capture it with a virtual camera, by projecting the scene to the 2D view plane [3–6]. Face recognition is an emerging and interesting application area of face modeling. A face model embeds information on the depth of image, providing substantial information in comparison with an image. Once a 3D model of a subject is acquired, vast number of synthetic training images under varying pose, expression and illumination conditions can be effortlessly rendered. These variations in the synthetically rendered images of a subject can be used to expand the training data set of a face recognition system, improving the accuracy and performance in face recognition [7, 8]. Algorithms for automated modeling of human faces would have diverse range of applications including but not limited to medicine [9, 10], video surveillance [11, 12], lip reading and virtual reality [13–15]. Requirements of the face modeling algorithm vary depending on the application context. For instance a surveillance application would require the face model to be both accurate and real time. However an application in plastic surgery would demand very high precision that could be attained at the expense of time. When it comes to movies and applications of the entertainment industry, accuracy becomes less important and rendering quality and smoother animations come to forefront. In this paper, we are introducing an automated face modeling algorithm mainly for the applications of recognition and animation of facial expressions. The availability of an anatomically accurate and customized face model of a given subject is very valuable in both application domains. An accurate face model enables both analysis and synthesis based algorithms for recognizing a facial expression and mimicking the recognized expression on the face model. In our application scenario, a facial expression analysis/synthesis engine utilizes our algorithm to fit a generic face model to a subject on the first frame of a video. The anatomical structure of this customized model is exploited for further analysis of facial expressions in the subsequent frames. Such applications do not require high precision in fitting the model to the subject, rather they require speed for real-time analysis and synthesis. The real challenge in modeling 3D objects is the acquisition of depth information for each sensed point. This information may be available by the choice of a sensing device such as a scanner or it may be extracted by use of correspondence points in multi-view capture. When observation(s) from one view of the object is available, modeling transforms into a depth estimation problem that can be achieved through shading, motion or prior knowledge in shape and texture. Such approaches are iterative and approximate in nature, hence they are computationally expensive and are often unsuitable for online processing. Our goal is to perform modeling in linear time for real time analysis of facial expressions. Our second contribution in this report is a face modeling algorithm to customize a 3D wireframe model (HIGEM) onto a single 2D face image. Since the face anatomy is embedded in the HIGEM model, our method does not require knowledge of the human anatomy, nor it requires any training data. The customization process is non iterative that runs in linear time in the number of wireframe vertices. We use 32 predefined facial feature points to fit our model onto different subjects with high accuracy. We compare our results with the results obtained with Procrustes analysis and Active shape model (ASM) techniques. The rest of this paper is organized as follows: Sect. 2 discusses the research work that has been carried out in this field. In Sect. 3 we present the proposed method. Sect. 4 illustrates the accuracy of the proposed method with experimental results. Finally in Sect. 5 we conclude this report.    2 Literature survey  To recapitulate, the target application field of our work is facial expression analysis and synthesis. Facial expression analysis is conventionally performed either on static images or video sequences. We aim to estimate and construct a 3D face model utilizing a single 2D image of the subject. This model can serve as prior information in analysis and synthesis of the facial expression that appears in the observation. Two-dimensional image and video inputs fail to provide information on the profile of the subject, i.e. the solution to the depth of the facial feature points is under determined. Since this problem cannot be algebraically solved, we propose an approximation to the 3D model of the head. Our database must include images (for model generation) and accompanying 3D data clouds to be used as ground truth (model evaluation). There are a significant number of 3D data sets available for research. A brief summary of a subset of existing 3D face data sets is proved in Table 1. In this research we utilize Photoface [26] and Bosphorus 3D [17] data sets. Any geometric method suggested for face modeling has to start the process with detection of facial features on the image of the subject. In this study we do not propose an algorithm for facial feature detection. We did not implement any existing algorithms either for the purpose of evaluating the accuracy of the customization algorithm independent of any other processes. It is nevertheless possible to augment the proposed algorithm with a facial feature detection technique to make it a fully automatic software for modeling of faces. Although there have been reliable algorithms for face detection, detection of facial feature points is still a challenging problem. Salient point such as pupils, eyebrows, nose tip and corners of the mouth can be detected [30–34] more reliably than regions with smooth textures. It is also possible to employ Gabor wavelets to improve the robustness of detection [35] or implement ASMs to increase the accuracy of the search [36]. Celiktutan et al. [37] proposed the use of several modalities and their fusions for detection of salient points such as eye and lip corners, eyelashes, nose and chin. The proposed methods achieve detection rates as high as 98.5&amp;nbsp;% for inner eye corners. Valenti et al. [38] used active appearance models and a constraint set they name as “virtual structuring element” for the same purpose. Valstar and Pantic [39] used Haar basis functions and GentleBoost for detection of 20 facial feature points. Various image based approaches have been utilized in automatic generation of human face models. These approaches can be distinguished by the type of the image data they employ. A single image, two orthogonal images, a set of images or a video sequence can be exploited in generating face models. There has been considerable amount of research on constructing face models employing multiple images captured from different angles. Stereo vision through two cameras can be utilized in constructing a model [40]. Sometimes as many as five cameras from different angles are exploited in the process of building face models [41, 42]. Same principle can be applied for varying mode of sensing, such as laser triangulation [43], time of flight and structured lighting [44]. Although triangulation and pattern lighting approaches can produce promising results, they require complicated set of apparatus to attain acceptable results, therefore they are not very practical for model generation in daily life. Moreover, they are computationally expensive since either the correspondence points between images or the curvature of the projected light pattern has to be extracted. A sequence of monocular images can be utilized for modifying the model, exploiting a single camera to acquire different poses of the face. Since our research falls into this category, we will provide an in-depth review of these approaches in the subsequent paragraphs. With the general form of the face at hand, it is possible to estimate the depth of feature points using shading [45, 46]. Commonly named as , this approach is to estimate the angles between surface normals and the vector to the light source using the intensity of the corresponding pixel. As such, the reflectance characteristics of the skin, the relative coordinates of the light source and boundary conditions for skin depth have to be known for this approach to be applicable. Schlizerman and Basri [47] used a generic model and the shape from shading approach to  the model to a single input image. They formulate the problem as an image irradiance equation assuming Lambertian reflectance and known lighting, albedo and surface normals. Researchers demonstrated their method on the USF database and reported 4.2  1.2&amp;nbsp;% error rate. Kittler et al. [48] provide an excellent survey of earlier shape from shading studies and their applications on face recognition. type approaches utilize time sampled sequences of monocular images to estimate 3D structure of objects [49–51]. The main challenge in these approaches is to identify correspondence points under moving target and/or camera conditions. This is in general an iterative process where camera properties, relative motion with respect to camera, and 3D coordinates of the correspondence points are computed simultaneously. Krinidis and Pitas [52] used a semi-automatic approach for fitting a wireframe model to a face image. The face model is a 2D mesh whose elements are springs with stiffness. In the first step they coarsely initialized the wireframe on the face image, then they manually matched model nodes with the corresponding positions of the face image. Using these correspondences driving force values that will cause the required deformation of the wireframe are calculated. One of the more popular techniques is to reconstruct observed texture in the input image by use of . Faces can vary widely, but variations of the face can be broken down into two main factors; changes in shape and the texture. Both of these features can also vary among the poses of the same individual due to changes in expressions and camera viewpoints. Statistical techniques are very suitable to capture these variations and to develop appearance based face models. The disadvantage of statistical methods is that they typically require hundreds of observations to derive reliable statistics. Blanz and Vetter [4] utilized a 3D face database and a morphable model for this purpose. They used principal component analysis (PCA) to obtain modes of variation in the database. The goal of the customization process is to estimate the coefficients of these variation modes so that their linear combination resembles the observation. Essa [53] used Moghaddam and Pentland’s [54] view-based and modular eigenspace methods for fitting 3D face model to a face in an image. The positions of the eyes, nose and lips are extracted automatically. According to the positions of these features a canonical face mesh is deformed and matched with the face image. Most studies in the literature pose reconstruction of face models by use of monocular images as a convex optimization problem and propose an iterative solution. Iterative solutions require computationally costly operations such as computation of the gradients, Jacobian matrices, pseudo-inverses and model updates. This hastle is unavoidable when the estimation of the depth has to be accurate, a situation commonly encountered in face recognition. Then again, there are quite a few applications in which we have to compromise accuracy for speed, avatar generation, on-line gaming virtual reality and facial expression recognition to name a few. In this research, we aim to customize an anatomically accurate generic face model to the subject’s face using only the first frame of her facial expression video. Quick and relatively accurate modeling of the face makes it possible to map the tracked motion of facial feature points onto the customized 3D model. Active shape model (ASM) approach is proposed by Cootes et al. [55]. This technique was successfully applied on modeling of human internal organs, hands and faces. A statistical shape model of the face object is built using a set of training examples. Pose and shape parameters are iteratively modified for a better fit. Cootes et al. proposed later on active appearance model (AAM) [56] approach for matching a generic and textured face model to input face image. AAM combines the statistical model of the shape and the intensity appearance of texture around the points of interest. The synthesized model is projected onto the face image and matching is done iteratively. Ahlberg [57] used a color-based algorithm for detecting the size and the position of the face and applied AAM search to perform customization. Dornaika and Ahlberg [58] ultimately proposed two appearance-based fitting methods. In their first method, they did a locally exhaustive and directed search in parameter space, and in the second one they decoupled the estimation of head and facial feature motion. They demonstrated the robustness of their fitting method on video sequences. Also, Dornaika and Ahlberg [59] designed a fast and reliable AAM search for tracking of faces. We implemented ASM to compare its results with the Nearest Neighbor Weighted Average algorithm. ASM is originally proposed for 2D models. In our research, we extended ASM to be used with a 3D generic face model. Our ASM implementation can be outlined with the following algorithm.        Align the 3D data clouds using Procrustes analysis (translation, rotation and isotropic scaling).         &amp;nbsp;           Apply PCA on the 3D data set to obtain the mean model m and eigenvectors a              &amp;nbsp;           Find the Jacobian of residual with respect to transformation parameters in 6 degrees of freedom.         &amp;nbsp;           Apply Gauss–Newton approximation to estimate the transformation parameters.         &amp;nbsp;           Apply the estimated transformation on  where  are the shape parameters.         &amp;nbsp;           Find the Jacobian of the residual with respect to shape parameters.         &amp;nbsp;           Apply Gauss–Newton approximation to estimate the shape parameters.         &amp;nbsp;           Go to step 3 until convergence.         &amp;nbsp;      We define the residual vector as the square of Euclidean distance between each facial landmark and the perspective projection of wireframe landmark. Align the 3D data clouds using Procrustes analysis (translation, rotation and isotropic scaling). Apply PCA on the 3D data set to obtain the mean model m and eigenvectors a. Find the Jacobian of residual with respect to transformation parameters in 6 degrees of freedom. Apply Gauss–Newton approximation to estimate the transformation parameters. Apply the estimated transformation on  where  are the shape parameters. Find the Jacobian of the residual with respect to shape parameters. Apply Gauss–Newton approximation to estimate the shape parameters.
Springer.tar//Springer//Springer\10.1007-s00138-013-0490-4.xml:Geometric steerable medial maps:Medial manifolds Reconstruction Medial representations Surface Comparison:  1 Introduction  Medial representations have gained increased popularity at describing [ 40,  44– 47] and segmenting structures [ 10,  21,  48]. While other surface representation methods model only the external surface of objects, medial representations can model also the interior of the shape by providing a radial perpendicular coordinate that extends from the medial surface [ 6]. In medical applications, this is particularly useful for addressing the following topics.     . The radial coordinate of medial representations allows parameterizing [15, 34] the (possibly diseased) parenchyma of organs, as well as their internal vascular system, powerful sources of information in organ functionality, analysis and diagnosis    . In medical imaging, techniques such as M-Reps [13, 28] and CM-Reps [39, 51] have shown the potential to describe complex shapes in a versatile manner. Using information of a medial surface for medical imaging segmentation has proven to improve segmentation results [29, 37]. It follows that deformable medial modelling has been used in a variety of medical imaging analysis applications, including computational neuroanatomy [35, 53], 3D cardiac modeling [36] or cancer treatment planning [11, 33].    . In shape analysis, medial representations can provide better information than Point Distribution Models (PDM), since they can model not only the shape but also the interior variations [52]. Medial manifolds of organs have proved robust and accurate to study group differences in internal structures of the brain [34, 35]. They also provide more intuitive and easily interpretable representations of complex organs [50] and their relative positions [25]. . The radial coordinate of medial representations allows parameterizing [15, 34] the (possibly diseased) parenchyma of organs, as well as their internal vascular system, powerful sources of information in organ functionality, analysis and diagnosis . In medical imaging, techniques such as M-Reps [13, 28] and CM-Reps [39, 51] have shown the potential to describe complex shapes in a versatile manner. Using information of a medial surface for medical imaging segmentation has proven to improve segmentation results [29, 37]. It follows that deformable medial modelling has been used in a variety of medical imaging analysis applications, including computational neuroanatomy [35, 53], 3D cardiac modeling [36] or cancer treatment planning [11, 33]. . In shape analysis, medial representations can provide better information than Point Distribution Models (PDM), since they can model not only the shape but also the interior variations [52]. Medial manifolds of organs have proved robust and accurate to study group differences in internal structures of the brain [34, 35]. They also provide more intuitive and easily interpretable representations of complex organs [50] and their relative positions [25]. To accurately address the above topics, the medial representation has to achieve a good reconstruction of the full anatomy and guarantee that the boundaries of the organ are reached from the medial surface. It follows that anatomical medial manifolds must be simple enough to allow an easy generation of the radial axis, but complete enough to allow a satisfactory reconstruction of the whole volume. Representations of the original anatomical geometry are accurate as far as the extracted medial manifold satisfies [ 30]:     : The medial manifold has to have the same topology (same number of foreground objects and holes) as the original shape.    : The resulting medial shape should be one pixel wide. The specific connectivity used to consider two pixels as adjacent, should be considered at this point.    : The medial structure should lie as close as possible to the center of the original object.  Many methods are based on morphological thinning of either a binary segmentation or, to ensure medialness, the distance map to the boundary [ 4,  8,  20,  27,  30,  32,  38]. Such methods require the topological definition of a neighborhood set and conditions for the removal of  , i.e. voxels that can be removed without changing the topology of the object. These topology definitions are trivial in 2D, but their complexity increases exponentially with the dimension of the embedding space [ 22]. Further, simplicity tests alone only produce (1D) medial axis, so additional tests are needed to know whether a voxel lies in a surface and, thus, cannot be deleted even if it is simple [ 20,  30]. Finally, small changes in surface and simplicity tests or in the order in which voxels are traversed generate completely different surfaces (as illustrated in Fig.&amp;nbsp; ). : The medial manifold has to have the same topology (same number of foreground objects and holes) as the original shape. : The resulting medial shape should be one pixel wide. The specific connectivity used to consider two pixels as adjacent, should be considered at this point. : The medial structure should lie as close as possible to the center of the original object. Surface tests might introduce medial axis segments in the medial surface, which is against the mathematical definition of manifold. In addition, since medial axes hinder the calculation of the radial coordinate, such medial structures are ill-conditioned for the generation of proper medial representations [44]. Consequently, they may require further pruning before their use in subsequent applications [3, 20, 30]. However, there is no easy way to tell which manifolds can be safely removed without hurting the capability of representation of anatomical structures. Pruning strategies rely on additional topological tests for the removal of unwanted medial axis [3, 30] or surface segments [20]. However, an aggressive pruning might break topology or remove important medial manifold segments for shape reconstruction. Another line of research approximates medial surfaces from a tetrahedral mesh of a set of points sampled on the object boundary [2, 3, 12, 31]. These methods provide an elegant conceptual description but present some practical limitations. First, the density of the boundary sampling required for the right medial topology is a priori unknown. This leads to a selective refinement of the initial sampling based on topological and geometric tests which complexity significantly increases for capturing anatomical finest details [12]. Second, in 3D, it is not always guaranteed convergence to the medial surface [23], the approximation is prone to fail at branches [12, 31] and might generate multiple spikes [3]. Energy-based methodologies [1, 41, 42] constitute a completely different approach, since medial points are characterized as local maxima of a potential map. A main advantage over topological inspired methods is that the geometric properties of the medial surface are determined by the definition of the potential map. The shape representation introduced in Ref. [1] relies on a potential map that represents distances to the object boundary avoiding the generation of extra medial branches at boundary corners. Medial surfaces are reconstructed by tracking the gradient of the potential map from the object boundary points. This introduces two main limitations. On one hand, it is prone to give non-connected medial surfaces [1]. On the other hand, the step is so computationally expensive that it is hardly feasible over dimension two. In a previous work [41], we explored the potential of energy-based methods combined with a Non-Maxima Suppression (NMS) scheme for extracting medial surfaces. Experiments on a reduced set of synthetic shapes showed the capability of energy-based methods for surpassing the performance of thinning methodologies in terms of medialness (thanks to the energy-based scheme) and thinness (thanks to NMS binarization). A main concern was a significant drop of the response at branches and the generation of internal holes in medial surfaces, which violated the homotopy condition. In Ref. [41], homotopy was partially recovered using morphological closing. Recently, we identified the theoretical weaknesses of existing ridge detectors to define a novel energy [42] capable of producing medial surfaces achieving a compromise between simplicity and reconstruction power for representation and parameterization of anatomical structures [43]. In this work, we contribute to the computation of medial manifolds from a theoretical and practical point of view. From the theoretical side, we propose a two-step method that combines the best of Refs. [41] and [42] to get medial surfaces fulfilling the three main properties (homotopy, thinness and medialness). From a practical point of view, we define a benchmark for validating the quality of medial surfaces for medical applications. Our two-step method for medial surface computation is based on the ridges of the distance map. We use a medial map (called Geometric Steerable Medial Map, GSM2) based on ridge detectors that combines the advantages of steerable filters and level sets geometry [41]. In a second step, we use NMS [9, 42] to binarize GSM2 and obtain a one pixel wide medial surface. A binarization based on NMS does not depend on any topological definition and, given that regardless of the space dimension, it only requires one direction to be defined, it scales well across the number of dimensions. For a reliable implementation of the methodology in clinical applications, we devote special attention to the analysis of the parametric values and their impact in the performance of the whole strategy. Our experiments define a solid validation protocol for statistical analysis of the capability of our method to fulfill the three main properties that a medial surface should satisfy regardless of the medial topology and volume geometry. The quality of medial structures is assessed on a benchmark of synthetic shapes of known medial geometry. The performance of our method is compared to existing topological thinning and pruning techniques. Finally, we present an application for representation of abdominal volumes that shows the reconstruction capabilities and higher ability of GSM2 to locate pathological deformations. This experiment also illustrates the potential for pruning medial surfaces consistently with the object boundary geometry. The contents are organized as follows. Section&amp;nbsp;2 introduces our operator for reliable computation medial structures, including details about parameter setting in Sect.&amp;nbsp;2.2. Section&amp;nbsp;3 presents our benchmark for validation of medial surface quality. Section&amp;nbsp;4 reports our experiments on synthetic shapes (Sect.&amp;nbsp;4.1) and volume reconstruction of medical volumes (Sect.&amp;nbsp;4.2). Finally, conclusions and future work are exposed in Sect.&amp;nbsp;5.    2 Medial surfaces capturing the essential geometry of volumes  The computation of medial manifolds from a segmented volume may be split into two main steps (as depicted in Fig.&amp;nbsp; ): computation of a medial map from the original volume and binarization of such map. Medial maps should achieve a discriminant value on the shape central voxels. Meanwhile, the binarization step should ensure that the resulting medial structures fulfill the three conditions: medialness, thinness and homotopy. Distance transforms are the basis for obtaining medial manifolds in any dimension. The distance map is generated by computing the Euclidean distance transform of the binary mask representing the volumetric shape. By definition, the maximum values of the distance map are located at the center of the shape at voxels corresponding to the medial structure. It follows that the medial surface could be extracted from the raw distance map by an iterative thinning process [30]. Two alternative binarizations that scale well with dimension are thresholding and NMS. Thresholding keeps pixels with medial map energy above a given value. Therefore, it requires that the medial map is constant along the medial surface. Non-Maxima Suppression keeps only these pixels attaining a local maximum of the medial map in a given direction. Unless the medial map maxima are flat, NMS also produces one pixel-wide surfaces. Further examination of the distance map shows that its central maximal voxels are connected and constitute a ridge surface of the distance map. We propose using a normalized ridge map with NMS-based binarization for computing medial surfaces. Ridges/valleys in a digital -dimensional image are defined as the set of points that are extrema (minima for ridges and maxima for valleys) in the direction of greatest magnitude of the second-order directional derivative [18]. In image processing, ridge detectors are based either on level sets geometry [26] or image intensity profiles [14]. The map described in Ref. [26] defines ridges as lines joining points of maximum curvature of the distance map level sets. This operator yields homogeneous ridge responses with a high medialness discrimination power. It is computed using the maximum eigenvector of the structure tensor of the distance map as follows. Let   denote the distance map to the shape and let its gradient,  , be computed by convolution with partial derivatives of a Gaussian kernel:        being   a Gaussian kernel of variance   and   and   partial derivative operators. The structure tensor or second-order matrix [ 5] is given by averaging the projection matrices onto the distance map gradient:            for   a Gaussian kernel of variance  . Let   be the eigenvector of principal eigenvalue of   and consider its re-orientation   along the distance gradient,  , given as:    for   the scalar product. The ridgeness measure [ 26] is given by the divergence:       The above operator assigns positive values to ridge pixels and negative values to valley ones. The more positive the value, the stronger the ridge pattern. A main advantage over other operators (such as second-order oriented Gaussian derivatives) is that NRM   for   the dimension of the volume. In this way, it is possible to set a threshold common to any volume for detecting significant ridges and, thus, points likely belong to the medial surface. However, by its geometric nature, NRM has two main limitations. To be properly defined, NRM requires that the vector   uniquely defines the tangent space to image level sets. Therefore, the operator achieves strong responses in the case of onefold medial manifolds, but significantly drops when two or more medial surfaces intersect each other. In addition, NRM responses are not continuous maps but step-wise almost binary images (see Fig.&amp;nbsp; , left). Such discrete nature of the map is prone to hinder the performance of the NMS binarization step that removes some internal voxels of the medial structure and, thus, introduces holes in the final medial surface. On the other side, ridge maps based on image intensity are computed by convolution with a bank of steerable filters [ 14]. Each filter is defined by second derivatives of (oriented) anisotropic 3D Gaussian kernels:    for  , the coordinates given by rotations of angles   and   that transform the   axis into the unitary vector  . We note that by tuning the anisotropy of the Gaussian, we can detect independently medial surfaces and medial axes. For detecting sheet-like ridges, the scales should be set to  \(\sigma _z &amp;lt; \sigma _x = \sigma _y\), while for medial axes, they should fulfill  \(\sigma _z &amp;lt; \sigma _x &amp;lt; \sigma _y\). The second partial derivative along the   axis constitutes the principal kernel for computing ridges:       The response of the operator is calculated as the maximum response for a discrete sampling of the angulation and the scale:       for   given by   and   and  . A main advantage of using steerable filters is that their response provides continuous maps which ensure completeness of the surfaces obtained by NMS binarization. Besides, since they decouple the space of possible orientations for medial surfaces, their response does not decrease at self-intersections (see Fig.&amp;nbsp;, center). Their main counterpart is that their response is not normalized, so setting the threshold for binarization becomes a delicate issue. The analysis above shows that geometric and intensity methods have complementary advantages and shortcomings. Therefore, we propose [ 42] combining them into the following Geometric Steerable Medial Map (GSM2):       Given that the properties of medial surfaces are determined by the medial map, the advantages of GSM2 are twofold. On one hand, steerable filters provide a continuous approximation to NRM semi-discrete maps with a more uniform response at self-intersecting points. On the other hand, because NRM maps have a sharp response at central voxels, GSM2 still provides a highly selective response at ridges. In this manner, GSM2 generates medial maps with good combination of specificity in detecting medial voxels while having good characteristics for NMS binarization, which requires a continuous response to avoid internal holes. Finally, GSM2 allows the algorithm to focus on the (shape) relevant medial structures, while small ridges, due to noise in the shape frontier, can be ignored. While working with medical image segmentations, this means that GSM2 allows to gather the anatomically relevant surfaces while largely ignoring small spurious manifolds. The usage of NMS allows to obtain the voxels with higher ridgeness value and obtain a thin, 1-pixel-wide medial surface. NMS consists in checking the two neighbors of a pixel in a specific direction,  , and delete pixels if their value is not the maximum one. Let   be a generic ridge map, then its NMS map along the direction   is given by:        for   and  . A thresholding of   produces 1-pixel-wide surfaces. The search direction for local maxima is obtained from the structure tensor of the ridge map, . Its eigenvector of the greatest eigenvalue indicates the direction of highest variation of the ridge image and that direction is perpendicular to the medial surface plane. NMS image must be binarized to generate a final medial surface image. As GSM2 produces consistent values along ridges [thanks to its normalized origin (NRM)], the value  suited to binarize the ridges can be obtained via basic histogram threshold calculation, such as Otsu thresholding. Unlike most of existing parametric methods, the theoretical properties of GSM2 provide a natural way of setting parametric values regardless of the volume size and shape. Our method depends on the parameters involved in the definition of the map GSM2 and in the NMS binarization step. The parameters arising in the definition of GSM2 are the derivation, , and integration, , scales of the structure tensor  used to compute NRM and the scales, , and orientations, , defining the steerable filter bank in Ref. (5). The derivation scale  is used to obtain regular gradients in the case of noisy images. The larger it is the more regular the gradient will be at the cost of losing contrast. The integration scale  used to average the projection matrices corresponds to time in a solution to the heat equation with initial condition the projection matrix. Therefore, large values provide a regular extension of the level sets normal vector, which can be used for contour closing [16]. Since, in our case, we apply NRM to a regular distance map with well-defined completed ridges,  and  can be set to their minimum values,  and . Concerning steerable filters parameters, scale depends on the thickness of the ridge and orientations on the complexity of the ridge geometry. The selection of the scale might be critical in the general setting of natural scenes [23]. However in our case, SGR is applied to a normalized ridge map that defines step-wise almost binary images of ridges (see Fig.&amp;nbsp;, left). Therefore, the choice of scale is not critical anymore. To get medial maps as accurate as possible, we recommend using a minimum anisotropic setting: . Finally, orientation sampling should be dense enough to capture any local geometry of medial surfaces. In the case of using the minimum scale, eight orientations, , are enough. It follows that GSM2 is given by:       for NRM computed over   and   computed setting  . The parameters involved in NMS binarization step are the scales of the structure tensor  and the binarizing threshold, . Like in the case of NRM, GSM2 is a regular function whose maximums define closed medial manifolds, so we set the structure tensor scales to their minimum values  and . Concerning , it can be obtained using any histogram threshold calculation, since GSM2 inherits the uniform discriminative response along ridges of NRM.    3 Validation benchmark  To address the representation of organs for medical use, medial representations should achieve a good reconstruction of the full anatomy and guarantee that the boundaries of the organ are reached from the medial surface. Given that small differences in algorithm criteria can generate different surfaces, we are interested in evaluating the quality of the generated manifold as a tool to recover the original shape. Validation in the medical imaging field is a delicate issue due to the difficulties for generating ground truth data and quantitative scores valid for reliable application to clinical practice. In this section, we propose a benchmark for evaluating medial surface quality in the context of medical applications. The benchmark is divided into two tests. The first test evaluates the quality of the medial surface generated, while the second one explores the capabilities of the generated surfaces to recover the original volume and describing anatomical structures. Surface quality tests start from known medial surfaces, which will be considered as ground truth. From this surfaces, volumetric objects can be generated by placing spheres of different radii at each point of the surface. The newly created object is then used as input to several medial surface algorithms and the resulting medial surfaces, compared with the ground truth.
Springer.tar//Springer//Springer\10.1007-s00138-013-0491-3.xml:Counting moving persons in crowded scenes:People counting Crowd density estimation Video interpretation and understading:  1 Introduction  Knowing the number of people present in an area is such an important issue in the framework of video analysis applications that an increasing number of papers on this topic have been proposed in the recent past. Among the applications where this issue is encountered, we can cite video surveillance (an excessive number of persons in an area may constitute a security or safety hazard), public transportation monitoring, and business intelligence (e.g. determining how customers are distributed within a large shopping mall). Despite the fact that recently some pioneering systems have been made commercially available, further improvements are still necessary, especially concerning their generality and flexibility. Many aspects make the problem really challenging: systems are required to work in real time on general purpose computers, possibly in parallel on different video streams coming from megapixel cameras, so as to supply up to date information on crowd density. Of course, the computational load is crucial but it is a less important issue than the expected accuracy, especially if the output is used for safety issues. The estimation accuracy of the number of people must be sufficiently high, even in the presence of dense crowds. To this concern, it is worth pointing out that in these situations only parts of people bodies appear in the image; the occluded parts generally cause significant underestimation in the counting process; it means that the partial occlusions must be forecast and suitably taken into account starting from the information about the crowd density. Another crucial point is the unavoidable presence of perspective distortions: people far from the camera appear small while the near ones are significantly bigger. Therefore, counting methods must deal with these perspective issues, in order to obtain an estimation independent of the local scale of the image. Moreover, it is convenient that the system is able to work with uncalibrated cameras, as a fine calibration is generally time consuming and demands for suitable technical skills, not always possessed by the end user. Consequently, the availability of simple tuning procedures that require no knowledge about the internal organization of the algorithm and depend only on simple geometric properties derivable from the scene, is an extremely desirable feature. The literature presents two different approaches. The direct approach (also called detection-based), relies on the individual detection of the single persons, using adequate segmentation and object detection algorithms; in this way, the number of people is then trivially obtainable. On the other hand, in the indirect approach (also called map-based or measurement-based), the number of people is estimated by measuring the occurrence of suitably defined features that do not require the separate detection of each person in the scene; these features are then somehow put in relation to the number of people. The direct approach presents the advantage that people detection is often already performed on a scene for other purposes; as long as people are correctly segmented, the count is not affected by perspective, people densities and, to some extent, partial occlusions. On the other hand, people segmentation is a complex task, often providing unreliable outputs, especially in crowded conditions, which are of primary interest for people counting. Recent and well-known examples of the direct approach are [3, 16] and [18]. Indirect approaches are based on the extraction of suitably defined measurements and raise the problem of finding an accurate correspondence between these measurements and the number of people. Some methods belonging to this category propose to base the people estimation on the amount of moving pixels [6], blob size [9], fractal dimension [11] or other texture features [15]. Despite their simplicity, promising performance in terms of estimation accuracy are obtained by the approach proposed in [1, 4, 7]; all them have been submitted to the PETS 2009 and 2010 contests on people counting, and achieved very encouraging results. In particular, in Albiol’s paper [1], the authors use the corner points (detected using the Harris’ algorithm [8]) as features. Although Albiol’s method has proved to be quite robust, confirming the validity of its rationale, its accuracy decreases in presence of highly complex scenes, with large depth variations (people moving in the direction of the camera) and highly crowded moving groups. The authors in [7] explicitly deal with the perspective effects and occlusions; a trainable regressor (the -SVR algorithm) is used to obtain the number of people as a function of the moving points, a function made complex by the above mentioned effects. Experimental results demonstrated the improvements with respect to the method by Albiol et al. However, this is obtained at the cost of complex set up procedures for training the -SVR regressor. In this paper, we present a method that is able to obtain performance comparable to those obtained by the method in [7], but at the same time it is much simpler to implement and to set up. The proposed approach deals with the perspective effects on the estimation by subdividing the entire scene in horizontal stripes; the latter have a size depending on their distance from the camera, justifying the hypothesis of a linear relationship between the number of feature points and the people contained. Moreover, a fully automated procedure for training all the needed parameters is presented; a brief video sequence taking a person walking around in the scene is analyzed for directly obtaining all the parameters needed by the system. The organization of the paper is the following: in the next two sections, we describe the proposed approach and the procedure for automatic training. Finally, we discuss experimental results and draw some conclusions.    2 Rationale of the method  In the literature, there are several interpretations of the problem of people counting, frequently with significantly different assumptions on the conditions under which the counting system is expected to operate. Therefore, before starting to describe our method, it is important to highlight the specific assumptions underlying our method. We work under the following hypotheses (which, anyway, are those used by most of the approaches in the literature): the camera is stationary; the only objects present in the scene are people; we are interested in measuring only the density of the flow of moving people, not the direction, speed or any other information. The generality of these assumptions is proven by the presence of numerous datasets and benchmarking activities, including contests, sharing these hypotheses. It is also useful to clarify what we mean by crowded scenes. From the viewpoint of the difficulty of the task, a scene in which individual people images do not overlap with each other is easy to deal with: a tracking algorithm can be applied, and then the count can be easily derived from its outputs. Seldom occlusions also do not present a great problem, since many tracking algorithms can effectively follow a person across a short occlusion. The problem become tougher when the people are partially occluded for a significant portion of the time they appear in the scene. Therefore, we consider as crowded scenes those in which most of the people in the scene are occluded for most of the time they appear. Now, let us turn our attention to the proposed algorithm. The underlying idea of our approach is that each person framed by a camera can be represented through a small number of salient points. Such points might be located at the boundary of the silhouette and in some other points with high discontinuities as eyes, mouth, nose, clothes, etc. Under the assumption that people are the only moving objects in the scene, the total number of persons can be estimated from the set of the detected salient points exploiting their motion information. A first attempt at expressing the relation between the salient points and the number of people was proposed by Albiol et al.&amp;nbsp;[ 1]:       where   is the estimated number of persons in the scene,   is the number of detected salient points, which can be associated to persons by using motion information, while   is a proportionality constant. Actually, the hypothesis of representing a person by a constant number of salient points is too simplistic as it does not take into account several issues as, just to name a few, the optical set up of the camera, the position of the person in the scene with respect to the camera, different appearances of the same person across the sequence but also among different persons, obstacles between the person and the camera causing occlusions, variations of the background in time and space, etc. Thus, a generalization of Eq.&amp;nbsp; 1 is required in order to take somehow into account the above-mentioned factors. Of course, the more general formulation would be an expression of the number of people as a function of the set of the salient points (i.e.&amp;nbsp;depending not only on their number   but on all the information associated to them):       where   are the detected salient points. However, since function   is unknown, and must be constructed through a learning process, its structure must be somewhat constrained. In order to have a formulation that is more general than Eq.&amp;nbsp; 1 while keeping a simple structure that allows for the application of a learning algorithm, we can reinterpret Eq.&amp;nbsp; 1 as stating that each salient point   gives an additive contribution to the count  , which is assumed to be independent of the point itself:       This reformulation lends itself very well to a generalization that extends considerably the ability to incorporate other factors in the counting without loosing the structural simplicity of the equation: instead of assuming that the contribution of each salient point is a constant, it can be considered as a function of the point itself. Thus, the equation is reformulated as:       Notice that this formulation makes very easy to include information that is local to each single point (and to its neighborhood); the underlying assumptions are that local information is sufficient for the counting problem, and that this information can be combined additively. Of course, the   function may not use all the available information associated with each  , but only a suitably chosen subset. An approach using this formulation is presented in [7] where the  functions depends on the distance of the persons from the camera and on the local salient points density. The latter information is adopted as an implicit estimate of the amount of occlusion due to high crowd density. An -SVR regressor is used to learn  from a set of training frames in which the points belonging to each person are manually given a different label. This method showed to be much more effective than&amp;nbsp;[1] when used on difficult scenes. Nevertheless higher performance is obtained at the cost of complex training procedures that have to be carried out for each camera installation. Notice that the performance of this method cannot be improved straightforwardly by incorporating more information in : in fact, while in theory, the accuracy of  would be increased given an infinite training set; in real cases, the higher complexity of the -SVR estimator would require a significant increment of the actual training set as demonstrated in the framework of the Statistical Learning Theory by Vapknik and Chervonenkis. This in turn would make the system unsuitable for actual use, due to the higher costs for its training. In this paper, we propose a method that provides a good compromise between the two opposite requirements of effectiveness and ease of deployment. The result is a method that is able to perform as well as the sophisticated method in [7] but maintaining the overall deployment simplicity of the method [1]. In order to achieve this result, our method is based on the following ideas:             The       function depends only on the distance of the salient point from the image plane:                         where       is the distance. Note that implicitly       depends also on the camera settings, since it has to be trained separately for each camera.         is modeled as a piecewise constant function, by having the scene divided into horizontal bands and using a constant value for each band; this simplifies both the computation (avoiding the need to perform an Inverse Perspective Mapping) and the training.   The learning of  is performed using an original automatic procedure, that only requires the acquisition of a short video sequence; the training does not require that the persons in the video are manually segmented, as needed by other techniques.  In the following subsections, we will present the details of our method. First, we will describe the salient point extraction and classification, and then estimation of the people count. The automatic training procedure will be described in Sect.&amp;nbsp; 3. The   function depends only on the distance of the salient point from the image plane:         where   is the distance. Note that implicitly   depends also on the camera settings, since it has to be trained separately for each camera. is modeled as a piecewise constant function, by having the scene divided into horizontal bands and using a constant value for each band; this simplifies both the computation (avoiding the need to perform an Inverse Perspective Mapping) and the training. The learning of  is performed using an original automatic procedure, that only requires the acquisition of a short video sequence; the training does not require that the persons in the video are manually segmented, as needed by other techniques. In the literature, there is a wide variety of salient points detectors and descriptors. Some comparative studies in [12] and [13] have demonstrated that Hessian-based detectors have to be preferred with respect to other approaches as they provide better performance in terms of both stability and repeatability. Drawing from the observation that real-world objects are composed of different structures at different scales, Hessian-based detectors, as many other ones, find salient points by analyzing the image at different scales. In the category of the Hessian-based detectors, the SURF algorithm in [2] has gained a large popularity since its first appearance, because of its effectiveness and efficiency. The interest points found by SURF are more independent of scale and, thus, of distance from camera than the ones provided by other detectors. They are also independent of rotation, which is important for the stability of the points located on the arms and on the legs of the people in the scene. The points detected are successively classified as static or moving. Under the assumption that persons are the only moving elements into the scene, the classification is aimed at pruning the static points, as they are not associated to people. We explore two different types of approaches for the classification of salient points: the  and the . With the first one, we estimate the motion vector associated to each salient point and discriminate between static and moving ones on the basis of the vector magnitude. With the second approach, we rely on the color intensity variations between a set of homologous pixels around the salient point in the current and in a previous reference frame. We expect that the first approach should assure better classification performance, but at a higher computational cost, than the second approach. Each salient point   detected in the position   in the frame at time   is attributed a motion vector  , calculated with respect to a reference frame at time  - , and is consequently classified:            The motion vector   is obtained using a block matching technique, by which the block   is matched to a set of candidate blocks in a reference (earlier) frame. Then,   is determined as the displacement of the best matching block in the reference frame with respect to the location of the block in the current frame. Matching is based on a criterion that measures the dissimilarity between two blocks. We used squared blocks (with sides of   pixels) and evaluated the dissimilarity of the block centered in   in the current frame   and a block shifted by   with respect to   in the reference frame  , as the mean of the absolute values of the color differences of the pixels in the two blocks:            where   is the number of the image color channels and   is the intensity value of the pixel in the  th color channel. Furthermore, block matching requires the definition of suitable searching algorithms for exploring a search area; the latter, possibly containing the candidate blocks in the previous frame, can be made more or less wide. A fully exhaustive approach extends the search everywhere in the frame with a significant computational expense without considering that the motion of the objects of interest (the persons) is much smaller than the frame size. Simply limiting the search procedure to a window centered on the considered block with a size slightly larger than the maximum possible motion of the persons would significantly reduce the number of candidate blocks without introducing estimation errors. Considering a squared search area with side , the number of candidate blocks analyzed following this approach is . Hereinafter, we will refer to this motion estimation algorithm as . A further reduction of the processing time is possible by adopting other search methods [ 10] (as  ,  ,  ,&amp;nbsp;...) which determine a suboptimal solution to the problem reducing the number of candidate blocks to analyze to   where   is a parameter called the step size; the search area has a size of   pixels. In Fig.&amp;nbsp; , it is shown an example of the analysis performed by the   algorithm. In order to reduce the effect of noise, it is possible to incorporate zero-motion biasing into the block matching technique. The current block is first compared with the block at the same location in the previous frame, before doing the search: if the difference between these two blocks is below a threshold (), the search is terminated resulting in a zero motion vector without analyzing the neighbor points. Zero-motion biasing allows to reduce the false motion, due to image noise, and the processing time, by eliminating searches; unfortunately, it may produce some false negatives by assigning a zero motion vector to a non static point. Hence, the right value of  has to be determined as the best trade off between the two opposite effects. Since we are not interested to the exact value of the motion vector, but only in discriminating between moving and static points, we propose to adopt an approach that simply classifies a point as static or moving if the difference between the blocks centered on it in the current and in the reference frame is below or above  . In particular,            where   is the interest point, while   measures the dissimilarity of the block in   in the current frame and the homologous block in the reference frame. We expect that this approach should preserve the same classification accuracy of the previous approaches, but could significantly reduce processing time as it has to analyze just one candidate block. According to the assumptions made at the beginning of this section, the total number  of persons into the scene is estimated using Eq.&amp;nbsp;5, which relates the contribution of each salient point  to its distance from the image plane  through the use of the  function. The distance from the image plane has been considered because, assuming that the camera lens has a negligible nonlinear distortion, the apparent size of the objects depends on this measure. The value of  could be computed from the position of the point within the image using the Inverse Perspective Mapping (IPM) [17], assuming that the points lie approximately on a common ground plane; however, this technique would require an accurate calibration of the camera parameters, which would complicate the deployment of the system. In order to overcome this problem, we adopt an approach based on two considerations exposed below. First, if the camera is properly aligned with the ground plane, the set of points having a given distance from the image plane will lie on a horizontal line of the image; this alignment is very easy to obtain during the camera installation if a great accuracy is not required, and it is routinely performed on cameras because a human viewing the scene would find it somewhat disturbing if it is badly aligned with the horizon. Thus, if we consider the points that are within a same narrow horizontal band of the image, we can assume that they have a very similar value for , and so we do not incur in a significant error if we use the same  value for all of them. Second, while the above-mentioned approximation is more accurate the narrower the band is, there is no real advantage in having a band whose height is smaller than the apparent height of a person. In fact, in that case, the accuracy on the estimate of  would be limited anyway by the error due to the fact that the salient points do not lie all on the ground plane, and, to a lesser extent, to the imperfect alignment of the camera and to the nonlinear distortion of the lens. Based on this considerations, our method partitions the image into horizontal bands whose height corresponds to the apparent height of an average person. Note that this height is not uniform across the image: it is larger at the bottom, that corresponds to an area closer to the image plane, and becomes smaller when approaching the horizon line. The value of  is assumed to be constant for all the salient points lying in a same band, and is determined using a training algorithm that will be presented later. Accordingly, Eq.&amp;nbsp; 5 is modified as:       where   is the band the point   belongs to. Notice that since the values of  are obtained using a training procedure, there is no need to compute explicitly the distance from the image plane corresponding to each band, and thus to obtain the calibration parameters for the IPM. Once the set of the weigths  of the bands have been determined, it is possible to calculate the total number of persons in the scene by Eq.&amp;nbsp;9.    3 Automatic training procedure
Springer.tar//Springer//Springer\10.1007-s00138-013-0492-2.xml:Using multiple sensors for reliable markerless identification through supervised learning:Interactive surfaces Human–machine interaction Machine learning:  1 Introduction  The Ca’ Foscari University in the last years has issued several art exhibitions in its own premises, augmenting with multimedia technology the presentation of selected artworks for improving the visitors’ satisfaction (Fig.&amp;nbsp; ). The early experiences [27] were built around a set of video installations distributed along the exhibition path, and interactive multimedia mobile devices to act as visitors’ companions, providing contextual information to augment the visitors’ knowledge about the exhibition contents. The overall initiative was part of a joint project, the  project, involving the Department of Environmental Sciences, Informatics and Statistics, and the Department of Philosophy and Cultural Heritage. The outcomes of such a mixture of traditional museography and interactive multimedia have been evaluated, in terms of visitors’ satisfaction, subjectively by questionnaires and objectively by tracing systems embedded in the mobile devices, logging user interaction styles and exploration of the exhibitions’ content; evaluation summaries are reported in several papers describing the project [2, 7, 27]. Recently, the project took a new road. The exhibition “William Congdon in Venice (1948–1960): An American Look”, held from May 5 to July 8, 2012, focused on the long stay of the artist, one of the protagonists of the American  , in Venice. The exhibition was showing the Congdon’s paintings together with giant photos of selected artworks of his Venetian period, message boards presenting letters and sketches, and with the projection of drawings, notes and graphic works. The tight relationship between visual art and Venice, celebrated during several centuries by many artists, has led the curators to change the role of technology in the exhibition, moving from individual mobile information devices to a mix of private and shared experience in selected information areas: the   aims at bridging the evolution of Venice in centuries with the artistic expressions, such as painting, photography and movies, depicting the town. To this end, three large interactive tables, each equipped with vision-based systems to track objects placed on it, have been built (Figs.&amp;nbsp; ,  ). The tables were decorated with maps of Venice pertaining to different historical and artistic periods: Sixteenth Century, Eighteenth Century, and the current days. The visitors could explore the city of Venice by placing and moving physical objects (cursors) on the map, activating the projection, on the wall surrounding the installation, of artworks, photographs and, for the more recent map, movies, related to the period referred by the map and to the location pointed by the cursor. On the Sixteenth Century table cursors were actually modified smartphones. These active devices were displaying information themselves and could also be used as personal devices for navigating the exhibition content independently from the table. The project offered several challenges both on the museographic and on the technological side. From the museographic point of view, the presence of a shared interaction space required the definition of rules to associate the projections to the cursors’ position and motion, allowing visitors to recognize the effect of their exploration. From the technological point of view, the size of the installation and the very dense and detailed maps decorating the tables posed serious constraints on the use of consolidated techniques based on fiducial markers; hence, a specially crafted technique was devised to associate the blobs as seen by the tracking system to the correct device. In the following sections, after a review of the relevant literature, we present a detailed overview of the proposed setup. A first set of experiments will then point out the infeasibility of traditional marker-based recognition methods and thus, the need for an alternative identification technique. Subsequently, we give an in-depth description of the machine learning approach used to classify the observed blobs by virtue of the relations between positional information and sensor data. Finally, an extensive experimental section assesses the viability of the proposed multiple sensors approach and investigates some interesting aspects related to the synchronization between the different sources of data.    2 Related work  Interactive multiuser tables and walls have proved to be a viable system to foster user participation and interest in many shared environments, among which educational and cultural environments such as museums and exhibitions have a leading role. They favor interaction among users and induce a sort of serendipitous discovery of knowledge by observing the information exploration performed by other users. Their use has been analyzed and evaluated in entertainment as well as in educational applications [1, 9, 20, 21]. Several technologies have been tested and evaluated in lab environments as well as in commercial products. Most of them only recognize and interpret the user touch and the placement of objects over the table, while the problem of associating the sensed information with specific users is solved in a few cases, often with sensors and equipment placed outside the table itself. Among the early multitouch, multiuser surface implementations ™, an interactive table produced by Mitsubishi Electric Research Labs (MERL), was able to recognize up to four different users by matching signals captured at users’ touch by small antennas placed under the table surface with receivers capacitively coupled to the users through their seats [12]. Such a user identification technique, while effective and robust, is oriented to a structured collaboration among the users and is not easily applied to highly dynamic environments like museums and exhibitions. A different technology is used in the Microsoft  interactive table, which uses five cameras and a rear projector to recognizes finger gestures as well as objects equipped with tags placed on the table surface. Gloves equipped with fiduciary tags are proposed by Marquardt et al. [25] to identify both what part of the hand and which user caused the touch. The  (FTIR) technology [17] has received greater attention as a cost-effective technology, able to trace many users with high frequency response; FTIR is based on infrared lateral illumination of a translucid surface, able to reveal small deformations caused by finger pressure. The problem of matching touches with users must, however, be solved by additional tracking systems based on analysis of the user position with additional external cameras and is subject to errors [14]. New opportunities for interaction in large shared spaces come from novel and promising methods for vision-based multitouch adopting depth sensors such as the broadly available Microsoft Kinect [13, 24]. Initially dedicated to active gaming, Kinect is now often proposed as the key component of systems implementing a more “natural” interaction style. While arm and body gesturing is viable in many situations, it requires a clear identification of the user and his/her interaction space, and is therefore unsuitable in crowded spaces or when multiple users are involved. Moreover, external tracking devices must be placed over the table and might not be suitable for scenarios where a compact and self-contained system is needed. Further, even when specially crafted physical objects are used instead of hands or fingers, the depth-based tracking is harshly hindered by body and arm occlusion and does not support recognition out of the box. In more recent years the widespread diffusion of mobile devices with rich interaction capabilities has suggested to couple personal (small) and public (large) screens for enhanced multiuser interaction. The personal devices are used both for input, allowing each of several users to provide direct interaction and own information to a shared system, and for output of local private information; the large shared screen acts as a collaboration and information sharing environment, guided by the input provided by the single users [15, 16]. The Calisto system [5] is a multitouch kiosk which can share information with user personal  devices; users can drag files and folders on the kiosk screen to , icons representing the personal devices connected; transfer occurs via HTTP and creates an information discovery environment shared among the kiosk users. Gestures on the personal devices are also interpreted to cause feedback from individual users to the shared kiosk. In this system, the identification of the user occurs when they connect to the kiosk. In [29] the coupling between a personal smartphone and a large shared screen occurs by touching icons with the smartphone and activating the flashlight. In [33] a shared environment is synthesized and projected on a wall by summing the actions performed by several users on their private devices. Users can select, upload and download objects in the shared space representing files, generating a collaborative environment. In such systems the user identification is easy because in the first two cases dedicated areas on the shared device are associated to individual private devices, while in the third system the shared space content is derived from the interaction only occurring on personal devices. Since personal devices like smartphones and tablet computers are often equipped with a set of internal sensors, it has been quite natural for researchers to take advantage of them to expand the range of possible user interactions in new and very creative ways. For instance, in [23] the author proposes to use the magnetic sensor to play virtual musical instruments with a touch-less gesture-based interaction. This is possible as the user wears on his finger a little magnetic ring that exerts on the internal compass sensor an influence more significant than that of the Earth’s magnetic field. In [11], a technique that uses accelerometer data to build a gesture-based control schema is presented. Specifically tilt gestures are used to manage a continuous interaction between a mobile phone and a larger display, and throwing gestures are metaphors for transferring documents between a handheld device and a storage system. The phone flashlight (when available) is used, besides in the already cited work by Schöning et al. [29], in&amp;nbsp;[19, 31] for light-based interaction between mobile phones and external large screens. In particular, the interaction can happen without the need for a wireless connectivity and thus is especially suitable for public spaces. While in the aforementioned examples the sensors have been used with a direct mapping between the values gathered and the actions triggered, in many cases this relation is much less defined. This is especially true when complex gestures or patterns that are not entirely predictable must be recognized. In those scenarios, the preferred solution is often the use of machine learning techniques. In [18], a large number of complex gestures is recognized by means of a fusion method based on features extracted from the time-domain and frequency-domain. Features are first fused together and dimensionally reduced through principal component analysis (PCA). Afterwards, a multi-class support vector machine (SVM) is used to classify the obtained vectors. The authors of [35] address the problem of gesture recognition with a technique called frame-based descriptor and multi-class SVM (FDSVM). It is a  approach that employs the SVM with a gesture descriptor combining spectral features and temporal features derived from 3D-accelerometers data. Gaussian random noise is added to the data to obtain a user-independent classification without the need of acquiring data from many different sources during the learning phase. Finally, in [32], the sensor data are collected during an entire day of normal mobile phone usage. An SVM-based classifier is used to recognize many common physical activities with the aim of obtaining a complete monitoring of the user’s lifestyle.    3 The context: a map-based multiuser art browser  The art exhibition mentioned in the introduction provided an opportunity to design and build three large () interactive tables equipped with the standard set of input and output devices such as cameras for blob detection and projectors for information display. While a focused effort has been made in order to create a generic and reusable system, the design of the table hardware and software is still the result of requirements partly bound to the interaction functions, partly imposed by the environment. Each table presents a high resolution diaphanous map of Venice (Fig.  -2) printed on a thick glass surface (Fig.  -1). A total of three tables has been built. Each one portraits a different period of the city history. The well-known lithography made by Jacopo De’ Barbari [ 28] that represents an aerial view of the island of Venice has been selected to represent the Sixteenth Century. The Napoleonic cadastral map was chosen to provide an overview of the city during the Eighteenth Century. Finally, a satellite view has been used to represent the modern era. The required interaction is based on placing or moving on the table objects representing the virtual visitor position in the town. These objects, that in the following will be referred to as , are smartphones that are equipped both with a display and some internal sensors, such as accelerometers and compass (Fig. -3). Relevant places are associated to paintings by artists of different epoques, portraying the city views related to the location selected by the user and the historical period expressed by the specific map. They are shown as pulsating spots on the map, attracting the user attention (Fig.  a). As the user moves the cursor over a relevant place, the spot is highlighted to confirm its detection and the corresponding artwork is projected on the walls surrounding the installation (Fig. -5); relevant information about the author and the picture are presented on the display of the cursor. Further, the cursors can be lifted from the table and used as a gesture-based remote control that allows the user to get near the projected paintings and still continue the browsing activity, updating the projection. To allow more users to interact with the table and to experiment a simpler interaction style, suitable for less skilled users, a few  cursors have also been used: they are small rectangular boxes decorated with the project logo, which can be placed and moved on the table like the active cursors, causing the same behavior of the active devices. In this case, obviously, there is no information processing and display on the cursor. Additional visual cues are generated if the cursor is placed near to a spot but not directly over it. A stream of light is generated, moving from the cursor to the nearest spot, suggesting a move (Fig. b). Each user is independent; at most 5–6 users can operate at the same time, distributed along the table sides with a comfortable amount of surrounding space to experience an open view of a part of the map. Such distribution assures also that the user position around the table allows the placement of the video projections on the walls in a regular and pleasant layout. Such layout is automatically arranged by an algorithm that tries to optimize space usage by dynamically resizing pictures and trying to place a new artwork directly in front of the user that required it. Since several new paintings could appear at the same time, special care must be applied to ensure that the user receives enough cues to visually associate the newly displayed painting with the action he/she just completed. The tables are operated by the users without help, but the installations are guarded by cultural mediators, personnel available to visitors to help them in case of need, and ready to explain both the table functions and the associated content. As for any multitouch system, one of the most critical choices is related to the technology used to detect the user interaction with the table surface. Given the large active surface (measuring 300  200&amp;nbsp;cm) using a touch sensitive overlaid plane was not an option for economic and practical reasons. Also the adoption of an external vision-based tracking system was not viable, since the large number of simultaneous users would cause unpredictable occlusion conditions. In this regard, our choice has been directed to classical blob detection by placing a number of cameras inside the table and oriented toward the surface. Specifically, we used infrared cameras (i.e. cameras that leave out the visible light) equipped with an 850&amp;nbsp;nm thresholded filter. This kind of camera is usually coupled with some source of infrared illumination, so that the visible light produced by the internal projector does not interfere with the blob-detection. To this end, two illumination techniques are usually adopted: the frustrated total internal reflection (FTIR) and the diffused illumination (DI). FTIR is based on the interference between the object in contact with the surface and infrared light tangentially diffused inside the thickness of the surface layer and trapped by the total reflection angle. Such interference causes the light to change direction and thus to escape from the surface layer toward the camera. By contrast DI implies the naïve illumination of the objects via a diffused light that passes through the surface and is reflected as it encounters an infrared-reflective obstacle.
Springer.tar//Springer//Springer\10.1007-s00138-013-0493-1.xml:Rapid blockwise multi-resolution clustering of facial images for intelligent watermarking:Evolutionary computation Graphics processing units Intelligent watermarking Clustering Population-based incremental learning Multi-objective optimization:  1 Introduction  Securing grayscale images is critical especially nowadays with the growing number of images transmitted and exchanged via the Internet. Grayscale images are widely used as medical images, biometric templates, or financial documents. These application domains rely on large volumes of grayscale images that need to be secured. Digital watermarking has been used to secure grayscale images by embedding watermarks into these images to ensure their authenticity. Finding optimal embedding parameters is a complex problem with conflicting objectives of image quality and watermark robustness. Image quality is associated with the distortion resulting from watermark embedding while watermark robustness relates to the resistance of the embedded watermark against manipulations on the watermarked image. In intelligent watermarking (IW), different computational intelligence techniques have been proposed to find optimal embedding parameters. Authors have proposed using evolutionary computation optimization techniques like Genetic Algorithms (GA) [15], Particle Swarm Optimization (PSO) [21], and combinations of GA and PSO [8] to find embedding parameters that maximize the fitness for both quality and robustness [17]. Most of these traditional methods are based on representing all cover image  pixels blocks in candidate solutions according to their positional order and iteratively improve the fitness until convergence is reached [15]. These methods use single aggregated objective [15, 19]. To date, few authors have proposed mutli-objective formulation [4, 12], where the two objectives are optimized simultaneously and multiple non-dominated solutions are located forming a Pareto front. This last approach provides more operational flexibility. Most of the EC techniques suffer from convergence problems due to the complexity of the search space associated with high-resolution images. This traditional representation [15] assumes at least 1 bit per block and equal number of bits embedded in all image blocks using even embedding scheme. Handling constraints for large-sized candidate solutions is computationally complex. Such constraints avoid using the same frequency coefficient for embedding more than once for the same image block. In this research, grayscale face image templates are considered to be secured. For this type of images, the traditional representation [15] implies that shifting slightly the face pixels inside the image is considered as a new optimization problem, and consequently costly re-optimizations are required. And thus watermarking a stream of high resolution grayscale face images results in a stream of computationally complex optimization problems. In this paper, a blockwise multi-resolution clustering (BMRC) framework is proposed for rapid intelligent watermarking. The proposed technique is capable of finding optimal embedding blocks and specify their embedding parameters in a computationally efficient manner. It also can handle watermarks of different lengths using proposed robustness score (RS) metric for block clusters. This proposed metric is used also to identify the optimal embedding clusters of blocks. BMRC is based on a multi-objective formulation which satisfies the trade-off between watermark quality and robustness and thus allows adaptability for different application domains, where the objectives priority vary, without the need for costly re-optimizations. During the training phase, the multi-objective optimization results, obtained on few training images, are stored in an associative block cluster memory (BCM). After the full optimization results are obtained, the optimal solution is selected from the resulting Pareto front based on the application domain priorities. This optimal solution represents optimal embedding parameters for all training image  pixels blocks; it is used to collect the most frequent embedding parameters for all image blocks having the same texture. This information is stored for multi-resolution clustering of face image blocks based on their texture features, where clustering resolution represents the number of clusters. The order of embedding for these multi-resolution block clusterings is determined using the proposed RS metric. BMRC uses an incremental learning scheme in training phase, such that the multi-resolution clusterings and their corresponding most frequent embedding parameters are calculated for the first training images and get updated for subsequent training images. During generalization phase, texture features are extracted from  pixel blocks of the unseen stream of images; then these blocks are categorized using the recalled multi-resolution clustering prototypes from BCM. The order of utilizing blocks categories for embedding is identified using RS, which is also used to calculate the empirical embedding capacity for these categories. The empirical embedding capacity is dependent on the watermark length and RS of block clusters identified in the image such that the block cluster of highest RS has the maximum embedding capacity, and it gets decremented until a threshold  of RS is reached. The watermark fitness is calculated for different resolutions stored in BCM, and then solutions are ranked to choose the optimal clustering resolution for each face in the stream. Proof of concept simulations are performed using the PUT database [7] of high-resolution face images and compared against reference method in terms of complexity and quality of solutions. Simulation results demonstrate that BMRC results in a significant reduction of the computational cost for IW by replacing costly optimization operations with associative memory recalls. The resulting solutions have nearly the same quality and robustness as those obtained with full optimization of each face image. The performance of BMRC is evaluated for different watermark length, and the robustness objective is considered of higher priority to reach up to 99.9&amp;nbsp;% of bits restored after manipulating the watermarked face image. A sensitivity analysis is performed on BMRC tunable parameters to evaluate the impact of these parameters on both the framework performance and the associative memory size. Parallel implementation using graphics processing units (GPU) is employed for the most complex functionalities of BMRC to evaluate its impact on the overall performance. This paper is organized as follows: Sect. introduces watermarking concepts, metrics, and terminology needed to understand the paper; then an overview for different IW approaches proposed in literature, and it also introduces population-based incremental learning (PBIL), and texture feature extraction for grayscale images. Section 3 describes the proposed framework for rapid blockwise IW for high-resolution grayscale facial images. The proposed experimental methodology is described in Sect.&amp;nbsp;. Results and analysis are presented in Sect.&amp;nbsp;5.    2 Intelligent watermarking of grayscale images  Digital watermarking is deployed in many domains to assure integrity and authenticity of the original signal via fragile and robust watermarking, respectively [17]. A fragile watermark is a type of watermark to ensure integrity, but it is broken if the watermarked image is manipulated or altered, while the robust watermark ensures authenticity and can be extracted after manipulating the watermarked image. Semi-fragile watermark considered in this paper is satisfying a trade-off between both the distortion introduced by the watermark and the watermark resistance to manipulations. Most digital watermarking techniques proposed for grayscale images use different transform domains to embed a watermark that minimizes the visual impact and to deal with the uncorrelated coefficients in the transform domain. The most commonly used transform domains in watermarking literature are discrete cosine transform (DCT) [15] and discrete wavelet transform (DWT) [8]. Using DCT transform inheriting robustness against JPEG compression which is based on DCT transform as well, the host image is divided into small blocks of pixels ( pixels), transformed to frequency domain, and watermark bits are distributed among these blocks by changing frequency bands coefficients of these blocks according to the value of the watermark bit to be embedded. Few authors have considered other transforms based on DFT [9] to improve robustness against geometric attacks since these transforms are more resistant to geometric manipulations. Digital watermarking system can be characterized using three main aspects: watermark quality, watermark robustness, and watermark capacity. Watermark quality measures the distortion resulting from watermark embedding; there are limits defined in literature [20] where the human vision cannot recognize the distortion resulting from the embedding. Watermark robustness measures the resistance to different manipulations and processing on the watermarked image; this is measured by the correlation between the extracted watermark after the manipulations and the original watermark. Watermark capacity measures the number of embedded bits per block given thresholds for watermark quality and/or watermark robustness. Watermark quality and robustness are commonly measured using weighted peak signal-to-noise ratio (wPSNR) and normalized correlation (NC), respectively. Peak signal-to-noise ratio (PSNR) is calculated between original image   and watermarked image   of resolution   using the mean squared error (MSE), where  , and   represents the index of pixels for width and height, respectively:            Weighted PSNR uses an additional parameter called noise visibility function (NVF) which is a texture masking function defined by Voloshynovskiy et al. [ 20]. NVF arbitrarily uses a Gaussian model to estimate how much texture exists in any area of an image. For flat and smooth areas, NVF is equal to 1, and thus wPSNR has the same value of PSNR. For any other textured areas, wPSNR is slightly higher than PSNR to reflect the fact that human eye will have less sensitivity to modifications in textured areas than smooth areas. Weighted PSNR shown in Eq.&amp;nbsp; 2 is proposed in the latest benchmarking for watermarking systems introduced by Pereira et al. [ 11].       The normalized correlation (NC) is calculated between embedded watermark   of resolution   where   and   represent the index of pixels for width and height, respectively, and the extracted watermark from the attacked image   using Eq.&amp;nbsp; 3. The watermark embedding/extracting algorithm considered in this paper is an algorithm proposed by Shieh et al. [ 15], where the original cover image is not required during extraction of the watermark; this reduces the required space needed to store the original cover images. Using this algorithm, the cover image   to be watermarked of size   is divided into   blocks and transformed into DCT domain where the resultant matrix   for each image block at row   and column   of cover image blocks has the upper left corner as DC co-efficient and the rest of matrix are the AC coefficients, where the DCT coefficients index   ranging from 0 to 63 for   blocks are placed in zigzag order. The DCT-transformed image   is then used to get the ratio between DC and AC coefficients   for all AC coefficients   using       Then polarities   are calculated using the Eq.&amp;nbsp; 5.            Next, the watermarked DCT coefficient   is obtained using the Eq.&amp;nbsp; 6. The index of DCT coefficients modified belonging to   referred to as embedding bands for block   with   equal to  . The embedding capacity for block   is defined as   in bits per block, and the watermark bits allocated for block at   row and   column  , where   represents the index of set of embedding bands and finally the watermarked image   is obtained using the inverse DCT for  . Modifications in certain frequency bands are less perceptible than others, and modifications in other frequency coefficients are more robust against manipulations. Many authors have, therefore, proposed using different evolutionary optimization techniques to find optimal frequency bands for embedding the watermark bits to maximize the fitness for both watermark quality and robustness objectives. The embedding parameters for frequency domain watermark embedding and extraction algorithms are represented using frequency coefficients altered due to watermark bits embedding which are commonly called embedding bands in literature. EC methods like GA and PSO have attracted authors’ attention due to simplicity of these techniques and the ease in adapting them to many different types of watermarking systems. Moreover, EC does not assume a distribution of the parameters space represented by selected frequency bands for embedding [15]. EC methods, inspired by biological evolution, are generally characterized by having candidate solutions which evolve iteratively to reach the target of optimization based on the guidance of objectives fitness evaluation. These candidate solutions are referred to as chromosome in GA and more generally individuals of the population of candidate solutions. In these traditional methods, all cover image blocks are represented in optimization candidate solutions, and the selected embedding bands are altered along optimization iteratively to maximize both the watermark quality fitness (QF) and robustness fitness (RF) simultaneously. All cover image blocks have to be represented in the optimization candidate solutions as shown in Fig.&amp;nbsp;  to allow distribution of watermark bits among cover image blocks. The optimization problem can be formalized as            where   represents the   block in cover image of resolution  , the total number of blocks equal to   represents the  th embedding band for block   , and the embedding capacity for block   is  . The first constraint considered ensures avoiding DC coefficient   for embedding, and the second constraint considered ensures avoiding using the same embedding bands for the same image block. The traditional optimization formulation [15] for IW problem implies that embedding capacity are equal for all blocks of the cover image and at least 1 bit per block is embedded. In watermarking literature, this is referred to as even embedding scheme where the embedding capacities are equal for all cover image blocks. From watermarking perspective [22], uneven embedding scheme is more suitable for better watermarking fitness where higher textured blocks are utilized for more bits to embed and smooth textured blocks are avoided for embedding. Many authors have proposed aggregating both quality and robustness fitness into one objective for simplicity utilizing different aggregation weights for the objectives to resolve the issue of different scaling of these different types of objectives and to favor one objective over the others using these weights. Shieh et al. [15] have used Genetic Algorithm for optimizing the aggregated fitness for both quality and robustness, while Wang et al. [21] have used Particle Swarm Optimization for optimization. Other authors [8] have proposed combining both GA and PSO for optimizing the aggregated fitness for quality and robustness. Different formulations for watermark embedding optimization have been evaluated and compared in literature [12]. Multi-objective formulation corresponds to the trade-off among different quality and robustness objectives. It provides multiple optimal non-dominated solutions (Pareto front) which gives a system operator the ability to choose among multiple solutions to tune the watermarking system [12] resolving the challenge of operating flexibility pointed out in [5]. The data flow of the multi-objective formulation is shown in Fig.&amp;nbsp; , where this formulation deals efficiently with conflicting objectives like watermark quality and robustness against different attacks. In this formulation quality fitness   and robustness fitness   are optimized simultaneously without favoring any objective over the other using aggregation weights. The fitness for both objectives are improved iteratively till stop criterion is reached, and optimal embedding bands   for all blocks   are concluded from a single non-dominated solution belonging to the resulting Pareto front. For single objective formulation, the fitness of both quality   and robustness   are aggregated into one objective to be optimized. The objectives can be aggregated using weighted sum [ 15,  18] or Chebyshev aggregation [ 19]. Population-based incremental learning (PBIL) method was proposed by Baluja in 1994 [1]. Its salient feature is the introduction of a real-valued probability vector. The value of each element of the vector is the probability of having a 1 in that particular bit position of the encoded chromosome. PBIL has proved efficiency with IW problem where utilizing the previous experience in subsequent generations ensures better convergence properties [12] compared with GA and PSO. Also the probability vector is considered a good representation for optimization landscape that can be recalled to reproduce the landscape without the need to go through complex iterations. Bureerat and Sriworamas [2] proposed changes to PBIL algorithm to handle multi-objective optimization problems. In this algorithm the probability vector is replaced with probability matrix, where each row in this matrix represents the probability vector to create sub-population individuals. Texture features are extracted from the grayscale images using  pixels blocks granularity. The most commonly used texture features can be classified into spatial features like gray-level covariance matrix (GLCM), and other domains features like discrete cosine transform (DCT), and gabor wavelet transform. Taking the computational complexity into consideration, using spatial features would have lower complexity compared with other domains’ features like DCT domain. However, the watermark embedding and extraction methods based on spatial domain would have lower robustness against different image alterations and lower watermark embedding capacity. In literature, many authors have considered DCT for extracting texture features of grayscale images. Yu et al. [24] have proposed zoning method using the most significant 39 coefficients. Sorwar and Abraham [16] have proposed selecting two different texture features based on lower coefficients, and directional edges coefficients, where the coefficients of the most upper and left regions represent vertical and horizontal edge information, respectively. Therefore, the traditional methods described in this section lack the support for uneven embedding to utilize more textured areas for more bits to be embedded. Also smaller watermarks need padding to satisfy the constraint of having at least 1 bit per block capacity for traditional formulations [15]. Moreover, the computational complexity is not affordable using modest computational resources, and thus a novel formulation is essential for IW a stream of high-resolution images. Most of authors in intelligent watermarking were focusing on single image watermarking and did not pay enough attention to high volume of grayscale images watermarking. Only for high-volume bi-tonal images, Vellasques et al. [18] proposed a high-throughout watermarking by considering optimization of a stream of images as single dynamic optimization problem. This approach is not efficient with grayscale face images [14] due to positional representation of blocks.    3 Rapid blockwise multi-resolution clustering  BMRC shown in Fig.&amp;nbsp;  finds optimal embedding bands in textured blocks for a stream of high-resolution face images using modest computational complexity. This is accomplished by replacing computationally expensive full optimization with memory recalls from an associative memory representing prior optimization knowledge. Face images blocks are clustered according to their texture, and then optimal embedding bands for all of blocks of same texture are selected together using prior knowledge stored in associative block cluster memory (BCM). Solutions recalled from BCM, representing different number of clusters, are proposed to be ranked using watermark fitness to find number of blocks clusters for each face image. The number of clusters is referred to as clustering resolution in this paper. This implements multi-hypothesis approach where all alternative solutions are stored and the hard decision to choose among these solutions is postponed. The training set  consists of training face images defined as , where  represents face image of index  from the training set  of resolution . The number of face images in the training set is equal to . For each training face image , the image is divided into  pixels blocks , where  with , and  defines the row and column index of  blocks, respectively. The total number of blocks , and thus . Training face image blocks  are transformed into DCT domain , where  with  defines the DC coefficient of the  block , and  defines the th DCT coefficient of the same block . The texture features  are extracted from , where  defines the most significant DCT coefficients from  for training face image . The texture feature vectors are defined as , where  defines the texture feature vector of block . This feature vector is defined as , where , and . The number of coefficients used to extract features is equal to , and  is the index of texture feature in the feature vector . After the full optimization process for face image , the optimal embedding bands  are concluded for face image , where  with  representing the optimal embedding bands for block . The embedding bands define the index of DCT coefficients which are modified during embedding the watermark. It can be defined as , where  is the index of the embedding bands in , and  is the number of embedding bands for block  representing the embedding capacity for . The generalization set of unseen face image  is defined as , where the size of generalization set equals . The subscript  is used instead of  for the data structures used in generalization phase. Thus , and  defines the  blocks, the DCT transformed blocks, and the texture features of the face image , respectively. Algorithm&amp;nbsp;1 describes the main steps of the proposed framework, where the training phase (lines 10–18) is performed on training set face image  , and the generalization phase (lines 8–14) is triggered using generalization face image  . BCM associative memory is populated (lines 1–7) with prior knowledge during the two-step training phase, where the first step finds the multi-resolution clusterings for face image blocks (line 5), and the second step calculates the most frequent embedding bands associated with each and every cluster prototype defined in the first step (line 6). Prior knowledge is recalled for different resolutions   (lines 10–13) and fitness is calculated for these resolutions (line 12), and finally solutions are ranked (line 14) to take the hard decision at the end of the process. The prior knowledge is represented by blockwise multi-resolution clustering of face image blocks  for different number of clusters  using texture feature vectors  of these blocks . The set of clusterings :{ , ..., , ...,  } are stored in associative memory. Each clustering  consists of cluster prototypes  with  representing th cluster for clustering resolution , where :{ , ..., , ...,  }. This set of clusterings SC is updated along training phase to update prototypes based on face images in the training dataset . The most frequent embedding bands () for all blocks belonging to the same blocks cluster are calculated for training set  using previous optimization results. These results are represented by optimal embedding bands  for all blocks  of training face image . For each clustering  there is  set, where .  is associated with cluster prototype  representing most frequent embedding bands using clustering resolution  for th cluster. The set of most frequent bands is defined as  and .  is ordered descendingly with respect to the frequency of occurrence of embedding bands, where  is the index of the most frequent embedding band for th cluster using resolution  represented by prototype , and  is the index of the least frequent band. The parameter  is tunable for the proposed system defining the size of  representing the maximum number of frequent bands stored in BCM. Robustness scores (RS) are used to identify the order of embedding for different watermark length and the embedding capacity of different blocks . For each clustering , there is a set  where :{ , , ..., , ...,  } with  representing the scores for th cluster using clustering resolution . The clusters of blocks whose  is equal or higher than  threshold are considered candidate embedding clusters  for resolution , where  and  defines the index of the clusters and the number of candidate embedding clusters equals . Even embedding would be a special mode of operation for the proposed system, where ranking clusters based on robustness scores  would not be needed during training phase. During generalization phase, the empirical embedding capacity calculation  would not be needed as well. The two phases now are presented in more detail in the following sections. The processing steps included in training phase are described in the following sections and shown in Algorithm&amp;nbsp;2. Full optimization of training face image   is performed (line 2), where all face image   blocks   are represented positionally as   in optimization candidate solution to find optimal embedding bands   for all blocks of  . After this optimization process, the prior optimization knowledge is concluded and stored in BCM. Texture features are extracted from face image   blocks (line 3); then the prior knowledge is represented by most frequent embedding bands for all blocks having the same texture features (lines 5–7).
Springer.tar//Springer//Springer\10.1007-s00138-013-0494-0.xml:A linear system form solution to compute the local space average color:Local space average color Color constancy Gray-world assumption:  1 Introduction  Color constancy refers to the outstanding capability of humans to recognize the color of the objects under a wide variety of illumination conditions. Due to the many benefits such a capability would bring to computer vision systems, much research has been carried out in this field (see [13] for a recent account). Although many different algorithms have been developed [1, 3] and a good level of understanding has been achieved [15, 17, 20], the problem remains largely unsolved [16]. A recent trend in the field [4] is to either employ several color constancy algorithms, and combine the results, or to analyze the image to assess which algorithm will perform better. For instance, for scenarios with ample gamut of colors, some researchers [6, 12, 19] have chosen to take into account the gray-world assumption to develop their methods [5], which state that the average surface reflectance is gray. Furthermore, a widespread practice is to assume that the illuminant is uniformly distributed through space, and only a few methods explicitly consider the presence of multiple light sources [2, 10, 11]. Ebner [6–10] has proposed an iterative formulation aimed at computing the color of the illuminant locally to tackle the more realistic situation of multiple illuminants coexisting in a scenario using the gray-world assumption. Ebner called  the local illuminant thus computed. The gray-world is too bold an assumption about how the world looks in general. However, there may be the need to make assumptions to gain intuition to build more comprehensive models. In this document, we present an alternative form solution to Ebner’s iteration scheme. In our proposal, we formulate the problem as a linear system and solve the resulting series. We show that the resulting matrices are sparse and hence admit a compact representation and a fast solution via LU factorization [14]. Furthermore, we show that the computing requirements are reduced at least by an order of magnitude. Also, in this document, we are only concerned with the computational aspects of the problem. A method to compute a constant descriptor for color, including chroma and lightness, is out of the scope of the present research, and it is still one of the most fascinating open problems in the field of computer vision. The rest of the paper is organized as follows. In the next section, we review the gray-color assumption and the referred iterative formulation. Then, in Sect.&amp;nbsp;3, after framing the problem as a linear system, we derive an alternative solution. There, we discuss the use of sparse matrices to reduce the space requirements and LU matrix decomposition to speed up the solution of the linear system. Next, in Sect.&amp;nbsp;4, we describe some experiments. A concluding section discusses the results and suggests directions for future research.    2 The gray-world assumption  In the gray-world assumption [ 5], it is expected that the observed color components of a given scenario will average to gray. As a consequence, any observed deviation with respect to gray may be attributed to the illuminant. That is, let the normalized representation of a color image and its observed space average color be   and  , respectively, for  ,   and  . We are going to assume that   and   are represented by matrices with dimensions   rows by   columns, where  . Furthermore, suppose that   and   are particular pixels of the color image   and space average color image  , respectively. Under the gray-world assumption, when the scene is lit with an illuminant different than white,   will undergo a deviation with respect to the gray line, represented by the unitary vector  , that can be expressed as       Thus, the correction   of the color   consists of compensating for the illuminant as       It is rarely the case that the illumination in the image is spatially uniform. Ebner [ 6– 10] addressed this problem by computing a local average of the color before applying the gray-world assumption. That is, given an initial estimate of the local average of a color band  , and an observation of a color component  , Ebner proposed the following iterative relation to compute the local space average:            where the average   is updated as       and  \(0&amp;lt;\rho \ll 1\) is a small positive constant. In this study, we assume absorbing conditions for points at the boundary, i.e., to compute the local average for pixels at the border, we use only the pixels inside the image limits. To compute the local space average, Ebner proposed either to build a resistive grid [ 10] or to execute ( 3) and ( 4) iteratively for a number of cycles in the order of the tens of thousands. In [ 10], Ebner also suggested the use of successive over relaxation (SOR) [ 22]. In the next section, we show that this formulation admits an alternative form of solution based on its representation as a linear system. Furthermore, we show that the alternative is faster and, for a given resolution, can be computed in a fixed number of operations.    3 A linear system alternative  Without loss of generality, the elements in   and   can be ordered in column-wise order, resulting respectively in the vectors   and  . This way, ( 3) and ( 4) can be expressed as       where  , for  , corresponds to the initial estimate of the value of the local space average and   is a weighted connectivity matrix [ 21], with the property  . The second iteration can be expressed as       To solve for   we can plug-in ( 5), for  , into ( 6). After some algebra, it results in       where   is the  -step weighted connectivity or the weight associated in taking into account any two pixels   and   in the  th iteration. It is easy to appreciate that the pattern that is emerging has the general form       We are interested in the behavior of ( 8) as   tends to infinity. In these conditions, the first half vanishes because   becomes zero. Among other things, it means that the final local space average does not depend on the initial estimated value of it. As for the summation, it can be simplified by subtracting   from  . That is, let the expansion of   be expressed as       Then, we multiply by  , resulting in            The subtraction of ( 9) from ( 10) will eliminate most of the terms except those at the extremes. Factoring out  , this results in       where the term   vanishes when  . Finally, we arrive at the expression       Note that, in practice, the matrix   is actually not computed. Instead, one can take advantage of the structure and sparsity of  . As it can be appreciated in Fig.&amp;nbsp; b,   is banded, i.e.,   for  \(j&amp;gt; i + q\) and  \(i &amp;gt; j + q\). Golub and van Loan [ 14] show that the LU factorization maintains the sparsity on   and  . They also demonstrate how to compute the factorization in   flops, and both the forward and backward substitution in   flops. On the other hand, to store the elements of the band requires   cells. Furthermore, it should be stressed that   and   need to be computed only once for a particular average definition and image resolution. In [ 10], Ebner offers an alternative to speed up the computation of the local space average color based on the use of SOR [ 22]. SOR techniques aim to accelerate the rate of convergence by introducing a factor   that increases the value of the estimate in the direction of the solution. Ebner’s iterative scheme can be described by            where now   is the local space average and   is an auxiliary variable that is updated as       and the SOR equation is defined as       After ordering   and   in column-wise form as   and  , respectively, ( 14) and ( 15) can be represented as       and       And the first iteration yields       We can continue the same process as before, expanding successive iterations and simplifying the expressions. At the end,  th iteration yields the form            When we analyze ( 19) for  , we notice that the first term is not guaranteed to fade unless  . Provided that, we can solve for   by subtracting it from   . Using the conditions for   previously stated, this gives the solution            In other words,       For simplicity, we prefer ( 12) for all of our computations.    4 Experimental results  The toy example in Fig.&amp;nbsp; illustrates some considerations in the construction of the weighted connectivity matrix , specially for issues related to boundary conditions. A 33 matrix is filled up with values from 1 to 9, with 5 in the center. Although for this example the central pixel is not included, for subsequent tests, we use a 4-connected neighborhood, with the central pixel included in order to compute the average in (3). The following describes the conditions at the borders. For the computation of , and depending on the number of neighbors involved, there are three cases. For the pixels on the corners, the vertical and horizontal neighbors are used, and the weight for each element is 1/2. For the pixels in the horizontal borders at the top and at the bottom rows, three pixels are used: the two lateral pixels and the vertical neighbor, and so a weight of 1/3 is used. The same rule applies for the vertical borders, the leftmost and the rightmost columns. In this case, three pixels are used, the lateral and the two vertical neighbors. The weight for each element is also 1/3. Finally, for the pixels in the center, a 4-connected neighborhood is used and the weight for each element is 1/4. The matrix   can be stored compactly in a   array, i.e., the central diagonal and two off-diagonal elements on each side. However,   and   require additional non-zero entries of size   [ 18]. This is important because carelessly defined, an   image will require as much as   cells to hold the weighted connectivity matrix  , i.e., four times as much space as the original image. Nonetheless, note that the vast majority of the entries of the linear system are zero and the non-zero entries are in or close to the main diagonal. The problem of space can be solved using sparse representation for matrices. As for the value of  , Ebner [ 10] points out that it determines the radius over which local space average color is computed. Larger values of   use small neighborhoods whereas smaller values use larger ones. Figure&amp;nbsp; c shows that for our toy example, the ratio of convergence spans between   and  . Out of this range, the numerical stability essentially breaks down. In a more realistic exercise, we obtained permission to use the images in [ 8] and applied our method. The color images have resolution   (rows   cols)  . We implemented Ebner’s methods to compute the local space average defined by ( 3), ( 4) and ( 13)–( 15) in Matlab. In both cases, we used   and a maximum number of iterations equal 10,000. Convergency was defined by the sum of the squared difference between the current and previous estimate of the local space average, with a value of  . In the case of the SOR version, we tried several values of  . In Fig.&amp;nbsp; a, we ran 30 times the three algorithms. The average time for ( 3) and ( 4) was 95.95&amp;nbsp;s. For ( 13)–( 15), the experimental results are more complicated. Depending on the particular values of   and  , the expressions may or may not converge. For instance, at  , 28,613 iterations have to be executed for   and 9,364 iterations at   to reach convergency (although Ebner recommends  \(\omega &amp;gt;1\) to achieve faster convergence). On the other hand, for  , the method requires 12,458 iterations for   and 2,333 iterations for  . For values of  \(\omega &amp;gt;1.3\), the method diverged for both values of  . As for our method, the average computing time was 1.97&amp;nbsp;s with  . In Fig.&amp;nbsp; , we show the results for other images. To evaluate quantitatively the relative change from the input to the output, we computed the root mean squared error (RMSE), in CIE L  color space, for the images in Fig.&amp;nbsp; . For the input, the first column in Fig.&amp;nbsp; , the RMSE between the first image and the second one, the first one and the third one, and the second one and the third one is 6.32, 10.3, and 33.74, respectively. As for the third column, the output, the RMSE is correspondingly 1.16, 2.20, and 1.04. As expected, the resulting images are more similar to each other.    Acknowledgments     1. Barnard, K., Cardei, V., Funt, B.: A comparison of computational color constancy algorithms. I: methodology and experiments with synthesized data. IEEE Trans. Image Process. 11(9), 972–984 (2002) 2. Barnard, K., Finlayson, G., Funt, B.: Colour constancy for scenes with varying illumination. In: ECCV, pp. 1–15 (1996) 3. Barnard, K., Martin, L., Coath, A., Funt, B.: A comparison of computational color constancy algorithms. II. Experiments with image data. IEEE Trans. Image Process. 11(9), 985–996 (2002) 4. Bianco, S., Ciocca, G., Cusano, C., Schettini, R.: Automatic color constancy algorithm selection and combination. Pattern Recognit. 43(3), 695–705 (2010) 5. Buchsbaum, G.: A spatial processor model for object colour perception. J. Frankl. Inst. 310(1), 1–26 (1980) 6. Ebner, M.: Combining white-patch Retinex and the gray world assumption to achieve color constancy for multiple illuminants. In: Pattern Recognition. Lecture Notes in Computer Science, vol 2781, pp 60–67 (2003) 7. Ebner, M.: A parallel algorithm for color constancy. J. Parallel Distrib. Comput. 64(1), 79–88 (2004) 8. Ebner, M.: Color constancy using local color shifts. In: ECCV, pp. 276–287 (2004) 9. Ebner, M.: Color Constancy. Wiley, West Sussex (2007) 10. Ebner, M.: Color constancy based on local space average color. Mach. Vis. Appl. 20(5), 283–301 (2009) 11. Finlayson, G., Funt, B., Barnard, K.: Color constancy under varying illumination. In: ICCV, p. 720 (1995) 12. Finlayson, G., Schiele, B., Crowley, J.: Comprehensive colour image normalization. In: ECCV, pp. 475–490 (1998) 13. Gijsenij, A., Gevers, T., van de Weijer, J.: Computational color constancy: survey and experiments. IEEE Trans. Image Process. 20(9), 2475–2489 (2011) 14. Golub, G., van Loan, C.: Matrix Computations. Johns Hopkins University Press, Baltimore (1996)
Springer.tar//Springer//Springer\10.1007-s00138-013-0495-z.xml:Multi-view dense 3D modelling of untextured objects from a moving projector-cameras system:Image-based modelling 3D modelling Image correlation Multi-view stereovision Bundle adjustment Pattern projection:  1 Introduction  Modelling a scene in 3D from images consists in mapping image pixels with real world 3D points. Since, the imaging process is a projection from a 3D space to a 2D space, some information is lost and one would need more than a single image to fully recover a real world point. The process is then the problem of retrieving a scene structure from a set of images. This is done by jointly estimating camera poses at acquisition times and inferring the structure from different point of views of the surface. With the development of stereo vision methods and 3D modelling from vision, scanning devices and methods have greatly increased. Industrial applications actors are increasingly using 3D modelling for a wide range of problems. Among the main applications, one can note reverse engineering, dimensional control, simulation, design, industrial maintenance, etc. In these applications, some objects can be challenging for vision technologies: untextured, with details or sharp edges, specular surfaces, and so on. Basic stereo vision setups, that is to say with simply a pair of cameras for instance, are often unable to accurately and densely model such objects. Untextured objects penalize matching process in stereo images, edges cause occlusions and ambiguities. To overcome these limitations, active vision systems propose to provide a measurable energy to the scene, often invisible or near-infrared regions of the light spectrum or using laser. Among active vision systems, structured light methods project a specific pattern on the scene. This pattern can be known and its projector is calibrated to act like an inverted camera in a stereo system, or can be unknown and used to add texture in a peak detection [4] or in an appearance correlation approach [39]. The projection can be a set of patterns with temporal coding. This imposes that the system is not moving during the projection. Structured light methods based on a single projection are, on the contrary, useable in moving devices, such as hand-held 3D modelling systems. Recently, important research efforts have been directed towards the design of small and portable 3D modelling systems [ 21,  38]. Hand-held systems allow an eased scanning process due to a more dexter handling compared to fixed systems. Most hand-held 3D modelling devices are based upon laser triangulation techniques or time-of-flight measurements. Hand-held structured light devices remain rare due to the projector limitations [ 13]. If the projector is placed in the room, independently from the scanning device, the operator has to take care of the occlusions of the lights causing shadows in the scene. Moreover, the setup of such a system can be complicated (more than one projector may be needed, the projectors have to be placed judiciously to cover the entire object,..), which is contrary to the initial purpose of simplicity of hand-held devices. If the projector is embedded in the device, the scene changes every time the device moves. Indeed, since the operator has to move the scanner to cover the entire scene, every acquisition leads to a new device position, and then to a new projector position, changing the projected light on the scene, that is to say changing the scene texture (Fig.&amp;nbsp; ). A classical approach for multi-view 3D modelling from cameras is to match appearance information in a set of images and optimize a system consisting of camera poses and scene surface parameters. Using a hand-held structured light device with an embedded projector does not allow such approach due to the appearance changing when the device is moving. Frequently, the modelling process operates as a range scanning task: 3D information—3D point cloud—is retrieved from images at every acquisition, and registration is achieved using geometrical methods [8, 33, 36]. However, the interest of using a multi-view approach has been demonstrated by a number of works [6, 15, 35]. This kind of approach utilizes more information in the modelling process. It tends to give more robust and accurate results when compared to standard pairwise approaches. Multi-view stereo (MVS) provides an efficient way to retrieve a complex, dense, and accurate shape from a set of images. A taxonomy and evaluation of MVS is presented by Scharstein and Szeliski [34]. Some of the best-performing methods are even available in a software package and allows to reconstruct very large scenes [14, 16]. Many MVS methods start from an initial surface, refined afterward during an optimization step. The initial shape can be acquired from  techniques or from traditional stereo methods [40]. In an industrial context, mechanical parts control applications for instance, a CAD model can be used as a reference in this process. Normals to the surface are often the optimized parameters as they encode the 3D position of points and the direction of the surface. MVS is known to perform particularly well for sufficiently textured scenes. Untextured areas are processed considering priors like surface smoothness. These priors may result in inaccurate results on the final 3D model. Most stereovision algorithms use local planar approximation models—correlation windows, for instance—that may oversmooth the surface. MVS approaches take advantage of the number of views to refine 3D surfaces using simple planar models. Other approaches use more complex surface models like surfels [5]. The complexity of these models make them hard to use in multi-view formulations. Among the important contributions, Pollefeys et al. [31] presented a complete  approach for off-line modelling of a scene acquired from a single uncalibrated mobile camera. This method tracks features in the image sequence to globally determine camera intrinsic and extrinsic parameters along with the geometry of tracked keypoints. In a second time, the 3D scene is densified by propagating pairwise-computed disparities to expected neighbors in other images. A recent evolution [23] of this kind of approach combined a probabilistic framework for real-time  and a densification using local  [41] to lower dynamic. The main drawback of these methods is the need of highly textured scenes. Recently, Wu et al. [44] proposed to combine structure from shading and multi-view stereo. However, applying these methods to industrial applications is not straightforward as a lot of constraints must be considered. Objects in industrial applications are most of the time untextured as they are made of a single material. When silhouettes are available, they are sometimes embedded in the process [18] to add constraints on the reconstructed model. Recent work on  (SLAM) has been oriented towards the use of visual data. Localization using photometric techniques has been explored to introduce optical odometry [27, 28], but suffers from drifts and rapidly causing large uncertainty. Davison [11] proposed an approach to sparse localization and mapping using a single mobile camera based on Kalman filtering. It benefits from information federation and loop closure to refine the last camera pose and the sparse model. This work has been extended to propose real-time implementations using active search in images [12]. Montiel et al. [22] solved the features delayed-initialization problem of the method using inverse-depth parameterization of features. Paz et al. [30] proposed the use of stereo camera systems to fully observe features, but limiting the observation field of the features. Sola et al. [37] considers the stereo system as a multi-cameras system, adding in the map points fully observed in the stereo view field, and partially observed outside. Again, these approaches require that the scene does not change—no mobile object or lighting condition changes are allowed—and are not adapted to a moving projector causing a change in the scene appearance. Geometrical approaches have been proposed to solve the pose estimation of range laser sensors [26], in robotics applications for instance. Nüchter et al. [25] proposed a framework to solve the simultaneous localization and modelling problem for robot navigation tasks in difficult areas exploration missions. Poses uncertainties can be taken into account by representing spatial transformations as random variables. Lu and Milios [20] introduced a stochastic approach for robot navigation applications. Other authors improved it with stochastic filtering [24], using Extended Kalman Filter for instance. These methods have been designed for sensor pose refinement but do not refine the scene model. Moreover they impose a sequential acquisition at relatively high dynamics to be consistent. Other geometrical approaches propose initial mesh refinements to match an input data set. Curless and Levoy [10] use a weighting on a signed distance function from input meshes to fill a global voxel grid approximating the actual 3D surface. Zharescu et. al. [45] applied their topological approach to 3D reconstruction. However, these methods can not jointly estimate camera poses and surface reconstructions. Vision systems modelling highly specular surfaces can be considered as appearance-change problems, since the specular reflections are a function of the camera pose and do not project the same way in all images. Some approaches [29] remove saturated image regions. The holes created are filled using a multiple view imaging system. Reflectance methods [43] propose to model the specular reflections when measuring the surface geometry. Zheng et al. [7] use a combination of several weighted functions to represent light reflections to retrieve reflectance properties of an object and retrieve its normal map. Pons et al. [32] proposed an original formulation of a scene flow problem to model non-Lambertian surfaces or illumination changes. However, the method requires a sequential acquisition and assumes that the scene is not completely modified by illumination perturbations. Vu et al. [42] extended the method to the refinement of a coarse initial mesh using a similar formulation and considering the visibility of points. However, this method is appearance-based and considers the extrinsic camera parameters fixed. Our approach is a -based global optimization with a novel observation scheme. It refines a coarse model previously reconstructed following a standard pairwise stereo approach. The initial approach adjusts the local homography around a correlated point minimizing the intensity differences in stereo images [3]. The optimization criterion is based on reducing the correlation error of points projected in pairs of stereo images. Correlation scores are observed through a luminance difference in images acquired at the same time and globally linked to every view observing the same points with a generalized formulation of homographies induced from tangent planes to the surface. This approach proposes a method for multi-view optimization of sensor poses and surface reconstruction using a least-squares formulation. It is similar to the approach of Chang et al. [6], but with a specific formulation for MVS problem. We show that our method brings a multi-view approach to the particular case of a texture changing over time. We also demonstrate that it is a consistent multi-view correlation-based approach for generic purpose problems allowing an accurate 3D modelling of objects even with sharp edges or details. It is also robust to the correlation window size changes, even with very small sizes. We first introduce our conventions and model in Sect.&amp;nbsp;2. The experimental device used in our application and for the evaluations is also briefly introduced. Our method is presented in Sect.&amp;nbsp;3. General considerations and our formulations are introduced with the simple stereo case (3.1). Then our method is generalized to the multiple stereo-images case (3.2) and some algorithm optimization is introduced (3.3). The method is evaluated in Sect.&amp;nbsp;4 after presenting our experimental protocol. Section 5 gives some conclusions and perspectives for improvements.    2 System model and principle  The method we propose is designed to address the particular case of multi-view 3D modelling of a texture-changing scene. For the purpose of the method, we used an experimental camera-projector setup. In the following chapter, we present this system and its operation along with some conventions. The imaging process of a camera allows the mapping of a real world 3D point   to a 2D image pixel  . It is a relation of the form       In Eq. ( 1),   is the intrinsic parameters matrix. It allows the mapping of a point expressed in the camera frame to an image pixel. The rigid transformation   allows the change between world and camera frames. In general, rigid transformations between points   expressed in frame   and points   expressed in frame   are noted   such that       In our system, we use two cameras rigidly attached. First camera frame is noted   and second camera frame is noted  . Each camera has an independent intrinsic parameters matrix, respectively,   and  . The transformation between cameras is noted  . The camera parameters are calibrated using a chessboard calibration target and following the method described by Zhang [46]. The chessboard has been modified to allow automatic initialization. Images are corrected to remove the effect of distortions prior to any processing, allowing the use of the linear models described previously. The system is hand-held. It is composed of two CCD  cameras with 8-mm lenses, a projector and an inertial sensor. The stereoscopic baseline is 140-mm long and the cameras are oriented with a  angle. The whole setup has been described and evaluated by Coudrin et al. [9]. The cameras and the projector are synchronized. We use a pattern projection to infer the dense 3D information from the pair of cameras. When the system is triggered, the pattern is projected on the scene. Then the two cameras acquire images simultaneously. The pair of images is used in a reconstruction algorithm, providing a 3D-point cloud by a stereovision surface growing method. Points are expressed with respect to the frame . To model an object completely, one has to move the vision system around the object. An acquisition at time  provides a set of 3D points expressed in the current first camera frame, . Point sets have to be registered in a common frame to retrieve the actual geometry of the scene. Registration can be solved using inertial sensing or surface descriptors matching for initial alignment and  algorithm or variants for refinement [21, 33]. Combining these algorithms makes the system more robust to symmetric ambiguity. This approach can lead to good results, but still suffers from some problems. Mostly, the accuracy of the results can be very low for actual industrial applications. The 3D surface reconstruction is based on the observation of the image neighborhood of every pixel and then produces an over-smoothing effect related to the neighborhood size, or noise near model discontinuities. In the case of reverse engineering, for instance, the smoothing effect can be problematic since it alters the sharpness of edges or suppresses details on the surface. Moreover, the registration process is independent. A multi-view approach tries to estimate jointly the camera poses and the structure scene. In this way, both informations can benefit from a global optimization process. In the approach introduced previously, the registration, being independent and using the surface discretization from the reconstruction, directly suffers from reconstruction errors or inaccuracies. We propose to improve the previous reconstruction by refining 3D points through multi-view optimization.    3 Multi-view optimization  Vision-based systems can capitalize a lot from using all images at a time to refine the 3D model. Since, in our problem, the pattern projector is moving along with the cameras, causing a change of the appearance of the scene at every acquisition, classical  methods cannot be used. Our method uses the estimation of the homography induced by the tangent plane to a surface point [17] in a pair of images taken at the same time. The expression of the homography is then generalized to every image pair observing the same point to produce a non-linear least-squares criterion to optimize. Let us assume that the system is at the origin of the world frame  . That is to say, in our notation, that the transformation between the world frame and the first camera is the identity transformation  . With this assumption, the relation between a 3D point expressed in world frame and an image pixel in image taken from camera   can be written from Eq. ( 1) as       with   being the third coordinate of the 3D point. From Eqs. ( 1) and ( 3), we can write a relation between the projections in stereo images from cameras   and          The structure of the observed scene is obtained by finding   in Eq. ( 4). This is achieved by determining the point   matching the point  . This is done by observing and matching luminance informations in both images. To avoid ambiguities, the matching point is found by comparison of local appearance around the pixel  . Surface can be approximated locally by tangent planes to observed points. Let   be a region around point  . Matching point to every   in image   can be found using the homography   induced by the tangent plane to the observed 3D point (Fig.&amp;nbsp; ). The tangent plane is defined using three points from region  . Let us define   as a square region in image   and choose, for convenience, its center  , and two corners   and   to define the plane. The 3D point associated to the pixel   is defined by the parameter  , as seen in Eq.&amp;nbsp;( 3). Let us define parameters   and   associated, respectively, to points   and   so that            The normal vector   to the tangent plane is defined using our three points:       The tangent plane   is defined, using point   and the normal vector  , as all points   satisfying       The coordinates of the tangent plane are then formalized as  . The intersection between this plane and any ray coming from the camera center of camera   through a pixel of image     is written as            In Eq. ( 8), the normal vector is written as   and   is the fourth coordinate of the plane  . Then, extending Eq.&amp;nbsp;( 4), matching point in image   to every pixel in the region   from image   can be expressed from                         This notation is equivalent to the one introduced by Hartley and Zisserman [ 17]. We introduce it because it allows a simpler adaptation of the homography expression to any configuration of cameras by simply composing rigid transformations, as it is shown in Sect.&amp;nbsp; 3.2. In the following, coordinates of the plane are supposed to be normalized by the fourth coordinate   and we introduce the notation            The generic expression of an homography in the two stereo cameras system is then expressed as       The structure of the scene, from two images acquired at the same time, is obtained by minimizing dissimilarities between the luminance function   in region   of image   and the luminance function   in the transformed region   of image  . If we note  , we try to find       Equation ( 13) introduces the minimization criterion used to find the parameters of a tangent plane from the correlation of two pixel regions in two stereo images. This problem can be solved using standard non-linear least-squares methods. One should note that the assumption that the surface can be approximated by planes is valid only if the region  is small enough. If the region is large, the surface will be overly smoothed from this approximation, but if the region is very small ambiguities are introduced in appearance correlation, causing noise. The choice of the region size is then a trade off between details and noise levels. Extending this criterion to multiple image pairs, as introduced in Sect.&amp;nbsp;3.2, not only allows the multi-view stereo global optimization when appearance is changing between acquisitions but also allows to robustly estimate the scene geometry with a reduced region size. We introduced in Eq.&amp;nbsp;(12) the expression of the homography relating two projections of a 3D point in two images acquired at the same time from a calibrated stereo system. Let us suppose, we take two acquisitions with our moving stereo system at time  and . The system is at the identity pose at time , then moves to be at unknown pose  at time . Figure   illustrates this setup and unfolds some relations that we need to introduce. Since frame   is the same as world frame, relation (1) has been introduced in ( 12). Relation (2) is similar to relation ( ), but the rigid transformation between the two cameras is the unknown transformation  . Moreover the camera is the same, but at different acquisition times, so the intrinsic parameters matrix remains the same. The homography for a point   in this case is       Relation (3) is, also, similar to relation (1) but this time both cameras have moved. The relation between them is still the known calibrated rigid transformation  . In this case, the homography for a point   is written from Eqs. ( 12) and ( 14) as       Note that, using our formulation of the homography, relations are easily expressed by simply composing suitable rigid transformations. Therefore, from Eq.&amp;nbsp;( 15), we can introduce our general expression for the homography in a pair of stereo images. Let us suppose that a point   has been observed from a squared region in image  , our two-camera stereo system at this acquisition time is referenced by frame  . After a motion, the system is referenced by frame  . The homographies for the observation of the region in images   and   taken from the system in frame   are expressed as                                      with   being the rigid transformation between frames   and  , composed by the poses of the system at acquisition times relative to the world frame. From Eqs.&amp;nbsp;( 16) and ( 17), we can formulate a global optimization criterion to refine points and camera poses from the model. Supposing the model has been initially reconstructed and registered ( 2.2), refining the model consists in building a relation with every point and every system pose. This involves to know how the pair of images taken from each system pose observes points. We have, then, to define a   on a 2-tuple composed from a point and a system pose. Let us define the set of system poses  , and the set of points  . The visibility function   is defined for a point   and a pose   as A point is defined as observable from a pose (Fig.&amp;nbsp; ) if it is projected inside images   and  , if the angles between the normal vector to the point and the cameras views directions are small enough, and if the point is not occluded. To determine the occlusion, the initial point cloud model is meshed and rays from camera centers to the considered point are tested against this mesh to check for intersections. If some intersections are found, the point is considered occluded. To avoid occlusions caused from a bad registration (multiple skins effect on coarsely registered objects), a tolerance threshold on intersection distances can be used. For the optimization process, we build a parameter vector  . It is composed from system poses parameters  , respectively, the three   angles and the three translation parameters, and from the reduced normal parameters of the points expressed by  . This parameter vector is estimated through a sparse Levenberg-Marquardt non-linear least-squares optimization method [ 19], minimizing the criterion       with                         Since the scene changes between each acquisition, the dissimilarities measurement using luminance functions can only be done between images acquired at the same time. Equation ( 20) extends our two images correlation criterion. Points and poses are related between acquisitions by the inherent geometry of the observed scene. We optimize directly the normal parameters and observe their influence on the correlation results in regions transformed in a consistent way between every image. By doing this, a robust estimation of our parameters can be achieved. This formulation still depends on the size of the region  , but since the system becomes heavily constrained, it is more robust to small sizes. Indeed, ambiguities are reduced into small size regions by the geometrical constraints imposed by the formulation of our system. We demonstrate in Sect.&amp;nbsp; 4 that it allows a more accurate measurement of a scene by reducing the smoothing effect of correlation approaches and the ability to observe smaller details or sharper edges. With a large number of points, the algorithm complexity leads to long computation times and memory size problems. Even with sparse implementations, refining every point from every acquisition is not easily manageable. However, not every points are needed to estimate finely the parameters of the problem. We propose a sparse-to-dense approach to model an entire object with our method. Keypoints are selected in a sampling process that tends to pick a given number of points maximizing their visibility in the set of acquisitions. Moreover, the points having the worst correlation score after the initial step and the points located near the edges are discarded. Camera poses and keypoints parameters are estimated strongly using the method described in Sect.&amp;nbsp; 3.2. The refined model obtained is sparse and contains only refined 3D keypoints. To densify the model, every remaining point from the initial model has to be refined. With a sufficient number of keypoints in the previous step, camera poses can be considered optimal.1 Since points, in Eq.&amp;nbsp;( 20), have parameters independent from other points—points are only dependent on the camera poses—every point can be refined separately. This reduces the complexity of the optimization process, and reduces memory issues and cache misses, improving execution time. Camera poses are fixed and each point is represented by a parameter vector  , and are estimated by minimizing       This approach is dependent on the initial process, since points have to be observed in the first place to be processed by our method. From the initial model, this approach allows to easily refine points in a time effective way, providing multi-view advantages to dense 3D modelling.
Springer.tar//Springer//Springer\10.1007-s00138-013-0496-y.xml:Development of an optical system for analysis of the ink–paper interaction:Image analysis and processing Ink–paper interaction Optical system Surface inspection Machine vision:  1 Introduction  The study of the ink–paper interaction is a great concern and assumes a high importance in the paper industry for improvement of the quality and performance of the inkjet printing process [1–4]. Two factors that play an important role in the study of the ink–paper interaction and, ultimately, in the printing process itself, are related with the spreading and the absorption that an ink drop reveals when it is in contact with the surface of a given paper. The ink spreading and absorption phenomena are dependent of the physical/chemical characteristics of the inks as well as of the papers [2, 5–9]. In this sense, it is expected that different combinations of inks and papers will result on different spreading and absorption distributions [9–12]. The ink spreading and absorption had been widely investigated by different researchers, over the years, using a variety of methodologies and equipments [6, 12–18]. Fundamentally, the performed studies regarding this matter are typically based in the observation of an ink drop deposited in the paper surface (sessile drop method), using a single lateral view [17], or both lateral and top views of the ink drop [14]. The views are then used to monitor the evolution of the ink spreading and absorption through time, and are also used to determine some parameters related with the ink–paper interaction [12, 17–19]. Among them, the contact angles of the ink drop, which correspond to the angles formed between the tangent plane to the liquid surface and the tangent plane to the paper surface, at the three phase intersection point [20, 21], assume a position of greater importance, being a common measure of the hydrophobicity of a surface. The contact angles are also important because they can provide information about the surface energy, heterogeneity and roughness [20–25]. Many different studies had been dedicated to the determination and interpretation of the contact angles, with a variety of different considerations. For instance, regarding the size of the drops, microliter droplets have been used in some studies [16, 18] whereas picoliter droplets were used in others [10, 11, 26]. Yet on this matter, several studies were also dedicated to the comparison of both cases [19, 20, 27]. The first option (microliter) has the advantage of being less dependent of local irregularities, and defects of the surface, but considers droplets with an order of magnitude hundreds of thousands to a million times bigger than the ones used in the inkjet printers. On the other hand, the second option (picoliter) has the advantage of using droplets at the same scale of the ones used in the printers, being a more realistic representation of what happens in a common and ordinary print. However, at this scale, very significant differences can be obtained when analyzing different droplets of the same liquid deposited on a porous and/or heterogeneous substrate such as the paper [20, 21]. Yet, in neither case (microliter/picoliter) the main directions of the paper sheet (machine and cross directions), are taken into account when monitoring the ink–paper interaction over time. This aspect is important because the direction on which the ink–paper interaction is being analyzed has a direct effect in the calculation of some of its corresponding parameters. For instance, considering the height of the ink drops, it is clear that no more than one lateral view is needed for its proper measurement since it has always the same value independently of which direction the ink drops are being seen. However, if the contact angles of the ink drops are considered instead, the case differs. The ink drops are not regular objects and the contact angles may change at both sides of the droplets. Moreover, the contact angles may also reveal changes according to which projection (dependent of the direction defined for the lateral view) is being seen. This happens due to asymmetries and distortions of the droplets, which may be caused by several different reasons such as, the roughness of the paper, local irregularities, heterogeneity, and others. Furthermore, because the paper consists of a multi-laminar structure of cellulose fibers, the way the fibers are arranged in the paper structure is also a point to be taken into account [28, 29]. In particular, if the cellulose fibers are randomly oriented in the paper structure, there is no preferential orientation of the fibers (isotropic case). On the other hand, if the cellulose fibers show a preferential orientation in the paper structure, different global alignments of the fibers can be noticed for the different directions of the paper (anisotropic case). Usually, during the papermaking process, the cellulose fibers tend to be globally aligned with the machine direction (coincident with the vertical direction of the paper sheet) whereas the cross direction (coincident with the horizontal direction of the paper sheet) reveals a lesser global alignment of the cellulose fibers. In addition, during the papermaking process, local variations of the orientation and distribution of the fibers may also occur on a paper sheet. These aspects may affect the ink–paper interaction leading to an unevenness of the spreading and absorption of the ink drops in the paper structure [28, 29]. In such way, when monitoring the ink–paper interaction, it is fundamental to have a top view and also two lateral views, aligned with the machine and cross directions of the paper sheet, to simultaneously observe the evolution of the ink drop shape over time. The top view is important to monitor the total area occupied by the ink drop and the asymmetries assumed by the ink drop over time. Concerning the profile of the ink drop, it is important to monitor its evolution over time considering the machine and cross directions of the paper sheet. From these profiles, one can calculate and monitor the contact angles and the base diameters along these two directions. Since the machine and cross directions correspond to extreme cases in terms of global alignment of the cellulose fibers, different values could be obtained for the aforementioned parameters at both directions. Having in mind these ideas, our research team based its line of work in the over mentioned studies regarding the sessile drop method, and developed an experimental optical system to be applied for the study of the ink–paper interaction at the paper surface [30], considering the two main directions of the paper sheet (machine and cross directions). The developed system considers three different views, two lateral and one from top, to provide a more complete monitoring of the event, and providing measurements performed under the machine and cross directions in simultaneous, to identify possible variations of the ink–paper interaction that might be manifested in these directions. In particular, the two lateral views of the system enable the determination of the ink drop contact angles, their height, base diameters, and volume. These views are also used to create a 3D and a 4D model of the ink drop interaction at the paper surface. The 3D model is fixed in time, being only spatially dependent  whereas the 4D model is both spatially and temporally dependent . The top view, on the other hand, is used to determine the spreading area of the ink drop, its aspect ratio, and orientation angle during the period of time that the ink drop is in direct contact with the paper surface. In terms of the aims of this work, our concern lies in the system description, complete characterization, test, and its usage under real conditions. With regard to the system characterization, the main parameters of the system were addressed and set having in mind its optimization. Relatively to the test of the system, several measurements using specific object samples with known dimensions were carried out to evaluate the accuracy of the obtained results. Finally, a commercial ink and common copy paper were used to perform an experiment under real conditions to dynamically collect data with the optical system which, after processing, enabled to address the main results of the analyzed ink–paper combination.    2 Description of the optical system  The experimental optical system implemented to study the interaction of ink drops at the paper surface is schematically presented in Fig.&amp;nbsp; . In particular, two different views of the system are shown in Fig.&amp;nbsp; a and b, top view and a lateral view, respectively, where it is possible to see the main equipment that composes the optical system (see Sects.&amp;nbsp; 2.1– 2.4 for full description of the system and the used equipment). A third view is also seen in Fig.&amp;nbsp; c, presenting the additional equipment used to control the implemented optical system. Basically, a paper sample is placed on the sample holder  , while the syringe   ejects an ink drop in direction to the paper sample. At the same time, an automatic trigger system starts the simultaneous acquisition of the three image detectors  , to register the ink–paper surface interaction under different views. Due to the complexity and the three-dimensional configuration of the system, the authors opted to schematize the two views of the system slightly different from reality, to make the observation easier. This can be seen, for instance, in the top view of the system, where the image detector  and the light sources – were represented using an exploded view (dashed square), to enable a direct observation of the sample holder, placed on a lower position relatively to the over mentioned elements. This can also be seen in the lateral view of the system, represented under a straight linear configuration. In this view, to denote that there are two different units of each element (image detectors, light sources and diffusers), they were all represented shifted to each other, along the  axis. Also, the corresponding angles of all these elements were not taken into account in the drawing. For simplicity, the full description of the system and the used equipment will be addressed in the next subsections considering a subdivision of the system into four different modules, designated as: acquisition module, illumination module, motion module, and control module. The acquisition module is composed by three image detectors – (DALSA—model Falcon 1.4M100), coupled with three zoom lens systems (OPTEM—model ZOOM 70XL). In particular, the three different image detectors are used to simultaneously observe and register the event (ink drop formation and impact at the paper surface) from three different views. The image detectors  and  are placed in the system horizontally and perpendicular to each other, resulting in the observation of the event from two lateral and perpendicular views, coincident with the machine and the cross directions of the paper (principal directions of a paper sheet). On the other hand, the image detector  is placed in the system vertically, resulting in the observation of the event from a top view. Due to the specific configuration of the system, the top view has the particularity of being observed with the machine and cross directions aligned with the diagonals of the acquired images. In particular, the machine and cross directions are aligned at  and  relatively to the horizontal direction, respectively. The two lateral views are used in the system to determine the height of the drop, and also to determine the corresponding base diameters and the contact angles for the two studied perpendicular directions. The lateral views are also used to create a 3D and a 4D model of the ink–paper surface interaction, based in the volumetric model from the silhouettes [31–33], through combination of the information provided by each view. Through the data associated to the generated 3D model, it becomes possible to estimate the volume of the ink drop as well. The top view, on the other hand, is used in the system to monitor the spreading and absorption behavior of the ink drop when in direct contact with the paper surface. Specifically, this view enables the determination of the area occupied by the ink drop, the corresponding aspect ratio, and the orientation of its major axis. Since the event under study suffers significant changes for little increments of time, especially at the beginning of the event, the image detectors must capture the event using high frame rates. Furthermore, to prevent dragging and defocus of the ink drops caused by their movement, small exposure times must be considered during the capture of the event. The illumination module is composed by six white light LED sources – (XLEDs Galaxy LED Spot White—MR16 6,500 K), used to illuminate the scene and the event from different locations. In this experimental system, light sources based in LED technology were specifically used because they provide high intensity levels of illumination associated with low heat dissipation [34]. In such way, the high intensity of the used lighting compensates the need of using small exposure times, for a correct discrimination of the ink drops relatively to the surrounding backgrounds of the acquired images. In addition, since the used lighting has low heat dissipation, eventual heat transfer that might occur to the ink drops is reduced to a minimum, which is important to avoid non-natural ink evaporation. In particular, the light sources  and  are used to illuminate the scene laterally, under the same directions of the image detectors  and . Since these image detectors observe the scene under retro illumination, two diffuser filters  and  were associated with the light sources  and . The reason for their inclusion was to obtain a more uniform and even illumination, avoiding the direct observation of the light sources by the image detectors. The remaining light sources – illuminate the scene from top for the image detector . Because this image detector observes the scene under reflected illumination, the use of a diffuser filter is not required in this case since the image detector does not directly observe the light sources. Furthermore, four light sources had to be used to illuminate the scene for the image detector  to match the same levels of intensity reached by the remaining two image detectors. This happens because less light arrives at the sensor of  (used under reflected illumination) than at the sensors of  and  (used under retro illumination). The motion module is composed by a linear motorized actuator  (Newport—model TRA25CC) assembled in permanent contact with a syringe , previously filled with ink. When the actuator is set for a given position, it starts to move making pressure in the syringe piston forcing the ink to exit the syringe from the needle. In this way, setting the actuator for a higher displacement will result on the formation of an ink drop with a bigger volume. On the other hand, setting the actuator for a lower displacement will result on the formation of an ink drop with a smaller volume. Therefore, controlling the exact position of the actuator will result in the formation of ink drops with specific volumes. The ink jetting device of the developed system works at the microliter range and, for this application in particular, ink drops with volumes of  and less were used (typically ranging from 0.1 to ). The system also offers the possibility of easily changing the needle tips and the syringe, or the entire ink jetting device, to form even smaller ink drops, below the microliter range. Because the weight of the formed ink drops is lower than the surface tension forces that sustain the ink drops in the needle [35], they cannot be detached from the needle by themselves. For this reason, a pneumatic system  (SMC—model MQMLB16H-100D) was integrated in the system, assembled with the syringe, to assist the detachment of the ink drops. Before an ink drop is formed, both the pneumatic system and the syringe are sent to a higher  position of the system as shown in Fig.&amp;nbsp;a and b. Then, an ink drop is formed and, to detach the ink drop from the needle, the pneumatic system is collected to the zero level on the  direction (reference position of the pneumatic system) forcing the syringe to perform a quick vertical movement and a sudden stop. This procedure results on the detachment of the ink drop when the reference position is achieved. The user can regulate the pressure of the pneumatic system at any time and its corresponding change will have a direct effect in the velocity of the ink drops when travelling toward the paper surface. In particular, increasing the pressure of the pneumatic system will result in a higher velocity of the ink drops and, therefore, in a more severe impact of the ink drops at the paper surface. On the other hand, decreasing the pressure of the pneumatic system will have the opposite effect. Yet, if the pressure is adjusted to a low value, the ink drops may not be detached from the needle. For this application, the pressure of the pneumatic system was adjusted at the minimum value that still allows the proper detachment of the ink drops. After being detached, the ink drop begins to travel toward the paper surface, placed in the sample holder . This holder was designed having in mind the proper fixation of the paper samples without affecting the deposition of the ink drops. In particular, the sample holder is hollow and contains several small holes in the extremities of its upper surface, these last being the only parts of the holder in direct contact with the paper samples. In addition, the sample holder is connected to a vacuum pump used to extract the air from the holder, generating a suction process through the holes of the upper surface of the holder. As a result, the paper samples, which are placed in the upper surface of the sample holder, are conveniently fixed for the entire capture of the event. Because the paper samples are only sustained by their extremities and the ink drops always land in the central part of the paper samples, the ink drops are sufficiently far away from the extremities to be affected by the generated vacuum. The control module is composed by a motion controller  (Newport—model SMC100CC), two frame grabbers  and  (DALSA—model X64 XCelera-CL Dual), a signal generator  (LODESTAR—model LS3002), a custom unit  (specially designed and implemented to control an external trigger  and the pneumatic system ), and a computer  (Intel(R) Core(TM) i7 CPU 950, 3.07&amp;nbsp;GHz, 2.99&amp;nbsp;GB of RAM), connecting all the above elements. The motion controller is used to setting the position of the linear motorized actuator and the frame grabbers are used to control the settings of all the three image detectors of the system. The image detectors  and  are both connected into the frame grabber , whereas the image detector  is connected alone into the other frame grabber . Concerning the signal generator , the custom unit , and the external trigger , they all work together to activate the pneumatic system  and to initiate the image acquisition process. Basically, the system works by initially elevating the pneumatic system together with the syringe and the motorized actuator. In this step, the ink drop is formed and lies static in the needle of the syringe. Now, to initiate the event capture, several instructions are executed beginning with the movement of the pneumatic system to its reference position. At this point, the ink drop is released from the tip of the needle and, simultaneously, one salient piece of the pneumatic system obstructs a linear beam of infrared light directed to a proper receptor. As a consequence of the occurred obstruction, the receptor sends a trigger signal to the custom unit to start the acquisition of the image detectors. To guarantee a simultaneous acquisition of the three image detectors, the same trigger signal needs to be sent to the image detectors any time a new image is acquired. In such way, the signal generator is used in the system to precisely control the triggering of the image detectors, send ing a certain number of pulses per second to the image detectors, according to the desired frame rate. For a simple and intuitive control of all the procedures of the optical system, a graphical software application was implemented using the MatLab programming language and the Image Acquisition and Image Processing Toolboxes. The software application is divided in five different parts, being the first dedicated to the video control, used to change the video source, and to change the settings of the image detectors such as the gain, exposure time, frame rate, bit depth and resolution. The second part of the software is dedicated to the motion control and is used to send position coordinates to the motorized actuator. Because this process is related with the formation of the ink drops, the equivalent volume of the formed ink drops, at the beginning of the event, is also determined through the displacement performed by the actuator. The third part of the software application is dedicated to the alignment, focus and calibration of the image detectors. The fourth part concerns the event capture (ink drop formation and impact at the paper surface) enabling the operator to choose the total duration of the event that will be registered. Furthermore, this part also contemplates the storage of the entire image data acquired by the image detectors. Finally, the fifth part concerns the processing of the previously stored data. This part includes the visualization of the acquired images during the event (2D), the visualization of the generated geometric model of the ink–paper surface interaction for a given instant of time, or through time (3D and 4D), the calculation of the several parameters regarding the ink–paper interaction, mentioned in Sect.&amp;nbsp;2.1, and also their corresponding graphical representation.    3 Characterization of the optical system  In this application, the image detectors were placed in the experimental optical system considering a working distance of 95&amp;nbsp;mm counting from the position where the event takes place (central position of the sample holder). Concerning the field of view, it was defined having in mind the dimensions of the ink drops, and the maximum ink drop spreading associated to them. In particular, the application was established to use a field of view of , which guarantees that the ink drops will be entirely seen by the image detectors during the entire event. In addition, a resolution of  pixels was used for each image detector to cover the entire field of view. Since the event under study suffers significant changes for little increments of time, especially at the beginning, the image detectors must use high frame rates to properly capture the event. However, the frame rate is dependent of the amount of data memory that can be accessed by the frame grabbers meaning that higher frame rates require the use of lower resolutions, and vice-versa. Having in mind the defined resolution ( pixels), the frame rate of the image detectors can be modified up to a maximum of 215&amp;nbsp;Hz (215&amp;nbsp;frames per second). Since this methodology is more focused at the beginning of the event (first seconds of acquisition), it is important to know exactly the behavior of the interaction of the ink drop at the paper surface, considering small increments of time. In particular, the use of the specified frame rate enables the register of a triplet of images (each from a different image detector) every 4.65&amp;nbsp;ms. Concerning the bit depth, the image detectors allow the selection of 10 bits (0–1,023 gray levels) or 8 bits (0 to 255 gray levels) outputs. In this application, the bit depth was defined at 8&amp;nbsp;bits since the gray levels associated to the specified bit depth are sufficient to correctly detect and extract the ink drops from the background of the images, requiring less memory to store the images. The exposure time is another important parameter of the acquisition process and must be settled correctly to obtain valid image frames. As the ink drops move toward the paper surface, the image detectors register and capture the event for a given amount of time (typically, several seconds). During the event, the ink drops suffer constant shape changes as they spread and are absorbed by the paper. For each frame acquired by the image detectors, a correct detection of the ink drops must be guaranteed, which is achieved by the absence of dragging or blur in the images caused by the movement of the ink drops. To study this issue, an experiment was carried out consisting on the image capture of ink drops with a volume of approximately  impacting in the surface of paper samples, for different values of exposure time ranging from 40 to . Values above  were not tested because they result in a high amount of dragging of the ink drops. Several exposure times were tested starting from  and then systematically decreasing the exposure time of the image detectors until achieving the minimum value of . The corresponding images acquired with the different tested exposure times were posteriorly analyzed in terms of blur and dragging. Particularly, the value of exposure time that must be used by the image detectors should correspond to the highest value that still provides sharp and perfectly focused images. The reason for this is to acquire valid images with no dragging and blur, associated with a maximization of the irradiance. In particular, Fig.&amp;nbsp;  shows two image frames acquired by one of the image detectors placed laterally in the system, at the instant of impact of an ink drop with the surface of a paper sample. An exposure time of   was considered in Fig.&amp;nbsp; a whereas an exposure time of   was considered in Fig.&amp;nbsp; b. From observation of Fig.&amp;nbsp; , it can be seen that the image on the left shows a significant dragging and blur effect resulting from the movement of the ink drop, whereas the image on the right shows to be perfectly in focus. In such way, the exposure time was set at   for the three image detectors since this value corresponds to the maximum exposure time that still allows the acquisition of valid images without dragging or blur. One other issue that plays an important role in the acquisition process is related with the intensity level registered in the images. In particular, it must be guaranteed an equalization of the three image detectors for application of the same image processing routines on each image detector. Furthermore, to correctly detect the ink drops in the acquired images, it must be guaranteed that the intensity level of the background is significantly different of the intensity level registered in the ink drops. To study these issues, an experiment was carried out consisting on the acquisition of reference images for all the three image detectors. In particular, the reference images of the image detectors  and  were acquired considering only their illuminated backgrounds (images acquired with the corresponding illumination sources  and  turned on). These reference images were acquired with a fixed exposure time of  but considering different values of gain, from 1,024 (minimum value) to 1,400 arbitrary units. Values above 1,400 were not tested since these values cause image saturation. For each acquired image, the corresponding maximum intensity values were determined to monitor the evolution of the intensity level of the images when the gain of the image detectors is increased. For the image detector , the reference images were acquired considering a paper sample fixed in the sample holder illuminated by the corresponding illumination sources –. The procedure of the experiment for the image detector  is the same used for the image detectors  and  with the exception that a paper sample is required in this case because the illuminated background of the image detector  is obtained by reflection. As before, the reference images were acquired with a fixed exposure time of  but, in this case, considering different values of gain, from 1,024 (minimum value) to 7,000 arbitrary units. For this image detector, values above 7,000 were not tested since these values cause image saturation. Figure&amp;nbsp;  shows the graphical representation of the evolution of the maximum irradiance for the three image detectors, considering four different measurements of reference images for different values of gain. From the graphic of Fig.&amp;nbsp; a, one can see that the maximum irradiance of the image detector   is slightly higher than the maximum irradiance of the image detector  . It can also be seen that the maximum irradiance increases with the increase of gain, and the images start to saturate for a gain value of approximately 1,320 for the image detector   and 1,260 for the image detector  . In such way, since the image detectors   and   have different responses in terms of irradiance, the equalization must be performed considering different gain values for each image detector. Moreover, to maximize the intensity level of the images, but at the same time avoiding image saturation that may difficult the differentiation of the ink drops relatively to the background, the gain was set considering the maximum irradiance as being   of the highest value that can be detected by the image detectors. Considering the specified range, the intensity level of the images is as high as possible with no risks of image saturation, because a secure margin still exists before reaching the maximum value. Concerning the graphic of Fig.&amp;nbsp;b, it can be seen that the maximum irradiance associated to the image detector  also increases with the increase of gain but, in this case, the images start to saturate for a gain value of approximately 6,500. To maximize the intensity level of the images acquired by the image detector , but at the same time avoiding the saturation of the images, the gain was set considering the same range specified earlier. However, it is important to note that the irradiance registered by the image detector  depends not only of the illumination but also of the reflectance of the analyzed paper samples. For this reason, the gain must be readjusted every time a different paper sample is analyzed. With regard to the depth of field [ 36,  37] of this application, an experiment was carried out to determine its corresponding range, which corresponds to the spatial zone seen focused by the image detectors relatively to the central position of the sample holder. In the performed experiment, a custom target consisting of several vertical black lines spaced approximately 0.30&amp;nbsp;mm was used to determine the focus zone. The target, schematically seen in Fig.&amp;nbsp; , was placed in the sample holder of the optical system, aligned with the   axis to be viewed by the image detectors at an angle of  . First, the target was faced for the image detector   and an image was acquired. Then, the same target was faced for the image detector   and a second image was acquired. Figure&amp;nbsp; a and b shows the images acquired by each image detector,   and  , respectively, being visible the zones that are focused and out of focus. From the above figures, it can be noticed that the central part of the images appears focused and start to defocus towards the periphery. In total, both the image detectors   and   reveal a 3.00&amp;nbsp;mm depth of field, in particular, 1.50&amp;nbsp;mm behind and 1.50&amp;nbsp;mm ahead of the focal plane of the image detectors (central vertical line seen in Fig.&amp;nbsp; a and b. In practice, this means that it is not required that the ink drops must fall exactly in the center of the sample holder. If the ink drops fall inside the depth of field zone, situation that is always guaranteed in the application, the ink drops will be seen by the image detectors in focus. On the other hand, the corresponding depth of field of the image detector  was not determined because this parameter is not important for this particular case. The image detector  is used to monitor the event from top, having the paper surface as reference level. In such way, the images of the ink drops captured by the image detector  are always in focus because at the surface level, there are no sufficient changes in the vertical position to promote serious defocus. Because the developed experimental optical system uses three different image detectors pointed at a specific position of the system to capture the event, a procedure for alignment, focus and calibration of the system must be carried out for each one of the image detectors. In particular, the alignment is fulfilled to adjust the position of the image detectors relatively to the reference planes of the system. The image detectors  and  share the same reference plane (reference plane 1) whereas the image detector  uses a different one (reference plane 2), perpendicular to the first. The focus procedure, on the other hand, is accomplished to adjust the focal plane of the image detectors to acquire the images as clear and sharper as possible. Concerning the calibration procedure, it consists on establishing the relation that exists between the object space and the image space. For simplicity, and because all the three procedures depend on each other, they were all merged together in a single real time process. To carry out the process of alignment, focus and calibration, a special 3D target was built consisting of a physical mounting displaying in two perpendicular planes (reference planes 1 and 2) specific marks that are used to perform each part of the procedure. A schematization of the built 3D target is shown in Fig.&amp;nbsp; . In terms of alignment, the position of the image detectors must be systematically adjusted, through the use of linear and rotational stages, until the black marks shown in the target coincide with the vertical and horizontal white lines of the image. These white lines were overlapped in the acquired images to assist the alignment task. Figure&amp;nbsp; a–c shows the three different images of the target acquired by each image detectors,  ,   and  , respectively, revealing to be conveniently aligned in both the horizontal and vertical directions.
Springer.tar//Springer//Springer\10.1007-s00138-013-0497-x.xml:3D segmentation of abdominal CT imagery with graphical models, conditional random fields and learning:Adrenal gland Machine learning Segmentation Abdomen Graph-cuts Probabilistic graphical models Random fields:  1 Introduction  The accurate detection and segmentation of organs, vessels and other regions of interest is a key problem in medical imaging. Accurate automated segmentation techniques aid in building more robust computer-aided diagnostic systems and help in the detection of legions, cancerous tumors and other abnormalities. However, manual segmentation of structures within volumetric imagery with voxel level accuracy is extremely tedious. The task is so laborious that even gathering training data to build and evaluate automated systems is a challenge. In this paper, we thus show how state of the art machine learning techniques and recently developed image features can be used to construct such systems. We focus on segmenting some key abdominal structures within computed tomography (CT) imagery using different amounts of user interaction. However, the techniques we present should be much more broadly applicable to different segmentation problems. We focus in particular on 3D or volumetric segmentation. Some examples of the anatomical structures we explore are given in Fig. , where we show a 2D axial image slice along with manual segmentation of some organs of interest. We also explore models that are specialized for the challenging task of adrenal gland segmentation. In Fig. , we show a sample 2D slice and the zoomed in region containing an adrenal gland along with a detailed manual segmentation. The CT imagery shown here and in subsequent figures comes from the liver segmentation data of [16].1 The adrenal gland is a common site of disease, and detection of adrenal masses has increased with the expanding use of cross-sectional imaging. Radiology is playing a critical role not only in the detection of adrenal abnormalities but in characterizing them as benign or malignant [ 29]. In order to facilitate this process, computer-aided diagnosis systems could build upon precise voxel level segmentations of the adrenal gland as a first step for subsequent automated image processing and classification steps. In both Figs.&amp;nbsp;  and Machine learning techniques are playing an essential role in modern medical image analysis systems. The problem of image segmentation can be particularly important as segmentations can both be useful in themselves as well as serving as input to subsequent automated processing techniques. Markov random fields (MRFs) provide an attractive framework for image segmentation. For example, previous work with some goals similar to ours has sought to create probabilistic atlases of the abdomen and then explored their application in segmentation using traditional MRFs which factorize into likelihoods derived from image properties and spatial priors in the form of MRFs [35]. Park et al.&amp;nbsp;[35] explored the importance of the probabilistic atlas and used unsupervised segmentation with  (MAP) configuration estimation using the Iterated Conditional Modes (ICM) algorithm of Besag&amp;nbsp;[2]. This type of approach was common at the time in that interaction potentials were set by hand, discriminative learning techniques were not used and Besag’s fast but local minima prone ICM algorithm was used for inference. In our work here, we use Gaussian mixture models to encode a form of statistical atlas as well. Such information can serve as a powerful feature encoding higher level semantic information. Recent insights have given rise to a new distinction between traditional MRFs which define a joint distribution over both segmentation classes and features and conditional random fields (CRFs) [ 25] which model the conditional distributions of the segmentation field  . CRFs have had a major impact in machine learning in recent years and our work here explores their application to 3D image segmentation in detail. CRFs were originally proposed for text and sequence processing problems where inference and learning are fast and convex. However, for 3D image segmentation the problems of inference and learning are much more challenging due to the loopy nature of the lattice structures needed for segmentation. Unlike a lot of previous work using MRFs for the task of image segmentation we model image volumes here with full three-dimensional (3D) graphical models as shown in Fig.&amp;nbsp; . Each 2D lattice corresponds to an axial slice of the medical image and each slice is coupled to adjacent slices via MRF interaction potentials. While both 2D and 3D inference and learning are challenging, fortunately both probabilistic inference and learning in lattice-structured graphs have advanced significantly over the past 10&amp;nbsp;years. For these reasons, and to create a self-contained manuscript with sufficient context to understand our approach, in Sect.&amp;nbsp; 2 we provide a compact but comprehensive review of the key developments in graphical modeling techniques that are most relevant to image segmentation, MRFs and CRFs. We also discuss the key issues of inference and their relationship to learning. Image descriptors based on histograms of oriented image gradients or HOG features have received a lot of recent attention in computer vision. For example, both the widely used SIFT descriptors of [28] and the person detector of [12] are based on different forms of HOG. Depending on the size and spatial extent of the HOG, such features are capable of capturing the entire shape of small organs or contour segments of larger organs. In our work here, we also explore the use of a state of the art HOG-based feature descriptor specifically designed for detecting anatomical structures and contours in CT imagery of the abdomen. We make a number of contributions in this paper which build upon and extend our preliminary work in Bhole et al.&amp;nbsp;[3]. First, we discuss and demonstrate how discriminative parameter learning in CRFs indeed leads to better segmentation performance compared to their generative MRF counterparts. Second, we demonstrate how HOG features are indeed well suited to the anatomical segmentation problem. We learn pairwise term parameters and use full 3D inference using either graph cuts, variational message passing or the max-product inference algorithm. Further, we provide (1) another set of experiments exploring the utility of HOG features at different image scales so the HOG windows cover different spatial extends of the images, and (2) a model and a set of experiments on the interactive segmentation of the extremely challenging adrenal gland. We believe our work is the first to provide this type of in depth comparisons for fully 3D MRFs, CRFs and hybrid techniques and the first to explore the important problem of adrenal gland segmentation. Our 3D CRF approach makes use of a novel technique for encoding a statistical atlas that fits naturally into our graphical modeling framework. Finally, we present a novel 3D sparse kernel CRF approach that is able to yield state of the art performance while avoiding the need to discretize or cluster input features. In the next section, we review some key influential recent developments in machine learning, computer vision and discuss the most pertinent recent work in medical image analysis. We then present our techniques and experiments in Sects.&amp;nbsp;3 and 4, respectively.    2 Recent work and important developments  Markov random fields started to receive increased attention in the medical image analysis around the time of the influential work by Zhang et al.&amp;nbsp;[56] on hidden MRFs for segmenting magnetic resonance (MR) imagery of the brain. This work presented a traditional MRF in the sense that in that they created a likelihood term for each pixel and a spatially coupled prior in the form of a random field. One of their contributions was to propose and explore the use of Gaussian mixture models for the local likelihood terms, thus introducing hidden variables and producing a hidden MRF. They then used the Expectation Maximization (EM) algorithm to perform unsupervised learning in the hidden MRF for segmentation. While exploring the problem of general interactive image segmentation Blake et al.&amp;nbsp;[5] used a similar Gaussian mixture model-based random field (GMMRF) approach along with a small amount of training data. This allowed one to learn MRF models in a supervised manner; however, the model still had the form of a traditional MRF defining the joint probability of features and segmentation class labels and did not learn pairwise term parameters. Lafferty et al.’s landmark work [25] on CRFs presented a new MRF approach based on directly modeling the conditional distribution of a field of labels. Their work focused on the use of chain-structured CRFs and applications in natural language processing. Soon after, Hebert&amp;nbsp;[23] explored the use of two-dimensional lattice structures for general image segmentation. While there has been an explosion of activity in the general computer vision community using CRFs, there has been comparatively less exploration of CRFs in the medical image segmentation literature. Some notable previous work includes Tsechpenakis et al.&amp;nbsp;[48] who coupled CRFs with deformable models for 3D eye segmentation. The deformable model captures shape information and is fed to the CRF model as observations. Lee et al.&amp;nbsp;[26] used pseudo-CRFs for segmenting brain tumors. As global inference in a true CRF can be expensive during learning, they broke the problem up into two components, one of them not depending on spatial interactions and another term that accounted for spatial interactions which they view as a regularizer. Bauer et al.&amp;nbsp;[1] used CRFs and SVMs for segmentation predicting tumor regions in brains. Monaco and Madabhushi&amp;nbsp;[30] model MRFs using conditional densities that they obtain from Markov Chain Monte Carlo methods. While modeling choices are a key part of solving the problem of image segmentation, the choice of image features is equally critical. Image features based on HOG features [12] have generated considerable interest in computer vision for tasks such as human detection in photographs. While HOG techniques have received an explosion of exploration in computer vision there has been comparatively less work in medical image analysis. There has been some recent work using HOG-like techniques such as that of Motwani et al.&amp;nbsp;[32] which used a HOG-like feature for segmenting brain structures from diffusion tensor (DT-MR) images. Graf et al.&amp;nbsp;[15] recently explored the use of a pyramidal HOG for detecting vertebrae in 2D CT imagery. We are motivated to use HOG-based features here as oriented gradients provide a way to capture shapes and partial curves through gradient profiles which can be more robust to variations across patients and different spatial contexts. To the best of our knowledge, our earlier work in Bhole et al.&amp;nbsp;[3] and the extensions we present here represent the first explorations of HOG features for multi-organ 3D CT image segmentation and our experimental evaluations confirm their utility. Other exemplary work on organ segmentation such as Seifert et al.&amp;nbsp;[40] used landmark detectors consisting of 3D Haar features for image volumes. They target full body segmentation and use six organs for segmentation. The landmarks are used in an MRF framework to obtain organ centers. After obtaining organ centers, marginal space learning classifiers which are a sequence of Probabilistic Boosting Trees are used to perform organ segmentation. They achieve full-body organ localization and segmentation using a hierarchical and contextual approach. In contrast, Ling et al.&amp;nbsp;[27] used a hierarchical approach, marginal space learning and steerable features to segment a liver and improve upon previous methods of shape initializations. Other recent work by Criminisi et al.&amp;nbsp;[11] has used regression forests to detect and localize abdominal organs. Regression forests could be integrated into the MRF approaches we explore here. Varshney&amp;nbsp;[50] provides a survey of the different approaches that have been used for segmentation of abdominal organs. Neural networks, level set methods, model fitting and rule-based methods have received particular attention. To the best our our knowledge, there is in fact no previous work on 3D volumetric segmentation of adrenal glands in particular. Our work is thus the first to explore this important problem and we provide our segmentations to the community as a resource. In the next section we provide a focused, concise review of the probabilistic graphical modeling techniques that we use to construct our models. Probabilistic graphical models are defined using graphical structures that consist of a set of nodes which represent random variables and edges that represent the relationships between the random variables. Directed graphs have edges with directions while undirected graphs do not have any associated direction with the edges. Bayesian networks are probabilistic graphical models constructed using directed acyclic graphs (DAGs). Undirected graphs can also be used to represent graphical models. Markov random fields are popular in computer vision and they can be illustrated by drawing undirected links between variables for which there exists a relationship that is modeled by the MRF. However, there is often ambiguity in the graph as relationships involving more than two variables are not explicitly represented using undirected edges between pairs of variables. For this reason and others in our discussion here, we will use another kind of graphical structure known as a factor graph [22], which is capable of expressing both Bayesian networks and MRFs within the same framework. As mentioned above, a Bayesian network is a DAG and it is one of the most common examples of a directed graphical model. It has no cycles or loops in its structure when one accounts for edge orientation. A Bayesian network defines a joint probability distribution consisting of the product of   conditional and unconditional probabilities for subsets of variables with the form       where   are the set of nodes that are parents of node  . An unconditional distribution is used for any node that has no parents. Loops may exist in a Bayesian network as long as all edges in the loop do not have the same orientation. However, such graphs must be converted into a higher order graph that forms a tree by grouping variables to perform exact inference for probabilities of interest. See Huang and Darwiche&amp;nbsp;[ 19] for the most popular approach to this transformation based on producing junction trees. Probabilities of interest can be computed exactly in a junction tree by passing messages between the cluster and their intersections. For the lattice-structured models, we use here such techniques quickly become intractable. The joint probability distribution of a MRF can be represented as a product of local potentials (functions) each of which depends on a subset of the variables of the graph such that       where, the   refers to the local functions and   refers to the subset of the variables for the local functions  . For an MRF, the local functions are chosen so that the conditional independence properties are satisfied.   is commonly called the partition function.   is a constant and does not depend on any variable or node of the graph which is in sharp contrast to what we shall see later with CRFs. As one can now see, factor graphs can represent both Bayesian networks and MRFs in a common visual framework as they simply provide a visual representation for the decomposition of a function concerning a set of variables into the product of local factors taking on subsets of variables. Since we will commonly be using two types of variables: observed or input variables and output or label variables, we rewrite the above equations using these variables. We use the notation   to refer to observed variables or input variables and   to denote the output or label variables. The equation of the joint distribution is thus written as:       Factor graphs [ 22] are more explicit about the factorizations that are implied by the graph as they explicitly include function variables that are connected to each variable within the argument of the function. Factor graphs are thus often easier to work with and we will often convert directed or undirected graphs to factor graphs for our work here. Factor graphs are bipartite graphs because they have two types of nodes: variable nodes and factor nodes. Variable nodes are connected only to factor nodes and vice-versa. Factor graphs are more general in that they simply illustrate the way in which a function of multiple variables factorizes into the product of local functions that involve subsets of variables. In other words, given a function   with a collection of   variables  , a factor graph illustrates how   factors into the product of local functions  , each having a subset   of the variables as arguments, i.e., for a function that decomposes into   local functions a factor graph expresses a factorization of the form       While we will often use this framework to illustrate the factorization of complex probability distributions, as we can see from ( 4), the framework itself is defined for general functions, not just probability distributions. Factor graphs also allow us to illustrate models that consist of a mixture of directed and undirected relationships or Bayesian networks and MRFs within a common graphical structure. Generative models are directed graphical models such that the input or observed random variables  always follow or come after (in topological ordering) the output random variables . Given the task of sampling from the distribution defined by a generative model, one can begin with the variables without a parent, drawing samples from unconditional probability distributions. One can then proceed to generate samples for children in the graph in a straightforward manner. As such it is common to refer to such models as generative models. In a generative model containing conditional distributions for observed variables , as with Bayesian networks, we draw arrows between the parent(s) and the child involved with the conditional probability. In the generative model, we model the  distribution over all the variables as . Using the chain rule, we see that to model the joint distribution, we need to model  as well as the prior distribution . In general there can exist dependencies between  variables and therefore it is difficult to model the relationship amongst the  variables and hence difficult to model . A discriminative model on the other hand models the conditional distribution   directly and hence the approach does not involve modeling either   or  . For generative and discriminative models with the same model complexity, focusing on the conditional distribution of the classification can have advantages. The graph of the discriminative model is shown with arrows going to the output label in the case of a Bayesian network, or as simply a set of factors in a factor graph which parameterizes a conditional distribution. In a discriminative probabilistic model   is often not treated as a formally defined random variable, and thus the Bayesian network for such models is not well defined. However, one can use the empirical distribution of  , i.e.,   to define a valid joint distribution consisting of  . The first row of Fig.&amp;nbsp;  illustrates a well-known generative discriminative pair, the naive Bayes model (left) and an analogous logistic regression model (right). The observed data or features of interest   are indicated by the shaded nodes. The naive Bayes graph factorization implies       The analogous probabilistic discriminative technique (i.e., with a similar factorization) is a logistic regression model, which we show in Fig.&amp;nbsp;  right. The model has the form:            Note how the feature functions   are explicitly dependent on the class label in ( 6). The model can also be written in the form:       where   refer to the feature functions and   are the parameters of the model using a suitable definition of feature functions and a single index. In logistic regression, the feature functions are often simply indicator functions times the value of the corresponding  . In our image segmentation example, if the random variable   denotes the voxel intensity, the feature function can be used as a switch to select specific   values for a particular range of voxel intensities. Now we extend this generative–discriminative pair to one where each of the models seen previously, the naive Bayes and logistic regression are connected to form a chain-like structure. The generative version of the model is the hidden markov model while its corresponding discriminative version is the linear chain CRF. Both these models and more complicated structures are shown in Fig.&amp;nbsp;. The discriminative versions of the models have been represented by factor graphs. It is important to note that many previous formulations of traditional MRFs in computer vision have used a conditional probability model for observations given labels, then coupled labels using a prior in the form of a lattice MRF structure. These correspond to a mixture of directed and undirected graphical models. Factor graphs are able to represent both these more traditional MRF formulations as well as CRF formulations. A more detailed comparison between generative and discriminative models is discussed in Bishop&amp;nbsp;[4]. Interestingly, Ng and Jordan&amp;nbsp;[34] have shown that generative models achieve their (lower) asymptotic test error rate with much less training data than corresponding discriminative model. However, given more data, discriminative models outperform the generative counterparts. There is some controversy over the practicality of this observation and this is discussed in Xue and Titterington&amp;nbsp;[55]. However, these insights suggest that there could be advantages of combining both generative and discriminative modeling approaches depending on the amount of the data. These hybrid models have shown to be promising as seen in [13, 31, 37]. Now we have seen that a CRF is a type of discriminative model, we define precisely what it is. A CRF simply means a random field which has associated with it the conditional distribution   where   would denote the output variables and   denotes the input variables [ 42]. Again, we mention for the general case of CRFs, the model can be written as follows:       where the feature functions   and parameters   depend on the clique. The product over   indicates that we take the product of the clique functions. In many computer vision and language processing applications however, the cliques have the same repeatable pattern and so the cliques can share the same feature functions and parameters. The equation above can thus be re-written as       where   is the normalization constant,   refer to the parameters of the model and   are the feature functions. The normalization constant is given as follows:  . The log-likelihood of the conditional distribution is given by:       Linear chain CRFs are now widely used for sequence processing applications [ 42]. As outlined in the original CRF work of Lafferty et al.&amp;nbsp;[ 25], linear chains have the advantage that exact gradients can be efficiently computed using a variation of the calibrated forward backward algorithm from the hidden Markov model literature, or simply the sum–product algorithm within the corresponding factor graph. Shallow 2D lattices are also well known in natural language processing; however, the simple addition of a second layer of hidden variables leads inefficient exact calculations and one must resort to approximate inference techniques during gradient-based learning. In computer vision 2D lattices are extremely popular, 3D lattices are often used for inference with hand-specified parameters as in practice as the computational challenges of learning are even more extreme. Fortunately, a combination of recent developments and insights has allowed us to perform full 3D inference and parameter learning in our work here. The impressive performance of kernel methods such as the support vector machine (SVM) [10, 53] and kernel logistic regression (KLR) [58] has lead to much interest in their use in structured settings. Structured models based on the SVM hinge-loss and maximum margin approach have been presented by Taskar et al.&amp;nbsp;[45] as the Max-Margin Markov Network (M3N), and Tsochantaridis et al.&amp;nbsp;[49], in a more general approach with similar objectives known as the structured SVM. The structured extension of the KLR is best represented by the kernel conditional random field (KCRF) [24]. In both cases, the feature functions  are replaced by kernel functions  and regularized using the  penalty. The main difference between SVM-based-structured models and the kernel CRF is the loss function, as the log-likelihood loss in the kernel CRF is replaced by the hinge loss function of the SVM. The main benefit of the choice of loss function is that it leads to a sparse solution, more easily seen in the dual formulation. The KCRF, on the other hand, requires alternative measures to ensure sparse solutions. In a full KCRF, the number of parameters can be very large as there is no restriction on the number of basis functions, typically the size of the entire training set. In Lafferty et al. [24], a greedy iterative approach is used to reduce the required number of kernel functions. However, the main benefit of the KCRF is that, as a fully probabilistic model, it allows for the use of standard probabilistic modeling tools. Learning and inference, therefore, can be accomplished using methods used in any CRF formulation. However, it requires many iterations to reach an optimal solution. Meanwhile, the M3N optimization is a Quadratic Programming (QP) problem that is usually very large in practice and therefore slow or sometimes impossible to solve. In both cases inexact methods are usually required. As the M3N derives from the SVM and CRF, the KCRF can be seen as a structured version of the import vector machine (IVM) [58]. The IVM is a KLR method which uses a greedy approach for kernel basis selection, in a manner similar to stepwise regression [18]. However, sparse methods such as the relevance vector machine (RVM) can also be used to derive sparse KLR solutions [46]. In the RVM, a Bayesian approach which uses a carefully chosen prior results in the desired sparsity. However, the Automatic Relevance Determination prior used in Tipping [46] requires an approximate Hessian, which leads to its own complexity issues. A “fast” version of the RVM was presented which did not require computation of a full Hessian was presented in [47], but resembles the IVM in that it uses a selection set chosen by a scoring function. Although the scoring function is more principled, it requires multiple solutions, bounded by the size of the selected set of basis functions, to a KLR problem that is already quadratic. An alternative approach is shown in Rim et al. [38]. In this case, rather than full Bayesian inference, the use of an Exponential hyperprior for the variance results in an Exponential prior, which approximates an  penalty, as opposed to an  penalty which will not generally yield sparse solutions. The resulting prior provides shrinkage and sparsity, approximating the penalty of the RVM in a MAP setting. The extension of the prior to the CRF model in this paper is straightforward in that the components of the method are broken into two alternating steps. First, the prior parameters (the variance of the prior) are updated and the weights of the local and interaction potentials are updated based on the updated variance parameter and choice of inference procedure. Details of our method are provided in Sect.&amp;nbsp;3.2.2. Learning can be more formally defined as an optimization procedure for finding the parameters of a given model. For learning in MRFs, there are two broad categories of techniques: those based on maximum likelihood (ML) and those based on variations of the maximum margin (MM) approach made popular by SVMs. Given labels   and features   in ML, one is typically interested in maximizing one of three probabilities over some training data: the log marginal probability of features   (ex. a quantity often used for unsupervised learning), the log joint probability   (ex. used for learning with complete data) and the log conditional probability of labels given features   (ex. discriminative learning). In these typical ML learning settings, learning based on some form of gradient descent will lead to computation for gradients that take the following forms:                                      where   represents the feature function in the exponent of the MRF model and   denotes the empirical distribution. We see that in each case we have to compute terms that are the difference between an empirical expectation and (a more complicated) model expectation. In the case of maximum margin-based learning one tries to maximize the margin between the ground truth solution and the next best set of labels and the problem can be formulated as a quadratic program. In ML-based learning with tree- or chain-structured graphs, inference is exact and one can compute the model expectations needed in (11), (12) or (13) exactly. Importantly, in linear chain-structured CRFs the process of conditioning on observations typically leads to inference in an even simpler chain-structured graph. In this setting, the ML optimization can be achieved effectively using a variety of well-known gradient descent-based optimization techniques. Recent work has shown that classical quasi-Newton optimization-based Broyden–Fletcher–Goldfarb–Shanno (BFGS) updates or on limited memory versions (L-BFGS), which approximate the Hessian matrix [8], can be extremely effective for CRFs as discussed in Sha and Pereira [41]. However, for even faster optimization with large data sets other recent work has revisited formulations using online learning or gradient updates over mini batches as well as additional methods to further accelerate learning using stochastic gradient descent by learning the learning rate [51]. In many problems in vision however, a lattice-structured model is used and there are potentially long range dependencies across an image such that one may wish to propagate information more globally. In our case here we use extremely loopy graphs, 3D lattices. While the use of CRF techniques does reduce some of the 3D graph complexity by not requiring inference for variables that have been conditioned upon, the problem of finding  in (13) remains fundamentally a problem of inference in a 3D lattice and quickly becomes intractable. There are in fact two important types of inference we need. The first is inferences about the marginal probabilities of variables, which is needed to compute gradients during learning. The second is inferences about the most probable configuration of the model, which is needed to provide a single, most probable segmentation under the model. In linear chains the problems are solved via variants of the forward–backward and Viterbi algorithms (or the sum–product and max-product algorithms), respectively. In loopy graphs, we must resort to approximate inference for both of these essential calculations. In maximum likelihood-based learning, one can view the role of approximate inference as simply a way to obtain an approximate distribution that will be used for the second expectation in the gradient equations of Eq. (11), for unsupervised learning; (12), for generative learning; or (13) for discriminative learning. Fortunately, as these expectations of interest involve functions of only subsets of variables we can reduce these expectation computations to the problem of first computing approximate marginal distributions over the variables of interest, then computing the final interaction with the local functions. To achieve this goal of computing the expectations of interest which corresponds to the problem of computing the approximate marginal inference, a variety of different techniques have been proposed. We use belief propagation (BP), variational methods and graph cuts for our models. We present the details of these algorithms here and explore the advantages and disadvantages of these techniques for learning in our models in detail in Sects.&amp;nbsp;3.3 and 4. Belief propagation [36] is a popular message passing algorithm that solves inference on trees exactly. It is also referred to as the sum–product algorithm. When belief propagation is used on general graphs that has cycles, it is termed as loopy belief propagation. In BP, messages (a combination of potential functions) are passed from the leaves of the tree to the root and then back from the root to the leaves to obtain the inference solution. The leaf node message values are initialized with unit values. For loopy graphs, there are no clear leaves and so all nodes are initialized to have initial messages of unit value. Since they have no clear root either, the idea here is to continue message updates in an iterative fashion. Though convergence might not be guaranteed, it has been observed to perform well in many practical applications and the approach is widely used for inference and learning in MRFs and CRFs. Max-product involves simply finding the max of the message values in place of their sums and allows us to find an approximation to the most probable explanation (MPE) over the joint distribution of the variables defined by the underlying model. The general technique of variational methods involves breaking a complex optimization problem up into a set of simpler solvable problems and finding an approximate solution using these simpler problems. With respect to inference in graphical models, variational methods involve a technique to formulate inference and estimation on smaller tractable graphs and obtain an approximate solution for the main graph.
Springer.tar//Springer//Springer\10.1007-s00138-013-0498-9.xml:Homography-based depth recovery with descent images::  1 Introduction  America has succeeded in landing both on the Moon and Mars and released rovers to explore the planetary surface, which encourages other countries and organizations to study the deep-space exploration technology, including European Space Agency (ESA), Russia, Japan, China, India, and other countries. Russia has also succeeded in landing on the Moon and explored the Moon with rovers. China just started the Moon Exploration Project and plans to send rover to the Moon surface. Landing to explore the Moon and Mars has attracted more and more researcher’s attention. To explore planetary surface like the Moon and Mars in detail, a rover should be sent to its planetary surface by a lander. To land on the surface, a site should be well chosen for safe landing and easy roaming. The global landing site can be selected with orbital camera images [1, 2], however, the local landing site should be chosen with the images taken by the landing camera during descent because the resolution of the orbit camera is restricted with the current technology. To choose the best landing site, the surface should be reconstructed with the descent images taken by the landing camera. Surface reconstruction with images is a general problem in computer vision. However, the discussed scenario differs from traditional reconstruction problem in some aspects. At first, the chosen landing planetary surface is mostly plain, which makes the traditional motion recovery from images unstable in the scenario. Second, the texture of the surface is not rich, which makes the extracted features sparse and the dense reconstruction hard. Third, the descending motion is nearly vertical to the surface during the near-planetary period and the epipoles lie in the near center of the images, which makes it impractical to rectify the adjacent frames in the manner that the traditional stereo techniques utilize. At last, in 1998 Mars Surveyor Lander and 2004 MER Mars Polar Lander missions, the descent camera all took photos during descent every time the altitude to the surface halved [1, 3, 4]. This large movements in the camera positions would produce a sequence of images doubling the resolution of the images at each adjacent frame, which makes it hard to image correlation and feature tracking. All these differences make the the planetary surface reconstruction from descent images a challenging problem. The problem discussed here is essentially depth recovery from monocular camera images. There have been many literatures about surface reconstruction or depth recovery with images by monocular camera, and the general reconstruction methods with monocular camera have been proven effective [5–8]. But few camera motions of the previous work are along the optical axis, and there was no such large movements in the camera positions that doubles the resolution in adjacent frames. Xiong et al. [3, 9, 10] in JPL studied this problem and proposed a feasible reconstruction method. They used Forstner features correspondence and the initial information of IMU and elevator to recover the camera motion, and compute the depth from spatial SSD correlation matrixes with descent image pairs. But their method is complex, and the feature matching and correlation computation are not robust. Pei et al. [11] studied 3D reconstruction with image pairs under camera motion along optic axis, but their reconstruction is not dense reconstruction. O’hara and Barnes [12] proposed a new shape from shading (SFS) technique and applied it to Mars Express HRSC (high resolution stereo camera) images. They used just one image to estimate the terrain shape, but the depth information could not be recovered. Kanade et al. [13, 14] proposed a factorization method that could recover the shape and motion from image streams that could be used in general case, but a large number of frames were needed. In this paper, we improved Xiong’s method to recover the depth from descent image pairs using a series of homography transformation induced by virtual parallel slicing planes. This work has similar characteristics as Xiong’s [3] method which uses adjacent descent image pairs to recover the dense exact depth information, but differs in three aspects. At first, SIFT features and epipolar lines are utilized in the motion refinement instead of Forstner features. SIFT features are invariant to scale change and image rotation, therefore it simplifies the features matching operation and is more robust. Second, the homography is utilized in the depth recovery, which makes the process more clearly. Third, a different correlation method is proposed to substitute SSD correlation and the match is optimized by penalizing changes of neighboring disparities. This paper is organized as following. In Sect.&amp;nbsp;2, we make a brief summary of the current surface reconstruction method and discuss their feasibility in the discussed issue. In Sect.&amp;nbsp;3, we analyze the characteristic of disparity and resolution change in descent images. The choice of adjacent images is elaborated. In Sect.&amp;nbsp;4, the method to recover the depth is presented. The first step is motion refinement by SIFT feature correspondence. The second step is depth recovery based on best correlation using a series of parallel slicing planes. The correlation problem is essentially analogous to conventional method and is discussed in Sect.&amp;nbsp;5. Section&amp;nbsp;6 shows the experimental result of simulative descending motion and error test. The results show that the improved method is valid and feasible in the discussed issue.    2 Overview of reconstruction methods  For landing terrain surface reconstruction, there are two solutions. One is using depth camera, i.e., 3D LiDAR camera. Depth camera can acquire the surface shape directly and quickly [15]. However, the resolution and work distance of current 3D camera is low, for example, the resolution of SwissRanger SR4000 is QCIF (), and its work distance is maximum 10&amp;nbsp;m. There are some onboard LiDAR system which can work in further range and can acquire the 3D data too. But the resolution is still low and it works like a scanner which needs lateral motion and exact motion information. The other is reconstruction with the images captured by the normal optical camera. There are two kind of ways to reconstruct the 3D shape with normal optical camera, one is binocular stereovision, the other is monocular vision. In the landing task, the distance between the lander and the planetary surface is large in most of the descent process. Due to the size constraint of the lander, it is impossible to set big baseline for binocular cameras. Then in most descent process, there is few parallax in binocular vision images. Therefore, the stereovision is not applicable in this task. Hence, there is only one downward-looking camera in the lander. For reconstruction with monocular camera, there are also many methods proposed. They can be divided into two categories, one is with single image, and the other with two or multi-images. The most common surface reconstruction method with only one image is SFS. SFS is a technique that computes the 3D shape information from a single monocular image under assumption that the light source direction is known and the surface is a Lambertian surface with constant albedo [16–18]. There are also some methods to estimate albedo [19], illuminant direction [20, 21] and both [22, 23]. O’hara [12] applied a new SFS technique to Mars Express HRSC images. Barmpoutis et al. [24] used SFS to 3D reconstruction of ancient inscriptions. SFS seems useful in planetary surface reconstruction task because the descent process seems ideal for the application of SFS. The far away Sun light is the ideal parallel light source and the light direction can be acquired by Sun sensor. The distance between the camera and the surface can be get by altimeter. The surface is rather smooth and coherent when the lander is far away from the surface. However, since it is impossible to calibrate some SFS parameters in the landing task, the reconstructed result is similar to but not precise as the true surface shape. Namboodiri [25] proposed another method to recover depth characterized by the defocus from a single image, but only relative depth was recovered and the assumption was complex. Structure from motion (SFM) is a general approach to recover the 3D shape of an object from observed multiple images [26, 27]. Given a set of observations of 2D point correspondences across the frames, it is possible to recover both the 3D positions of the points and the underlying motion between frames [28]. Szeliski [29] and Molton [30] studied SFM problem when feature points came from planes. Dellaert et al. [31] even proposed a SFM method without correspondence. However, the camera motion in these researches is generally a lateral movement, while the motion of landing camera is nearly along the optical axis. There are also reconstruction methods with images along optical axis movement. Depth from focus (DFF) is a technique that utilizes an imagery along optical axis to recover the shape [32, 33], which apparently seems the plausible solution to the discussed issue. However, DFF requires that the motion is strictly translation along the optical axis, and each image is partially clear and partially blurry. But during the lander descents, there is minor but not negligible lateral movement. Furthermore, due to the depth of field of the lander camera, there is no distinction between clearness and blur in the imagery. Depth from defocus (DFD) [34] used differences in blur between two scene images at same position under different len apertures as cue to estimate the depth. In the landing task these conditions cannot be guaranteed. Depth from optical flow is another way for 3D reconstruction with two or multi-images. Prazdny [35] studied self-motion estimation and relative depth recovery based on optical flow. Heeger et al. [36] proposed a simple method to recover the 3D motion and depth from optical flow which could solve the translation and rotation of the camera separately. Adato et al. [37] proposed an algorithm to recover shape from specular flow. Depth from optical flow relies on the accurate computation of flow field, but the dense and accurate computation of optical flow is also difficult. JPL developed a two-stage method to compute the depth from the descent imagery [3, 9, 10]. They used adjacent image pairs, one was taken at the half altitude of the other image, to recover the depth. Experiments apparently validated their proposed method. But in their paper, they did not elaborate why they used adjacent images that was taken at the half altitude of the other. Their method is also complex in image projective warping. For image projective warping, Vizireanu et al. [38–40] studied the interframe interpolation methods based on morphological shape decomposition.    3 Analysis of descent images  For space landing missions, it is likely that the motion will be nearly perpendicular to the planetary surface. In American 1998 and 2004 Mars lander experiments, the descent camera all took photos during descent every time the altitude to the surface halved. In other words, there would be roughly a scale factor of two between adjacent frames in the sequence. A similar scenario is likely in future missions. The large change of scale prohibits us from tracking many features across several frames. For most features, we limit our correlation and depth recovery to adjacent frames for this reason. In this section, we will model the descent process and analyze the characteristic of disparity and resolution change in adjacent descent images. Figure&amp;nbsp;  shows the scheme of the descent camera, where   is the len’s optical center of camera,   is the reference frame of camera with   coherent with optical axis,   is the image sensor,   is the visual field of the surface which is supposed to be a plane perpendicular to the optical axis,   is the focal length of camera and   is the height of the lander to the surface. Suppose   be the intrinsic parameter of descent camera which has been carefully calibrated in advance and kept unchanged. Let   be the time when the first image is captured and the camera reference frame at   be world frame. For any point   on the terrain its image point is,       where   is homogenous coordinate. At  , the lander descents distance  . Suppose the descent motion is strictly along the optical axis, the new image point of   is,       where  . Note that,        and  . Suppose   and subtract Eq.&amp;nbsp;( 2) from Eq.&amp;nbsp;( 1),       Let   denote the coordinate of   relative to the image center(main point).   denotes the disparity of point   between   and  , then we get,       From Eq.&amp;nbsp;( 4), it can be seen that the point disparity is dependent on its distance to the image center and the ratio of descent height to elevation. The relation is shown in Fig.&amp;nbsp; . When  lies in the image center, the disparity is always zero. When  is 1&amp;nbsp;pixel distant from the image center, it requires that  to make there 0.5 pixel disparity. This means that if a dense reconstruction map near the image center is required, the heights of two adjacent images should be halved in order that the disparity of neighbor pixel around image center could possibly be detected. Here, we suppose the minimum error for subpixel feature extraction is 0.5 pixel. Note that the requirement is independent with the camera intrinsic parameters. However, under this requirement, the disparity would vary greatly from image center to border. For example, if the resolution of the camera is , then the disparity of border pixel will be 256 pixels while zero pixels at the epipoles. Furthermore, the terrain resolutions vary greatly too. From pin-hole model of camera, we know that   and  . By combining this we get,       where   is the area of view field and   is the valid area of CCD sensor. When the lander camera is given,   is constant. Equation&amp;nbsp;( 5) shows that the viewing area of lander camera is just determined by the height when the lander camera is given. Let the first image be taken at height   and second image at  , then the scale of view field change is:       The scale is very big, which means the second view field is just quarter of first view field, but the second image is four times clearer than the first image. In the concrete task, there are vibration and lateral movement in the camera motion. But in total, they are very small compared with the forward motion. Therefore, the above model is applicable to this scenario.    4 Homography-based depth recovery  The method discussed here intrinsically belongs to SFM, which first should recover the motion from images, and then recover the depth information by triangle model. Therefore, to recover the depth, at first the camera motion is refined which aims to recover the accurate motion analysis that the epipoloar constraints can be strictly recovered. Depth is then recovered which uses a series of parallel plane-induced homography-based projective warpings to calculate the correlation to decide the depth. We need precise camera motion between two adjacent frames such that epipolar constraints are satisfied to subpixel accuracy. We may expect that the lander camera is rigidly attached to the lander and the lander onboard inertial measurement unit (IMU) can give the accurate motion information. But unfortunately, due to the cumulative drift error of IMU, it is unrealistic to expect the lander onboard inertial sensors to track the camera motion information with high precision, therefore fine camera motion information should be recovered from two or more frames. It is well known that with just image features correspondence, the rotation of camera can be estimated but the translation just can be estimated under an unknown ratio. Even worse, plain planetary surface is chosen as landing terrain. Generic motion recovery from planar feature correspondences is ill-posed because the rotation and translation are difficult to distinguish. However, although the onboard IMU cannot give high precision motion information, it provides a good initial motion estimation and the accurate motion should be near to the initial values. Therefore, this can be utilized to eliminate the singularity. Feature correspondence are the basis of motion estimation and rich set of feature correspondences are needed for fine motion estimation. Compared with other image features, SIFT feature has not only position information, but also direction information which is helpful to find correspondence. Furthermore, SIFT can provide rich set features and is robust to changes in image resolution and scale [41], which fits the situation that there is large-scale change in descent image pairs. Therefore, SIFT is utilized to extract features from adjacent images. Figure&amp;nbsp; a shows an example of the extracted SIFT features from adjacent images. After SIFT features are extracted, they are firstly matched with Euclidean distance. Then RANSAC method is used to exclude matches that do not meet epipolar constraints. Figure&amp;nbsp; b shows the final feature correspondences. The SIFT feature correspondences provide a rich set of observations to constrain the camera motion, even though the relationship between the locations of the correspondent features and the camera motion parameters is highly nonlinear. Let the correspondent features   at time   is   and   is  . The camera motion between time   and   is composed of a translation   and rotation  . The fundamental matrix between these two adjacent images is:       where   is the inverse symmetrical matrix induced by  . The eplipolar line of feature   in the first image is:       Let  , then the distance between correspondent point to epipolar line is,       Then the objective function we are minimizing is,       where   is the number of matched features. Since these features come from a nearly plane, to avoid the singularity problem, a penalty term is added to the objective function that prefers motion close to the initial motion measurement from lander inertial sensors, suppose to be   and  . The overall optimization objective function is,       We perform nonlinear minimization using the Levenberg–Marquardt algorithm. In the motion refinement, we have extracted feature points from the image pairs and found the correct match. Using the triangle theory, we can recover their depth information. However, the feature points are sparse, we want to recover dense depth map for every point or pixel. Therefore, we want to utilize a method like reconstruction with binocular vision. But the window correlation method used in binocular vision is not applicable to this problem. The first reason is that the two images have great changes in the resolution. The second reason is that the epipole lies near the image center. If we resample one image along the epipolar line to make them resolution equivalant as in stereo, then the images will be oversampled near the epipole while undersampled at the image boundaries.
Springer.tar//Springer//Springer\10.1007-s00138-013-0499-8.xml:High-dimensional MRI data analysis using a large-scale manifold learning approach:Sampling Brain tumor Manifold Progression:  1 Introduction  With the rapid advancement of diagnostic imaging technology, multi-dimensional, large-scale, and heterogeneous medical data sets are generated routinely in clinical imaging exams. For instance, magnetic resonance (MR) diffusion tensor imaging (DTI) has become a routine component of the brain MR imaging exams in many institutions. Together with the traditional T1, T2 or fluid attenuated inversion recovery (FLAIR)-weighted MRI scans, this new imaging modality has provided additional information and has shown potential for better brain tumor diagnosis. However, interpreting these large-scale, high-dimensional data sets simultaneously is challenging [1, 2]. For example, quantitative maps of apparent diffusion coefficients (ADC) derived from DTI imaging have been reported as useful indicators in distinguishing tumor tissue from surrounding edema by Sinha et al [3]. But other researchers found that the differentiation has not always been successful [4–6]. Lam et al. [7] reported that ADC values were not useful in identifying tumor types. These claims have been challenged by a number of researchers whose results have shown that high-grade gliomas have lower ADC values while low-grade ones are opposite [5, 6, 8, 9]. Analysis performed on fractional anisotropy (FA) values also showed contradictory results in FA’s ability for differentiation of enhancing tumor from edematous brain cancer [3]. Similar situations can also be found in meningiomas diagnosis. A significant difference in peritumoral ADC and FA values between low-grade meningiomas and high-grade gliomas has been reported by Bastin et al. [10]. This possibly reflects the presence of tumor-infiltrated edema in gliomas. On the other hand, Lu et al. [11] showed that there is no statistically significant difference of ADC and FA values between intra- and extra-axle lesions. No significant difference was found for ADC values between peritumoral hyperintense regions or peritumoral normal-appearing white matter and high-grade gliomas [12]. The above contradictory results may be due to methodological variations which include, for example, mismatch of ADC values with the biopsy examination results [1], the use of diffusion gradients applied in a single direction instead of along the three orthogonal directions, heterogeneous study group, differently categorized tumor types, patients having previous surgeries, adjuvant therapy, or use of steroids [2]. A single MRI scan may not provide enough discriminating power to reliably differentiate various tissues. Experiments using large-scale high-dimensional data sets from our recent studies [13, 14] have shown that one can efficiently differentiate brain tumor tissue from tissues in progressed and normal regions. In the previous study, we integrate all available MRI scans into a high-dimensional data set and perform a classification task in the high-dimensional space. Our results [13–15] indicated that an individual MRI scan is probably insufficient to reliably distinguish different tissue types. But taking advantage of multiple MRI images may lead to a successful classification because those different tissue types are located in different regions with possible overlaps in the constructed high-dimensional space. Due to the fact that significant correlations exist among these multi-dimensional images, a hypothesis is made that low-dimensional geometry data structures (manifolds) are embedded in the high-dimensional space. Those manifolds might be hidden from human viewers because it is challenging for human viewers to interpret high-dimensional data. When correctly extracted from the high-dimensional space, the hidden manifolds may provide particularly useful information for brain cancer studies. For example, one may investigate the residence of cancer and normal tissues on the manifolds to derive rules to accurately classify cancer regions. Moreover, the bridge manifolds connecting the cancer and normal tissue manifolds may provide hints for identifying cancer progression trajectory. This knowledge can be used for predicting future tumor growth and allow for better patient management. For example, tumor treatment can be adapted depending on the aggressiveness of the tumor growth. Many manifold learning algorithms essentially perform an eigenvector analysis on a data similarity matrix whose size is , where  is the number of data samples. The memory complexity of the analysis is at least . This is not feasible for very large data sets in terms of both computational and storage requirements for a regular computer. To solve this problem, statistical sampling methods are typically used to sample a subset of data points as landmarks. A skeleton of the manifold is then identified based on the landmarks. The remaining data points can be inserted into the skeleton by a number of methods such as Nystrom approximation, column-sampling, and Local Linear Embedding (LLE) [16–18]. To keep a faithful representation of the original manifold, effective sampling should be considered. Undersampling will distort true embedded geometry structures and thus lead to subsequent manifold learning failure. Oversampling may introduce unnecessary noise. For example, landmark multidimensional scaling (MDS) performs poorly for randomly chosen landmarks if the data is noisy (contains outliers) [19]. Also, data may sometimes collapse to a central point in the low-dimensional space if certain “important” samples are missing [18]. In this paper, we present a large-scale manifold learning schema for brain tumor study. First, a set of landmarks from a large data set was selected based on an importance function learned from the data. Next, a manifold skeleton was learned using the Isomap algorithm [20]. The remaining data points were then inserted into the skeleton using the LLE method [21]. There were several parameters to be optimized including the number of landmarks needed and the number of neighbors in the Isomap algorithm. Two cost functions were designed for optimizing the parameters. The method was applied to MRI data sets from several brain tumor patients aiming to identify normal, tumor and progressed tissues on the manifold. The contribution of this paper consists of two parts: (1) the integration of the existing methods toward a novel application and (2) a new sampling approach for large-scale manifold learning.    2 Method  In this section, the process of reducing a high-dimensional MRI data set into a low-dimensional manifold is discussed. The proposed method incorporates multiple signal processing methods including standardization, sampling, dimensionality reduction, feature selection and classification. In contrast to other manifold implementations that view an image as a whole, the proposed method considers each pixel location as a feature vector. This allows for decisions to be made locally. The MRI data of brain tumor patients were collected using various MRI scans including FLAIR, T1-weighted, post-contrast T1-weighted, T2-weighted, and DTI. Five scalar volumes were also computed from the DTI volume including ADC, FA, max-, min-, and middle- eigenvalues, thus yielding a total of ten image volumes for each visit of every patient. Each patient went through a series of scans with an interval of one or two months. A rigid registration was utilized to align all volumes to the DTI volume of the first visit using the vtkCISG toolkit [ 22]. After registration, each pixel location can be represented by a ten-dimensional feature vector corresponding to the ten MRI volumes. Figure&amp;nbsp;   shows images from the ten registered MRI volumes. Two visits were selected in this study and denoted as “visit 1” and “visit 2” where visit 2 showed an expanded tumor region. Hyperintensity regions were defined on the FLAIR scans as tumors. A similarly sized region far away from the abnormal regions was also defined as a highly confident normal region for training purposes. Figure   shows example MRI slices overlaid on the defined tumor and normal regions. Many current nonlinear dimensionality reduction techniques in the literature require an eigen decomposition of an   matrix. There are roughly 65k (each slice contains 256   256 pixels) high-dimensional data points in one MRI slice. Even segmenting just the brain region can usually yield over 30k points. Thus, applying a nonlinear dimensionality reduction technique directly on the data is computationally expensive. Therefore, an advanced sampling technique was developed to select a set of landmarks based on local curvature variations. A manifold was learned from the set of landmarks to produce a manifold skeleton. The remaining data points were then inserted into the skeleton using the LLE algorithm [ 21]. After that, a Gaussian mixture model (GMM) classifier was trained using sampled points from the tumor and normal regions defined at visit 1. Using the model, a posterior probability map was found for the entire brain region. By adjusting the threshold of this probability map, a tumor region and a progressed tumor region can be predicted. The system diagram of the proposed framework is shown in Fig.&amp;nbsp;  . In this section, each of these steps are described.  Different MRI scans do not follow a standard scale. Intensity values in MRI images can vary greatly between separate acquisitions. This is the case even for imagery taken from the same patient, using the same machine, and with the same technician. Intensity scaling was performed as a preprocessing step for MRI images to ensure that the intensity ranges for similar structures are consistent between MRI acquisitions [ 23]. This step is essential if direct data comparisons are to be made between data acquisitions. In this study, the g-scale standardization was employed for its robustness to abnormal structures [ 23]. The presence of abnormal structures such as lesions can create problems for other techniques that scale images using the global histogram [ 24]. The hyperintensity region of the abnormal structure can cause an unpredicted increase in the overall intensity of the image histogram and skew standardization values. In contrast, the g-scale standardization finds the largest g-scale region which avoids including abnormal structures. A g-scale region is essentially a large fuzzy connected region. Intensity markers are found within the largest g-scale region to perform a linear transformation to a standard scale. These markers are features derived from the intensity histogram such as the mean, highest intensity, and lowest intensity. The largest g-scale regions of MRI data tend to exclude abnormal structures thus reducing its impact on the standardization. The transformation performed is a piece-wise linear histogram transformation based upon markers found in the largest g-scale region. Figure   shows the g-scale transfer function. The main challenge in the g-scale standardization is to find a consistent structure in MRI images that can give reliable intensity markers in the presence of abnormal structures. The largest g-scale region of a typical MRI scan is a large white matter region [ 23]. To segment this region, [ 23] uses a fuzzy selector approach that separates the image into a multitude of fuzzy connected regions. Each of these regions are referred to as g-scale regions. More specifically, a g-scale region is essentially a fuzzy connected region where adjacent points satisfy a homogeneity condition  \(\left| f\left( c\right) -f\left( d\right) \right| &amp;lt;\theta _{g}\). Here,   and   are arbitrary neighboring points,   is the homogeneity threshold,   is the feature vector at a point, and   represents a dissimilarity metric. As shown in Fig.  , the largest g-scale region selected a large white matter region while avoiding abnormal structures since the transition to these areas do not satisfy the homogeneity condition. Once the largest g-scale region was found, a transformation to a standard scale was made by assigning three marker values to the g-scale region,  ,  , and  , where   denotes the mean intensity value, and   and   represent the upper and lower bound pixel values in the g-scale region. In order to exclude outliers in the g-scale region, a percentage   of the data points with pixel values smaller than the lower bound or larger than the upper bound were trimmed. Assuming that values for a standard scale were obtained, each intensity value   can be transformed to the standard scale through the following equations:            where   is the standard g-scale mean,   is the standard g-scale lower bound, and   is the standard g-scale upper bound. Those standard values were found by training as described below. The standard g-scale intensity markers were found in the training phase by averaging marker values over a set of MRI images. In this study, 60 training samples having abnormal regions were selected. It is important to note that the derived sequences have absolute values and thus, the standardization algorithm was only applied to the acquired scans, FLAIR, T1-weighted, post-contrast T1-weighted, T2-weighted, and DTI. In our study, the largest g-scale region was found from the FLAIR sequence and used as a reference for all sequences. This is because the largest g-scale region in the FLAIR modality is more consistent in selecting similar structures than the other sequences. For each training sample, the mean intensity value , lower bound , and upper bound  were computed. The average of the mean values  was calculated for the entire training set to produce . Similarly,  and  can be derived. This was repeated for the other acquired sequences to produce standard values for each sequence using the g-scale region found in the FLAIR sequence. The transformation function in Eq.&amp;nbsp;(1) was then used to transform each sequence to standard values. Ideally, all images will have the same scale after standardization.  To keep a faithful representation of the original manifold, landmarks used for the manifold skeleton learning should be carefully selected from the original data. Ideally, landmarks should be the smallest subset that can preserve the geometry in the original data. Figure   shows an explanatory dataset to illustrate the basic idea of LCV. Heuristically, to preserve the data structure after sampling, more data points should be kept near area ’A’ rather than area ’B’ in Fig.   because data structures near ’A’ change more abruptly. Based on this observation, an importance value was assigned to each of the points by computing the local curvature for it. For each data point in the data set, a neighborhood is formed by selecting  -nearest neighbors where   is a parameter specifying the number of neighbors. The local curvature   of each point   was found using the equation [ 25]:    where   represents the smallest singular value.   is an orthonormal basis of the tangent space of   neighbors. Normalization was performed by scaling of the magnitude of the tangent space projection  , where   is the mean of the points in the neighborhood. A probability density function,  , was obtained by normalizing the curvature values such that they sum to one,       Samples were then selected from the probability density function using importance sampling. It can be seen from Eq.&amp;nbsp;( 2) that the probability value for each point   is directly correlated to its curvature value  . Thus, points with higher curvature have higher possibilities of being selected. A small set of   representative points was selected yielding  . Figure   shows an example set of the points selected using LCV.  The overall goal of nonlinear dimensionality reduction is to represent the nonlinear distances along an intrinsic geometric data structure in the original feature space with Euclidean distances in a lower-dimensional manifold with the structure preserved. The Isomap method reduces a high-dimensional data set while preserving the geodesic distance between data points. If the -dimensional original input space is defined as  and the -dimensional Euclidean space  as the lower-dimensional manifold, then the problem can be summarized as finding a mapping of . Ideally, geodesic distance pairs in  will be equivalent to Euclidean distance pairs in . The Isomap method is a widely used nonlinear dimensionality reduction technique in many applications including wood inspection [26], visualization of biomedical data [27], and head pose estimation [28]. Isomap first constructs a  -nearest neighborhood graph   by connecting   to its  -nearest neighbors in  , where   is the number of data points in  . Based on  , Isomap then computes a similarity graph  , where   is the geodesic distance between data points   and   in  . Geodesic distance can be obtained efficiently using Dijkstra’s algorithm [ 29]. Once the similarity matrix   is established, Isomap utilizes the traditional MDS technique to reduce dimensionality by minimizing the following cost function:       Here,   represents a matrix of Euclidean distances   and   represents the   matrix norm   and   is the second-order variation of the geodesic distances that converts the geodesic distance into an inner product.    where    and       is a matrix of squared distances,   is a centering matrix and   is the identity matrix. The minimum of the cost function in Eq. ( 3) can be found by setting the first   eigenvectors of the matrix   as the coordinates for  . The residual variance of the data is decreased with each successive eigenvector. Once the number of eigenvectors reaches the intrinsic dimensionality of the manifold, the residual variance that is reduced by successive addition of eigenvectors bottoms out. The result of performing nonlinear dimensionality reduction is a  -dimensional representation of the sampled points where  \(d^{\prime }&amp;lt;d\). In other words, a mapping is made such that   and  .  Successful manifold learning depends on many factors such as parameter choices for the learning algorithm and numerical stability of the algorithm. In this study, the need arises to determine the number of landmarks to be selected and the number of neighbors to be used by the dimensionality reduction algorithm. Two cost functions below are proposed,    and    where   is the stress function and   represents the training accuracy after manifold learning. Intuitively, if the nonlinear manifold in high-dimensional space is successfully unfolded, the Euclidean distance between points   and   in the low-dimensional space will be the same as that of the geodesic distance in the high-dimensional space. If a set of labeled training data is provided, which is the case in our study, the   value is a good criterion to verify the manifold learning. Otherwise, the stress function can be used in an unsupervised manner requiring no label information.  Once we learned the manifold skeleton, we inserted the remaining data points into it using the LLE algorithm [ 18,  21] utilizing for each data point  , its  -nearest landmarks  ’s in the original high-dimensional space. A linear model was then computed to reconstruct   by minimizing the following function:    To embed the data point   into the lower-dimensional manifold, we reconstructed it in the low-dimensional space as   using the weights   derived above    where   were the low-dimensional representations of the landmarks  . An example of the LLE application is shown in Fig.  , where the red dots are landmarks comprising the manifold skeleton, and the yellow square is a data point to be embedded into the skeleton. After this step, all the original data points are converted to the  -dimensional nonlinearly reduced manifold space.  After the manifold was learned, the intrinsic dimensionality of the data was found to be around four or five. However, the learned manifold dimensions are usually not equally important for the subsequent classification. In this step, a feature selection method was performed to identify important dimensions. The post-manifold dimensions were ranked by the Fisher score [ 30], which is a widely used supervised feature selection method and is defined for a two-class problem as,    where   and   are the standard deviations of class 1 and class 2 for the   feature.   and   are the means of class 1 and class 2 for the   feature, respectively. In the experiments, the Fisher scores for features beyond the third highest were found to be two orders of magnitude smaller than the highest score. The lower scores were discarded in the subsequent classification step. After this step, the data set was reduced to three dimensions which can be easily visualized.  Finally, the data set was applied to a Gaussian mixture model (GMM) for classification. A benefit of using GMM is that a posterior probability mapping can be generated. The algorithm is explained in the Appendix since it is very common in the literature. The training samples used are the same points that were selected in the LCV sampling step. This is because the ground truths of these points are already known. Here, the training was performed in the reduced dimensional feature space, or more specifically, the three-dimensional space after Fisher score dimension selection. The hypothesis is that if the bridge manifold is correctly unfolded, a point can be classified as either normal or abnormal depending upon its distance to each cluster in manifold space. By applying the trained GMM model to all remaining points, a probability map can be produced for the MRI data set. The final classification can be computed by thresholding the GMM classification probability.  Small regions of false positives may occur far away from the known tumor region. Since these isolated regions are unlikely to be tumor regions, small clusters marked positive were considered as noise and discarded. This step was performed on the binary classification of the GMM classifier. A morphological close operation was applied to smooth the boundary of the predicted regions. The number of pixels contained in each disjoint cluster was found and only the largest region was defined as the final predicted tumor region.    3 Experiments and results  Figure  a–d shows results for the “Swiss roll” data set. Figure&amp;nbsp; a is the original data set having 2,000 data points. Figure&amp;nbsp;  b shows the learned results based on 900 randomly selected landmarks (  = 0.4527). Figure&amp;nbsp;  c is the result based on 900 landmarks selected based on the LCV concept (  = 0.4293). Figure  d illustrates the result for a very large Swiss Roll data set having 20k data points. A direct manifold learning for this data set is computationally expensive, so we utilized the manifold skeleton learned in Fig.  c and inserted all the 20k data points into the skeleton based on the LLE algorithm. Results in b is slightly worse than that in c as expected. Five other testing methods were implemented in this study for comparison.  The system diagram of the proposed method is shown in Fig. . First, MRI data sets were standardized using the g-scale standardization method. Second, the LCV sampling strategy was employed to randomly select a set of landmarks inside the defined tumor and normal regions for manifold skeleton learning. Third, the remaining data points were then inserted into the skeleton using the LLE algorithm. Fourth, three dimensions were selected by the Fisher score based on the selected landmarks and a GMM model was trained using the selected dimensions. Note that, only those selected landmarks from LCV sampling were utilized for GMM training. And finally, the trained GMM model was applied to the whole MRI scan to produce a probability map for abnormal tissue. A crisp classification was also generated based on the probability map by thresholding. 
Springer.tar//Springer//Springer\10.1007-s00138-013-0500-6.xml:Multi-cue hand detection and tracking for a head-mounted augmented reality system:Hand detection Tracking Human–computer interaction Augmented reality Hand-pose estimation 3D graphical user interface:  1 Introduction  Augmented Reality (AR) is the synthesis of real world and virtual (computer generated) graphics. In contrast to Virtual Reality (VR) in which the user is engaged in an entirely artificial world, in AR applications virtual imagery of objects is superimposed over the real world displayed to the user via wearable, hand-held or static display. With the decrease in costs of off-the-shelf hardware for sensing and computing, and the increase in processing speeds, AR systems have become commercially appealing, and applied to a number of different areas, such as design&amp;nbsp;[21], medical imaging&amp;nbsp;[16, 43], military&amp;nbsp;[15], education&amp;nbsp;[24] and gaming&amp;nbsp;[28]. Moreover, as the necessary hardware is getting smaller, wearable AR setups are becoming more and more popular in comparison to their static counterparts. This is due to the fact that the maximum interaction with a scene can be achieved when the user is mobile during operation and such freedom demands a portable and easy to carry wearable setup. The majority of the previous AR research focuses on pose tracking and virtual object registration for precise information overlay. New research efforts are shifting towards mobile phone applications because of their market penetration. However, considerably less research has been done on  with virtual objects in AR settings&amp;nbsp;[27], especially for wearable systems. When wearable systems are considered, the world around the user can serve as a 3D interaction space in which the Human–Computer Interaction (HCI) is performed. Necessary information such as tools, menus, etc. for HCI can be visualized in this space. Moreover, auxiliary devices for interaction and control such as keyboard, mouse and joystick are not desired since they introduce extra weight and cost to the system. The human hand is a natural input device to communicate and interact with the immediate surroundings. Therefore, utilization of the hands would be a natural choice as an interaction device. We designed a new HCI methodology for the use with a Head-Mounted-Display(HMD)-based AR system with stereo cameras, which exploits the user’s hands as an interaction device instead of other auxiliary equipments. The 3D position and the pose of the hand is utilized to render the graphical components. The 3D trajectory from the user’s hands is converted to click and drag events which trigger the widgets and control tools similar to traditional interfaces with mouse interaction from Windows, Icons, Menus, Pointers (WIMP) interfaces&amp;nbsp;[ 40]. This allows users to select and use tools, and supports many familiar widgets, such as menus, slider bars, text labels and icons (Fig.&amp;nbsp; ). Our work broadens the freedom of the user by eliminating extra hardware or markers for interaction. With the proposed system the user can manipulate and control the AR setup and interact with the environment. For precise menu overlay and usage, the user’s hands need to be localized and tracked with high precision. In order to achieve robust, accurate and real-time operation, we propose a novel hand detection and tracking method that probabilistically combines different visual cues such as color, depth, curvilinearity and intensity. This enables robust and precise hand detection and tracking under challenging conditions. The 3D trajectory and the 3D pose are extracted from the tracked hand. In the following section, related work is presented. Then the framework is described, followed by a detailed description of the detection and tracking algorithms. Afterwards, we present experimental results. In the final section, we conclude and discuss the future research.    2 Related work  A wide variety of auxiliary equipment is employed for vision-based HCI including color glove&amp;nbsp;[46], LEDs&amp;nbsp;[37], infrared transponders&amp;nbsp;[34] and markers&amp;nbsp;[39]. However, utilization of additional devices decreases the mobility of the system while increasing the hardware complexity. Some of these systems limit the performance of the user since holding a device which occupies one of his hands restrains the freedom for interaction. It is highly impractical or even impossible for some users (such as surgeons) to utilize these devices during operations. A more natural way of interaction, without any external hardware, is by use of bare hands, which has been investigated in many research efforts related to AR and HCI. There have been many attempts to detect hands and gestures in a 2D image plane from a single color camera and use them as a mouse replacement. Most of these approaches use only the skin color information to segment the hands&amp;nbsp;[2, 27, 29]. Although the skin color detection is efficient and easy to implement, the robustness of such methods suffers from variances in lighting conditions and backgrounds in which the hand is not the only skin-colored object. Another common approach to detect hands is background removal&amp;nbsp;[3, 42], in which the motion in an image is detected by subtracting the background and hand detection is performed on foreground regions. Also the combination of skin color and motion detection (via background subtraction) is used&amp;nbsp;[8, 35, 50]. However, background subtraction is not an option for a moving wearable setup since neither the foreground nor background is static. Detecting faces prior to the hand segmentation&amp;nbsp;[31] is another popular method to improve the skin color model from detected faces and reduce the search space to the spatially close regions to the face. These methods are mostly designed for teleconferencing applications in which the user is positioned in front of the camera and are not applicable to wearable systems where the cameras are not able to see the user’s face. Model-based techniques are also used for hand segmentation and gesture recognition with monocular setups&amp;nbsp;[9, 26]. In [9], the 3D pose of the hand is found from a single image using texture and shading. However, the proposed method assumes a static camera and a light source that is fixed relative to the camera. In [26], flock tracking of hands is introduced. They combine KLT features with color for tracking hands. However, the proposed method has the same drawbacks as other single camera methods when the background is of a similar color with the hands. Although 2D methods are useful for segmenting hands in a single view and can be extended to a stereo version, alone they are not able to provide robust 3D position of the hands or fingers and therefore are not suitable for our application. 3D model-based hand shape and pose detection is also employed&amp;nbsp;[11, 12, 30] and they mostly rely on fitting articulated 3D hand models on 2D image features. However, they are either confined to desktop environments/static backgrounds and require a precise hand segmentation or computationally too expensive for real-time applications. Different sensors besides standard cameras that are used for hand segmentation and gesture recognition are Time-of-Flight (TOF) cameras&amp;nbsp;[17, 44], thermal cameras&amp;nbsp;[1, 36] and structured light systems&amp;nbsp;[32]. But additional sensors to a standard stereo camera (which is also used for head-pose estimation in our framework) are highly undesirable since they introduce extra weight, power consumption and cost to the wearable setup. Recognizing hand gestures by using stereo vision is studied in multiple research efforts. The disparity cue provided by the stereo camera can help to solve the problems of monocular methods such as background subtraction and occlusions. However, most researchers utilize the depth information after 2D segmentation to locate the 3D position of the hand&amp;nbsp;[2], which still suffers from the drawbacks of 2D setups such as cluttered backgrounds. Assumptions such as static background/camera (for background subtraction) and a fixed distance between the camera and the user&amp;nbsp;[19, 49], and the visibility of the user’s face in the image&amp;nbsp;[10, 33, 45] are also employed in different research efforts. However, these assumptions are not valid in our case where the camera is moving and the face of the user is not visible. In&amp;nbsp;[5, 38], other multi-cue hand detection and tracking methods are presented. However, both systems are monocular and do not utilize the depth information. In&amp;nbsp;[38], Petersen and Stricker used a color tracker to estimate the main orientation of the hand which makes their system sensitive to lighting changes and similar colored backgrounds. In&amp;nbsp;[5], the presented algorithm does not involve tracking and uses only parallel lines and curvature to detect hands.    3 System overview  In order to realize the GUI&amp;nbsp;[ 40], the user’s hands must be detected and tracked. In this paper, we focus on the hand detection, tracking and pose estimation. The performance constraints and the requirements are derived from the GUI and we developed a system as depicted in Fig.&amp;nbsp; . The proposed method combines color, gradient, intensity and depth cues in order to detect hand/finger regions while achieving robust, accurate and real-time operation. The 6 DOF pose of the hand in the camera coordinate frame is approximated by a plane and its surface normal, and the center of the segmented hand is used to render a planar menu around it. Afterwards, the extracted regions are tracked between the frames in order to compute their trajectories. Finally, the trajectories and the pose are used to overlay necessary tools on the display for interaction or recognize the “click” gesture or perform drag. The proposed method enables the HCI system to be used in dynamic environments with a cluttered background and a moving camera since the combination of multiple cues eliminates the drawbacks of such environments. Moreover, the pose estimation method is robust against outliers that are originated from erroneous disparity estimates since the best plane is approximated by using the inliers. We used the video eyewear (glasses), Cinemizer OLED 720p (Carl Zeiss AG, Germany) to display the content. Two LifeCam HD-5000 (Microsoft, USA) webcams are stripped and mounted in front of the Cinemizer providing a full stereoscopic 720p resolution pipeline. The cameras can output color images with 720p resolution at 30 frames per second (fps). They are mounted approximately 6.5&amp;nbsp;cm from each other (shown in Fig.&amp;nbsp; ) as with human eyes, and are calibrated off-line to find the rotation and translation between the camera sensors (extrinsic parameters) as well as the camera matrices (intrinsic parameters). Synchronization of the cameras for the acquisition of the stereo images is achieved in software by using multi-threading, in which each camera has its own thread for image acquisition. The weight of the Cinemizer is 115&amp;nbsp;g and the attached stereo rig is 65&amp;nbsp;g.    4 Hand detection, tracking and pose estimation  Local gradient magnitude and orientation are utilized to detect curvilinear structures in multiscale. These structures are strong candidates for fingers of the user’s hand. However, there are other structures that are similar to fingers and exist in the background. These false detected regions are eliminated by integrating color and intensity cues. Moreover, the depth cue is involved in the detection process to reduce the search space and decrease false detections. Afterwards, a score representing the similarity to a hand is given to each pixel by combining all the cues. A gradient directed parameter-free probabilistic bottom–up aggregation method by using these scores is performed to group hand-like pixels and very small regions or outliers are eliminated. Section&amp;nbsp;4.1 introduces a color score using a color model created prior to operation. In Sect.&amp;nbsp;4.2, a curvilinearity score is defined which gives higher scores to curvilinear structures in the scene. We introduce a depth score in Sect.&amp;nbsp;4.3 using disparity maps, and Sect.&amp;nbsp;4.4 explains how these scores are combined into a global score. Section&amp;nbsp;4.5 introduces the tracking algorithm which utilizes the calculated global scores to track the hand between consecutive frames. Finally in Sect.&amp;nbsp;4.6, we explain the hand pose estimation algorithm that detects the 3D location and the surface normal of the hand by approximating the tracked hands with planes. Prior to hand segmentation, a color model for skin color is created. Many visual cues such as color, motion, texture and shape can be used to represent and segment human body parts. Among these features, color has an important role in detecting human presence in an environment due to the distinctiveness of skin color and the fact that the major similarity between different skins lies in the chrominance rather than intensity&amp;nbsp;[18]. Color is employed in detecting skin in an image, due to its robustness against scale variation, affine transformations and occlusion. Color space selection is an important issue for successful skin color modeling&amp;nbsp;[48] and various color spaces have been used to detect skin pixels&amp;nbsp;[41, 50]. We preferred the HSV color space, and use hue (H) and saturation (S) values since H and S are independent of the brightness (V) which makes it more robust compared to the RGB color space. Most methods that have been proposed to create a skin color model can be separated into two categories: parametric and non-parametric methods&amp;nbsp;[22]. Jones and Rehg [22] compared histogram color models (non-parametric) with Gaussian mixture models (parametric) and found histogram models to be superior in accuracy and computational cost. Therefore, we decided on the non-parametric color model in our research and used a Parzen window density estimation [13] method to eliminate the drawbacks of histogram-based methods such as bin location selection and discontinuity. Given training skin pixels   with color values  , the value of the estimated probability density function at the point   (the probability of measuring a color value   from a skin pixel class  ),   is given as       where   is the number of training skin pixels,   is a window function and   is the smoothing parameter called the bandwidth. A bivariate Gaussian kernel would be a proper window function choice since the contribution of each training pixel to the density estimation is equal to each other and decreases as the distance to the point increases. Therefore,   is selected as       Then the overall contribution of the training pixels   to the skin color model and the estimate of the probability of measuring a particular color value   from a skin pixel is given as       where   is the kernel bandwidth. The suitable kernel bandwidth   must be carefully chosen since too small a bandwidth might result in too little resolution while too large a bandwidth might lead to an over-smoothed density estimate. We considered the standard deviation of the color values,   and   of the training skin pixels and select   as       The selection of the kernel bandwidth using the standard deviation reflects the variation in the model into the resolution by adjusting the smoothing via variation. In our experiments, the created color model has   and   and therefore   is selected as  . The same procedure is repeated with non-skin training pixels to create a non-skin model  .       Training images captured under different illumination conditions are manually labeled as skin and non-skin pixels, and used in the training step. The final models are given in Fig.&amp;nbsp; . The probability of being a skin color pixel (probability mass function) is used as a score,  , for the final decision process. It is constructed by using the given skin and non-skin densities as a Bayesian posterior       where   and   can be defined as the ratio of the total number of skin and non-skin pixels in the training images The created hand models are robust against the illumination changes up to some level. However, as the user moves and the lighting conditions change drastically, the created models fail to represent the hands. We increase the robustness of the system by employing a second group of models that are calculated by using the pixels that are classified as skin in the most recent   frames. Then the final score for the pixel   is defined as       This rule encapsulates the current dynamics of the environment as well as the initial conditions. The parameter   can be adjusted according to the level of expected variation, and setting the value   and   gives relatively good adaptation in our case. Setting the   to small values increase the adaptation of the final color score which might corrupt the color score when the detected hand regions are not accurate and therefore results in failure in color scoring. However, as long as the   values are chosen conservatively, the filtering process will never decrease the performance. A finger can be approximated as a curvilinear structure (a line with finite thickness) in an image since it is surrounded by almost parallel edges, and it has a uniform color/intensity inside the finger. This representation can be exploited to extract cues for detecting candidate hand regions. Therefore, we constructed a filter utilizing gradient information to detect the center of curvilinear structures in multiple scales and calculate a curvilinearity score for each pixel using the response of this filter. A line structure (a finger) can be represented at different scales depending on the distance between the camera and the hand. We use a bar-shaped profile to represent a 1D segment of a finger region as shown in Fig.&amp;nbsp; . The center of the line represents the center of the finger. An ideal line centered at   with width   and height   can be represented as The lines with the profile given above can be detected by convolving the image with the second derivative of the Gaussian smoothing kernel       and locating the points where the convolution has a minimum&amp;nbsp;[ 6]. However, the convolution with the second derivative of the Gaussian smoothing kernel will be approximately zero for small   values such as   and there will be two distinct minima between  . In order to have a clear minimum at  , the center of the line, the sigma should be   &amp;nbsp;[ 25]. Also, to keep the maximum effect of the filter inside the line (between  /2 and  /2),   should be less than or equal to  /2. Moreover, the convolution of the line profile with the first derivative of the Gaussian kernel will be 0 at   for all  \(\sigma &amp;gt;0\). If we set the   and write the second-order derivative as a discrete derivative of the first derivative, then       where   is a proportionality constant. If we omit the constant   and write the above equation as the convolution with the Dirac-delta function  , then it becomes The convolution with the first derivative of Gaussian detects the right and left edges (at   and  ) of the bar-profile and the convolution with   gives the minimum response at   as shown in Fig.&amp;nbsp; . A 2D line encapsulates the characteristics of a 1D bar-shaped profile in the direction perpendicular to the line. Therefore, we can extend the 1D approach to 2D images by using the gradient direction. In order to detect the center of the 2D line section, we calculate the local orientation of the line and then apply the line filter for range of scales in this direction. The location with the minimum filter response is selected as the center of the line. Local orientation can be found by calculating the direction of the gradient vector at the edge pixels of the line. The direction of the gradient at pixel   is       where   and   are the derivatives of the image   at pixel  .
Springer.tar//Springer//Springer\10.1007-s00138-013-0501-5.xml:CM-BOF: visual similarity-based 3D shape retrieval using Clock Matching and Bag-of-Features:Non-rigid Bag-of-Features Local feature 3D shape retrieval:  1 Introduction  How to efficiently and effectively retrieve 3D models based on their shapes has become an active topic in several research communities including computer vision [30, 46], pattern recognition [40, 52], computer graphics [10, 38], and multimedia [12, 56]. With the development of various kinds of 3D shape retrieval benchmarks (e.g., PSB [49], ESB [20], McGill [50], NSB [14], etc.) and the successful organization of the 3D SHape REtrieval Contest (SHREC) [2], more and more researchers have been attracted to work in this area and a large number of algorithms have been proposed. Feature extraction plays an important role in 3D shape retrieval methods. Ideally, a good shape descriptor has the following desirable properties: (1) High discrimination; (2) Efficient shape matching; (3) Compact representation; (4) Efficient feature extraction; (5) Invariance to similarity transformations (for non-rigid 3D shape retrieval, descriptors should also be isometry-invariant [46]); (6) Invariance to shape representation; (7) Invariance to shape degeneracies and noises. Generally speaking, based on the shape descriptors used, existing 3D shape retrieval methods can be loosely classified into four categories [11]: statistics-based, graph-based, transform-based, and view-based. Recent investigations [9, 11, 30, 37] show that view-based methods with pose normalization preprocessing obtain significantly better performance in retrieving rigid models than other approaches and more importantly they satisfy almost all characteristics mentioned above. Among all kinds of view-based approaches, the visual similarity-based method which follows the idea that “if two 3D models are similar, they also look similar from all viewing angles” is considered to be the most powerful one. Therefore, despite having several intrinsic drawbacks (e.g., discarding invisible information of an object), visual similarity-based approaches are still without doubt the most popular and practical methods in the field of 3D shape retrieval. Probably because of the high computational complexity of shape matching for local features, existing visual similarity-based methods all utilize global shape descriptors to represent 2D views, which hinders the further improvement of retrieval performance. As a matter of fact, local features have been widely used in many computer vision applications [35] and usually methods that employ local features result in better performance than other methods using only global features. Consequently, it is reasonable to infer that similar computer vision techniques can be applied into content-based 3D object retrieval, especially for the view-based 3D shape retrieval methods. In this paper, we propose a new visual similarity-based 3D shape retrieval approach (CM-BOF) using Clock Matching and Bag-of-Features. More specifically, the method describes each view as a word histogram built by vector quantization of the view’s salient local descriptors and employs an efficient multi-view shape matching scheme to compute the dissimilarity between 3D objects. An overview of the CM-BOF method is presented as follows: First, a 3D model is properly aligned to the canonical coordinate frame so that the normalized pose could be well suited to draw standard three-view images and then depth-buffer views are captured on the surrounding vertices of a unit geodesic sphere. Afterwards, for each view we extract salient local features (e.g., SIFT [ 33]) which are subsequently quantized into a word histogram using the Bag-of-Features approach. Finally, according to the properties of the geodesic sphere previously used, an efficient shape matching is carried out to measure the dissimilarity between two objects by computing the minimum distance of all (24) possible matching pairs. To the best of our knowledge, our work [ 28] is the first to employ local features in the visual similarity-based method for 3D shape retrieval, and this article is the extended version of the conference paper. However, our previous method [ 28] can only deal with rigid 3D models (see Fig.&amp;nbsp;  for some examples). In fact, existing visual similarity-based methods are all essentially unsuitable to distinguish and recognize non-rigid objects. This is because, as shown in Fig.&amp;nbsp; , even if two articulated 3D models are generated from the same object, they still may look quite different from many viewing angles. This paper extends our previous work [ 28,  29] to make the method also well suited for the retrieval of non-rigid 3D shapes. Specifically, when applying the CM-BOF approach to retrieve non-rigid 3D objects, multidimensional scaling (MDS) should be utilized before pose normalization to calculate the canonical form for each object. By doing this, excellent performance can be obtained by the proposed method in applications of both rigid and non-rigid 3D shape retrieval. The major contributions of this paper are twofold.        A novel visual similarity-based 3D shape retrieval framework is proposed, where the Bag-of-Features method is utilized to describe each view as a word histogram and the objects are compared by an efficient multi-view shape matching scheme. Moreover, using MDS embedding, the proposed method can also obtain excellent performance in the application of non-rigid 3D shape retrieval.         &amp;nbsp;           Exhaustive experiments are carried out carefully to investigate the influence of the number of views, codebook, training data, and distance function. Perhaps surprisingly, our results show that, in contrary to the traditional Bag-of-Features, the time-consuming clustering is not necessary for the codebook construction of our method.         &amp;nbsp;      The rest of the paper is organized as follows. Section&amp;nbsp; 2 discusses previous work. Section&amp;nbsp; 3 presents an detailed description of our method. Experimental results are then shown and analyzed in Sect.&amp;nbsp; 4. Finally, we provide the conclusion of this paper in Sect.&amp;nbsp; 5. A novel visual similarity-based 3D shape retrieval framework is proposed, where the Bag-of-Features method is utilized to describe each view as a word histogram and the objects are compared by an efficient multi-view shape matching scheme. Moreover, using MDS embedding, the proposed method can also obtain excellent performance in the application of non-rigid 3D shape retrieval. Exhaustive experiments are carried out carefully to investigate the influence of the number of views, codebook, training data, and distance function. Perhaps surprisingly, our results show that, in contrary to the traditional Bag-of-Features, the time-consuming clustering is not necessary for the codebook construction of our method.    2 Related work  Based on the shape descriptors used, existing 3D shape retrieval methods can also be classified into the following two categories: global feature-based methods and local feature-based methods. For more information about the development in 3D shape retrieval, we refer the reader to a recent survey [56]. Most of the existing 3D shape retrieval methods belong to this category. So far, a large number of 3D global shape descriptors have been proposed such as D1 [3], D2 [38], spherical harmonic descriptor (SHD) [23], 3D wavelet descriptor [24], skeleton descriptor [53], Reeb graph descriptor [4], light field descriptor (LFD) [10], DESIRE [59], and so on. Since our 3D shape descriptor is designed to be able to measure the visual similarity between two objects, we pay more attention to the visual similarity-based methods, which have been considered as the most discriminative approaches in the literature [2, 49]. Among these visual similarity-based methods, LFD [10] method is perhaps the most famous one. In the LFD method, each 3D model is represented by 100 silhouettes (10 views per group) rendered from uniformly distributed viewpoints on the half side of a unit sphere and the silhouette is encoded by a feature vector consisting of 35 Zernike moments and 10 Fourier coefficients. They measured the dissimilarity between two objects by the minimum distance of 6,000 view group matching pairs, considering all possible situations. LFD method avoids pose normalization via an exhaustive searching which inevitably aggravates computational cost. To address this problem, Lian et al. [30] developed a multi-view shape matching scheme for properly normalized generic models. The experiments showed that, with the same image descriptors, retrieval performance including discrimination, spatial requirement and searching speed could be considerably improved compared to the original LFD method. Also, Daras et al. [12] achieved accurate rotation estimation using the combination of plane reflection symmetry and rectilinearity to normalize each 3D object, and then represented the model as a set of 2D binary view images. In each image, 2D Polar-Fourier coefficients, Zernike moments and Krawtchouk moments are extracted to generate the view’s shape descriptor. As a matter of fact, pose normalization has been widely applied in many visual similarity-based 3D shape retrieval methods [8, 9, 42, 44, 47]. The major difference between them is the feature vectors they used to describe views. For instance, 2D Fourier coefficients [42], the elevation descriptor [47], and the depth-line descriptor [9] have been employed to represent depth-buffer views. Similarly, silhouette views have been described using the 2D shape distribution descriptor [44] and 1D Fourier coefficients [8]. The methods discussed above all utilized the images captured from viewpoints located on the sphere. Recently, Papadakis et al.&amp;nbsp;[41] proposed a 3D shape descriptor using a set of panoramic views, which were obtained by projecting a 3D model to the lateral surface of a cylinder. The panoramic views are described by their 2D Discrete Fourier coefficients as well as 2D Discrete Wavelet coefficients. Papadakis&amp;nbsp;[39] further improved the retrieval performance of the method [41] using a local relevance feedback technique that basically shifts the 3D shape features closer to their centroid in feature space. 2D local features have been proven to be very successful in many applications (e.g., image retrieval [21], object classification [15], video data mining [51], etc.) and a vast number of 3D local features (e.g., 3D spin image [22], harmonic shape context [16], 2.5D SIFT [32], etc.) have also been developed. However, considerably less work has been reported to apply local features in 3D shape retrieval. This is mainly due to the high computational cost of shape matching for huge amounts of local descriptors extracted from 3D objects. Local feature-based 3D shape retrieval is an interesting and promising research topic, since it has intrinsic capability of solving non-rigid 3D shape retrieval [26] and partial 3D shape retrieval problems [17]. Funkhouser and Shilane&amp;nbsp;[17] selected distinctive multi-scale local features, which are calculated via spherical harmonic transformation, and applied priority-driven search to efficiently achieve partial matching. Gal et al.&amp;nbsp;[19] proposed a curvature-based local feature that describes the geometry of local regions on the surface and then constructed a salient geometric descriptor by clustering together a set of local descriptors which are interesting enough according to a given saliency function. Geometric hashing was utilized to accelerate the partial matching of salient local features. Tal and Zuckerberge&amp;nbsp;[55] decomposed each object into meaningful components, and then, based on the decomposition, they represented the 3D model as an attributed graph that is invariant to non-rigid transformations. Wang et al.&amp;nbsp;[61] proposed intrinsic spin images (ISIs) generalizing the traditional spin images [22] from 3D space to N-dimensional intrinsic shape space, in which ISIs shape descriptors are computed from MDS embedding representations of original 3D shapes. More recently, Lian et al.&amp;nbsp;[26] made a comparison of methods for non-rigid 3D shape retrieval and found that a large percentage (more than 60&amp;nbsp;%) of these state-of-the-art approaches utilize local features to represent non-rigid 3D objects. Bag-of-Features, which is a popular technique to speed up the matching of image local features, has recently been introduced into local feature-based 3D shape retrieval. Liu et al.&amp;nbsp;[31] presented a compact 3D shape descriptor named “shape topics” and evaluated its application to 3D partial shape retrieval in their paper [31], where a 3D object is represented as a word histogram constructed by quantizing the local features of the object. Spin images, calculated on points randomly sampled on the surface, are chosen as the local descriptors. Li et al.&amp;nbsp;[25] introduced a weak spatial constraint to the method proposed in [31] by partitioning a 3D model into different regions based on the clustering of local features’ spatial positions, but the improvement was limited. Toldo et al.&amp;nbsp;[57] applied a more sophisticated mesh segmentation method to decompose a 3D object into several subparts. Each segmented region is represented by one descriptor and then a word histogram is generated by assigning all subpart descriptors of the object into visual words. More recently, Bronstein et al.&amp;nbsp;[6] employed multiscale diffusion heat kernels as “geometric words” and used the “Bag-of-Features” approach to construct compact and informative shape descriptors for 3D models. They also demonstrated that considering pairs of “geometric words” allows creating spatially sensitive bag of features with improved discriminative power. The work that is most relevant to our paper is [37], in which Ohbuchi et al. reported a view-based method using salient local features (SIFT [33]). They represented a whole 3D object by a word histogram derived from the vector quantization of salient local descriptors extracted on the depth-buffer views captured uniformly around the object. Their experiments demonstrated that the method resulted in excellent retrieval performance for both articulated and rigid objects. To some extent, the CM-BOF method proposed in this paper is quite different from the BF-SIFT algorithm presented in [37]. Basically, our approach is a visual similarity-based method, following the idea that “if two 3D models are similar, they also look similar from all viewing angles”; while BF-SIFT [37] is a “global Bag-of-Features based method”, which represents a 3D model as a single word histogram via the vector quantization of its local features. Moreover, several new techniques have been developed to make our method be well suited for practical applications of both rigid and non-rigid 3D shape retrieval, and results also demonstrate that our method could markedly outperform BF-SIFT [37] in terms of retrieval accuracy.    3 Method description  In this section, we first present an overview of our method and then elaborate on the details of each step in the corresponding subsection. Since the method is mainly based on the   approach and a multi-view shape matching scheme (named   for the sake of convenience and intuition), we call it “CM-BOF” algorithm in this paper. The CM-BOF algorithm, depicted in Fig.&amp;nbsp; , is implemented subsequently in four steps:          Normalize 3D objects with respect to the canonical coordinate frame to ensure that their mass centers coincide with the origin, they are bounded by the unit sphere, and they are well aligned to three coordinate axes. For the application of non-rigid 3D shape retrieval, before pose normalization, the canonical form of each object is calculated using MDS.         &amp;nbsp;             Capture depth-buffer views on the vertices of a given unit geodesic sphere whose mass center is also located in the origin and then extract salient local features from these range images.         &amp;nbsp;             For each view, quantize its local features into a word histogram using a pre-specified codebook. Normally, the codebook is obtained off-line by clustering the training data that are randomly sampled from the feature set of all models in the target database. However, the codebook of our method can also be directly built using randomly sampled  local feature vectors. This has been verified by the experiments described later.         &amp;nbsp;             Carry out an efficient multi-view shape matching scheme to measure the dissimilarity between two 3D models by calculating the minimum distance of their 24 matching pairs.         &amp;nbsp; Normalize 3D objects with respect to the canonical coordinate frame to ensure that their mass centers coincide with the origin, they are bounded by the unit sphere, and they are well aligned to three coordinate axes. For the application of non-rigid 3D shape retrieval, before pose normalization, the canonical form of each object is calculated using MDS. Capture depth-buffer views on the vertices of a given unit geodesic sphere whose mass center is also located in the origin and then extract salient local features from these range images. For each view, quantize its local features into a word histogram using a pre-specified codebook. Normally, the codebook is obtained off-line by clustering the training data that are randomly sampled from the feature set of all models in the target database. However, the codebook of our method can also be directly built using randomly sampled  local feature vectors. This has been verified by the experiments described later. Carry out an efficient multi-view shape matching scheme to measure the dissimilarity between two 3D models by calculating the minimum distance of their 24 matching pairs. As shown in Fig.&amp;nbsp;, for the application of non-rigid 3D shape retrieval, the 3D canonical form of each object should be computed before the procedure of pose normalization. While, for rigid 3D shape retrieval, we only need to normalize the pose for the original model. Details of the  and  are presented in Sects.&amp;nbsp;3.2.1 and 3.2.2, respectively. Since the key idea of our method is to measure the visual similarity between 3D objects with arbitrary poses, it is preferable if all models can be well normalized to the canonical coordinate frame in the same manner. Therefore, we normalize the objects by a recently proposed approach [ 30] which combines the PCA (principal component analysis) based and the rectilinearity based pose alignment algorithms to obtain better normalization results. As we know, PCA (here we use the continuous PCA [ 60]) is the most prominent tool for accomplishing rotation invariance by solving three principal axes of a 3D object. While the basic idea of the rectilinearity-based method (only suitable for polygonal meshes) is to specify a standard pose through the calculation of the model’s rectilinearity value. Three steps of the composite method are described as follows.          For a given 3D mesh, translate the center of its mass to the origin and then scale the maximum polar distance of the points on its surface to one.         &amp;nbsp;             Apply the PCA-based and the rectilinearity-based method, respectively, to rotate the original model to the canonical coordinate frame and then store these two normalized meshes in memory.         &amp;nbsp;             Calculate the number of valid pixels of three silhouettes, projected on the planes , for the two normalized meshes generated in the previous step. And then select the model, which yields the smaller value, as the final normalization result.         &amp;nbsp;      Two normalization examples are displayed in Fig.&amp;nbsp; . Note that, the method only performs incomplete pose normalization for rotation transformation. More specifically, only the positions of three axes are fixed for the model normalized by the composite method, that is, the direction of each axis is still undecided and the  -axis,  -axis,  -axis of the canonical coordinate system can be located in all three axes. That also means 24 different orientations are still plausible for the aligned models, or rather, 24 matching operations should be carried out when comparing two normalized objects. For more details of the pose normalization algorithm, we refer the reader to our previous paper [ 30] where convincing experimental results have been obtained to illustrate the advantage of this approach, in the context of pose normalization and 3D shape retrieval, against other methods. For a given 3D mesh, translate the center of its mass to the origin and then scale the maximum polar distance of the points on its surface to one. Apply the PCA-based and the rectilinearity-based method, respectively, to rotate the original model to the canonical coordinate frame and then store these two normalized meshes in memory. Calculate the number of valid pixels of three silhouettes, projected on the planes , for the two normalized meshes generated in the previous step. And then select the model, which yields the smaller value, as the final normalization result. Non-rigid objects are commonly seen in our surroundings. Take Fig.&amp;nbsp; a for an example, a human being might appear in several distinct postures that could inevitably be identified as different kinds of objects using most existing methods. In order to properly and efficiently measure the dissimilarity between two non-rigid objects, it is preferable that the models can be represented as some shape descriptors which are invariant or approximately invariant under isometric transformations (e.g., bending and articulation). Unless otherwise specified, isometric transformation mentioned in this paper means the transformation which preserve the geodesic distance between every pair of corresponding points on the surface. Based on the fact that the geodesic distance between every two points on a surface remains unchanged under isometric transformations, a bending invariant representation (i.e.,  ) can be obtained by applying MDS to map the geometric structure of the surface to a new 3D Euclidean space, in which geodesic distances are approximated by Euclidean ones. This idea is originally proposed in [ 13], where three different MDS techniques are also compared. Examples of 3D canonical forms obtained using Classical MDS and Least Squares MDS are shown in Fig.&amp;nbsp; b and c, respectively. It should be point out that typically MDS cannot be used in the application of rigid 3D shape retrieval. This is mainly due to the fact that models derived from the same object after applying different isometric transformations might be classified into different categories in the case of rigid shape retrieval, while the MDS embedding generates isometry-invariant representations for those models. For example, human models in the PSB database are classified into the following three categories: human beings in normal poses, walking, and with their arms out, respectively. If we apply MDS on these models, very similar canonical forms might be obtained and thus most probably they will be classified into the same category. That could obviously reduce the performance of rigid 3D shape retrieval methods. The basic idea of MDS techniques is to map the dissimilarity measure between every two features in a given feature space into the distance between corresponding pair of points in a low-dimensional Euclidean space. More specifically, MDS map each feature     to its corresponding point     in a  -dimensional Euclidean space   by minimizing, for example, the following stress function:       where   denotes the dissimilarity between the feature   and  ,   denotes the Euclidean distance between two points (i.e.,   and  ) in  ,   is the weighting coefficient, and   Specifically, in this paper,   and   stand for a pair of points on the original 3D mesh, while   and   denote the corresponding points on its canonical form. For our purpose of generating 3D canonical forms, the dimension of the Euclidean space is chosen as   and the geodesic distance   is selected as the dissimilarity measure between the pair of points on the original surface. A standard optimization algorithm to solve the minimization problem of cost functions like  (Eq.&amp;nbsp;(1)) is the Least Squares technique. However, it is not easy to calculate the closed expression for the first derivative of this nonlinear function. A simple but effective solution is to use the numerical computing technique with iterative majorization. The idea is applied in the SMACOF (scaling by maximizing a convex function) [5] algorithm to minimize the stress function . As demonstrated by experimental results in [13], the Least Squares MDS method using SMACOF obtained better minimization (see [13]) for the stress function (1) compared to other MDS techniques. Thereby, we choose to apply the Least Squares MDS embedding with the SMACOF algorithm in our method. Here, we briefly review the SMACOF algorithm (more details can be found in [5]). Minimizing the stress function   is equivalent to minimizing the following function:       or       where                                      Applying the Cauchy–Schwartz inequality and some basic algebraic operations, we have            where   is the approximation of  , and the elements of the matrix   are defined by                    and the   matrix   is given by                         where   is the vector that occupies the  th column of a   identity matrix. Let the derivative of   be 0, that is       we get the minimum of  . Finally, the result of the minimization problem can be computed by       where   is the Moore–Penrose inverse of  . By setting all weights   to 1, Eq.&amp;nbsp;( 13) can be rewritten as       In practice, given a threshold  , calculating Eq.&amp;nbsp;( 14) iteratively until  \(E^{\prime }_{\mathrm{S}}(X^{(k)})-E^{\prime }_{\mathrm{S}}(X^{(k-1)})&amp;lt;\varepsilon \), we obtain the final solution   for the nonlinear minimization problem of the stress function  . Here, we use the matlab source code that is publicly available on the web site of the book [ 7] to implement the SMACOF algorithm. As the calculation of geodesic distances and the SMACOF algorithm are both computationally expensive, 3D meshes should be simplified before the MDS embedding procedure. In this paper, the number of vertices on the simplified mesh is experimentally chosen as 2000, and it takes about 120&amp;nbsp;s on average to construct the canonical form for a 3D model under our experimental settings. After pose normalization, 3D meshes (or their canonical forms) have been well aligned to the canonical coordinate frame. Then, we capture their depth-buffer views on the vertices of a given unit geodesic sphere whose mass center is also located in the origin. The geodesic spheres used here are obtained by subdividing the unit regular octahedron in the way shown in Fig.&amp;nbsp; . These kinds of polygonal meshes are suitable for our multi-view based shape retrieval mechanism, mainly because of the following three reasons. First, the vertices are distributed evenly in all directions. Second, these geodesic spheres enable different levels of resolution in an intuitive manner. The coarsest (level-0) one is obtained using a unit regular octahedron with 6 vertices and 8 faces. Higher levels can be generated by recursive subdivisions. Third, since all these spheres are derived from an octahedron, given the positions of six vertices for the original octahedron, other vertices can be specified automatically. Moreover, all vertices are symmetrically distributed with respect to the coordinate frame axes. That means, when comparing two models, only 24 matching pairs need to be considered for the feature vector in an arbitrary level. After view rendering, a 3D object can be approximately represented by a set of depth-buffer images from which we extract salient SIFT descriptors, as presented in [ 33]. The SIFT descriptor is calculated, using the   matlab source code developed by Vedaldi and Fulkerson&amp;nbsp;[ 58], in the following two steps. First, obtain the scale, orientation, and position information of the salient points detected by the different-of-Gaussian (DoG) approach. Second, compute SIFT descriptors for the local interesting regions which are determined by the scale and position of the salient points. Here, the SIFT descriptor, which is actually a 3D histogram of gradient location and orientation, is computed using its default parameters, where the location is divided into a   grid and the gradient angle is quantized into eight orientations. This results in a feature vector with 128 elements. The feature is designed to be robust, to some extent, against similarity transformation, affine distortion, noise and illumination changes of images. Figure&amp;nbsp;  shows some examples of SIFT descriptors extracted from the range images which are scaled, rotated, and affine transformed. It can be seen that the SIFT descriptor is stable to various changes of 3D viewpoints, which is a desirable property for our visual similarity-based 3D shape retrieval method to compensate its dependence on the stability of pose normalization. Directly comparing 3D models (or their canonical forms) by their local visual features is time-consuming, especially for the 3D shape retrieval methods using a large number of views. To address this problem, we quantize the SIFT descriptors extracted from a depth-buffer image into one word histogram so that the view can be represented in a highly compact and discriminative way. Before vector quantization, a codebook (also named as ) with  visual words should be created. Usually, the codebook is generated via off-line clustering. More specifically, huge numbers of feature vectors are first randomly sampled from the feature set of the target database to form a training set. Then, the training set is clustered into  clusters using the K-means algorithm. At last, centers of the clusters are selected as the feature vectors of visual words in the codebook. Since the spatial requirement and calculating time of the K-means clustering are significant, many other algorithms [18] (e.g., kd-tree, ERC-tree, and Locality sensitive hashing) have been applied to alleviate the computational cost. While, as we can see from the experiments described in Sect.&amp;nbsp;4.2, clustering is not necessary for the codebook construction of our method. In other words, randomly sampled local feature vectors can be directly used to create the vocabulary and these two codebook construction approaches result in almost the same discriminative power for 3D shape retrieval. By searching for the nearest neighbor in the codebook, a local descriptor is assigned to a visual word. Then, each view can be represented using a word histogram whose th bin records the number of th visual words in the depth-buffer image. To obtain satisfactory discrimination capability, usually the histogram should have thousands of bins. Let the number of views be 66 and the number of visual words in the codebook be 1,500, without optimization, the 3D shape descriptor would be of dimension . In fact, with the observation that, for our method, the average number of salient points in a view (with size ) is only about 30, we can represent the histogram in a better way that not only makes the shape descriptor highly compact but also significantly improves the efficiency of dissimilarity calculation. Figure&amp;nbsp;  shows an example of the data structure for our 3D shape descriptor, where only the information (i.e., bin No. and bin value) of some bins, whose values are not equal to zero, appears in the feature vector. Experimental results show that, considering the method with 66 views and a 1,500-dimensional codebook, on average the new data structure requires about 30 times less spatial storage and performs approximately 21 times faster for feature comparison.
Springer.tar//Springer//Springer\10.1007-s00138-013-0502-4.xml:A new fusion scheme for multifocus images based on focused pixels detection:Image fusion Multiscale Top-Hat (MTH) transform Focused regions:  1 Introduction  Image fusion is to combine substantial information from several source images of the same scene, so that the fused image, which has a better description of the scene than any single source image, will be more suitable for human and machine perception or further image processing tasks. Due to the limited depth-of-field of optical lenses, it is usually impossible to acquire an image that contains all relevant objects in focus. However, sometimes people like to acquire images that have large depth-of-field, i.e., the images are in focus everywhere. As one of the main research fields of image fusion, the multifocus image fusion technique can be employed to create a single image where all objects are in focus. It has been used as an effective tool in many different fields such as biomedical imaging and computer vision. And the fused image obtained from the technique can give a better view for human or machine perception. So far, many image fusion methods have been proposed to merge multifocus images. Basically, these methods can be classified into two categories, one of which is the spatial domain-based methods, and the other is the transform domain-based methods [1]. The simplest fusion method in spatial domain is weighted averaging of input images. However, this method often produces unsatisfactory results, because the features that appear in one source image and not in other images are rendered in the composite image at reduced contrast [2]. In addition to the weighted fusion method, blocks or segmented region-based techniques were developed to merge the source images [3–5]. However, the block-based methods may easily produce block effect which affects the appearance of fused image significantly. And the segmentation-based methods are strongly dependent on the segmentation algorithms, which are usually complicated and time consuming. More importantly, the segmentation algorithms are of vital importance to the quality of the fused image [6]. Apart from the above-mentioned methods, several fusion schemes based on the focused regions detection have been developed in recent years [7, 8]. The advantage of these methods is that the pixels within the focused regions can be successfully selected as the pixels of the fused image. However, these methods may easily produce discontinuity or erroneous results at the focused border regions, because the boundary of focused regions cannot be determined accurately. These effects will reduce the quality of the fused image. In order to reduce the discontinuity and pixels selected wrongly, Chai and Li proposed a fusion method based on the focused regions detection and multiresolution [9] in virtue of the property of the multiscale transform (MST)-based method. However, for this method, it is very difficult to determine the window size, which is very important in detecting the pixels of the transition zones. In the transform domain-based methods, the usually used transform methods include the discrete wavelet transform (DWT) [10], lifting stationary wavelet transform (LSWT) [9], log-Gabor transform [11], contourlet transform [12, 13], and nonsubsampled contourlet transform (NSCT) [14, 15], etc. Compared with the block-based or region-based methods, the transform domain-based fusion methods can successfully overcome the disadvantages mentioned above and avoid the discontinuity in the transition zone between focus and defocus [16]. However, due to the complexity of image content, these methods are likely to experience the coefficients of the fused images selected mistakenly. This can lead to the loss of some useful information of the source images. To overcome the disadvantages of the spatial domain-based method, a new fusion method based on focused regions detection and dual window technique is proposed in this paper. The main contributions of this paper can be described as follows: first, a new and effective focused regions detection method is proposed based on the improved MTH (IMTH) transform. Second, an initial fusion method based on MST and a dual window technique are presented, respectively, and used to restrain the discontinuity in the transition zone. Third, according to the focusing properties of the source images pixels and the initial fused image, a fusion scheme is developed to achieve the final fusion result. Experimental results demonstrate that the proposed method outperforms the traditional fusion methods in visual perception and quantitative measurement. The rest of this paper is organized as follows. Section&amp;nbsp;2 describes the new multiscale Top-Hat transform theory in detail. And the Sect.&amp;nbsp;3 presents the method employed for detecting the pixel of the focused regions. In Sect.&amp;nbsp;4, the proposed fusion strategy is described. Experimental results and discussion including performance analysis are presented in Sect.&amp;nbsp;5. Finally, some conclusions are given in Sect.&amp;nbsp;6.    2 Improved multiscale Top-Hat transform  Mathematical morphology is an important technique in the field of image processing, such as image interpolation [17, 18], image fusion [19], image enhancement [20], etc. In morphology, the objects in an image are considered as set of points, and the basic operations are defined between two sets: the object and the structuring element (SE). The shape of the SE plays a crucial role in extracting features or objects of given shape from the image. A morphological operation with a scalable structuring element can extract features based on shape and size simultaneously [19]. The basic morphological operations are erosion, dilation, opening and closing. By combining these basic operations, some important compound operations can be obtained. If we use   to denote the input image,   denote the structuring element, the dilation and erosion operations of   and   are defined, respectively, as follows.                         where   denotes the operator of dilation and   denotes the operator of erosion.   and   are the domains of   and  , respectively. Based on the above two operations, the morphological opening and closing operations of   and   can be defined, respectively, as                         The opening and closing Top-Hat operations of   and   are defined, respectively, as                         The results of the Top-Hat operator for opening and closing filters can present the bright and dark features around the pixel  . These features are very important in determining which pixel is in focus. However, in the Top-Hat operator, only one structuring element is used that means the Top-Hat operator can extract image features at only one scale corresponding to the size of the used structuring element. To be effective at all scales, the multiscale Top-Hat transforms for opening and closing are defined as follows [ 7,  19]:                         where   is the scale factor. If   is convex, in discrete domain, the   could be calculated by using Eq. ( 9).       By convention   when  . In Eqs. ( 7) and ( 8), the   contains all the bright features that have size greater than or equal to   but less tan  , and   contains all the dark features within the same range of size [ 7]. So, the maximum value of the two Top-Hat operations can present the bright and dark features of the image, simultaneously, and can be described as:       In order to determine which pixel is in focus, the initial decision map, namely, the initial detected results of the focused regions can be generated using Eq. ( 11) for a pair of source images denoted by   and  .            where   and   generated by Eq. ( 10) denote the features of the source images   and  , respectively, at the  th scale and position  . Obviously,   contains all the image features within the range   [ 7]. That means the features within   are not contained in  . In order to extract the important features of the source image and accurately decide whether a pixel is focused or not, the Top-Hat transforms defined as Eqs. ( 7) and ( 8) should be improved. Similar to literature [ 20], the modified multiscale Top-Hat transform for opening and closing can be defined, respectively, as follows for scale  .                         Through varying scale factor  , the multiscale results of the new Top-Hat transforms could be obtained. These results can present the bright and dark details within   scale. All of these details are very important and useful in distinguishing focused pixels. Here, the maximum value of the two transforms, which can present the image features within   scale, is employed to distinguish the focused pixels. It can be expressed as       Generally speaking, pixels with greater values of   are more possible in focused regions. Therefore, whether a pixel is focused or not can be decided according to  .    3 Improved MTH transform for decision map generation  To simplify the discussion, we consider the processing of just two source images A and B. Moreover, throughout this work, we assume that the source images have been appropriately registered. With the  , the initial decision map can be generated as            where   and   calculated by Eq. ( 14) denote the focus values of the source images   and   at the location   in scale  , respectively. ‘ ’ in   indicates that the pixel at position   in image   is in focus, and can be selected as the corresponding pixel of fused image, otherwise the pixel in   is focused and can be extracted to construct the fused image. However, in practice, comparing the focus value, namely, the DTH value is insufficient to determine all the focused pixels. There are thin protrusions, thin gulfs, narrow breaks, small holes, etc. in map. According to the theory of imaging we know that the regions, either out of focus or in focus, are always continuous in the interior of these regions. So the defects mentioned above should be corrected for a good fusion quality. In this paper, an interesting method is suggested to generate the final decision map, and the method can be algorithmically presented as follows:              Employ the fill holes filter to remove these isolated background holes of the binary image       . The operation can be expressed using Eq. (      16). In this case, the holes are the areas of dark pixels surrounded by lighter pixels.                             where ‘      ’ stands for the fill holes operation.              &amp;nbsp;                 Perform an anti-white transform on the binary image       , and then the fill holes filter is applied to remove these isolated background holes, which are the areas of lighter pixels encompassed by dark pixels in binary image       . Finally, the anti-white transform is applied on the modified binary image again. All of these operations can be expressed by Eqs. (      17)–(      19).                                                                                                                                               &amp;nbsp;           Apply the closing operation on the binary image  to smooth sections of contours, fuse narrow breaks and long thin gulfs, and fill gaps in the contours.         &amp;nbsp;                 Unfortunately, holes located at the boundary of the binary image        cannot be removed simply using the above operations. In order to remove the holes, and produce a pleasing decision map, a threshold        should be set to remove the holes smaller than the threshold. The operation can be realized by alternating sequential filter formed by Eq.&amp;nbsp;(      20) and anti-white transform.                             where ‘      ’ is the function of Matlab, which is used to remove the small objects or holes that have fewer than        pixels.              &amp;nbsp;      Hereto, a complete decision map is established and denoted as  . Based on the decision map, the fused image can be constructed directly by combining focused pixels from the source images. However, this may produce undesirable side effects such as discontinuity or erroneous results in the transition regions between focus and defocus because the boundary of the focused regions cannot be determined accurately. All of these effects will affect appearance of the fused image. Employ the fill holes filter to remove these isolated background holes of the binary image  . The operation can be expressed using Eq. ( 16). In this case, the holes are the areas of dark pixels surrounded by lighter pixels.         where ‘ ’ stands for the fill holes operation. Perform an anti-white transform on the binary image  , and then the fill holes filter is applied to remove these isolated background holes, which are the areas of lighter pixels encompassed by dark pixels in binary image  . Finally, the anti-white transform is applied on the modified binary image again. All of these operations can be expressed by Eqs. ( 17)–( 19). Apply the closing operation on the binary image  to smooth sections of contours, fuse narrow breaks and long thin gulfs, and fill gaps in the contours. Unfortunately, holes located at the boundary of the binary image   cannot be removed simply using the above operations. In order to remove the holes, and produce a pleasing decision map, a threshold   should be set to remove the holes smaller than the threshold. The operation can be realized by alternating sequential filter formed by Eq.&amp;nbsp;( 20) and anti-white transform.         where ‘ ’ is the function of Matlab, which is used to remove the small objects or holes that have fewer than   pixels.    4 The proposed fusion strategy  As we know, the MST-based image fusion method can avoid the discontinuity in the transition zones between focus and defocus [16]. In order to restrain the discontinuity, the transition zones can be fused by the MST-based method. However, the transition zones are irregular and cannot be decided accurately. It is very difficult to apply the MST-based method to the transition zones, directly. To this problem, the MST-based method is applied to the source images and an initial fused image is generated. Then the fused result of the transition zones can be determined by detecting the corresponding zones of the fused image. The work presented in the following subsection is concerned with a design of the initial fusion strategy. In our proposed method, in order to restrain the discontinuity in the transition zone, a novel fusion method based on multiresolution is proposed and utilized to merge the transition regions between focus and defocus. In MST domain, one of the well-known MST methods for image fusion is the DWT. However, the DWT lacks the shift-invariance and causes pseudo-Gibbs phenomena around singularities [15]. Compared with DWT, the lifting stationary wavelet transform (LSWT), which is designed based on the lifting scheme [21], can overcome the disadvantages of the DWT in image analysis. It is a redundant scheme, as each set of coefficients contains the same number of samples as the input. Moreover, the LSWT possesses the shift-invariance. When it is introduced into image fusion, the impacts of mis-registration and the pseudo-Gibbs phenomena around singularities on the fused results can be reduced effectively. Moreover, the sizes of different subbands and the source images are identical. It is easy to find the relationship among different subbands, which is advantageous for designing fusion rules [22, 23]. Besides, the fusion performance of the LSWT is close to that of the NSCT [24]. However, theoretically, the computational complexity of the LSWT is much lower than that of the NSCT. By considering both fusion result and computing complexity, in our study, we adopt the LSWT as the multiresolution image fusion method for our method. The overview schematic diagram of the proposed MST fusion method is shown in Fig.&amp;nbsp; . For 1-level LSWT decomposition, the input images are decomposed into one lowpass subband and three highpass subbands in different directions. Then, some fusion rules are employed to fuse the lowpass subband coefficients and the highpass subbands coefficients, respectively. Finally, apply the inverse LSWT to the fused coefficients and thus obtain the fused image  . Apart from the LSWT discussed in the above paragraph, the fusion rules for different subbands are another important thing in the presented method. In the following subsections, we will focus on the design of the fusion rules for different subbands. The coefficients in the lowpass subband of the coarsest scale represent the approximation component of the source image. So, the ‘average’ scheme can be adopted for the lowpass subband coefficients. However, this will reduce the fused image contrast and lose some useful information of the source images. To improve the quality of the fused image, a clarity measure should be defined to determine which coefficient is from the clarity region. For multifocus image fusion, many typical clarity measurements, namely focus measures, have been suggested and can be used to determine which pixel or coefficient is from the focused region. As one of them, the Sum-Modified-Laplacian (SML) proposed by Shree K. Nayar can provide a better performance than other measures, such as spatial frequency, image gradient, etc. [4, 25]. So, we can use SML as the focus measure to select coefficients from the clear parts of the lowpass subbands. Suppose   denotes the gray value of pixel at position   in an image, the SML is defined as                         where ‘step’ is the variable spacing between coefficients which compute the derivatives;   and   determine the local area sizes whose center is located at  . With the SML, the proposed SML-based fusion rule can be described as            where  ,   and   are the lowpass subband coefficients at the coarsest scale   and position   of the source images  ,   and the fused image  , respectively, while   and   are the corresponding SML values of   and  . For simplicity, we name the fusion rule as ‘SML-max’. The highpass subband coefficients with large values of the variation generally are likely to correspond to pixels with sharp brightness changes or salient features in the corresponding source image. From Eq. ( 21), we know that the   can measure the variation of pixels. Therefore, it is reasonable to utilize   as the focus measure to select coefficients from the clear parts of the highpass subbands. Here, an improved ML based on local idea is developed and used as the clarity measure to merge the highpass subband coefficients. The expression of the improved ML is defined as       where   and   determine the local area size. According to  , the proposed fusion rule, namely, selection mode for the highpass subband coefficients can be given as            where  ,   and   denote the highpass subband coefficients at the  th scale,  th direction and location   of the source images  ,   and the fused image  , respectively. The   values of   and   are denoted by   and  , respectively. In practice, the fused result of the transition zones can be considered as the corresponding part of the initial fused image . Note that the local area sizes  and  can be  or , because they always can produce a good fusion performance. For discussion purposes, we name the proposed LSWT-based method as ‘LSWT-SML’. The following work presented in this paper is concerned with a design of novel and effective fusion scheme to produce the final fused image. According to literature [ 9], we know that the ‘sliding window’ technique can be employed to detect the transition zones. Unfortunately, the size of the window is very hard to choose. If the window size is too large, some pixels of the focused regions will be classified into the transition zones. Moreover, if the window size is too small, some pixels of the transition zones will be considered as the pixels of the focused regions. Even worse, in this case, the focused pixels of one source image may be judged as the focused pixels of the other source image. These imperfections have adverse effects on improving the quality of the fused image. To avoid the disadvantages, a ‘dual sliding window’ technique is developed and used to solve this problem in our proposed method. In the technique, the size of one window, which is used to identify the transition zones, should be large enough to ensure that the focused border pixels are all detected properly. The other window size should be relatively small to make sure that most pixels detected by the large window can be classified correctly. The technique and our proposed fusion method can be realized by the following steps.              Let        be the number of bright pixel in a        window whose center is at        in the decision map       . Then, the        can be computed as below:                                            where        is the large window size.              &amp;nbsp;                 Compare the values        and        to determine which pixel is in focus. For the focused pixels, the fusion scheme can be defined as                                            where        is the gray value of pixel at position        in the final fused image       .        demonstrates that the pixel located at        in image        is in focus. So, it can be selected out to compose the corresponding pixel of the final fused image       , directly. Whereas        indicates that the corresponding pixel of image        is in the focused region, and it can be selected as the pixel of the final fused image. In addition,       \(0&amp;lt;Z(x,y)&amp;lt;M_{1}N_{1}\) means the pixel at location        may locate at the transition zones between focus and defocus, and the corresponding pixel of the fused image        can be generated by executing the following steps.              &amp;nbsp;                 Calculate the root mean square error (RMSE) and correlation coefficient (CC) of each pixel within        window between the input images (       and       ) and the initial fused image        using the following equations, respectively.                                                                                                                                                                                                                        where       ,       ,        are the pixels values at the        coordinates of the initial fused image        and the input images       ,       , respectively.        is a        window whose center is located at       .              &amp;nbsp;                 Compare the values        and       ,        and        to determine which pixel is in focus. The decision map        of the transition zones can be described using Eq. (      33), when        or       .                                            where                             or else, the       , where ‘      ’ in        indicates that pixel at position        in image        is in focus, and ‘      ’ indicates the pixel at the same position in image        is in focus, otherwise the pixel is located in the focused border region. In this step, it is very probable that        and        output a incorrect result due to the complexity of image content. In other words, only use        in these windows may be not enough. This can lead to the result that the pixels in these windows may be classified mistakenly. In view of the above-mentioned problem, the        and the        are both adopted in the proposed method to ensure that the comparisons can produce an objective outcome.              &amp;nbsp;                 If       , the pixel located at        in image        is extracted to compose the corresponding pixel of the fused image       . If       , the corresponding pixel of the source image        is selected as the final fused image pixel. Otherwise, the corresponding pixel of the initial fused image        is selected as the pixel of the fused image       . The fusion procedure can be formulated as                                            It should be explained that the pixels with        are possible in the focused border regions. These pixels are always mixed by focused pixels and out of focus pixels. It is very difficult to tell whether a pixel belongs to focused part or not. In order to restrain the discontinuity in the transition zone and erroneous results, the corresponding pixel of the initial fused image        is retained as the final fused image pixel.              &amp;nbsp;                 Repeat the steps from        to        until all the pixels of the decision map        have been visited. After the above steps, the final fused image        is generated. The fusion procedure can be formulated as                                            Apart from the above explanations, we should mention that the size of the second window must be properly small so that more pixels within the larger window can be classified accurately. For       , we have investigated different window sizes and found that a        window size for all of       ,       ,        and        will be a good choose. And the first window size        may be        or       , because these sizes can generate pleasing fusion results in most cases. At last, Fig.&amp;nbsp;                    &amp;nbsp;
Springer.tar//Springer//Springer\10.1007-s00138-013-0503-3.xml:Towards a balanced trade-off between speed and accuracy in unsupervised data-driven image segmentation:Parallel algorithms Image sampling Spatial resolution Image segmentation:  1 Introduction  By the segmentation of an image, we mean the partitioning of its pixels. Image segmentation itself is a broad discipline that is applied as a crucial intermediate step in several different assignments of pattern recognition, computer vision, and high-level image understanding. As most tasks in these fields are relatively easy for the human observer, algorithms designed for segmentation often try to get inspiration from the biological procedure of human visual perception [9, 64]. Its latest computational/algorithmic interpretation is modeled as the interactive processing of two streams with reverse direction: the data obtained by top-down (or ) analysis and data gained via bottom-up (or ) analysis [11]. Bottom-up information stands for the set of attributes acquired directly from the raw input material, and top-down information represents the , semantic or acquired knowledge that is embedded in the segmenter. Consequently, generic, multipurpose segmentation frameworks are easier to design utilizing data-driven methods, because they use a finite set of rules in low-level attribute spaces in which both local and global features can be extracted on demand. On the other hand, in real-life tasks, target objects have diverse appearance and in most cases complex hierarchy, such that they can be better isolated, when additional top-down information is available [5].  [76], as the synthesis of top-down and bottom-up approaches is often referred to, is still under heavy research, as so far no successful attempts have been made to find a representation applicable in both approaches. The difficulty of the initiative is that compact taxonomies, efficient for top-down methods, are very abstract for bottom-up procedures, whereas pixel-based representations are hard to aggregate into useful high-level information required by the knowledge-driven direction. Another major difference between the data-driven and knowledge-driven approaches is that while a bottom-up system can be employed by itself, a top-down structure requires the help of cues obtained via a bottom-up analysis [12, 23, 31, 59]. For this reason, state-of-the-art segmentation algorithms either choose to apply area-specific top-down information, thereby restricting themselves to a given segmentation task, or they follow the data-driven scheme and utilize low-level properties with high descriptive power, but with somewhat lower accuracy on the object level [43]. Our investigation is centered around the unsupervised sub-branch of clustering methods that follow the bottom-up scheme and is applied to generic color images. The motivation behind this choice is that these methods are widely used in practical scenarios due to their autonomous nature, relatively low complexity, and discrete length rule collection. We present a universal image segmentation framework inspired by the mean shift algorithm [ 21] that returns a detailed output at low running time demand using generic bottom-up information only, but on the other hand, it can easily be adapted to the priorities of any specific segmentation task (Fig.&amp;nbsp; ). Our contribution is summarized in the following four key elements:         Multipurpose applicability.              &amp;nbsp;            Reduced computation demand with preserved image information through adaptive sampling.              &amp;nbsp;            High segmentation quality utilizing a nonlinear pixel–cluster mapping system. Accuracy is pursued using a single-parameter system that registers the strength of the bond between a pixel and the mode of a cluster, subject to their spatial distance and color similarity. This way each picture element is associated with a class having the most similar characteristics. The key element for both the sampling procedure and the voting algorithm is the , which is calculated implicitly during the segmentation phase with no overhead.         &amp;nbsp;            Fast operation due to parallel design.              &amp;nbsp;      We demonstrate the capabilities of our segmentation algorithm through comprehensive numerical evaluation. Aspects of our analysis include output accuracy using numerous metrics measured on public datasets, as well as the running time demand assessed on high resolution images. Multipurpose applicability. Our framework returns a two-level output: the result of the data-driven segmentation that has a structure based purely on the characteristics of the image and the result of a subsequent merging process that utilizes a set of similarity rules. This scheme offers the possibility to directly inject alternative information (such as high-level, semantic, top-down metadata) or additional, task-dependent rules into the merging procedure with respect to the characteristics of the given task. Reduced computation demand with preserved image information through adaptive sampling. The segmentation algorithm utilizes adaptive sampling such that sampling frequency is based on the local properties of the image. Homogeneous image regions get clustered fast, initializing only a few large kernels, while spatially non-uniform regions, carrying fine details are processed using more kernels of smaller sizes that provide extensive information. While preserving the content of the image, this intelligent scheme reduces both the computational requirement and the memory demand, enabling the segmentation of large images as well. High segmentation quality utilizing a nonlinear pixel–cluster mapping system. Accuracy is pursued using a single-parameter system that registers the strength of the bond between a pixel and the mode of a cluster, subject to their spatial distance and color similarity. This way each picture element is associated with a class having the most similar characteristics. The key element for both the sampling procedure and the voting algorithm is the , which is calculated implicitly during the segmentation phase with no overhead. Fast operation due to parallel design. The framework exploits the benefits of many-core platforms that can make the segmentation much faster, especially when dealing with a large amount of data. The rest of the paper is organized in the following way: in Sect.&amp;nbsp;2, we briefly summarize the basics of data-driven segmentation and discuss its ongoing trends along with the most prominent clustering solutions available in the literature. The mean shift algorithm is considered in detail to prepare the description of our proposed segmentation framework that is explained in Sect.&amp;nbsp;3. This section is divided to two subsections that deal with the two main phases of the algorithm step by step. Section&amp;nbsp;4 contains the experimental design. We not only discuss the metrics and the databases used for the evaluation of our framework, but also summarize the four main analytical aspects that are required for the extensive evaluation of an adaptive algorithm applied in the high resolution domain. In Sect.&amp;nbsp;5, we show and briefly review the results obtained on three test image databases, finally, Sect.&amp;nbsp;6 concludes the findings of this paper.    2 Bottom-up image segmentation  In this section, we cover the basic notions of data-driven segmentation and briefly summarize the strengths and challenges connected to the approach. The second part of the section gives an overview of the most frequently used image segmentation algorithms that follow the bottom-up scheme. In the field of image segmentation,  are typically characteristic attributes of a single pixel that are either original/provided (such as e.g., color channel intensity or the topographic position in the mesh) or derived (such as edge information or the impulse response of a filter). The  is formulated via the concatenation of the features, and its dimensionality (and consequently, the feature space representation of a pixel) equals the number of contained features. In the algorithmic level, pixels are represented in an abstract form by  (FSEs), such that  denotes the function from picture element indices  of the input image  (where ) to the feature space . Then, . This numerical representation puts quantities of different properties into a unified frame, consequently image processing algorithms can utilize generic methods from various fields, such as machine learning, data mining, neural networks or combinatorics. In real-life scenes, objects have varying appearance due to changes in lighting, occlusion, scale, viewing angle, color, etc. To cope with the lack of high-level information, data-driven techniques (such as cue combination [6, 48], various types of graph-based segmentations [26, 58, 60], mixture model fitting [51], superpixels [57] or the mean shift algorithm [21]) apply various low-level features, and utilize different similarity metrics to enable the formulation of perceptually meaningful clusters. Since, the processing is conducted at the pixel level, computational complexity of these algorithms is often superlinear or in some cases even quadratical subject to the size of the input (i.e., the number of pixels). In practice, the actual size of the input is also an important factor, because that is what influences the running time besides algorithmic complexity. The two main components of the input size are the resolution of the image and the number of features assigned to the pixels. Increasing the number of features (i.e., dimensions of the feature space) can lead to a better output quality, as it can increase the discriminative power of an algorithm, but this direction does not lead to a universal rule of thumb for two reasons. Reason one is the curse of dimensionality [32], when the data becomes sparse due to the extended number of dimensions, such that robust discrimination becomes difficult. Reason two is that handling such a feature space may lead to a heavy memory load with frequent accesses, which influences the running time in a highly nonlinear manner above a certain image size. The second aspect of complexity is related to the number of pixels (i.e., number of elements in the feature space), since most tasks in computer vision can highly benefit from using images of increased resolution, as a consequence of which the amount of data to be processed will grow. For this reason, several acceleration techniques have been proposed, since the birth of the algorithms mentioned above, with the aim of reaching higher segmentation speeds while maintaining the same quality level. Speedups are either achieved in a lossless way, such as in case of algorithmic optimization and parallelization techniques, or in a lossy manner, which in one way or another involves the reduction of processed data. The main difference between these two methodologies is that the approaches belonging to the former category normally do not affect the output quality, whereas the latter ones usually have a negative impact on it, the extent of which can only be judged with respect to the speedup gained. Despite the lossy processing, most of such acceleration techniques do not give the user any control over the quality of the segmentation output, eroding this way the benefits of the increased resolution. In this subsection, we consider segmentation methods that build upon the bottom-up segmentation approaches listed above with the aim of achieving the highest possible speedup, while maintaining a reasonably small (if any) quality corruption. Cue combination [73] used in the field of segmentation is a relatively young technique having ancestors coming from the field of boundary detection [42, 44]. The latest variant was introduced by Arbeláez et al.&amp;nbsp;[6] in 2011, who designed a composite segmentation algorithm consisting of the concatenation of the globalized probability of boundary (gPb), the ultrametric contour map, and the oriented watershed. The method utilizes gradients of intensity, color channels, texture, and oriented energy (the second derivative of the Gaussian), each at eight orientations and three scales resulting in a sum of 96 intermediate stages. Their optimal composition into a single detector is obtained using previously trained parameters. Such a vast palette of features enables the algorithm to be one of the most accurate data-driven segmentation techniques available [6]. The price on the other hand is an enormous computational complexity, resulting in a runtime of several minutes for a single image. Catanzaro et al.&amp;nbsp;[16] successfully sped up the computation of the gPb by mapping it to a GPGPU architecture. The drawback of the parallel implementation lies in the emerged memory demand of the contour detector, which extremely increases the cost of the hardware required. In 2004, Felzenszwalb and Huttenlocher [26] described an unsupervised graph-based segmentation algorithm, where each pixel is assigned to a node. Edges between nodes have weights representing the dissimilarity between the two connected nodes. The procedure carries out pairwise region comparison and performs cuts to find a minimum spanning tree (MST). The novelty given by Felzenszwalb is that the segmentation criterion is adaptively adjusted to the degree of variability in neighboring regions of the image. To improve on this, Wassenberg et al.&amp;nbsp;[67] designed a graph-cutting heuristic for the calculation of the MST. Parallel computation is enabled by processing the image in tiles that result in MSTs. The component trees are connected subject to region dissimilarity and hence, a clustered output is obtained. The system works with a performance of over 10&amp;nbsp;mega pixel/s on high resolution satellite photos. However, since the article does not give any high resolution segmentation example, they do not provide a numerical evaluation for the low resolution examples displayed either. Salah et al.&amp;nbsp;[58] consider image clustering as a maximum flow–minimum cut problem, also known as the graph cut optimization. The aim of this algorithm is to find the minimum cut in a graph that separates two designated nodes, namely, the source and the target. Segmentation is done via an implicit data transform into a kernel-induced feature space, in which region parameters are constantly updated by fixed point computation. To improve segmentation quality, the procedure computes the deviation of the transformed data from the original input and also a smoothness term for boundary preserving regularization. The paper presents an extensive overview of segmentation quality including grayscale and color images, as well as real and synthetic data. The algorithm reaches excellent scores in most benchmarks, however, in some cases image size normalization was necessary due to unspecified memory-related issues. Further in this field, Strandmark and Kahl [62] addressed the problem of parallelizing the maximum flow–minimum cut problem. This is done by cutting the graph to subgraphs such that they can be processed individually. Subgraph overlaps and dual decomposition constraints are utilized to ensure an optimal global solution, and search trees are reused for faster computation. The algorithm was tested both on a single machine with multiple threads and on multiple machines working on a dedicated task. Test fields include color images, CT, and MRI recordings, all processed with over 10 million samples per second, however, parallelization speedups were not in all cases present. The lack of quality indicators does not allow the reader to observe output accuracy. The normalized cuts spectral segmentation technique was published by Shi and Malik [60] in 2000. Being different from graph cuts, it performs graph partitioning instead of the maximum flow-minimum cut optimization problem. Edge weights represent pixel affinities that are calculated using spatial position and image feature differences. Cuts are done by observing the dissimilarity between the observed sets as well as the total similarity within the sets. The algorithm has a few difficulties. First off, the final number of clusters is a user parameter that needs to be estimated. Second, graph partitioning is computationally more complex than the previously described optimization problems. Third, minimizing the normalized cut is NP-complete. Fourth, memory requirements of this technique are quadratical. To overcome the third problem, Shi traced back the cut operations to a regular eigenvalue problem using approximation. As an alternative, Miller and Tolliver&amp;nbsp;[47] proposed spectral rounding and an iterative technique to reweigh the edges of the graph in a manner that it disconnects, then use the eigenvalues and eigenvectors of the re-weighed graph to determine new edge weights. Eigenvector information from the prior step is used as a starting point for finding the new eigenvector, thus the algorithm converges in fewer steps. Chen et al.&amp;nbsp;[17] aimed at handling the memory bottleneck arising in the case, when the data to be segmented is large. Two concurrent solutions were compared: the sparsification of the similarity matrix achieves compact representation by retaining the nearest neighbors in the matrix, whereas the Nyström approximation technique stores only given rows or columns. To achieve additional speedup, most matrix operations were encapsulated into a parallel scheme; finally, both approaches were extensively tested for accuracy and speed discussing many particular details. Results indicated that the approximation technique may consume more memory and has a bit worse output quality, but works faster than the sparsification. Despite its usual role as being only a preprocessor, the superpixels method is also discussed due to improvements of the latest corresponding papers present. The algorithm was originally introduced by Ren and Malik [57] and is technically a variant of the graph cuts. The normalized cuts algorithm is utilized to produce a set of relatively small, quasi-uniform regions. These are adapted to the local structure of the image by optimizing an objective function via random search that is based on simulated annealing subject to the Markov Chain Monte Carlo paradigm. As the procedure requires multiple runs, the segmentation is relatively slow (in the magnitude of several dozens of minutes for a small image) and requires the training of certain parameters. For a more consistent output, Moore et al.&amp;nbsp;[50] added a topographic constraint, such that no superpixel could contain any other, also they initialized the algorithm on a regular grid to reduce computational complexity. The algorithm also utilizes precomputed boundary maps that can heavily affect the output quality. Another fast superpixel variant (called ) was proposed by Levinshtein et al.&amp;nbsp;[39], who utilized a computationally efficient geometric flow based on the level-set algorithm. As a result, segments had uniform size, adherence to object boundaries, and compactness due to a constraint which also limited under-segmentation. Another variant, called  (SLIC) was proposed by Achanta et al.&amp;nbsp;[2]. The algorithm is initialized on a regular grid, then cluster centers are perturbed in a local neighborhood, to the lowest gradient position. Next, the best matching pixels from square neighborhood around the cluster center get assigned to the cluster using a similarity measure based on spatial and color information. Finally, cluster centers and a residual error are recomputed, until the displacement of the center gets adequately small, and connectivity is enforced by relabeling disjoint segments with the labels of the largest neighboring cluster. The algorithm has been reported to achieve an output quality better than turbopixels at a lower time demand due to its linear computational cost and memory usage. Ren and Reid [56] documented the parallelized version (called GPU SLIC, or gSLIC) that achieved a further speedup of 10–20 times compared to the serial SLIC algorithm, such that it runs with 47.61&amp;nbsp;FPS on video stream with VGA resolution. The main difficulty of mixture models used for image segmentation lies in the estimation of the parameters used to build the underlying model. In 2007, Nikou et al.&amp;nbsp;[51] described a spatially constrained, hierarchical mixture model for which special smoothness priors were designed with parameters that can be obtained via maximum a posteriori (MAP) estimation. In 2010, further improvements were introduced by the same authors [52]: the projection step present in the standard EM algorithm was eliminated by utilizing a multinomial distribution for the pixel constraints. In both papers, extensive measurements were performed to evaluate the speed and the output quality of the algorithms. The proposed enhancements make the algorithm accurate, but computationally expensive, furthermore, the number of clusters remains a user parameter. Yang et al.&amp;nbsp;[71] proposed to model texture features of a natural image as a mixture of possibly degenerate distributions. The overall coding length of the data is minimized with respect to an adaptively set distortion threshold. Thus, possible redundancy is minimized and the algorithm can merge the data points into a number of Gaussian-like clusters using lower dimensional structures. Good output quality is verified by several different measurements, however, the running time is measured in the magnitude of minutes. Mean shift image segmentation procedure was introduced by Comaniciu and Meer [ 21] in 2002, highly building upon the work of Cheng [ 19] and Fukunaga and Hostetler [ 28], and since its main idea is playing a key role in our framework, we go a bit further into its details and acceleration strategies. Origins of the algorithm are derived from kernel density estimation (KDE), which is a robust tool for the analysis of complex feature spaces. Let the feature space be a  -dimensional Euclidean space that is constructed from the input domain via a mapping. Selection of the adequate feature space can highly depend on the given task, but its proper selection results in the benefit that characteristic features get represented in the form of dense regions. Thus, if we consider the feature space as an empirical probability density function, the dense regions will induce high probability. Such local maxima of the function are called  . Image segmentation using KDE is done via retrieving the position of the modes, and associating a subset of data with them based on local properties of the density function. As a preliminary step towards mode seeking, let   denote a set of feature points in a feature space   with distribution  . The kernel density estimator of this set can be written as       where  \(h&amp;gt;0 \) is the bandwidth of the non-negative, radially symmetric kernels of a function   (such as the Gaussian, or the Epanechnikov) that integrates to one because of the normalization constant  \(c_{k,d} &amp;gt; 0 \), and   is the profile of kernel   for  . Modes of the density function are a subset of the positions where the gradient of the function is zero. Mean shift is an iterative hill climbing algorithm that steps towards the steepest ascent in each iteration. Also, it is proven to converge into locations where the gradient of the estimate is zero [ 19], which enables it to find the modes without explicit estimation of the density. By following the transformations given in [ 21, Sect. 2.1], the density gradient estimator can be written in the form of            where   is the profile of kernel  . The second term of this equation represents the difference between the weighted mean of kernel   and its centroid, and is called mean shift (see Fig.&amp;nbsp; ). Comaniciu utilized the algorithm on a joint feature space consisting of color data (referred to as  ) and topographic image coordinates (referred to as  ). Thus, a feature point is considered to be a five-dimensional vector in the form of  , where   and   represent the three-dimensional range coordinates in the selected color space and the spatial coordinates in a two-dimensional mesh of pixel  , respectively. In case the kernel is Gaussian, its property of separability can be exploited and the mean shift vector in the joint feature space can be written in the form of       where   is the newly calculated position of the mean at iteration   is the set of pixels in input image   and   are, respectively, the range and spatial coordinates of the current position of the mean in the feature space,   and   are the range and spatial coordinates of the FSEs within the support of the kernel,   and   are the respective kernel bandwidth parameters, finally,   denotes the Gaussian kernel:       The iterative   retrieving the local maxima of the probability density function operates the following way. For  :        Initialize .         &amp;nbsp;           For \(t &amp;gt; 0 \), compute the new mean  of kernel  using (3), and center the kernel window into this position.         &amp;nbsp;                 If                                                   is satisfied for a given threshold       , then continue with step 4, otherwise go to step 2. (Note: this paper refers to the phenomenon of meeting this criterion as       , for which the time instant is denoted by       .)              &amp;nbsp;           Store the feature space position of  into the  .         &amp;nbsp;      A subset   that converges into a small tolerance radius of a   location is the   of that mode. The FSEs in the basin of attraction belong to its cluster and inherit the color information of mode  . Sets of mode candidates lying in a close neighborhood are joined together into a single mode. Robustness of a pixel-cluster assignment for a given pixel can be tested by observing the position of the saturation in the case when the mean shift iteration is reinitialized from a slightly perturbed seed point (see the   in [ 21, Sect. 2.3]). Subsequent to the saturation of all kernels initialized from   feature points, the image pixels can be decomposed into   non-overlapping segments of similar color defined by their respective modes  . Clusters with an element number smaller than the     are eliminated. Initialize . For \(t &amp;gt; 0 \), compute the new mean  of kernel  using (3), and center the kernel window into this position. If                is satisfied for a given threshold  , then continue with step 4, otherwise go to step 2. (Note: this paper refers to the phenomenon of meeting this criterion as  , for which the time instant is denoted by  .) Store the feature space position of  into the  . The proposed algorithm has a computational complexity of  [19], with the main bottlenecks being the calculation of the weighted average and the retrieval of neighboring pixels in the feature space. As the time demand of the naïve version is highly superlinear in terms of the size of the input data, a large number of techniques aim to accelerate the algorithm. Detailed discussion of all of them is out of the bounds of our paper, thus these strategies are summarized in two major groups. The first group of methods achieves faster segmentation performance via modifying the algorithm itself. Such methods include    applying an iteratively increased bandwidth [22], the quasi-Newton method [72], or combining the mean shift with either Newton’s method [13] or Gaussian blurring [14] to speed up the convergence of kernels;   utilization of multiscale region merging with respect to the minimization of a minimum description length principle [41];   usage of neighborhood subsets [13, 30] that involve elements located within a distance and exploit that the cost of retrieval in the spatial domain is negligible compared to the cost of retrieval in the whole space, or locality sensitive hashing [29] that estimates the nearest neighbors;   the sparse expectation maximization (EM) technique [13], recasting the original EM problem to the maximization of an energy function.  The second group of methods focuses on reducing the content of the feature space. The most prominent variants include    spatial discretization [13] or the path assigned mean shift [54], which reduce complexity by merging overlapping trajectories with kernel iterations alternatively calculated in parallel [77];   estimation of the KDE (referred to as KDE-paring) [27], or approximation of the underlying mixture model [75], then running the iterative mean shift on this subset, leading to a highly reduced complexity;   utilization of KD-trees for adaptive sampling of the feature space [69] on a many-core system;   computation of the density function on a coarse grid [53] followed by the utilization of separable Gaussian convolution to extract its modes.  Finally, we mention the EDISON system [ 20] that is a popular tool for the evaluation of mean shift due to its public availability and straightforward usability. This application implements the mean shift segmentation algorithm as published by Comaniciu and Meer [ 21], and operates in the Luv color space. Optionally, the EDISON offers speedup strategies present both during the mean shift iterations (using a path assigned strategy) and during the subsequent mode merging (using region adjacency graphs and graph contraction). The system works fast due to its advanced C++ implementations. applying an iteratively increased bandwidth [22], the quasi-Newton method [72], or combining the mean shift with either Newton’s method [13] or Gaussian blurring [14] to speed up the convergence of kernels; utilization of multiscale region merging with respect to the minimization of a minimum description length principle [41]; usage of neighborhood subsets [13, 30] that involve elements located within a distance and exploit that the cost of retrieval in the spatial domain is negligible compared to the cost of retrieval in the whole space, or locality sensitive hashing [29] that estimates the nearest neighbors; the sparse expectation maximization (EM) technique [13], recasting the original EM problem to the maximization of an energy function. spatial discretization [13] or the path assigned mean shift [54], which reduce complexity by merging overlapping trajectories with kernel iterations alternatively calculated in parallel [77]; estimation of the KDE (referred to as KDE-paring) [27], or approximation of the underlying mixture model [75], then running the iterative mean shift on this subset, leading to a highly reduced complexity; utilization of KD-trees for adaptive sampling of the feature space [69] on a many-core system; computation of the density function on a coarse grid [53] followed by the utilization of separable Gaussian convolution to extract its modes.   
Springer.tar//Springer//Springer\10.1007-s00138-013-0504-2.xml:Moment-based alignment for shape prior with variational B-spline level set:Level set method Shape alignment Segmentation B-spline Geometric moment:  1 Introduction  Image segmentation is one of the fundamental research topics in image processing and computer vision, and has been extensively used in a wide range of applications, such as medical image analysis [1–3], remote sensing [4], robotics, surveillance [5], and so on. Segmentation can be considered as the process of partitioning an image into several parts including foreground and background regions. To address the segmentation problem, various methods have been proposed in the literature, such as thresholding, edge detection, region growing, K-means clustering, and active contours. Among them, active contours, especially level-set-based active contour models (ACMs) [6] have been shown to be a promising approach for performing this task [7]. In level-set-based ACMs, there has been a rising interest in region-based models [8, 9] which are less sensitive to noise and initial position while compared to edge-based approaches [10, 11]. In the region-based models, Mumford–Shah (MS) model [12] has been seen as the fundamental approach and widely studied. The main idea of MS model is to approximate an input image as piecewise smooth functions with regular boundaries by minimizing an energy functional. To simplify the minimization problem, Chan and Vese represented the MS energy functional within level-set framework [9, 13]. Other works extending the approach of Chan and Vese, such as mean separation [14], histogram separation [15], and Bayesian model [16], have been developed. These level-set-based models, aside from advantages such as handling topological changes, often need to calculate the level-set curvature [17] and require a re-initialization step to maintain the regularity of the level-set function [18]. In addition, as indicated in [19, 20], to satisfy numerical stability constraints, level-set-based models often require choosing small time steps, thus leading to slow convergence. One of the promising approaches to overcome above drawbacks of level-set-based active contour models is to represent the level-set function as a linear combination of continuous basic functions [17, 21, 22]. In particular, Bernard et al. [17] proposed a variational B-spline level-set model, in which the level-set function is expressed as the combination of continuous basic functions using B-splines. The minimization of the energy functional is then directly performed with respect to the B-spline coefficients. By this way, the B-spline level-set model has the advantage of fast convergence to the segmentation solution. Though the B-spline level-set approach in [17] possesses advantageous properties over the conventional level sets, this model, like above-mentioned models which use the image intensity information alone, cannot deal with images in the presence of clutter and occlusion. In such cases, a natural approach is to take into account a prior knowledge about the shape of desired object. The prior or reference shape may be either a given silhouette [23–25] or the outcome of a training procedure from a set of shapes [26, 27]. There have been many shape prior models that aim at matching a desired shape in segmented image with prior shapes [23, 26, 28–30]. For example, Rousson et al. [28] constructed the shape prior as the difference between two level-set functions corresponding to the evolving shape and the prior one. Paragios et al. [30] introduced a registration algorithm for matching the evolving shape and a reference shape. The matching energy is then minimized by optimizing a set of pose parameters in a variational framework. Leventon et al. [31] proposed to use Principle Component Analysis (PCA) to estimate the shape features of the training shapes which are assumed to be distributed according to a Gaussian distribution. Tsai et al. [32] applied PCA to a collection of signed distance function representations of the training shapes to create a set of eigenmodes. The parametric shape model is then derived by a weighted sum of such eigenmodes. Bresson et al. [33] also employed PCA in the shape prior model including an average shape and the mode of variation on the space of implicit functions. Cremers et al. [27] introduced the kernel density estimation to estimate the distribution of prior shapes. Other works related to statistical shape priors could be found in [34, 35]. In most shape prior-based approaches, a fundamental step is to align the shapes such as the training shape and the shape to be segmented, to account for possible pose variations. In common shape prior-based level-set models, the pose optimization problems are generally expressed by coupled partial differential equations (PDEs). The pose parameters such as rotation, scaling, and translation associated with the pose optimization problems are then derived by solving the underlying Euler–Lagrange equations via gradient descent approach. However, iteratively solving these coupled PDEs often suffers from challenges while choosing the order of pose parameters and suitable time steps for corresponding equations of pose parameters to satisfy severe numerical stability constraints. From the above analysis, in this study, we propose to introduce a shape prior-based active contour model that aims at overcoming above shortcomings. Particularly, in the shape alignment process, instead of iteratively solving coupled PDEs when optimizing pose parameters, we utilize the theory of moment invariants and shape normalization [36, 37] which takes into account the affine transformation to compute the transformation. In addition, for a fast convergence to segmentation solution and avoiding level-set re-initialization, we represent the level-set functions as linear combinations of continuous basic functions expressed on B-spline basics. Notice that the proposed model shares common aspect with the work of Foulonneau et al. [38] in utilizing moment invariants to avoid solving PDEs, but differs in approach. In particular, in [38], the Legendre moments are utilized to define shape descriptors in the shape prior model and directly incorporated into the energy functional. Meanwhile, in this study, the geometric moments are used in the shape normalization process, to align the reference shape with respect to the evolving shape. In other words, in the proposed model, the moments are used for aligning the shapes in a slightly distinct procedure, and are not taken into consideration in the energy functional. The main contributions of this work can be outlined as the following: first, in solving the shape alignment problems, we utilize the theory of moment invariants and shape normalization to account for affine transformation including translation, rotation, and scaling. This avoids increasing the number of complex-coupled partial differential equations as in common shape prior-based level-set models. Second, we employ the kernel density estimation on a set of training shapes to reconstruct a deformable shape which is then used as the reference shape in a shape prior model. By this way, the shape prior model for a set of prior shapes is simplified to the case of a single prior shape. Third, we express the level-set functions as linear combinations of B-spline basic functions for a fast convergence to the segmentation procedure. The remainder of this paper is organized as follows. Section&amp;nbsp;2 reviews the general form of B-spline level-set model, the moment-based shape description, and the shape normalization procedure. The description of the proposed shape prior model for image segmentation is presented in Sects.&amp;nbsp;3 and&amp;nbsp;4. The implementation and experimental results are given in Sect.&amp;nbsp;5. We end the paper with a brief conclusion in Sect.&amp;nbsp;6.    2 Background  Let   be a bounded open subset of  , let   be a given  -dimensional image, and let a curve   divide the image plane into two regions,   and  . In [ 9], inspired by the well-known Mumford–Shah model [ 12], Chan and Vese proposed the energy functional defined as:            where   is an adjustment coefficient,   is the length of the curve  , and   are values, respectively, representing the intensity means of two regions   and  . In level-set framework, Eq.&amp;nbsp;( 1) can be reformulated as:            where   is the Heaviside function, which is 1 if   and 0 if  \(\phi &amp;lt;0\), used as an indicator for object and background regions in the image,   is the Dirac function,  , and   represents the level-set function that satisfies            Bernard et al. [ 17] proposed an alternative approach to represent the level-set function in ( 2). In this formulation, the level-set function   is expressed as a linear combination of B-spline basic functions [ 39]       where   is the uniform symmetric  -dimensional B-spline of degree  . The coefficients of B-spline representation are gathered in  .   is a scale parameter which directly influences the degree of smoothing of the interface [ 17]. Using Eq. ( 4) to represent the level-set function in ( 2), then minimizing the energy functional,  , with respect to each B-spline coefficient  , we obtain       where            Then, a gradient descent equation on the B-spline coefficients [ 40] can be obtained as       where   correspond to the gradient of energy relative to B-spline coefficients, which is given in ( 5) and ( 6), and   is the iteration step. The set of geometric moments [ 41] of a two-dimensional function   is commonly defined as       where  ,   are nonnegative integers,   is the order of the moment. Moments naturally provide region-based shape descriptors, e.g. the zero order moment, , is the area of an object. The first order moments are used to locate the image centroids or center of mass computed by       The central moments can be defined as Shape normalization [36, 37, 42] aims at making the shape invariant under translation, scaling, and rotation. The shape normalization procedure can be summarized as follows:  The covariance matrix is calculated via the second-order central moments computed from Eqs. ( 8– 10) as This step allows shifting the original of coordinate system to the center of the shape, and rotating the coordinate system according to the eigenvectors of the covariance matrix . Eigenvalues of covariance matrix  ,   and  , are calculated as:               Let  \(\mathbf{e}_1 = \left[ \begin{array}{ll}e_{1x}&amp;amp;e_{1y} \end{array} \right] ^\mathrm{T}\) be the eigenvector associated with  , and  \(\mathbf{e}_\mathbf{2} = \left[ \begin{array}{ll} e_{2x}&amp;amp;e_{2y}\end{array}\right] ^\mathrm{T}\) be the eigenvector associated with  . The eigenvectors are computed as       where  . Consequently, we can construct a rotation matrix  ,            We then transform the coordinate system by first translating the original to the shape center and then multiplying the coordinates with matrix  . Each object pixel location   is transformed into the corresponding location   as In this step, we change the scale of two axes according to the eigenvalue   and   so that the shape is in its most compact form and with a normalized size [ 36]. That is, for each object pixel location  , the new location   is obtained through a transformation defined by a scaling matrix               where   is a system-wide constant,   as suggested by Leu [ 36]. The result of the first three steps represents the compacting shape, which is expressed as After the above compacting process, the resulting shape is invariant to translation, and scaling. To make the compacting shape invariant to rotation, Wang et al. [42] adopted the idea of Teague [43] to characterize the size and orientation of the shape. The approach can be briefly presented as follows: If we only consider three second-order central moments,  , and  , the original shape can be approximated to a constant irradiance ellipse of the same size, orientation, and centered at the shape centroid location [ 43]. The orientation of the shape is defined as the angle between the  -axis and the semi-major axis [ 42,  43] expressed as:       More detail for choosing the values of ellipse tilt angle   with various cases of the sign of   and  could be found in [ 43]. Finally, by rotating the compact shape by angle  , we get the normalized shape, which is invariant under translation, scaling, and rotation. The transformation equation from the original to the normalized shape is achieved as:                    The normalization of shapes is illustrated by an example in Fig.&amp;nbsp; . In this example, the shapes in Fig.&amp;nbsp; a and c are in different poses including scaling, translation, and rotation. The normalized results of these two shapes are respectively given in Fig.&amp;nbsp; b and d. As can be seen from this figure, the shape normalization approach yields the same normalized result for both images. The shape normalization procedure presented above is applicable for non-symmetric shapes. For symmetric shapes, however, it might lead to unsatisfactory results when handling object orientation. This arises from the fact that many moments of symmetric shapes are zero, hence some normalization constraints, such as constrains for choosing the rotation angle in [42, 43], might not be well defined. To address this problem, many approaches have been proposed such as the works of Pei and Lin [44], Shen and In [45], Heikkila [46], Suk and Flusser [47]. In this study, following the approach of Suk and Flusser, we utilize complex moments to perform a further step called normalization to second rotation. The process of normalizing the shape to second rotation can be outlined as the following. Given a symmetric shape, by normalizing the shape through the procedure presented in steps 1–4, we obtain a resulted shape and call it the preliminarily normalized shape. We then calculate moments,  , of the preliminarily normalized shape, and compute its complex moments as:       where  . We then select the first non-zero complex moment except the moments used in previous normalization steps [ 47,  48], and calculate the second rotation angle as       where   and   are the imaginary and real parts of the complex moment  , respectively. Finally, by rotating the preliminarily normalized shape by angle , we get the normalized version of the given symmetric shape, which is invariant to translation, scaling and rotation. An example showing the convergence of the normalization procedure for a symmetric shape is given in Fig.&amp;nbsp; . In this figure, a star image with rotationally symmetric shape and the rotation version of the given shape, as shown in Fig.&amp;nbsp; a and&amp;nbsp;d, are used for normalization. By the normalization stage using steps 1 to 4, the normalized versions of the two shapes are in different orientation, as shown in Fig.&amp;nbsp; b and e. This illustrates that using the first rotation only could not handle symmetric objects. By contrast, when using the second rotation as a further step, we obtain the normalized shapes which are approximately similar, as depicted in Fig.&amp;nbsp; c and f.    3 Active contour model with a shape prior  The energy functional of the proposed model is given as the following equation       where   is the shape energy measuring the non-overlapping areas between the evolving shape and the reference shape,   is the data fitting term in the Chan–Vese model [ 9], and   is a weighting factor expressing the amount of contribution of the shape term to the energy  . The shape prior energy is defined as the similarity distance between the evolving shape and the reference one [ 23,  24] given as follows:       where   denotes the Heaviside function, which is 1 if the argument is non-negative and 0 otherwise,   is a signed distance function for the reference shape. In [ 23,  24], the alignment between the evolving shape and the prior one is implemented via a set of gradient descent equations which correspond to parameters accounting for translation, rotation and scaling. For a more efficient computation, we exploit the shape normalization approach, as presented in Sect.&amp;nbsp; 2, with the note that   and  effectively binarize the shape embedding functions   and  , respectively [ 49]. Let   be a prior template which has been normalized, and let   be an estimate shape for the target object, i.e.,  . We normalize the target shape   by computing a transformation  . Consequently,   is aligned to   by reversing the normalization procedure on   using the transformation computed for   [ 49], i.e.,  . In implementation   might be replaced by   which represents the reference shape. A demonstration of shapes before and after alignment can be seen in Fig.&amp;nbsp; . The data term is defined as:            The level-set function   is expressed as a linear combination of B-spline basic functions detailed in Sect.&amp;nbsp; 2 as:       It is noted that, in the data term, we omit the length term of the B-spline level-set model since the shape prior term is capable of controlling the smoothness of the zero level-set of the level-set function   [ 23]. The energy functional of the proposed model is expressed as follows:            In numerical approximation, the Heaviside function   is approximated by a smooth function   defined by       where   is a positive constant. The derivative of   is the smoothed Dirac delta function       Thus, the energy functional in ( 28) is approximated by            We use standard gradient descent method to minimize the energy functional   in ( 31). Keeping level-set function   fixed, we minimize the energy functional in ( 31) with respect to   and  . By calculus of variation, it can be seen that the constants that minimize   satisfy the following Euler–Lagrange equations                         From ( 32) and ( 33), we obtain       Then, keeping   and   fixed, we minimize the energy functional ( 31) with respect to level-set function  . Note that, because we use the B-spline formulation in ( 4) to represent the level-set function  , we therefore perform the minimization of ( 31) with respect to B-spline coefficient  . Such minimization implies computing the derivatives of ( 31) with respect to each B-spline coefficient  , expressed as follows                    Following the modified version of the gradient descent with feedback step adjustment algorithm [ 40], we present the B-spline coefficients as:       where   is an iteration step. In summary, the segmentation algorithm with shape prior can be outlined in following steps:        Choose an initial level-set function and B-spline coefficients.         &amp;nbsp;           Calculate the two intensity means inside and outside the evolving contour according to Eq. (34).         &amp;nbsp;           Normalize the reference shape and evolving shape using the normalization procedure in Sect.&amp;nbsp;2. Then, align the reference shape with the evolving shape.         &amp;nbsp;           Evolve the level-set function  to obtain B-spline coefficients and the level-set function for the next iteration.         &amp;nbsp;           Check whether the evolution of the level-set function has converged, or a specified maximum iteration number is reached. If not, return to step 2.         &amp;nbsp;
Springer.tar//Springer//Springer\10.1007-s00138-013-0505-1.xml:Recursive Bayesian fire recognition using greedy margin-maximizing clustering:Mixture of Gaussians Data clustering Recursive Bayesian estimation Fire recognition:  1 Introduction  Fire can cause serious damage to life and property in private and public areas. To minimize the loss, there have been studies concerned with early fire detection through automatic fire alarm systems utilizing sensors such as heat, smoke, and flame detectors. Recently, video sensors such as closed-circuit television (CCTV) cameras have received much attention in the field of automatic fire systems due to several advantages over conventional sensors: (i) their detection range is large and they do not need to be distributed densely, so they can successfully monitor large or open spaces; (ii) humans can easily verify fire alarms raised by the automatic systems without visiting the locations, since video sensors provide intuitive visual information; (iii) they can be complementary to conventional sensors by assisting to reduce false alarms [26] as well as providing different types of information such as the locations of fires [20]; and (iv) the existing video sensors installed for surveillance purposes can be reused for fire safety purposes by integrating the hardware with software components for vision-based detection methods&amp;nbsp;[28]. Furthermore, they can be utilized in other applications. For instance, Warhade et al. [27] adopted some fire detection methods for robust segmentation of shot boundaries under sudden illumination changes caused by fire in content-based video retrieval. For these reasons, research on the detection of fire in video through image processing and vision technology has received a great deal of attention. Color and dynamics are the two main features used to characterize fire in videos. Thus, tasks to detect fire candidates are generally based on the examination of these two features in images. Fire colors have been examined through a certain kind of color model such as a color histogram [10, 15, 23], a mixture of Gaussians [19, 25], or a rule-based generic color model [5, 24] in RGB or HSV color space. A rule-based generic color model in YCbCr color space has also been designed [2]. The dynamics of fire has been inspected carefully in terms of temporal and spatial aspects. To analyze the temporal aspect of fire, researchers have exploited temporal variations in the intensity of fire-colored pixels and have performed wavelet analysis on collected color values for a particular pixel over specific periods of time [10, 23, 25]. To analyze the spatial aspect of fire, the intensity variation of a fire-colored region [10, 25], the growth rate change of the region [5, 14], and the roughness of the regional boundary [5, 24] have been examined. Furthermore, some studies have revealed that the spatial features have time-varying characteristics&amp;nbsp;[15, 19]. All of these methods ultimately detect fire through a combination of hard decision rules by inspecting the aforementioned features based on manually given thresholds and, therefore, are not generally applicable to real environments. To overcome this limitation, many researchers have adopted various pattern recognition techniques and have developed classifiers for recognizing fire patterns. In the early studies by Töreyin et al.&amp;nbsp;[24] and Ko et al.&amp;nbsp;[16], respectively, the hidden Markov model (HMM) and support vector machine (SVM) were employed to recognize spatial and temporal features of fire. However, both of these studies still used a hard decision rule to combine the results from the recognition with those from an examination of the other fire features. Subsequently, the irregular characteristics of fire and aspects of uncertainty in recognition have been more seriously considered, and probabilistic approaches have been utilized in this regard. Borges and Izquierdo&amp;nbsp; [1] employed a naive Bayes classifier to categorize a fire-colored area. Their set of features contains some spatial properties of fire, one of which is the skewness representing the shape of the distribution of values in the red channel. They train the likelihood distribution as a Gaussian for the features, but presupposed information about the positions of fire-colored areas is utilized in their probability model. At roughly the same time, Ko et al.&amp;nbsp;[17] adopted hierarchical Bayesian networks in which they represented individual fire features and the conditional dependencies through a directed acyclic graph (DAG) model. Using this model, they verified each fire-colored pixel. Their set of features describes the skewnesses of the colors and wavelet coefficients for values collected for a fixed pixel over the preceding frames. The distributions of the features were trained by the nonparametric method. Ko et al.&amp;nbsp;[18] advanced the probabilistic model and proposed fuzzy finite automata to classify a fire-colored block in an image. They divided the possibility of fire into four connected states from very high probability to very low probability. Transitions among the states are controlled by fuzzy state transition probabilities, where the transition probabilities are changed according to likelihoods for observations about fire at each time. Through this model, they attempted to reduce the false alarms caused by moving fire-colored objects, such as humans, cars, and lights, compared with their previous work&amp;nbsp;[16, 17]. However, they assumed that high regional intensities and irregular frequency are mainly induced by fire and that fire usually exhibits upward motion. Unlike the probabilistic approaches, the online adaptive decision fusion framework proposed by Gunay et al. [12] permits human intervention to update the combination weights used in compound decisions about several fire features. However, the performance of their system was reported in the limited case of wildfire. When fire detection methods are applied to real environments, there are several important factors to consider. One of these factors is dynamic textures, such as those of flowing water, steam, smoke, and tree foliage in wind&amp;nbsp;[11]. According to&amp;nbsp;[4], dynamic textures cause many problems in video analysis tasks, especially for outdoor scenes. For instance, the appearance of moving objects induced by dynamic textures often obstructs the segmentation of interesting video targets such as humans in surveillance applications&amp;nbsp;[6]. In fire detection using video, it is a problem that dynamic textures cause frequent false alarms when they present fire colors in scenes. Since the false alarms are related to the productivity and cost of fire detection systems, frequent false alarms need to be handled seriously. Recently, Habibolglu et al. [13] actively considered environments with dynamic textures and attempted to discriminate between fire and fire-like instances. To do so, they proposed a feature combining the color and spatiotemporal characteristics of fire, which are described by a covariance matrix. The feature is extracted for a fixed block from blocks collected over some frames and is tested by an SVM. However, it is highly probable that the proposed feature is dependent on textures. This is because the method has a weakness if the images have few textures or only slight changes in texture. Such images are mainly observed when captured from long distance or when blurred due to moving cameras. Aside from dynamic textures, there are other factors that may complicate the fire detection problem for video. These include considerable variations in flame features related to the size, color, shape, and movements of the fire&amp;nbsp;[20]. Since a video captures a wide area, flames may cover very small areas in images. Flame colors differ according to the combustible materials present and the varying intensities of natural light. It is difficult to extract stereotypical patterns for the shape and spreading speed of fire, since the condition of the atmosphere is constantly changing. Additionally, the visual features of fire also can be contaminated owing to smoke and camera motion. The smoke arising with fire may hide or deteriorate the visual features. Camera motion also should be considered, since it can produce somewhat blurred images. The motion may be exhibited not only by pan-tilt-zoom (PTZ) and hand-held cameras but also by fixed CCTV cameras exposed to fierce winds in outdoor spaces. In this paper, we propose a novel fire detection approach in which fire-candidate blobs are categorized as fire or non-fire under recursive Bayesian estimation. Fire is not a temporal phenomenon; rather, it is spatially and temporally continuous. We attempt to interpret the phenomenon of fire as a hidden Markov process. In this framework, a hidden class is predicted using previous beliefs about classes, and the prediction is corrected using current observations. Once fire-candidate blobs are detected, they are tracked across frames to reveal their identities and lifetimes to apply the Markov process smoothly. The estimation of true class using prediction and correction is repeated for each identified blob throughout its life cycle. The natural advantage of this approach is that, as class-specific features become more frequently observed for the blob, the true class of the blob is gradually revealed during its lifetime. That is, uncertainty about the categorization of the blob gradually decreases. The greedy margin-maximizing clustering (GMMC) algorithm is specially devised to reduce the Bayes error in the classification. It learns color clusters to model the feature space while attempting to maximize the in-cluster margins within a class and between classes. The reason to maximize the margins is that in-cluster overlaps lower the discriminative power of class-conditional likelihoods&amp;nbsp;[9] and, therefore, lower the accuracy of the classification. To further improve the detection accuracy, we developed two methods, -time delayed decision (-TDD) and on-line learning of transition probability (OLTP). Through the -TDD method, tracking information from  time steps ahead is utilized in the current recognition to suppress false alarms caused by temporarily fire-like blobs. Additionally, classification results from preceding times are reflected by the OLTP method in the current classification as a form of transition probability. Through this method, we can determine the current class of an identified blob by considering the majority of previous classification results. This method makes our classifications of fire-candidate blobs more robust, even when class-specific features are temporarily contaminated at the current time or the blobs were misclassified at a previous time. With this distinctive approach, we attempt not only to achieve high accuracy but also to overcome the aforementioned difficulties in vision-based fire detection using videos. For various reasons, the proposed approach is competitive in applications to real environments. First, camera motion scarcely affects the fire detection in our approach, but it considerably affects that in other recent approaches. This is because some recent approaches require background estimation as an initial stage for detecting fire candidates&amp;nbsp;[17] or because qualities of their features are somewhat affected by images blurred due to camera motion&amp;nbsp;[13]. Second, our approach utilizes a minimum of presuppositions about fire. While we assume that spatial and temporal localities exist in fire, most recent approaches utilize presumptions about specific patterns in the positions, motions, and distances of fire&amp;nbsp;[1, 13, 17, 18]. These advantages in real environments make our approach competitive with most conventional approaches proposed in the literature. The remainder of this paper is organized as follows: Sect.&amp;nbsp;2 gives a detailed account of the proposed approach; Sect.&amp;nbsp;3 describes experiments and comparative analyses with contemporary approaches. Finally, in Sect.&amp;nbsp;4, we reach some conclusions about our research.    2 Fire recognition  For fire detection, our approach first detects fire-candidate blobs that are formed by connected pixels exhibiting fire features in colors and motions (Sect.&amp;nbsp; 2.1). Then, the multiple blobs detected are tracked over frames to discover their identities (Sect.&amp;nbsp; 2.2). Each identified blob is categorized as either fire or non-fire by recursive Bayesian estimation at each time during its lifetime from first appearance to disappearance (Sect.&amp;nbsp; 2.3). The overall flow of the proposed fire recognition procedure is illustrated in Fig.&amp;nbsp; . Some images showing the output of each stage are arranged for three consecutive frames below the flow diagrams. The input images are shown in the first column of the output. The second column shows mask images that segment the areas of the fire-candidate blobs detected. The tracking results are drawn on the input images in the third column, where the bounding areas and identification numbers (IDs) of the blobs are indicated by red boxes and black numerals. The last column shows the classification results of the tracked blobs, where blobs categorized as fire are drawn in green. According to related studies, fire has distinct color characteristic and dynamics. We notice that these two features are basically represented in images by some range of colors and frame-to-frame changes in intensities. Therefore, to detect fire-candidate blobs, we first examine the two features in pixels of an input image. To detect pixels showing fire colors, each pixel is investigated with a rule-based generic color model&amp;nbsp;[ 5], through which the pixels with high values of red color and saturation are isolated. The fire-colored pixels and their intensities are recorded in an image   at time  . To detect movement of the fire-colored pixels, each pixel is investigated with a three-frame differencing rule&amp;nbsp;[ 8], which examines whether the pixel has significant intensity changes over three consecutive images,  , and  . The fire-colored pixels exhibiting significant movement are recorded in a mask image,  , as expressed in the following equation:            where a pixel is denoted by its position   in the image and   is a threshold. Then, the moving fire-color pixels must be formed into blobs by connecting such pixels for analysis of the blob units in later stages. To acquire fire-colored blobs, we perform a connected component analysis on  . We again investigate the movement, and blobs satisfying the following equation are isolated:            where   and   represent the set of pixels in a blob and the cardinality of the pixel set, respectively, and   is a threshold. The equation checks how many pixels showing fire colors a blob contains and whether the movement of the blob is significant in proportion to its size. The thresholds,   and   in Eqs.&amp;nbsp;( 1) and ( 2), are determined experimentally using training videos. Fire-colored blobs that do not satisfy Eq.&amp;nbsp;( 2) are not of interest, so they cannot proceed to the next stage. We assume that fire has temporal and spatial localities. The temporal locality means that, with high probability, the latest-appearing fire will reappear. The spatial locality means that, with high probability, the location of the reappearing fire will be in the areas surrounding the latest fire location. With these assumptions, multiple moving fire-colored blobs are tracked through the two-round association that finds correspondences between the blobs across frames. Through the blob tracking process, we find out which current blobs are identical with which previous blobs, and those having the same identity share the same ID across frames. This information is utilized in the next stage. In the first round of association, the previous blobs at   are assigned to current blobs at   using the spatial occlusion between these. We designate a blob   and a set of blobs at frame   as   and  , respectively. It is said that a previous blob   is spatially occluded by a current blob   if  . If spatial occlusion exists between   and  , then   is assigned to  . The systematic process of assignment from   to   is detailed in&amp;nbsp;[ 7], and the correspondence is neither one-to-one nor onto. As a result, each current blob is assigned one of the events in    . The event of a blob gives information about the existence of spatial occlusion and the circumstances of that occlusion. When a blob has an   event, this represents that it spatially occludes no previous blob and so has newly emerged. When a blob has a  , or   event, this represents that it spatially occludes some previous blob, in spite of different situations. A previous blob that is not spatially occluded by any current blob is assigned a   event. Examples of the five events are illustrated in Fig.&amp;nbsp; . In the figure, black boxes represent input images while colored ellipses, rectangles, and diamonds indicate blobs and their coverage areas in two consecutive images. In case (i),   has  , since it spatially occludes no previous blobs. In case (ii),   has  , since it spatially occludes  . In case (iii),   has  , since it spatially occludes two individual blobs,   and  . In case (iv),   and   have  , since they spatially occlude  . In case (v),   has   at the current time, since it is not spatially occluded by any current blob. In the second round of association, we perform verifications of the assignments and make as many current blobs as possible have  events, especially for the blobs undergoing  and  events in the first round. To do this, we measure the similarities between related blobs using properties that describe the appearance and geometry of the blobs, such as their colors, sizes, and positions. More specifically, the event of a current blob is remapped to any of the events in   that have attracted our interest. The remapping process is described in Algorithm&amp;nbsp;1, where  indicates the event of a blob. The operators  and  indicate logical conjunction and disjunction, respectively, and the operator  indicates existential quantification. To measure the color similarity between two blobs, the Bhattacharyya distance is calculated using the hue-saturation histograms of the two. To measure the geometric distance between two blobs, the Euclidean distance between the centers of the two is calculated. After the two-round association, we assign IDs to all current blobs with , and we cause all current blobs with  to inherit the IDs from the corresponding previous blobs. Through these processes, we can reveal the identity of a blob and check its lifetime from appearance to disappearance. Each identified blob should be categorized as fire or non-fire at each time during its lifetime. Generally, a classification task includes two major processes: training and recognition. In our recognition process, the category of a blob is recognized by a classifier that implements recursive Bayesian estimation. The application of the classifier and the details of the recognition process are explained in Sect.&amp;nbsp; 2.3.1. In our training process, Gaussian clusters are learned to model our feature space for each category. To reduce the Bayes error in this process, the GMMC algorithm is specially devised in Sect.&amp;nbsp; 2.3.2. The overall process of the classification task is illustrated in Fig.&amp;nbsp; , where the algorithms required in the two processes are indicated by Alg. and their names. Additionally, two methods developed for improvement of the fire detection accuracy are explained in Sect.&amp;nbsp; 2.3.3. We model our classifier as a probabilistic inference framework employing recursive Bayesian state estimation. In this framework, we repeatedly calculate or update the belief about the state of a blob, which in our case is the class of the blob, on the basis of the most recent observation and the previous beliefs related to the states. This is performed for each time during the lifetime of a blob. The overall algorithm for recognizing the class of a blob is described in Algorithm&amp;nbsp;2, where the general recursive state estimation algorithm is modified for our problem. We describe the algorithm for blobs that make their first appearance in the first frame. For blob   having an ID of  , the hidden class and the measurement according to the class are denoted by   and  , respectively. We assume that a set of parameters   for class-conditional likelihood probability has already been learned. Step 1 is performed only when a blob appears. In this case, the Bayes rule is applied, where we assume that the prior probabilities are equal for all states. Steps 2 to 4 are repeated until the event associated with the blob is changed from   to  . In the repetitions, Step 2 uses the previous state beliefs to calculate the state prediction probability that the blob is in a given state at the current time. This is computed by multiplying the state transition probabilities with the previous state beliefs and summing over the states. The state transition probabilities are initially set by experiments, and then they are pruned by online learning, as explained in Sect.&amp;nbsp; 2.3.3. Step 3 corrects the prediction using the current time measurement. It is implemented by taking the product of the class-conditional likelihood probability, the predicted probability, and the normalization factor  . Finally, Step 4 determines the class of the blob by comparing the state beliefs. Figure&amp;nbsp;  shows an example of how the recursive steps are applied for a tracked blob. The first row in the figure shows the consecutive results of tracking a blob (    3) in time. In the images, the bounding area of the blob is represented by a red box and the ID of the blob is represented by a black numeral at the center of the box. In the second row of the figure, the recursive estimation process for the blob is modeled graphically. When the blob appears in the first frame, the recursive process starts and the blob is classified by Steps 1 and 4 according to the result of the Bayes rule. When the blob is tracked at the next time, the blob is classified by Steps 2 to 4 according to the results of state prediction and correction. This process is repeated until the blob disappears. In what follows, we will introduce our feature of fire and explain how to calculate a class-conditional likelihood probability based on this feature, as required for Steps 1 and 3. As a feature of fire, we choose color values in RGB channels. This is because the colors of fire are considered to be one of the most intuitive features of fire and have been consistently used from the early stages of research on the detection of fire in video. We model the feature space as a set of clusters for each class, and each cluster is represented by a Gaussian distribution. Thus, the measurement   is composed of a set of RGB color vectors obtained from all the pixels in the region of a blob. The likelihood probability is formalized as follows:            where a pixel in the region of a blob having an ID of   produces one color vector   and the measurement   is a set of vectors  s over a set of color vectors  . The   returns a class label in which the mean of a Gaussian is Mahalanobis-closest to   among the means of all the Gaussians in fire and non-fire classes. The   returns 1 if the   is satisfied and 0 otherwise. Thus, the more that a given class   is equal to the class label returned by  , the higher the likelihood probability of the given class. We explain the algorithms for learning the clusters of Gaussians to represent our feature space statistically. The learned clusters form a mixture of Gaussians for each class, so all the means and covariances form a parameter set  . The notation used in this section is summarized in Table&amp;nbsp; 1, which lists the symbols with their simple descriptions. We wanted the mixture of Gaussians for each class to maximize the class-conditional likelihood probability. According to Dasgupta’s [9] study, when clusters greatly overlap in the data, the Gaussians learned from such clusters do not produce good estimates of the clusters. We suppose that the poor estimates due to in-cluster overlapping are a major cause of deterioration in the discriminative power of the likelihood probability. Therefore, the maximization of the likelihood probability is hindered. With our problem, since there are many similar colors between the fire and non-fire classes, we are likely to obtain poor estimates when learning these using techniques such as expectation maximization (EM). For this reason, we propose our GMMC algorithm instead of using EM. The GMMC algorithm iteratively increments clusters while attempting to maximize the in-cluster margins within a class and between classes. As shown in Fig.&amp;nbsp;, four algorithms constitute the GMMC. The overall shell of the GMMC is expressed in Algorithm&amp;nbsp;3, which controls iteration. The main tasks performed in each iteration are described in Algorithm&amp;nbsp;4, which calls Algorithms&amp;nbsp;5 and&amp;nbsp;6 within its body to achieve the goal. In the following, we explain the four algorithms one by one in detail: The GMMC algorithm starts with empty parameter sets   and   with constant number   in Algorithm&amp;nbsp;3. Initially, for each class,   clusters are obtained in lines  4 to  9 from a randomly selected video   in a training dataset. Then, the remainder (lines  11 to  13) is repeatedly performed until the total number of learned clusters exceeds   or there is no video left in training dataset  . In each repetition,   clusters are newly obtained from one of the training videos through  , and their parameters are accumulated in  . The  (Algorithm&amp;nbsp;4) carries out three tasks. The first task (in line&amp;nbsp;1) is choosing the next video in  through  (Algorithm&amp;nbsp;5). The  evaluates the likelihoods for all videos in  using the previously learned clusters of the class and then returns the index of a video with many features producing the worst likelihood. The second task (in lines&amp;nbsp;2 to 5) is attempting to eliminate overlapping areas and, consequently, to leave in-cluster margins between the clusters already learned and the clusters to be learned. To do so, the algorithm performs a two-round revision targeted at the chosen data. For the fire class, the features extracted from the chosen video only undergo within-class revision, which discards feature vectors within  Mahalanobis distance of any mean  in the already learned . The  is the within-class marginal distance, and it is manually given based on experiments. Additionally, for the non-fire class, a between-class revision of the remaining features is executed. This is a task similar to the within-class revision, except that the Mahalanobis distance is measured from each feature to any mean  in  using  instead of . The  is the between-class marginal distance and is also manually given based on experiments. The third task (in lines&amp;nbsp;6 to 15) is started if the quantity of remnants is enough to produce clusters, whereupon new clusters are obtained from the remnants. Through the aforementioned revision, we can teach the new clusters to include more varied colors in addition to the already learned colors. Additionally, non-fire clusters do not learn colors that are very similar to the fire colors already learned. This prevents a true fire-colored blob from being classified as non-fire, even though false alarms sometimes occur for true non-fire blobs. In the case of learning clusters for the non-fire class, the new clusters undergo the  procedure (Algorithm&amp;nbsp;6). The purpose of  is to avoid fresh clusters unintentionally influencing previously learned clusters. In this procedure, lines&amp;nbsp;4 to 14 check whether the fire class is more likely to generate the feature vectors extracted from training videos in  by a set of clusters including fresh clusters. If so, the fresh clusters are considered to have a bad influence. Lines&amp;nbsp;15 to 25 execute a task similar to the last one, except that they check the feature vectors extracted from the videos in . If it is found that the non-fire class is more likely to generate the feature vectors extracted from the fire videos by the set of clusters, the fresh clusters are considered to have a bad influence, as before. Returning to , if the fresh clusters are determined to be invalid at line&amp;nbsp;9, the clusters have a chance to be re-learned at line&amp;nbsp;7. In this case, the features participating in producing the invalid clusters undergo one more between-class revision with the distance  increased by . This causes the number of remaining features to decrease because of the increased distance  as the features undergo the revision once again. If the reduced features are too small, the video produces no clusters. On the other hand, if the clusters are valid, their parameters and the video producing them are accumulated in  and . Finally, the selected video is removed from the training dataset at line&amp;nbsp;15. Two methods, -TDD and OLTP, are developed to improve the flame recognition accuracy. Non-fire blobs can temporarily have fire colors due to sudden changes in the reflection and refraction of light and movements by wind. In numerous instances, such blobs disappear soon after appearing but can be temporarily categorized as fire blobs. To avoid this misclassification, tracking information from  time steps ahead is utilized in the current recognition in the -TDD method. More specifically, there is a time difference of  between tracking and classification for blobs. By tracking blobs, we determine whether there exists a blob whose event is labeled  and whose lifetime is within  at the current time. For each such blob, the likelihood of the non-fire class is made high by applying  and , where  is close to 1. We assume that  is a small integer and is manually given. Meanwhile, we suppose that the true class of a blob is more likely to be the specific class to which the blob is most frequently mapped. Thus, we design the OLTP method for reflecting the classification results from before the current time in a classification at the current time. To this end, we first collect all of the classification results up to the current time for a blob and count the number of transitions between classes. The numbers are recorded in the form of transition probabilities and applied as such. For instance, if most results of recognizing a blob indicate fire during some period of time, it is learned that the probability of transition from fire to fire is relatively high and the transition probabilities in the other cases are relatively low. Then, the learned transition probability for the blob is utilized in the prediction step of the recursive estimation for the blob after a certain period of time from its first appearance. As a result, if a blob is found through online learning to have a relatively high probability of staying in a specific class, it is more likely to fall into that class, even though its likelihood probability for that class is low at the current time. Moreover, even if the previous belief for that class has been relatively low, the blob is still more likely to fall into that class. In contrast, if there is little difference in the transition probabilities between two classes, the likelihood probabilities have more power to differentiate between the classes for each blob.    3 Experimental results  We collected diverse videos for experiments to consider various difficult situations in real environments. The collection included various fire situations, such as forest fire, oil plant fire, landfill fire, and experimental fire indoors and outdoors. The areas, colors, and shapes of fire vary in the collection. The smallest area of fire is covered by about 0.4&amp;nbsp;% of the pixels in the whole image. The combustible materials, weather conditions, and observation times are somewhat different in each video. There is a video capturing the rapid spreading of fire on grass in a forest, and some videos have severe smoking with fire. The collection also includes some videos containing a variety of dynamic texture instances, such as flowers, foliage, grass, vegetation, trees, ponds, smoke, traffic, and an outdoor parking lot. The video resolution is 320 by 240 pixels. We used totally 23 and 46 videos for the fire and non-fire classes, respectively. For the fire class, there were 20 and 9 videos in the testing and training datasets, respectively. Among the training videos, six were also used for testing. For the non-fire class, there were 33 and 16 videos in the testing and training datasets, respectively. Among the training videos, three were also used for testing. Some videos in the datasets were acquired from the Firesense project&amp;nbsp;[ 3] or from video-sharing websites such as YouTube. Among the videos in the non-fire dataset, 30 videos were downloaded from the dynamic texture database DynTex [ 22]. Some sample images from the test videos in the non-fire class are shown in Fig.&amp;nbsp; . All of the programs used in the experiments for our approach and the previous approaches are written in MATLAB. These were run using a PC with an Intel Core i7 CPU (2.93&amp;nbsp;GHz) and 8&amp;nbsp;GB of memory. To learn the parameter  , the GMMC algorithm (Algorithm&amp;nbsp;3) was performed with the training dataset and with   set to 2. The variables   and   started at 1, and   was set to one of the elements in the sequence   for each trial. The number of training videos in the non-fire dataset was about twice that in the fire dataset. This was because we could not forecast how many non-fire colors would be learned and which non-fire colors would be similar to fire colors. Among training videos, only seven videos had a chance to produce clusters directly for the fire class and only nine videos had such a chance for the non-fire class. As a result, the algorithm produced two mixtures of Gaussians, which have 14 and 18 Gaussians for the fire and non-fire classes, respectively. Sample images acquired from the training videos are shown in Fig.&amp;nbsp; . We implemented and tested our proposed approach in two different ways, called Approach 1 and Approach 2. In Approach 1, recursive Bayesian estimation was implemented with mixtures of Gaussians obtained from the GMMC algorithm. Subsequently, the 1-TDD method was applied to utilize tracking information. Approach 2 added the OLTP method to Approach 1. These were compared with two previous approaches&amp;nbsp;[13, 25], referred to as Approach 3 and Approach 4, which are two contemporary approaches to detecting fire in video. Approach 3 uses hard decision rules to verify the temporal and spatial properties of fire with given thresholds. We set the thresholds through experiments with the training videos to make the fire detection rate as high as possible. Approach 4 recognizes fire blocks in video using an SVM, with covariance-based features extracted from blocks at fixed positions in the video. For this approach, we trained the features for each class with the same training videos. All the parameter values required for the training and testing were properly set as described in&amp;nbsp;[13]. We also compared the GMMC algorithm to EM. For this, we performed EM on the training videos 100 times for each class and obtained 100 mixtures for each class. Each mixture had the same number of Gaussians as ours. From among the 10,000 mixture pairs resulting from the combination of the mixtures of the two classes, the best pair needs to be selected. Our criterion for the selection was the  -separation introduced in&amp;nbsp;[ 9], where   represents the degree of overlap among Gaussians. For example, if two Gaussians are 2-separated, they are almost completely separated, and if   is 0.5, they overlap significantly. Thus, to select well-separated mixture pairs, we measured the  -separation between mixture   for the fire class and mixture   for the non-fire class, as described in Eq.&amp;nbsp;( 4). In the equation, we assumed that mixtures   and   had totals of   and   Gaussians, respectively, where   denotes a Gaussian distribution with a given mean and given covariance, and   is a function defined in&amp;nbsp;[ 9] that returns the   value given the two Gaussians. Then, we selected the pair of mixtures   and   with the maximum average  -separation value. We refer to the results of Approach 1 with the clusters learned from EM (not from the GMMC algorithm) as the results of Approach 1 with EM.       All of the aforementioned comparison results are listed in Table&amp;nbsp; 2 for the fire test videos and in Table&amp;nbsp; 3 for the non-fire test videos. In these tables, the fourth to eighth columns list the number of frames detected as fire and the percentage with respect to the total number of frames. All frames in the test dataset for fire had real fire and those for non-fire had no real fire. Thus, the percentages in Tables&amp;nbsp; 2 and&amp;nbsp; 3 represent fire detection rates and false alarm rates for each video, respectively. The last row, labeled  , lists the collective results from all the videos. The numbers in the row are acquired by summation of all the numbers of frames detected as fire. The percentage next to each number is the proportion of the number to the sum of all the total numbers of frames. The collective results are graphically depicted in Fig.&amp;nbsp; . While Approach 3 yielded moderate performance in detecting fire, it also yielded the worst performance in terms of the number of false alarms. More precisely, the poor fire detection results were produced by the videos containing smoke together with fire or containing fast-spreading fire. The worst false alarms were raised for the videos containing dynamic textures with colors and dynamics similar to fire. This is because the authors of Approach 3 supposed that only fire has significant temporal and spatial variations in an image and, therefore, overlooked instances of dynamic textures other than flames. Approach 4 produced a high detection rate of over 80&amp;nbsp;% for most of the fire videos. However, it produced a very low detection rate for some fire videos, such as those containing image regions with fewer textures or few time-varying textures or those filmed when the camera was shaking. Meanwhile, the worst false alarms were raised for the videos containing fire-colored regions with high texture or greatly time-varying textures. Through the results, we confirmed that Approach 4 is useful in restricted environments in which the regional textures are clearly observable by cameras. To compare EM-learned Gaussians to GMMC-learned Gaussians, we analyzed and compared the experimental results of Approach 1 and Approach 1 with EM. Even though Approach 1 with EM yielded a moderate false alarm rate, it failed to detect fire in that its collective detection rate was less than 60&amp;nbsp;%. In measurements of the average -separation of the Gaussians learned using the two techniques (EM and GMMC), EM gave an average of 1.6936 with a variance of 1.4333, while the GMMC algorithm gave an average of 1.7024 with a variance of 1.0092. The measurements and the comparison results give two indications. First, the colors differ little between the fire and non-fire classes, and in such a case the mixtures of Gaussians learned by techniques like EM fail to maximize class-conditional likelihood probabilities. Second, in the same case, the GMMC algorithm produces Gaussians better separated between classes, and such mixtures of Gaussians contribute to maximizing the likelihood probabilities. With the clusters generated by the GMMC algorithm, Approaches 1 and 2 produced the highest detection rates and the lowest false alarm rates in the comparison results. To enhance the accuracy of fire detection, -TDD was adopted for Approach 1. Through this method, as shown in the results, Approach 1 effectively suppresses false alarms issued for temporary fire-like phenomena, such as blinking red lamps and fluttering red flags. Through the OLTP method, Approach 2 issued fire alarms more correctly than Approach 1. This is because the majority of previously acquired classification results to some extent reveal hidden information about class. Therefore, Approach 2 can robustly classify a fire-colored region even when class-specific features are contaminated at the current time or the region was misclassified at a previous time. We analyzed videos producing relatively low detection rates using Approach 2. We found that in the process of detecting moving fire-colored blobs some fire-candidate blobs were missed due to weak intensity changes in some videos. However, the high false alarm rates with Approach 2 mostly came from the videos containing colors very similar to fire colors, since the GMMC algorithm prevented these colors from being utilized in training for the non-fire class. To analyze the computational complexity, Approaches 2 and 4 are compared in terms of their computation time. We measured the time required to process all the frames in each video and converted the result to a unit of frames per second (fps). The experiments were conducted without code optimizations for the two approaches. Approaches 2 and 4 yielded processing rates of 13.45 and 9.50 fps, respectively. According to this result, Approach 2 is 1.4 times faster than Approach 4 and runs in near real-time. This is because the fire analysis of Approach 2 is limited to moving fire-colored blobs. Approach 2 mainly consumes computing resources in the stage for tracking multiple fire-candidate blobs, since that stage performs many searches to build a relationship between blobs across frames.
Springer.tar//Springer//Springer\10.1007-s00138-013-0506-0.xml:A machine vision system to estimate cotton fiber maturity from longitudinal view using a transfer learning approach:Transfer learning Cotton maturity estimation Feature-based domain adaptation:  1 Introduction  Cotton is a natural industrial raw material which is cultivated in many parts of the world for producing a number of textile products such as yarns and fabrics. The US is a world leader in cotton production, i.e., first exporter of raw cotton fibers with about 41&amp;nbsp;% of the total world exports, and third producer after China and India. To classify millions of cotton bales produced each year, it is necessary to develop more descriptive and accurate fiber quality measurements than the ones available today. Accurate fiber quality assessment involves monitoring a number of variables, of which length and maturity are especially important. Generally, longer fibers are desirable because they tend to be stronger and finer, allowing the production of finer yarns. Similarly, more mature fibers are desirable because they produce better quality yarns and fabrics, and have better dye affinity. Thus, to improve processing and avoid excessive waste caused by short and immature fibers, the development of better cotton quality measurement technologies is of paramount importance to the global textile industry. Fiber length represents the physical length of the cotton fibers from end to end and is measured from longitudinal view using image analysis. The development of our length module has been presented in previous publications [1, 2]. This paper focuses primarily on the maturity module. In this work, fiber maturity   (theta) is the degree of cellulose deposition in the secondary cell-wall of a fiber, and is defined in terms of the perimeter and area of a cross-section as illustrated in Fig.  a.   is a unit-less value in the range  . A low value of   indicates immature cotton, whereas a high value of   indicates mature cotton. A reference set consisting of 104 reference cottons have been created to capture the variability in cotton [ 3]. The direct and traditional (reference) method (AATC 20A-2008 section 14) [4] of measuring theta is the microscopic evaluation of hundreds of fiber cross-sections using image analysis as illustrated in Fig. b. According to this approach, fibers are first cut into 1 micron segments using a microtome and then imaged under a microscope. Finally, image analysis techniques [5] are applied to compute two features of interest for every cross-section; namely, the perimeter () and the area of secondary cell-wall (). The maturity of a cross-section is then computed using the formula for  as presented in Eq. (1). Typically, a mature fiber cross-section is more circular in shape because of a thick secondary cell wall whereas an immature fiber cross-section has a thin and flat appearance. There are a few issues with the reference method for maturity measurement. First, it is tedious due to the fine detail involved in sample preparation. Furthermore, it cannot estimate the average maturity of a single fiber since it cuts a mixture of fibers into small segments. In addition, some theta values are probably biased due to imperfect feature measurements made during image analysis. With improved image resolution and better image analysis, the amount of bias could be minimized.       where   perimeter of fiber cross-section,   area of secondary cell wall of the fiber, i.e. the region enclosed between the central lumen and the cross-sectional boundary, defined as follows,       where   fiber radius,   lumen radius. A number of different systems have been proposed to overcome the limitations of the reference method for measuring cotton fiber maturity. Some of the systems presented in this section are also mentioned in an earlier review paper [6]. All of these systems measure maturity indirectly, i.e. they do not use the cross-sectional view for feature measurements. One can broadly classify these systems as (1) routine—a system which is commonly accepted as a means for measuring fiber maturity, and (2) experimental—a system which is in the design or validation stage. The routine systems comprise of the fineness and maturity tester (FMT) and the advanced fiber information system (AFIS). FMT, developed by the Shirley Institute, measures the average maturity of a cotton sample by measuring the air-flow through a 4.0g plug of fibers at two compression levels [7]. It cannot measure the sample maturity distribution. In addition, the specimen needs to be conditioned prior to testing. AFIS, developed by Uster Technologies (Switzerland), measures a number of parameters of raw cotton fibers such as length, maturity, neps and trash content. Sample preparation involves preparing 0.5&amp;nbsp;g of fibers into a 25&amp;nbsp;cm long sliver. The length and maturity module of AFIS measures the length and maturity of individual fibers in the sample by analyzing the electro-optical signal received as an individual fiber passes through a light beam in mid-air. The maturity measurements include both the average maturity of a cotton sample and the sample maturity distribution. However, its length measurements are known to overestimate the short-fiber content, i.e., the proportion of fibers less than 1.27&amp;nbsp;cm long due to mechanical breakage of fibers during processing. In addition, some length measurements are biased due to the presence of fiber crimps which are not accounted for. Therefore, its length statistics such as mean and distribution could be improved. Some experimental systems that are currently in development use polarized light microscopy (PLM) and image analysis methods (IAM) to quantify maturity indirectly from longitudinal views. PLM studies show that the maturities of cotton fiber are related to the interference colors produced when they are imaged under crossed polarizers, as investigated initially by Schwartz and Hotte [8]. Under PLM, mature fibers appear yellow or yellow–green in color, whereas immature fibers are blue and dead fibers are bluish-red or violet. Standard Test Method ASTM D1442-00 [9] summarizes the procedures to estimate cotton fiber maturity based on PLM and involves the manual counting of mature and immature fibers according to their color. In practice, this standard suffers from poor precision and high variance due to the human subjectivity of the technicians performing the measurements. SiroMat [7] which is developed by Commonwealth Scientific and Industrial Research Organization (CSIRO) and the Australian Cotton Industry overcomes the issues with ASTM D1442-00 by automating the process of scanning and PLM color analysis. Sample preparation requires the immersion of a small amount (2&amp;nbsp;mg) of 1&amp;nbsp;mm cotton fiber snippets in castor oil (to improve image contrast) and sandwiches them between two glass slides before imaging. The system is trained by learning a multiple linear regression model which correlates the color measurements from PLM made using image analysis with the maturity ratios measured from FMT for a number of known cotton samples. Like AFIS, SiroMat can also quantify the sample maturity distribution of a given sample. CottonScope [10], developed by Cottonscope Pty Limited (Australia) is licensed to use SiroMat technology and makes several improvements to it. It measures both cotton fiber maturity and fineness of weighted fiber snippets (50&amp;nbsp;mg) using PLM and IAM and reports both averages and distributions of these properties. It is relatively user-friendly, operates quickly (measures 20,000 fiber snippets in 40&amp;nbsp;s), and improves sample preparation by using water instead of castor oil. It is trained by learning a linear regression model which correlates yellowness with maturity for six reference cotton samples. Rodgers et al. [10] report an average linear relationship  between the theta measured using PLM and the theta measured using the cross-sectional image analysis (reference method). CottonScan [11, 12], also developed by CSIRO, measures fiber fineness by using IAM to measure the total length of a known mass of cotton fiber snippets. It has an option to use the fineness results to estimate the average maturity of a sample via Lord’s micronaire equation [13]. However, there do not appear to be any published results indicating the correlation between CottonScan and reference or routine methods. The Fraunhofer diffraction principle (FDP) has been applied by researchers [14, 15] to measure the diameter of a single fiber (which is also referred to as fiber ribbon-widths or just fiber width). It is expected that fiber widths are related to cotton fiber properties such as maturity and fineness, with thicker fibers indicating higher qualities [14]. In a typical FDP system, a laser beam is passed through a single fiber and the intensity distribution of the diffraction fringes is projected on a linear charged-couple-device (CCD) camera. This fringe distribution depends on the width of the fiber, the wavelength of the light beam and the diffraction angle (where the first fringe minima occur). Therefore, the computed fiber width is based on the distance between the positive and negative minima of the first-order fringes. In their study using three cottons bales, Zhang et al. [14] report a correlation coefficient  between the average fiber widths measured using the FDP optical system and the fiber widths estimated using the perimeter information of the cross-sectional method. Adedoyin et al. [15] performed a more detailed study where they measured the fiber widths at different locations of a single cotton fiber for approximately a thousand cotton fibers from each of the ten cotton bales. They report that the fiber width measured using their FDP optical system has a strong linear relationship with the perimeter  and area  of the cross-sectional method. However, this method is not used to estimate the maturity of single cotton fibers. Even though several systems are available to measure cotton fiber length or maturity or both, there is an increasing interest to use a system that can measure both length and maturity simultaneously, completely and accurately. Therefore, we have developed a new line-scan imaging system to address the shortcomings of the existing systems. The characteristics of the proposed system will be compared to the existing systems, and how it helps to overcome some of the current limitations. First, there is an interest in determining the bivariate distribution of length and maturity of a cotton bale because this will give better insight about a cultivar than just individual distributions of length and maturity alone. To achieve this, hundreds of longitudinal images of complete cotton fibers are acquired using the proposed system. Each image consists of the entire view of a single fiber, and not a mixture of fiber snippets (from different fibers) as in the PLM-based systems such as the SiroMat and the CottonScope. The length module of our system applies an algorithm [1] to measure the length of a complete cotton fiber. This length measurement does not suffer from the fiber crimp bias as in the AFIS system and is accurate (off by only 1&amp;nbsp;% from the true value). Fiber localization information from the length module is also used to initialize the maturity module. In the maturity module, features that are related to fiber maturity are first extracted, and this information is then used to train a maturity model for average fiber maturity estimation. Thus, the proposed system allows the simultaneously measurement of length and maturity of a complete cotton fiber from a single test image. In addition, this paper presents a novel approach based on transfer learning to train the maturity module of the proposed system. During training, the bias of the proposed system must be adjusted to match the bias of the reference system even though their datasets exist in different feature spaces. In our feature-based transfer learning approach, a domain adaptation step is first used to match the distributions of the measured features (low-level information) of the examples between the two different views (the cross-sectional and longitudinal views). In a subsequent step called model generation, a common maturity model, which is based only on the low-level information, is first trained to transfer the maturity knowledge from the reference system to our system. An auxiliary independent model is then trained to establish the relationship between maturity and the measured features for the proposed system. This approach is different from the approach used in the current systems where they only investigate the correspondences between the sample means (high-level information from the two views) and then train an independent maturity model based on this correspondence. Since low-level information is closer to the definition of maturity, our system should be able to estimate maturity with higher accuracy. The rest of this paper is organized as follows. In Sect. 2, the proposed system for measuring cotton fiber length and maturity is described. In Sect. 3, the transfer learning approach used to train the maturity module of this system is described. This section also includes information on the different types of cotton materials used in the studies to generate a library of cotton fiber images, and the different types of features extracted from these images to generate the target domain datasets. In addition, it also describes the different maturity models that are trained for the proposed system using these datasets. In Sect. 4, performance measure of the maturity models on test datasets is presented. Finally, in Sect. 5, conclusions and directions for future work are presented.    2 Proposed system  The goal of the proposed system is the simultaneous measurement of cotton fiber length and maturity from a single longitudinal scan of a complete cotton fiber. This will help the cotton experts to study the relationship between length and maturity for different cultivars. This is a challenging task from a single scan because of the aspect ratio of cotton fibers; the physical dimensions of a typical cotton fiber are 20 microns by 25,000 microns, i.e. an aspect ratio of 1:1,250. No cotton fiber is ever perfectly straight. The length measurement requires a large field-of-view (FOV) to capture the intricate turns of a complete cotton fiber whereas the maturity measurement requires high magnification to capture the textural differences between mature and immature fiber types. The proposed system achieves a compromise between the length and maturity modules. The proposed system consists of a single monochrome line-scan camera (Basler L-801k, Germany) attached via a lens tube (Infinitube Special, USA) to a 5 /0.225 numerical aperture lens objective (Mitutoyo, Japan). This setup provides a FOV of approximately 8&amp;nbsp;mm in width which is wide enough to encapsulate the turns of a cotton fiber, and an image resolution of 1&amp;nbsp;  per pixel which is sufficient to capture the fiber texture. Longitudinal images of a small portion of mature and immature fibers acquired are shown in Fig.  a, b. It is evident from this figure that there are textural differences between mature and immature fibers which can be quantified using image analysis. Sample preparation consists of randomly selecting a single cotton fiber from a population of fibers using tweezers, and then placing it on a 2.54&amp;nbsp;cm  7.62&amp;nbsp;cm clean and transparent glass slide. The fiber is typically positioned as straight as possible on the slide. After fiber placement, another identical glass slide is used as a cover slide to hold the fiber securely in place before the sample is placed on a Y-translation stage. The length of the acquisition area is determined by the operator before scanning and is 40&amp;nbsp;mm in most cases. Therefore, the scanned area is  and it takes 20&amp;nbsp;s to scan this area for a single fiber. Before image acquisition is initiated, a Z-focus stage (miCos GmbH ES-50, Germany) provides automatic fine focusing, and an X-translation stage (miCos GmbH VT-80, Germany) is used to manually position the fiber properly under the camera’s FOV. During image acquisition, a Y-translation stage (miCos GmbH VT-50, Germany) moves the sample between a DC-powered line-light source and the line-scan camera. After acquisition is complete, image data is transferred from the camera via a high-speed Camera-Link interface to the computer’s frame-grabber (National Instruments NI-1428, USA). The acquired image is then compressed (without loss of quality) using the libpng library and saved to a hard drive as a portable network graphics (PNG) file. A typical acquired image is 8,160 pixels by 40,000 pixels in size where the width is determined by the number of detectors in the linear CCD array of the camera and the length is influenced by the traveling distance sufficient to encompass the length of a scanned object. The following steps are taken to ensure that the cotton fiber images acquired using the system have low-distortion and are consistent for feature measurements.        All system components are securely fastened to an optics bench on an isolation table&amp;nbsp;to eliminate the ambient vibrations, and the translation stage velocity adjusted to eliminate the microscopic vibrations of the operating parts during acquisition. This ensures that the boundary of a cotton fiber in an image is smooth and reduces the uncertainties in fiber width measurements.         &amp;nbsp;           Image distortion in the form of changes in pixel scaling and blurring across the FOV was measured by imaging a ‘MR-1 Gellar Traceable’ micron ruler (Micro Gellar, USA).&amp;nbsp;Pixel scale refers to the numbers of pixels required to represent a physical distance of 1 micron. Blurring is a function of the focus stage position. Due to imperfections in the optics, off-axis focus near the sides of the ruler image were found to differ from best-focus in the center&amp;nbsp;by up to 100 microns, beyond the 14-micron depth of field of the objective lens.&amp;nbsp;However, as Fig. a shows, focus was found to be consistent and within the depth of field in the center 4&amp;nbsp;mm of the&amp;nbsp;8&amp;nbsp;mm FOV.&amp;nbsp;Figure b shows that the horizontal pixel scaling varies by less than 0.5&amp;nbsp;% over this range, from 1.001 to 1.005 pixels/micron, and increases to 1.025 pixels/micron (i.e., 2.5&amp;nbsp;% increase in magnification) at the sides of the 8&amp;nbsp;mm FOV.&amp;nbsp;As the system uses a line-scan camera, longitudinal pixel scaling is a function only of the translation stage velocity, which is consistent and produces no measurable variation along 40&amp;nbsp;mm of travel. &amp;nbsp;&amp;nbsp;&amp;nbsp;To ensure that parts of a fiber do not fall in the out-of-focus regions, an operator places a fiber on a glass slide as straight as possible. As a result, the majority of the fiber will fall in the center of the final image where it is in focus and minimally distorted. This will not impact the length measurement because those portions of the fiber that fall in the out-of-focus regions are still differentiable from the background and because no vertical distortions have been measured in the direction of travel. However, the out-of-focus segments are not suitable for feature measurements in the maturity module and are eventually removed.         &amp;nbsp;           Some features related to maturity estimation are sensitive to the brightness level of an acquired image. To resolve disparities in brightness levels between two pixels in the same image and between pixels across different images, two types of brightness corrections are applied to each pixel of an acquired image, namely (i) photo-response-non-uniformity (PRNU) flat-field correction, and (ii) region-based brightness correction. PRNU is a built-in function of the line-scan camera which is trained by exposing the camera to a specimen-slide when it is illuminated by a line-light but does not have any objects present on the slide. This way, the PRNU algorithm learns the differences in the sensitiveness of the camera sensors when exposed to the same amount of light and makes the necessary corrections to ensure that all the pixels on the training line image are of uniform brightness. The same corrections are then applied to all line images captured when objects are present on the slide. However, it has been observed that there are some minor variations in brightness between parts of the final image (e.g. top vs. bottom) when the direction of travel is significant (more than 5&amp;nbsp;mm). Therefore, PRNU is sufficient for brightness correction in the field-of-view but not in the direction of travel. Aging of the halogen light bulb and changes in the ambient lighting will affect acquisition for fibers that are scanned days or weeks apart. It is necessary to update the PRNU correction on a daily basis because changing input conditions render previous PRNU correction values obsolete. To alleviate differences in brightness between different images, as well as between different parts of the same image, another brightness correction is needed. A region-based brightness correction is applied to ensure that the backgrounds of all the images have an 8-bit pixel intensity level of 255. The correction is applied on non-overlapping sub-images of size 100 pixels by 100 pixels. Since the fiber width is only around 20 microns, every region is guaranteed to contain background pixels. The intensities of all the pixels in a region are rescaled proportionately such that the brightest pixel (corresponding to a background pixel) in the region has a value of 255, as shown in Fig. a, b. This process is applied independently to all the regions of a single image and repeated for every image to ensure that all the images in the cotton fiber library are at the same brightness level. &amp;nbsp;&amp;nbsp;&amp;nbsp;The brightness correction step ensures consistency of the measured features and potentially improves the discriminatory power of a model using these features. For instance, features 1 through 5 listed in Table 1 are not impacted by brightness correction because these features are based on physical measurements (widths) of a cotton fiber. Features 6 through 9 are intensity-based features measured relative to the background intensity. Since pixel intensities are rescaled proportionally, the distributions corresponding to these features shift without loss of class information. Lastly, features 10 through 13 are the texture features and it is expected that the computed values of these features will be impacted by different brightness levels. In general, for a given texture feature, brightness correction actually improves the separations between the distributions of different classes.         &amp;nbsp; All system components are securely fastened to an optics bench on an isolation table&amp;nbsp;to eliminate the ambient vibrations, and the translation stage velocity adjusted to eliminate the microscopic vibrations of the operating parts during acquisition. This ensures that the boundary of a cotton fiber in an image is smooth and reduces the uncertainties in fiber width measurements. Image distortion in the form of changes in pixel scaling and blurring across the FOV was measured by imaging a ‘MR-1 Gellar Traceable’ micron ruler (Micro Gellar, USA).&amp;nbsp;Pixel scale refers to the numbers of pixels required to represent a physical distance of 1 micron. Blurring is a function of the focus stage position. Due to imperfections in the optics, off-axis focus near the sides of the ruler image were found to differ from best-focus in the center&amp;nbsp;by up to 100 microns, beyond the 14-micron depth of field of the objective lens.&amp;nbsp;However, as Fig. a shows, focus was found to be consistent and within the depth of field in the center 4&amp;nbsp;mm of the&amp;nbsp;8&amp;nbsp;mm FOV.&amp;nbsp;Figure b shows that the horizontal pixel scaling varies by less than 0.5&amp;nbsp;% over this range, from 1.001 to 1.005 pixels/micron, and increases to 1.025 pixels/micron (i.e., 2.5&amp;nbsp;% increase in magnification) at the sides of the 8&amp;nbsp;mm FOV.&amp;nbsp;As the system uses a line-scan camera, longitudinal pixel scaling is a function only of the translation stage velocity, which is consistent and produces no measurable variation along 40&amp;nbsp;mm of travel. &amp;nbsp;&amp;nbsp;&amp;nbsp;To ensure that parts of a fiber do not fall in the out-of-focus regions, an operator places a fiber on a glass slide as straight as possible. As a result, the majority of the fiber will fall in the center of the final image where it is in focus and minimally distorted. This will not impact the length measurement because those portions of the fiber that fall in the out-of-focus regions are still differentiable from the background and because no vertical distortions have been measured in the direction of travel. However, the out-of-focus segments are not suitable for feature measurements in the maturity module and are eventually removed. Some features related to maturity estimation are sensitive to the brightness level of an acquired image. To resolve disparities in brightness levels between two pixels in the same image and between pixels across different images, two types of brightness corrections are applied to each pixel of an acquired image, namely (i) photo-response-non-uniformity (PRNU) flat-field correction, and (ii) region-based brightness correction. PRNU is a built-in function of the line-scan camera which is trained by exposing the camera to a specimen-slide when it is illuminated by a line-light but does not have any objects present on the slide. This way, the PRNU algorithm learns the differences in the sensitiveness of the camera sensors when exposed to the same amount of light and makes the necessary corrections to ensure that all the pixels on the training line image are of uniform brightness. The same corrections are then applied to all line images captured when objects are present on the slide. However, it has been observed that there are some minor variations in brightness between parts of the final image (e.g. top vs. bottom) when the direction of travel is significant (more than 5&amp;nbsp;mm). Therefore, PRNU is sufficient for brightness correction in the field-of-view but not in the direction of travel. Aging of the halogen light bulb and changes in the ambient lighting will affect acquisition for fibers that are scanned days or weeks apart. It is necessary to update the PRNU correction on a daily basis because changing input conditions render previous PRNU correction values obsolete. To alleviate differences in brightness between different images, as well as between different parts of the same image, another brightness correction is needed. A region-based brightness correction is applied to ensure that the backgrounds of all the images have an 8-bit pixel intensity level of 255. The correction is applied on non-overlapping sub-images of size 100 pixels by 100 pixels. Since the fiber width is only around 20 microns, every region is guaranteed to contain background pixels. The intensities of all the pixels in a region are rescaled proportionately such that the brightest pixel (corresponding to a background pixel) in the region has a value of 255, as shown in Fig. a, b. This process is applied independently to all the regions of a single image and repeated for every image to ensure that all the images in the cotton fiber library are at the same brightness level. &amp;nbsp;&amp;nbsp;&amp;nbsp;The brightness correction step ensures consistency of the measured features and potentially improves the discriminatory power of a model using these features. For instance, features 1 through 5 listed in Table 1 are not impacted by brightness correction because these features are based on physical measurements (widths) of a cotton fiber. Features 6 through 9 are intensity-based features measured relative to the background intensity. Since pixel intensities are rescaled proportionally, the distributions corresponding to these features shift without loss of class information. Lastly, features 10 through 13 are the texture features and it is expected that the computed values of these features will be impacted by different brightness levels. In general, for a given texture feature, brightness correction actually improves the separations between the distributions of different classes.    3 Transfer learning  Transfer learning is a machine learning framework which allows knowledge to be transferred from a source problem to a target problem in order to improve learning in a target problem [16]. Under transfer learning terminology, the reference method of measuring maturity using image analysis of microscopic cross-sections is called the source system, and our proposed approach of measuring maturity from longitudinal images of cotton fibers is called the target system. Transfer learning is applied to this problem to transfer the maturity knowledge from the source system to the target system and then use this knowledge to learn a suitable maturity model for the target system. Under transfer learning, each system is associated with a  and a . A   is a set which describes the input characteristics of the system. It is made up of two components: a feature space  and a marginal probability distribution . A domain example is  where  represents the  feature of the example. In addition, the marginal distribution of the  feature is given by . Let  represent the source domain, and  represent the target domain. The set  consists of  training examples, hence  where each feature vector  is in  dimensional space. Similarly,  is in  dimensional space.  and  denote the marginal feature distributions in the source and target domains respectively. A   is a set which describes the output characteristics of a problem. It is also made up of two components: a label space  and a model or functional mapping . In a classification task, the label space consists of a finite number of possible labels or classes. In a regression task, the label space takes on real values. The model is a function which predicts the label for a given example . One can also write  which accommodates cases where examples could belong to more than one class. The goal of transfer learning is to learn  for the target system. Let  denote the source task and  denote the target task.  and  are sets consisting of the class labels in the source and target domains respectively. If one assumes that the same classes occur in both domains, then one can write . Let  and  denote the posterior distributions for a given example. Pan and Yang [16] surveyed and categorized different transfer learning approaches. In this paper, only a feature-based approach is presented because the feature sets between the two systems are expected to be related. This relationship is derived from the fact that both systems are involved in measuring cotton fiber maturity. Feature-based transfer learning is essentially a two-step process, (1) a domain adaptation step which bridges differences in the feature distributions between the two domains, and (2) a model estimation step which uses traditional machine learning theory to fit a suitable model to the adapted data. Traditional machine learning algorithms alone are not suitable because the source and target systems have different feature spaces or different marginal distributions. The domain adaptation step of the presented feature-based approach is motivated by the algorithm in [17]. The flowchart for training the cotton imaging system using feature-based transfer learning is shown in Fig.  . The same flowchart is applicable for learning two different types of models in the target domain, (1) a model   for estimating the average maturity of a complete fiber, and (2) a model   for estimating the average maturity of a fiber segment. The first model will be used to estimate the average maturity of test fibers during the testing phase. In addition to providing additional maturity information, the second model will also help ascertain if the transfer learning approach is suitable for this application. The two models will use distinct datasets. For training, transfer learning requires two different datasets, (1) a cross-section dataset available from the source domain, and (2) a longitudinal dataset that is generated using our system. This study uses a subset of 8 reference cotton classes from a total of 104 reference cottons [3] provided by the Fiber Biopolymer Research Institute (FBRI) at Lubbock, Texas. The first five cottons are used to generate the training datasets, and the last three cottons are used to generate the test datasets for the validation studies. For each cotton class, two identical samples consisting of a few thousand fibers are prepared carefully. Fibers from the first sample are cross-sectioned and image analysis [ 5] is performed on these cross-sections to determine perimeter, area of the secondary cell wall, and hence the maturity of the cross-sections. In addition, instead of using the area of a cross-section, it is more logical to use the thickness of the cross-section because this will give a better measure of the amount of cellulose that light has to travel through. The thickness   of a cross-section is computed using Eqs. ( 3)–( 5), where   and   are the perimeter and cell-wall area respectively,   is the radius of the cross-section and   is the radius of the lumen (before collapse).                                      Denote the source domain data for a cotton sample as   where   denotes the source domain features (perimeter and thickness) derived from its cross-sections, and   denotes the maturity computed for the cross-sections using Eq. ( 1). Fibers from the second sample are imaged individually using the target system and two types of target domain datasets are generated, namely, (1) a target domain dataset pertaining to complete cotton fibers, where  denotes the average features measured from a cotton fiber, and (2) a target domain dataset pertaining to fiber segments, where  denotes the features measured from a cotton fiber segment. Using the first dataset, one learns a model  to predict the average maturity  of a complete cotton fiber, whereas using the second dataset one learns a model  to predict the maturity of a cotton fiber segment. In our previous publications [18, 19], feature extraction algorithms were applied to measure features from complete cotton fiber images. In this paper, feature extraction algorithms are also applied to measure the same features from cotton fiber segments. To improve processing speed, some changes have been made to the feature extraction algorithms when applied on cotton fiber segments. These changes are discussed in the next section. Before feature extraction, the medial axis of a fiber is determined from the length algorithm, as described in [ 1]. The medial axis of the fiber tells us the location of the fiber in the image, and this information is used to crop the fiber image into smaller and more manageable images called tiles. The portion of a fiber included in a tile is called a fiber segment. The region-of-interest of a tile is determined based on the length, width and orientation of the segment that is encapsulated within it. The width never exceeds 60 pixels because this is greater than the maximum fiber width. The length of a segment (along the medial axis) is set to 150 pixels, although this number can vary by 10&amp;nbsp;% depending on the orientation of the fiber. The length of a segment needs to be long enough such that meaningful features can be measured from it. Tests performed showed that not enough discriminatory information can be extracted from very short segments. In addition, 50&amp;nbsp;% overlap is included between adjacent segments, as illustrated by the three segments in Fig.  . Overlap is introduced to ensure that maturity estimations along the fiber change gradually.
Springer.tar//Springer//Springer\10.1007-s00138-013-0507-z.xml:Evaluating the effect of diffuse light on photometric stereo reconstruction:Photometric stereo 3D face reconstruction Ambient (diffuse) light:  1 Introduction  Photometric stereo techniques use images of an object that are captured from the same viewpoint but under different illumination directions&amp;nbsp;[38]. When a varying albedo field is considered, a minimum of three illumination directions is required&amp;nbsp;[3]. The images are fused appropriately and the orientation and albedo of each surface facet are recovered. Compared with conventional stereo reconstruction techniques, the advantages of photometric stereo include its ability to operate on featureless objects, the absence of feature matching errors and its computational simplicity. The main reconstruction stage of photometric stereo requires each input image to be associated with a particular 3D illumination vector&amp;nbsp;[3]. This requires the subject to be illuminated in turn by various directional illumination sources. In real-life situations, apart from the intended directional component, ambient light is also present, thus, introducing a diffuse light component that reduces the directionality of the 3D illumination vectors. This work aims at assessing the robustness of photometric stereo in the presence of various levels of ambient light. In particular, various illuminance ratios of the diffuse to the directional light component are considered, and the output reconstruction quality is systematically assessed. By doing this, this work provides a reference that indicates up to what levels of the diffuse light component photometric stereo renders useful reconstruction results. In our evaluation, the popular human face class is employed for surface reconstruction. This is an object class with a wide range of emerging applications, which also comprises an appropriate platform for assessing photometric stereo reconstruction quality, due to the considerable surface variation and object convexity. This paper is structured as follows: Sect.&amp;nbsp;2 gives a brief account of photometric stereo and flatfielding. Section 3 presents the related literature and identifies our contributions. Section&amp;nbsp;4 discusses the experimental methodology that is employed for controlling the diffuse to directional illuminance ratio and collecting intensity measurements during the frame capturing process. In Sect. 5, a metric for the evaluation of 3D face reconstruction is introduced. Section 6 presents and evaluates the outputs of photometric stereo reconstruction for various illuminance ratios. We conclude in Sect. 7.    2 Background  This section presents the motivation of this work and the considered assumptions, sets up the notation that is employed throughout the paper and gives a brief overview of photometric stereo reconstruction and flatfielding. When light is propagated through an attenuating medium, such as fog, mist, haze or murky water, its power decreases and it deviates from a straight line course. This is due to the angular scattering of the flux at each point within the volume of the medium, which causes scattering effects of complex trigonometry [11, 15, 25, 28–30]. To simulate such effects, various mathematical models are proposed in the literature, which approximate the real-world light attenuation [26]. An analytical light propagation model for a directional collimated beam source is proposed in [22], while a simplified version of this model that targets underwater reconstruction is presented in [27]. Including in the processing chain, a model of the attenuating medium increases the overall computational complexity, while, since such a model is only a mathematical approximation of the complex real-world scattering phenomena, approximation-related errors are propagated to the main photometric stereo reconstruction step. Furthermore, the method of [27] relies on solving a system of non-linear equations, whose optimization does not always converge and whose final solution is dependent on the initial approximation. For attenuating media such as fog, mist, cloud, haze and various water media, assessing the reconstruction quality for a certain medium intensity and resulting incident light is not feasible, as the medium density cannot be systematically adjusted; while a photometer is not applicable for the case of water media. However, the ambient light that is generated by a floodlight, whose intensity is increased in discrete steps by employing a dimmer, consists, for the propagated directional light, an attenuating medium whose strength is gradually increased in a fully controlled fashion; while the incident light can also be measured with a photometer. Thus, a lookup table that associates the attenuation strength of the medium with the photometric stereo reconstruction quality can be extracted to provide a guideline for the reconstruction accuracy of pure photometric stereo for situations, where the intervening medium attenuates the intended directional illumination. As considered in the classical photometric stereo approach, it is assumed that the light sources are point sources at infinity, and thus, the intensity of the incident light is the same everywhere. Moreover, it is assumed that the projection is orthographic,  the camera is far from the object relative to the object size. Finally, Lambertian reflectance is assumed with varying albedo values across the surface, and no shadows, occlusions or interreflections are considered, which is justifiable for convex objects. The  -plane of our coordinate system coincides with the image plane, whereas the   axis is lined up with the optical axis of the imaging system. A patch   with coordinates   on the 3D object surface is projected on the image plane at pixel  . The object height at   is  , where   is the function describing the object surface; whereas the gradient components are defined as   and  . If   denotes the surface normal vector at  , then the unit surface normal is       Let   be the number of images that constitute the input set of the reconstruction process. In each one of these images, the object is illuminated by a single point source. The   illumination vectors are non-coplanar. The illumination vector from   to the  th light source is denoted with  , and its gradient is  . The unit illumination vectors in the direction of the   light sources are Let   denote the measured image intensity for patch   in input image  , and let   denote the actual reflectance albedo of that patch. The equation that relates the measured pixel intensities with the actual reflectance values&amp;nbsp;[ 21] is       Combining the   measurements for  , we obtain       where the rows of matrix   are the   illumination vectors   and the components of vector   are the   image intensity measurements. If vectors   are non-coplanar for  , then matrix   is nonsingular and can thus be inverted, therefore, giving       If &amp;nbsp; , then the surface gradient components at   are            The unit normal at surface patch   is computed by ( 1) and ( 6), whereas the reflectance albedo is       Thus, photometric stereo calculates the normal and albedo for each patch of surface  . Since we have considered varying albedos, a minimum of   is required for solving ( 5). To remove the effect of the inverse-square law of light propagation, flatfielding can be employed to correct appropriately the raw input images&amp;nbsp;[8]. The standard flatfielding process employs a flat calibrating device of uniform color, which is captured under the same imaging conditions as the object of interest, to produce a set of reference images. In these images, the variation of intensity across the device’s surface is solely due to variations in illumination and noise. To smooth out the high frequency noise and avoid subsequent noise amplification, a low order polynomial fitting may be applied to the grey values of the reference images&amp;nbsp;[31]. Let   denote the  th raw input image, and let   be its greyscale version. Let   denote the 2D low order function, which models the  th illumination field. Images   are divided pixel-by-pixel with the points of the corresponding illumination field, and, to preserve the initial dynamic range, all quotients are multiplied with the average grey value of  . Thus, for every point   of the image plane, a new pixel value   is computed as    3 Literature survey  In this section, we present the current state of the art in the general field of photometric stereo, with a focus on issues related to directional and ambient light, and state our contributions. Since the seminal paper of [38], a large number of papers on photometric stereo have been published. To target non-Lambertian objects, [4] and [2] examine photometric stereo in the presence of both highlights and shadows. In [18], the challenging case of having only two available images without occlusion is dealt with. Instead of adopting the standard orthographic model, perspective projection is considered in [34]. Photometric stereo is combined with multiview stereo and/or structure from motion, to deal with non-static objects and enable full 3D reconstruction [17, 24, 35, 36, 40]. Such methods overcome the inability of traditional photometric stereo to deal with varying viewpoints, by computing the pixel correspondences across different images. The work of [23] removes the interreflections in a photometric stereo setup, by separating the indirect from the direct lighting. A thorough overview of photometric stereo techniques and the related theory is presented in [3]. The use of photometric stereo has been extended to a number of interesting applications and environments. In [16], photometric stereo is employed to model white cloth, based on the observation that when a white object is illuminated with red, green and blue light; the RGB value reflected by each pixel is in one-to-one correspondence with the local surface normal. In [27], photometric stereo is performed in scattering media, by including the properties of the medium in the image formation model. In recent works, photometric stereo has been employed for the reconstruction of human faces. The algorithm of [41] recovers class-specific albedos and surface normals for objects of the human face class, utilizing an observation matrix that consists of multiple human faces under different illuminations. This is in contrast with traditional photometric stereo that considers the appearance of solely a single object under different illuminations. The use of photometric stereo reconstruction for face recognition purposes is evaluated in [39]. Moreover, a hardware system that performs face reconstruction by applying near infrared illumination, in addition to the standard visible light sources, is presented in [13]. All the above works assume that each input image that participates in the reconstruction process can be associated with a particular 3D illumination vector that represents the direction of incident light. Recently, a number of techniques have been proposed to estimate the directional light. Typically, such methods employ calibrating devices. A Lambertian sphere is used in [7] for the detection of multiple light sources, whereas a specular sphere is employed in [42] and [10]. A pair of calibrating spheres is used in [33], and the difference sphere that is derived from the two image regions corresponding to the spheres is employed. In [1], a planar device with specific patterns on its surface is constructed and used. In [37], the calibrating device is a cube, whereas [19] employs a reference object of known shape and material. To estimate the light vectors in an uncalibrated manner, assumptions related to the position of the light sources are typically employed. The method of [9] estimates light direction, by considering differential image pairs and using the assumption that the light sources lie on a circle. To find the light source position, [14] takes as an input a 3D geometric model of an object with specular reflection and implements an iterative relaxation scheme. Contrary to the estimation of directional illumination, the presence of ambient light is not adequately covered in the literature of photometric stereo. In [20], the ambient light intensity is included in Eqs. (3) and (4) as an additive unknown quantity that needs to be calculated during reconstruction, along with the directional light intensity that needs to be precomputed prior to reconstruction. Thus, the minimum number of input images increases by one, and, apart from its direction, the intensity of directional light also needs to be precomputed. In fact, the directional light estimation methods that are reviewed above only render the direction of light, and thus, precomputing in addition the intensity of light would complicate light estimation tasks. Contrary to [20], this work does not include the directional and ambient light intensities in the photometric stereo system, but treats ambient light as a diffuse light component that degrades the contrast created by the intended directional illumination. Thus, we consider only the unit illumination vectors , which are estimated with a Lambertian sphere [7]. The presence of ambient light can also be accounted for by employing the analytical method of [6], which considers general illumination conditions. The work of [6] goes beyond the representation of directional light with a 3D illumination vector and represents the general illumination conditions with low order spherical harmonics. All the above light estimation methods, with the exception of [6], consider the approximation of incident light with a 3D illumination vector. This simplifies both the light estimation and surface reconstruction processes. This work investigates if and when the above approximation is accurate and how much it affects the reconstruction quality, for various levels of the diffuse component relative to the directional component of the incident light. Thus, this work treats the diffuse light component as a degrading factor to the intended light directionality and assesses the photometric stereo robustness to various degradation levels. Quantizing and visualizing the robustness of the method is critical, as it indicates when the vast majority of theoretical photometric stereo and light estimation approaches that assume 3D illumination vectors are valid. This is of significant practical value, as it can increase the number of algorithmic options that are suitable for a given application, beyond the limited set of computationally complex approaches that deal with general lighting conditions&amp;nbsp;[6]. In our assessment, the human face class is employed as the target object class. This class provides convex objects of considerable surface variation and is, thus, suitable for a fair comparative evaluation of the photometric stereo reconstruction quality under varying illumination ratios. To the best of our knowledge, this is the first time that an evaluation of the effect of the diffuse light component on photometric stereo reconstruction is presented in the literature.    4 Evaluation methodology  Let   and   denote the illuminances in   ( ) that correspond to the ambient&amp;nbsp;(diffuse) and directional light components, respectively. We are interested in the illuminance ratio       With respect to the value of  , there are three types of situation.        For , the directional and diffuse light components have the same illuminance.         &amp;nbsp;           For \(\lambda &amp;lt;1\)              &amp;nbsp;           For \(\lambda &amp;gt;1\)              &amp;nbsp;      In the remainder of this section, we discuss our methodology of generating and measuring varying   values, by employing both directional and diffuse light sources. For , the directional and diffuse light components have the same illuminance. For \(\lambda &amp;lt;1\), the directional component is stronger than the diffuse component. For \(\lambda &amp;gt;1\), the directional component is weaker than the diffuse component. In the employed experimental setup, the directional light component is generated by flashes that are hardwired to a camera&amp;nbsp;(Fig.&amp;nbsp; a). Such a system may capture images at high frame-rates, if a fast camera is used. To measure   a flashmeter is employed. Since the photographed subject is accessible, incident light measuring is applied as this is more accurate than reflected light measuring&amp;nbsp;[ 32]. Thus, a flashmeter is placed at the position of the subject and in the direction of the lens axis, as shown in Fig.&amp;nbsp; c. The flashmeter measures the aperture value  &amp;nbsp;[ 32]. Let   denote the exposure value and let   denote the shutter speed time value. The relationship between   and   is as follows       Let   denote the shutter speed in   and let   denote the exposure time in s. It is       The shutter speed   and the shutter speed time value   are related as follows       Illuminance   is computed as       where   denotes the ISO arithmetic speed and   denotes incident-light flashmeter calibration constant. The values of  and  are predetermined, as it will be explained in Sect. 6.1. Thus, once the  measurement of the directional light is rendered by the flashmeter,  is rendered by Eq. (13), after employing Eqs. (10)–(12). The diffuse&amp;nbsp;(ambient) component is generated by a flood-light, which is placed exactly behind the camera, thus illuminating the subject uniformly&amp;nbsp;(Fig.&amp;nbsp;b). The intensity of the flood-light is controlled with a dimmer switch. Therefore, various levels of ambient light are generated, resulting in a variation of the illuminance ratio  for a given . By setting a flashmeter to ambient operation&amp;nbsp;[32],  can be computed after measuring the value of . The computational process is the same as for  and Eq. (13) now renders , instead of . In our evaluation, we have included eight experiments, each associated with a specific directional illuminance. The illuminance values are demonstrated in Table&amp;nbsp; 1. Each experiment includes   tests, each corresponding to a particular ambient illuminance. The ambient illuminances corresponding to tests 1–16 are the same for all eight experiments. Thus, each test corresponds to a certain position of the dimmer that is attached to the flood-light. For test  , no ambient light is considered. As the test number increases, the ambient light gradually becomes higher, and for test  , the maximum intensity of the flood-light is employed. For tests 12–16, apart from the flood-light rendering high intensities, the room-lights are also on.
Springer.tar//Springer//Springer\10.1007-s00138-013-0508-y.xml:Charting-based subspace learning for video-based human action classification:Human motion analysis Charting Subspace learning Human motion classification:  1 Introduction  A fundamental problem in human motion analysis [23, 27] is the extraction of motion features like sequences of 3D joint angles of a skeletal model (body postures) or as single objects from video sequences (location tracking), which are later interpreted (classification) for various applications. Challenges include high-dimensional search space, complex and unpredictable nature of human motion, noisy images or motion information and similar appearance of certain actions, e.g. a fast walk and a slow jog. Human motion classification has a wide array of applications in the areas of biomedical applications, sports, security and surveillance [22]. In biomedical applications, classification of gait is an important example [22]. In addition, the gait signature can also be used in security applications [9, 22]. An important application for human motion classification algorithm is in the automatic initialisation of subspace (low-dim) human motion tracking algorithms. Subspace tracking algorithms are, typically, initialised manually by visually identifying the test sequences [14, 16]. In this paper, we propose a full-body articulated human motion classification algorithm using charting, a non-linear subspace learning algorithm. To perform classification in the low-dimensional subspace, we propose a multi-layered classification framework applicable to multiple-view sequence of skeletal poses, as obtained from our markerless tracking system [15] or from marker-based mocap systems. The multi-layered classification performs pruning of candidate actions at each layer, reducing the complexity of classification search until only a few candidate actions with minor variations remain in the final layer. Here a detailed classification search is performed using multi-dimensional dynamic time warping (MDDTW) to account for variable time sampling in actions. We present and compare two variations of our classification framework. First, each action is mapped to a single low-dim space learnt from all the known actions. Second, each action is mapped separately to its own subspace. We report good classification accuracies on the HumanEva dataset [34], CMU motion capture dataset [8], HDM05 dataset [24] and ACCAD dataset [7], comparable or better than state-of-the-art classification systems. To our best knowledge, our work differs from the current literature at least in two ways. First, the use of charting to generate a latent subspace for articulated body motion classification has not been reported before. Second, we compare experimentally and evaluate single-subspace and multiple-subspace classification frameworks. Our article is organised as follows. In Sect.&amp;nbsp; 2, we discuss the related approaches to human motion analysis. Section 3 provides a brief overview of charting. In Sect.&amp;nbsp;4, we present our proposed subspace classification algorithm. In Sect.&amp;nbsp;5, we present our classification experimental results on various datasets and compare results with state-of-the-art classification algorithms. We also perform a detailed analysis of the system behaviour changing system parameters. Finally, in Sect.&amp;nbsp;6, we present our conclusions and possible future extensions.    2 Related work  Vision-based human action classification assigns an action label to an input video sequence. This requires extracting discriminative features from video sequences and a dictionary of suitably represented actions. The extraction of discriminative features is essential for accurate human motion classification. We organise the human action classification literature using the dimensionality of feature spaces. In high-dimensional classification algorithms, the features used can be further classified as either image-based [6, 10, 41] or skeletal [21]. Image-based features are extracted directly from the video sequences, while skeletal features, or poses, are obtained from motion capture data or from tracking algorithms. Amongst these two features, image-based features are more widely used in human motion classification systems, especially monocular systems which do not require motion capture equipment. Silhouette-based features, such as binary motion energy image [3], R-transform [11], 3D space-time features [10], appearance and position context descriptors [26], and 3D silhouette-based voxels [40] are examples of image-based features. Several human motion classification algorithms are based on skeletal features. For instance, Zsolt et al. [12] propose a system based on action primitives which are sub-sequences derived from complete action sequences. Lv et al. [21] model the dynamics of 3D joint position using HMM and learn a weak classifier for each joint position, which are then combined by multi-class AdaBoost. A few algorithms combine tracking and classification [12]. An alternative approach, addressing computational complexity associated with high-dimensional classification algorithms, is learning low-dimensional subspaces for image or skeletal features, using dimensionality reduction (DR). Several linear and non-linear techniques have been proposed in the machine learning literature. Principal component analysis (PCA) and probabilistic PCA [20] are the linear DR techniques used, e.g. by Urtasun et al. [38] and Sidenbladh et al. [33]. However, the mapping between the original feature space and the latent subspace is in general non-linear, and indeed non-linear DR techniques like Isomap [36], locally linear embedding [29], Gaussian process latent variable model (GPLVM) [19], and local linear co-ordination (LLC) [30] are used to learn the human action manifold. Among the non-linear algorithms, in terms of space structure preservation, Isomap preserves the geodesic distances on the surface of the subspace represented as a graph (), while LLE belongs to a class of algorithms which preserve . LLC [30] and charting [4] preserve both global and local properties by performing a global alignment of several locally linear models. In our work, we use charting to reduce dimensionality and provide an overview in Sect.&amp;nbsp; 3. The major advantage of charting over competing non-linear methods that estimate the subspace density model, e.g. GPLVM, is that charting’s subspace can be estimated in closed form and is not prone to local minima. Charting is very closely related to the local coordination of global models method (LLC) [30], which also describes the data with a finite set of Gaussians (parameterized as factor analyzers). However, LLC simultaneously fits the model and assigns global coordinates, and is prone to local minima. Charting performs better than LLC when learning a smooth subspace with noisy data, as each chart MAP estimation depends on all other charts; bringing non-local information about subspace shape into the local description of each neighbourhood, ensuring that adjoining neighbourhoods have similar covariances [4]. Thus, even in the presence of locally linear noisy points which are dense perpendicular to the subspace, the local chart are oriented parallel to the subspace as part of a globally optimal solution. In Lawrence et al. [20], different algorithms are compared based on high-dimensional data proximity preservation, availability of forward and inverse mappings between high-dimensional data and subspace data, suitability for non-linear data, probabilistic approach, convexity (algorithms with unique solution are considered convex algorithms). All the above criteria are relevant for human motion analysis; specifically, proximity preservation, smoothness and forward mapping play an important role in classification. In addition to satisfying all the criteria, charting also contains an intrinsic dimensionality estimation mechanism, useful to obtain the embedded subspace with its estimated dimension. Similar to the high-dimensional classification algorithms, the low-dim classification algorithms can also be categorised based on the features from which the low-dim subspace is learnt. The low-dim subspace classification algorithms focus on learning an accurate low-dim representation of high-dimensional features to reduce the computational complexity of high-dimensional classification algorithms [2, 35]. In subspace classification, image features are comparatively more popular than skeletal features with the predominant feature being silhouettes. Wang [39] uses LPP with image silhouettes to learn the subspace for action classification. Chin et al. [35] learn silhouette subspaces using local linear embedding [29]. Niebles et al. [25] calculate patches of normalised space and time derivatives, and reduce the dimensionality using PCA after smoothing. Recently, Blackburn et al. [2] used Isomap to learn the subspace representation for smoothed silhouette sequences. Dynamic time warping (DTW) is later used for matching test trajectories with database trajectories. Some algorithms derive the low-dimensional subspace for skeletal features. In Han [17], the subspace for multiple actions are learnt from 3D skeletal features using hierarchical GPLVM. In addition, there are subspace systems which again function as combined tracking-classification systems [28]. Chen et al. [5] use switched-GPDM to perform simultaneous action tracking and classification in the subspace. Similarly, Jaeggli et al. [13], track in a subspace learnt using LLE and perform action classification. Raskin et al. [28] propose two systems for simultaneous action tracking and classification using HGPLVM [28] and GPLVM [18]. Recently, a few algorithms focus on enhancing the inter-class distance in the subspace. Such classification algorithms can be termed as discriminative classifiers [32, 37]. In Shyr et al. [32], a novel subspace estimation technique called sequence kernel dimension reduction is proposed for classification. Urtasun et al. [37] propose a discriminative GPLVM for action classification. Our concise review of action classification literature suggests that image-based features are more widely used. This can be primarily attributed to the fact that image-based features are directly accessible from video frames, unlike skeletons which require further processing. It must be noted, however, that skeletons are directly available in applications using marker-based mocap system; e.g., video production and biomedical motion analysis. In this article, we present full body articulated multiple view human motion classification using low-dimensional skeletal features. Charting is used to learn action subspaces (low-dimensional) from 3D joint angles (skeletal features). Moreover, we derive discriminative subspace motion patterns and key-frame-based representations from skeletal features, which are used in a multi-layered classification scheme.    3 Charting  Charting constructs a non-linear mapping from the high-dimensional joint angle space,  , to a low-dimensional manifold in  , where    \(&amp;lt;\)  . The mapping preserves local geometric relations in the manifold and is pseudo-invertible, so that the reverse mapping is also learnt. The steps involved in charting include: (a) estimating directly the intrinsic dimensionality of the manifold from the training data; (b) obtaining locally linear, low-dimensional patches ( ), and merging them into a single low-dimensional space ( ); (c) computing the forward and reverse mappings between the high-dimensional and low-dimensional spaces. We provide a brief overview of all the steps and refer the readers to [ 4] for a more detailed explanation.         Estimating the intrinsic manifold dimensionalityGiven a -frame sequence of -dimensional joint angle vectors, , the intrinsic dimensionality is estimated using a point-growth process. A ball of radius or scale , centered on each point, is grown and the number of data points contained in it, , is recorded. To estimate the locally linear scale  and intrinsic dimensionality , the growth rate at each point for various values of  are tracked, . The maximum of  gives an estimate of the locally linear scale  and intrinsic dimensionality  as  at linear scale and lower at the other scales.         &amp;nbsp;            Charting step              &amp;nbsp;            Connecting step The final step connects local charts, specified by GMM. Firstly, PCA is used in each chart to reduce the dimensionality in each local chart. Specifically, a low-dimensional representation, U , of the th chart is obtained by PCA using the reference frame of the first  eigenvectors of the chart’s covariance matrix, . The goal of connection step is to sew together all charts into a single global low-dimensional subspace using a weighted average projection.         &amp;nbsp;      To do this, each lower-dimensional chart,   is projected to the global low-dimensional co-ordinate system   using an affine transform, say   for the  -th chart, and probability-based weighting  , which is the probability that the  th chart generates point  . Estimating the intrinsic manifold dimensionalityGiven a -frame sequence of -dimensional joint angle vectors, , the intrinsic dimensionality is estimated using a point-growth process. A ball of radius or scale , centered on each point, is grown and the number of data points contained in it, , is recorded. To estimate the locally linear scale  and intrinsic dimensionality , the growth rate at each point for various values of  are tracked, . The maximum of  gives an estimate of the locally linear scale  and intrinsic dimensionality  as  at linear scale and lower at the other scales. Charting step The high-dimensional dataset is partitioned into locally linear partitions by fitting a Gaussian mixture model (GMM) density to data. The parameters of GMM are learnt with two constraints: (a) the data in each partition has minimal loss of variance between high and low-dim space, and (b) neighboring charts should span maximally similar subspaces. The first criterion is obtained by fitting a GMM and maximising the likelihood. The second criterion is achieved using a prior-based on cross entropy ensuring an alignment and overlap of neighbouring charts. Given the likelihood and prior, the posterior of GMM over the parameters is formulated, and MAP estimate is used to obtain the optimal parameters. Connecting step The final step connects local charts, specified by GMM. Firstly, PCA is used in each chart to reduce the dimensionality in each local chart. Specifically, a low-dimensional representation, U , of the th chart is obtained by PCA using the reference frame of the first  eigenvectors of the chart’s covariance matrix, . The goal of connection step is to sew together all charts into a single global low-dimensional subspace using a weighted average projection. Given the solved affine transform and the probability-based weighting function, the single global co-ordinate system is obtained, which is a smooth low-dimensional representation of the high-dimensional data.       Conversely, the posterior mean inverse mapping reconstructing the joint angle vector from its corresponding manifold point is:       where   is the probability of  th chart generating x,   is the PCA operator used in the connecting step and   is its pseudo-inverse. Note that inverse mapping is not used in our classification framework. The learnt subspace representation was shown to preserve the geometry of high-dimensional local neighbourhoods in the subspace [ 4]. In charting, similar poses in the high-dimensional joint angle space are mapped to the same region in the low-dimensional subspace, which is illustrated in Fig.&amp;nbsp;  . This property arises as charting falls into the class of subspace learning algorithms that perform a global alignment of locally linear model, preserving the local neighbourhood structure. The neighbourhood geometry preservation property can be used to model the subspace dynamics, as the periodic cycles of an action are well represented in the learnt low-dimensional subspace as shown in Fig.&amp;nbsp;  . In addition, the subspace for an aperiodic action is also illustrated in Fig.&amp;nbsp;  . In addition to preserving the geometry of local neighbourhoods, charting also preserves the high-dimensional spacing between the poses, or frames, in the subspace. An illustration of the spacing preservation is shown in Fig.&amp;nbsp; . The spacing between high-dimensional poses are obtained as the distance between two position vectors, where each position vector corresponding to the vector defined between the origin and pose in a frame. Similarly, the spacing between subspace co-ordinates are also obtained as the distance between two position vectors. As shown in Fig.&amp;nbsp; , charting preserves the spacing between the poses in the subspace.    4 Charting-based human motion classification  Classifying the articulated motion is a challenging task because of the complex and generally unpredictable nature of human movements and the high-dimensional search space, with typically (20–50) degrees of freedom for skeletal pose [12] and (\(&amp;gt;\)100) degrees of freedom for image-based descriptors like shape context and HOG [6, 26]. Further challenges are posed by variations in motion style within the same action (intra-class variations), and by similarities between different actions, e.g., slow running and jogging. A good human action classification approach should be able to generalize over intra-class variations, while providing good inter-class discrimination. Based on our review of subspace human action classification literature, we observed that subspace human action classification systems either learn a single subspace from examples of multiple actions [37] or learn separate subspaces for each action (multiple subspaces). In this work, we attempt to evaluate both the approaches, by proposing a multi-layered classification framework using both single subspace and a multiple subspace. In our work, we are particularly interested in addressing high-dimensional search, while maintaining a high classification accuracy. We present a classification framework in a low-dimensional subspace learnt by charting. The extracted skeletal features from the video sequences are represented using their subspaces. Specifically, the pose in each frame corresponds to a point in the subspace, and the entire estimated sequence of poses is represented by a curve in this subspace (Fig.&amp;nbsp; ). Motion classification is based on the comparison of subspace trajectories that represent different actions. Two categories of algorithms exist in classifying a human actions sequence in the subspace. In the first, the focus is on enhancing the discrimination among the classes during the subspace learning process. Discriminative GPLVM and sufficient discriminative DR algorithms belong to this category. In the second, the inter-class distance is not enhanced, but computationally expensive trajectory or curve matching algorithms are incorporated within the classification framework, e.g., the Frechet distance [28] or DTW. Our work belongs to the latter category; we use multi-dimensional dynamic time warping in a multi-layered classification framework to match the embedded test trajectory with the learnt subspace trajectories. The set of candidate actions (subspace) is pruned at each layer and in the final layer, where only similar classes with subtle variations remain, multi-dimensional dynamic time warping, is used to classify the query. We explored learning action subspaces in two ways: firstly, learning a single subspace for all actions (one subspace for all actions); secondly, multiple subspaces for multiple actions (one subspace per action). As illustrated in Sect. 4.1, few variations occur in the single and multiple subspace structures, which is accounted for in our multi-layered classification framework. We adopt a  framework in the subspace. We devise a subspace classification framework using skeletal poses. In addition, we identify the minimum sub-sequence length allowing reliable recognition (over the set of actions learnt in this work). We call such sub-sequences . Discriminative high-dimensional D skeletal pose information is well captured, in 2D subspace, by two features: (a) absolute position of a point (pose in a frame), and (b) relative position of neighbouring points (poses in successive frames), i.e., length of the vector connecting the two points. While spatial co-ordinates alone are sufficient to classify very different actions like walking and punching, spacing greatly aid to distinguish similar actions differing by speed, like walking and jogging, especially in the multiple subspace framework. The training set of subspace feature vectors is given as  , with position and spacing co-ordinates in the  th partition of the single subspace (single subspace classification) and  th separate subspace (multiple subspace classification). Each subspace feature vector,  , in the training dataset of a given partition is represented as:       where   gives point co-ordinates, and   is the spacing between neighbouring points. Next, to obtain a concise representation of the training set of feature vectors, we cluster   using k-means obtaining feature-vector cluster centers  . Finally, each subspace training feature vector   is assigned the label corresponding to the nearest cluster center. The clustering of feature vectors is performed to reduce the computational complexity in our multi-layered search. The assignment of labels to each subspace feature vector is necessary for efficient retrieval in the final layers of our algorithm, as explained in detail in Sects. 4.2.2 and 4.2.1. In addition to obtaining cluster centers for actions, we also exploit a key-frame representation of actions, within our multi-layered search, and perform a quick and efficient search in the initial layer. In this regard, we compute a set,  , of key-frames in subspace, for each partition, as done in animation. Specifically, we define our key-frames as the starting and ending points of low-curvature regions in subspace, currently using an empirical threshold on the angles between consecutive vectors. Such regions are separated at instants in which an action changes significantly (Fig.&amp;nbsp;  ). Given our extracted key-frames, we generate a key-frame feature vector,  , as
Springer.tar//Springer//Springer\10.1007-s00138-013-0509-x.xml:A rule-based event detection system for real-life underwater domain:Fish detection Fish recognition Trajectory classification Behaviour understanding Event detection:  1 Introduction  The continuous progress in digital cameras and information storage capacities, with consequent cost reduction, led to an exponential proliferation of video-surveillance applications, specifically developed for investigating events and behaviours in human-centered applications [1–3]. This explosion also generated new possibilities for investigating diverse domains, such as fauna monitoring, through the use of embedded cameras which have low impact and low interference with the natural environment [4–6]. In fact, nonintrusive visual observation plays a crucial role for researchers in domains such as ecology that address complex questions about natural environments and about the behaviours and interactions of their living organisms. An interesting example is the EcoGrid project1 which collected many terabytes of videos with the aim of monitoring forest animals and fish living in Taiwan. And other examples can be found with cameras filming animals such as bird nests, wolves, or foxes. However, it is unrealistic to assume people can fully investigate all the generated videos, because it requires a lot of time and concentration while being error prone. Therefore, machine vision techniques are highly envisioned for automatically mining such data. The Fish4Knowledge project2 contributes to this direction. It develops video processing tools to support marine biologists that study fish populations of the coral reefs of Taiwan’s shores. Marine biologists are mainly interested in specific or unusual behaviours, such as migration, preying, schooling, or mating. Their analysis of the changes in behaviour patterns, and of the behaviour characteristics of different species, allows them to detect and study the environmental conditions, such as pollution and climate change [7, 8]. To address their needs, we propose here an automatic event detection system. It is able to recognize specific fish behavioural events by detecting fish activities and fish interactions in the videos collected for the Fish4Knowledge project. More specifically, the events of interest are identified by integrating user-defined rules with the analysis of fish behaviours modeled using hidden Markov models. Although in the literature there exist many approaches for event detection (as reviewed in the next section), they are mainly tailored for different usages in domains ranging from sport to airport monitoring for security purposes. To our knowledge, our proposed system is one of the first approaches addressing behavioural event detection for the underwater domain. A previous attempt was proposed by Spampinato et al. [9] but it aimed at detecting environmental events (e.g. typhoons and storms), and at investigating how fish behaviour changes when such events occur. In addition to the event detection system, we propose an intelligent user interface (UI) that supports and speeds up the ground-truth annotation process. The Fish4Knowledge video repository contains over 500,000 videoclips that are 10 min long. It is practically impossible to manually analyze this amount of videos for selecting the video excerpts needed for the training of the machine vision classifiers and for the performance evaluation. The UI supports the definition of rules that allow the retrieval of potentially relevant video excerpts. User can select and label these excerpts, and use them as the ground-truth for event detection. The user-defined rules target co-occurrence of fish, since biologists are mostly interested in detecting activities involving solitary fish and pairs of fish. The main contributions of our paper to the discussion on event detection in video-surveillance are the following:          In the remainder of the paper, Sect.&amp;nbsp; 2 reviews the mainstream approaches for event detection in human-centered applications. Section&amp;nbsp; 3 describes the flowchart of our event detection system, describing in detail the components employed for fish detection, tracking and recognition, trajectory modeling and classification. Section&amp;nbsp; 4 presents the user interface for video labeling, the species-specific events it targets, and their meaning for marine biologists. Sections&amp;nbsp; 5 and  6, respectively, discuss the performance of our system, and the concluding remarks and ideas for future developments. First, we evaluate the applicability of object detection, recognition and trajectory modeling approaches on noisy environments affected by low video quality (e.g., due to limit in network bandwidth), background sudden changes (e.g., sudden light changes due to the gleaming of the sun on the water), massive presence of background objects (e.g. plants, corals). Second, we propose a motion-based event detection system which exploits trajectories extracted in a complex real-life use case. Indeed, our videos are more cluttered and denser than the ones usually tackled in human video-surveillance systems. Moreover, fish movement is more complex than human’s, since fish are featured by non-rigid and erratic motion. Finally, we encounter partial or total overlapping of fish due to the unconstrained motion in three dimensions. Finally, we propose a UI for video labeling that supports users in the complex task of collecting event ground-truth. It uses the output of the low level processing modules (i.e., the fish detection, tracking an recognition) to facilitate the exploration of videos. The user-defined rules allow to retrieve fish co-occurrences of interest, in order to distill the number of videos to be inspected and labelled by users.    2 Related works  Many different event detection approaches have been proposed in literature. They can vary regarding the following points:      humans have been the typical object targeted by event detection algorithms. Depending on the kind of event that is targeted, the difficulty of the task can be relatively easy (e.g. changing speed, changing direction, chasing [10]) or hard (e.g., a specific action, such as using an object or waving at people [11]). Another kind of target is urban vehicles, e.g., for traffic monitoring, or for anomalous and illegal behaviour detection [12]. This kind of applications are usually easier than those involving humans, since vehicles move on constrained paths and their appearance does not change a lot. Another case study are animals. In this case, besides implying motion dynamics that differ from human ones, complications arise from the technical difficulties in filming wildlife and from the complexity of representing animal-related events [13].     the two main application fields that were investigated by the research community are video-surveillance (e.g. [10, 14]) and sports (e.g., [15, 16]). In video-surveillance, cameras are typically stationary, and the strongest requirements concern the risk of false alarms: no one would want a bank video-surveillance system to alert the police every time something passes by the field of view of the camera. In sport, the scene’s background is changing, both because of camera movements and because of scene cuts. Thus the estimation of the absolute position of players is difficult. Further, the scene switchings themselves may provide information on what happens in the match [15].     when the application domain and the kinds of event to detect are completely known, some approaches explicitly define an event in terms of combination of low-level motion properties or simple actions [10], and apply heuristic rules [17] or finite state machines [18]. Other approaches [19, 20] apply machine learning techniques (e.g., hidden Markov models [19] or dynamic Bayesian networks [21]) to learn typical event patterns, given the features used to describe the targets’ actions (e.g., people trajectories). The former approach allows to explicitly describe the types of event of interest, thus making the detector more accurate. However, this approach can detect  the events it was instructed for, and it requires that the targeted events are defined, and definable, as a sequence of easy-to-detect sub-events. The latter approach, on the other hand, automatically learns how to recognize events of any kind (e.g., by clustering trajectories, or learning motion patterns). It has the advantage to discover unseen data patterns, thus providing results for scientists to investigate. But since the events are inferred by the algorithm, the main disadvantage is that end-users must be particularly careful when selecting the algorithms and their parameters. This condition how the approach could fit a specific context.  In this section, we will describe a few examples of state-of-the-art event detection algorithms, varying among all of the above-mentioned targets, contexts and algorithm types. humans have been the typical object targeted by event detection algorithms. Depending on the kind of event that is targeted, the difficulty of the task can be relatively easy (e.g. changing speed, changing direction, chasing [10]) or hard (e.g., a specific action, such as using an object or waving at people [11]). Another kind of target is urban vehicles, e.g., for traffic monitoring, or for anomalous and illegal behaviour detection [12]. This kind of applications are usually easier than those involving humans, since vehicles move on constrained paths and their appearance does not change a lot. Another case study are animals. In this case, besides implying motion dynamics that differ from human ones, complications arise from the technical difficulties in filming wildlife and from the complexity of representing animal-related events [13]. the two main application fields that were investigated by the research community are video-surveillance (e.g. [10, 14]) and sports (e.g., [15, 16]). In video-surveillance, cameras are typically stationary, and the strongest requirements concern the risk of false alarms: no one would want a bank video-surveillance system to alert the police every time something passes by the field of view of the camera. In sport, the scene’s background is changing, both because of camera movements and because of scene cuts. Thus the estimation of the absolute position of players is difficult. Further, the scene switchings themselves may provide information on what happens in the match [15]. when the application domain and the kinds of event to detect are completely known, some approaches explicitly define an event in terms of combination of low-level motion properties or simple actions [10], and apply heuristic rules [17] or finite state machines [18]. Other approaches [19, 20] apply machine learning techniques (e.g., hidden Markov models [19] or dynamic Bayesian networks [21]) to learn typical event patterns, given the features used to describe the targets’ actions (e.g., people trajectories). The former approach allows to explicitly describe the types of event of interest, thus making the detector more accurate. However, this approach can detect  the events it was instructed for, and it requires that the targeted events are defined, and definable, as a sequence of easy-to-detect sub-events. The latter approach, on the other hand, automatically learns how to recognize events of any kind (e.g., by clustering trajectories, or learning motion patterns). It has the advantage to discover unseen data patterns, thus providing results for scientists to investigate. But since the events are inferred by the algorithm, the main disadvantage is that end-users must be particularly careful when selecting the algorithms and their parameters. This condition how the approach could fit a specific context. The typical target of event detection algorithms are people, since most practical applications involve the analysis of human actions, ranging from security to entertainment. However, depending on the kind of events into consideration, the way people are modeled can vary. In some applications, the simple identification of a moving person is sufficient for the event detection purpose. In this case it consists of an analysis of trajectories to find those which match, or do not match, a learnt pattern of anomalous behaviours. This strategy also applies to vehicle behaviour analysis, and is typical of a video-surveillance application. For example, in [22], a clustering method for trajectories is presented, which can be applied both to improve tracking performance (by predicting the position of an object at time  according to the best-matching cluster at time ) and to detect anomalous trajectories (by evaluating how frequently each cluster is matched, and by considering clusters with few elements as “anomalous”). In this approach, clusters actually represent relatively short segments of trajectories and are organized in a tree structure. Trajectories can be decomposed in segments belonging to different clusters. In Porikli et al. [20], histograms and hidden Markov models (HMMs) based on objects’ features (such as speed, color, size, aspect ratio) are used for trajectory description. This allows to integrate temporal information in the description of motion. In other applications, the events to detect consist of specific actions performed by a human. In this case, a trajectory-based representation of the human involved is not sufficient. The single limbs may have to be detected and tracked and, even harder, matched against a known moving pattern. In [11], the authors present a work on the detection of short events, such as picking objects or waving hands, in crowded environments. It typically involves difficulties due to interferences between humans. A spatio-temporal segmentation is performed on the video and is compared with the templates describing the events of interest. The template-matching engine is able to detect parts of event (sliced both spatially and temporally), for a more robust recognition. The main limit is that the event model is built from a single example, with no fusion mechanism implemented for defining a template from multiple samples. Event detection in a  involved different approaches to analyze human behaviour. In crowd analysis, from a computer vision point of view, the techniques used for individual targets are not appropriate, since the concepts of “motion detection” and “tracking” acquire a different meaning. Nevertheless, there are suitable techniques [23] that allow to tackle the event detection problem. For example in [24], the authors compute the crowd optical flow and use unsupervised feature extraction to encode normal crowd behaviour. Spectral clustering is applied to find the optimal number of hidden Markov models to represent the usual motion patterns. These HMMs are then used for analyzing new crowd scenes and for detecting abnormal events. In the context of natural and ecology studies, event detection is applied for the analysis of animal behaviour. As an example, in [13], a three-stage framework for event detection is described. It targets hunt detection in wildlife videos. The first layer of the framework extracts low-level description and motion information from the videos, while coping with camera motion. The second layer uses a neural network for the classification of the moving blobs, and it segments the input video clip into shots. The third layer applies user-defined event inference rules (e.g. a state machine) to verify whether a sequence of shots matches a target event. Another example is the work proposed in [25] which focuses on high-level events related to crowds of fish. It mainly addresses investigations on fish schooling characteristics. In particular, the proposed method exploits Lagrangian particle dynamics from fluid mechanics so as to consider the trajectories of fish as small particles in the fish flow. One of the most investigated context is video-surveillance, since it finds immediate use in security applications. The method proposed in [14] compares detected trajectories with a set of trajectories which are typical of intrusion patterns. It raises an alarm if a close match is found. A Gaussian mixture background model and a color-based blob tracker are used to detect and follow foreground objects. The tracker may generate more than one trajectory, because of occlusions or distinct movements of different body parts. Similar trajectories (in time and space) are merged into a single one. Then the comparison between an input trajectory and a database model is performed through a scale- and translation-invariant distance metric proposed by the authors. It also allows to quickly scan the database for possible matches. In [19], the authors describe an anomaly-detection system, with a use case for video-surveillance in a shop. Each trajectory is modeled by a HMM, and a distance matrix between all training trajectories is built. Multi-dimensional scaling (MDS) is applied to project trajectories onto a low-dimensional space. The projected vectors are then clustered using k-means. All cluster’s trajectories are used to train a HMM representing the whole trajectory pattern. This method allows to both detect anomalous trajectories within the training set, and to perform online evaluation of new trajectories by computing the matching likelihood with the cluster HMMs. Sports video clips have been one of the researchers’ favourite contexts for event detection. Typically, the purpose of the existing methods is to detect salient parts of the videos (e.g. a team scoring a goal) for summarization. In some cases, this requires just a method to infer when something interesting is happening (e.g., through super-imposed graphics, camera movement, crowd views). In other cases, it is necessary to recognize the specific kinds of events of interest. In [15], the authors propose a sport video summarization system based on the detection of interesting “plays”. Given a specific sport (case studies includes baseball, American football and Japanese sumo), a set of inference rules is defined to describe an interesting play in terms of sequence of scenes. For example, in baseball a play usually starts with a pitching scene, and if after a scene cut the camera is shooting the field, then the current play continues, otherwise the current play ends. Scene cuts are detected by comparing the colour histograms of two consecutive frames. Scene types are identified using features based on field colours and their spatial distribution, and the position of players and umpires. The rule inference matching scheme can be implemented by training a hidden Markov model with ground-truth play shots. In another work [16], the authors put particular focus on “field sports”. They analyze videos in the search of features which may be an indication of interesting events, such as crowd images, audio activity, on-screen graphics, or scoreboard changes. A Support Vector Machine is then trained using the features computed from 210 events from different sports. Given the different kinds of targets and contexts, it is extremely important to choose the best approach for describing events of interest and for matching such descriptions with the actual visual information contained in the video. A common way to handle this task is to describe an event as a set of simple actions which can be easily recognized (e.g., an object moving in a certain direction or approaching another object, a set of speed variations). Such sets of simple actions can be recursive if necessary. In [17], the authors use trajectory data and a priori information on the scene to define three abstraction levels in the event recognition process: i) image features (e.g., size, speed, position, distance from reference objects), ii) mobile object properties (e.g., entering a certain area, approaching reference objects or other actors), and iii) scenarios (e.g., combinations of mobile object properties). Similarly, in [10] different scenarios are modeled with “basic properties” (e.g., trajectory, speed), states (e.g., a situation involving a set of actors at a certain time, or for a certain period) and events (variations of states). Other works describe methods for event detection which aim at being as generic as possible to address a wider scope of application context. In such cases, little or no a priori information is provided. The typical approach consists of using trajectory data (which can be represented in several ways, e.g. point sequence, histograms, hidden Markov models) and a clustering algorithm. It aims at identifying common motion patterns which can be associated to predefined events, or which can define new kinds of behaviour. Porikli et al. [20] propose a method for the detection of unusual events based on spectral clustering. Histograms and HMMs use objects’ characteristics such as speed, color, size, or aspect ratio. They serve as features for trajectory description. For each feature, an affinity matrix is built where the th element shows how similar the th and  th objects are, according to that feature. The affinity matrix are then decomposed using a certain number of the largest eigenvalues. After further transformations, a correlation matrix is computed. Clustering consists of grouping the elements whose results are highly correlated. In [12], the authors apply a grammar rule induction framework to learn event rules. A clustering approach based on [26] is used to identify simple motion patterns. Hidden Markov models are trained to model each cluster, and are used as detectors of primitive events. A grammar induction algorithm is then applied to build the set of event rules. The induction algorithm evaluates grammar according to the Minimum Description Length principle [27]. Finally, in [28] the authors present a feature for event detection named Extended Relative Motion Histogram of Bag-of-visual-Words (ERMH-BoW). It aims at describing both the entities involved in an event and how they evolve. Instead of using raw motion distribution, which is noisy, the motion information of visual words is applied. Motion relativity histograms are adopted to handle problems caused by camera movements. Support Vector Machines using the ERMH-BoW descriptor are then trained to detect events in video clips. To summarize, to our knowledge the literature does not contain event detection approaches working on animals in their natural environment. In fact, the mainstream approaches operate on controlled labs [29, 30] with constant light conditions and high background-object contrast. Of course these conditions greatly simplify the task of mining the recorded videos. For human behaviour recognition, the most explored methods adopt visual concepts (as a direct representation of the scene) instead of using concepts that are more sensitive to viewpoint changes, such as trajectories and silhouettes [31]. This is not necessarily true in the cases of animals. Applications usually record animal and insect behaviour (both in controlled environments and in the natural environment) with rather fixed cameras whose viewpoints do not change often. Further, the structure of animal body (that varies more than human body) worsen the performance of visual concepts with respect to indirect scene representation (e.g., trajectories, body parts) [32].    3 The proposed system  The proposed system supports an event detection process organized in three main steps: (i) the detection, tracking and recognition of fish occurrences; (ii) the labeling of ground-truth video footage, on the basis of user-defined rules that retrieve potential event occurrences; (iii) the modeling of fish trajectories that allow our classification module to learn and detect the fish behaviours of interest. More specifically, the framework of our system is shown in Fig.&amp;nbsp; , and involves the following stages:        The fish detection is carried out with an approach based on background modeling.         &amp;nbsp;           The modules for fish tracking and species recognition identifies the individual fish for each species of interest. These low-level results are stored in a database.         &amp;nbsp;           User-defined rules are specified through our web user interface, on the basis of the previously stored descriptions of fish occurrences. They describe the behaviours of interest, i.e., specific co-occurrences of fish from specific species.         &amp;nbsp;           A rule-based selection of potential events is performed. It provides a set of video excerpts that are potentially valid for a ground-truth dataset. The results are submitted to user validation and labeling, which are also performed through our UI.         &amp;nbsp;           The user-labeled fish trajectories are used as a training set for the trajectory classification module.         &amp;nbsp;           Finally, the trajectory classification module, the learnt behaviour models, and the user-defined rules are used to perform the event detection.         &amp;nbsp;      To understand better our event detection process, we report on a use case targeting the detection of pairing behaviours for the    3 species. The user-defined rule specified that this behaviour occurrence implies that two fish from this species co-occur within a specific timespan. The ground-truth trajectories were labeled using our UI. The pairing behaviour model for that species was learnt by our HMM-based trajectory classification module. Then, when a new video is processed, if the user-defined rule holds and if fish trajectories are classified as “pairing”, then this specific event is detected and provided as output. The fish detection is carried out with an approach based on background modeling. The modules for fish tracking and species recognition identifies the individual fish for each species of interest. These low-level results are stored in a database. User-defined rules are specified through our web user interface, on the basis of the previously stored descriptions of fish occurrences. They describe the behaviours of interest, i.e., specific co-occurrences of fish from specific species. A rule-based selection of potential events is performed. It provides a set of video excerpts that are potentially valid for a ground-truth dataset. The results are submitted to user validation and labeling, which are also performed through our UI. The user-labeled fish trajectories are used as a training set for the trajectory classification module. Finally, the trajectory classification module, the learnt behaviour models, and the user-defined rules are used to perform the event detection. The video processing modules shown in the flowchart are described in detail in the following sections.
Springer.tar//Springer//Springer\10.1007-s00138-013-0511-3.xml:Image-based magnification calibration for electron microscope:Electron microscope Image matching Correspondence matching Scale estimation Phase-only correlation:  1 Introduction  Electron microscopes can be used to measure and observe cells, cell organelle, the fine structure of biological membranes, atoms, etc., and have a 1,000 times greater resolution than light microscopes. To achieve accurate measurement and observation of the object using electron microscope images, the correspondence between the scale of the electron microscope image and the actual size of the object must be obtained. This task is known as magnification calibration of the electron microscope. The high-accuracy magnification calibration method is required for accurate measurement and observation of the object. In general, magnification calibration is performed using standard calibration samples containing line, grating or spherical patterns with known spacing [1, 4, 7, 12, 13]. For example, microscopic glass spheres [4], polystyrene latex spheres [1], polyoma and a spherical animal virus [12], crystals of bovine liver catalase [7], monodisperse gold nanoparticles [13], carbon gratings and carbon graphites are used as the internal or external standard calibration samples. The internal standard calibration sample such as an atomic pattern of the target object is included in the target object and is used to simultaneously calibrate the magnification and measure the target object, while the external one such as a grating pattern is not included in the target object and is used to calibrate the magnification in advance. In the basic calibration method, a carbon grating is used in the low magnification setting (i.e.,  order), and an atomic lattice of the target object is used in the high magnification setting (i.e.,  order). When we focus on the magnification range between these two magnification settings, there are no applicable standard materials. Hence, it is difficult to perform reliable magnification calibration of the electron microscope over this range, and the current magnification calibration method using the standard calibration samples may include a maximum of 5&amp;nbsp;% scale error [11]. In addition, an alternative method has not yet been proposed. Addressing this problem, we propose a novel framework of magnification calibration using a multi-stage scale estimation approach. We consider a sequence of microscope images of the same target object, where the image magnification is gradually increased so that the final image has a very large scale factor  (e.g., ) with respect to the initial image. The goal of this framework is to estimate the overall scale factor  between the image captured with the correct magnification setting and the image captured with the uncalibrated magnification setting. Since the overall scale factor  is obtained by the product of a sequence of estimated relative scale factors, a high-accuracy scale estimation algorithm for estimating the relative scale factors between adjacent images is indispensable. Moreover, in high-magnification range over  100,000, the geometric transformation between sequential images is represented by not only scaling but also translation, rotation and sometimes perspective distortion due to slight movement of the object during image acquisition, which is known as drift distortion. Hence, a scale estimation algorithm has to be robust against geometric transformation. To achieve high-accuracy magnification calibration of the electron microscope, we propose a high-accuracy scale estimation algorithm using correspondence matching based on phase-only correlation (POC). Phase-based correspondence matching [10] is an efficient method of sub-pixel correspondence matching, which employs (1) a coarse-to-fine strategy using image pyramids for robust correspondence search and (2) a POC-based sub-pixel image matching technique for finding a pair of corresponding points with sub-pixel displacement accuracy. In addition, to evaluate the accuracy of scale estimation algorithms, we propose a quantitative performance evaluation method of scale estimation algorithms using Mandelbrot images. The Mandelbrot set [3] is a mathematical set of points having distinctive boundaries and two-dimensional (2D) fractal-like shape. Considering the Mandelbrot set as 2D signals defined in the continuous space, we can generate images transformed with arbitrary parameters without interpolating pixels, since the Mandelbrot set has infinite resolution. The use of Mandelbrot images makes it possible to generate the image sequence with a very large scale factor  (e.g., ). Experimental evaluation using Mandelbrot images as precisely scale-controlled images shows that the proposed scale estimation algorithm can estimate the scale factor with approximately 0.1&amp;nbsp;% scale accuracy from the image sequences with the overall scale factor  and  of the initial scale errors. This paper also describes an application of the proposed algorithm to the magnification calibration of an actual scanning transmission electron microscope (STEM). The main contributions of this work are summarized as follows:        A novel framework of magnification calibration using a multi-stage scale estimation approach for electron microscopes is proposed. The proposed framework allows us to perform magnification calibration of electron microscopes within the magnification range for which no applicable standard materials exist.         &amp;nbsp;           A scale estimation algorithm using the phase-based correspondence matching technique is proposed. The proposed algorithm can estimate the overall scale factor  with approximately 0.1&amp;nbsp;% scale error.         &amp;nbsp;           A quantitative performance evaluation method using Mandelbrot images for scale estimation algorithms is proposed. Since the proposed method generates the image sequences whose scale factors are precisely controlled, we can correctly evaluate the accuracy of scale estimation algorithms even for the overall scale factor .         &amp;nbsp;      The rest of the paper is organized as follows: Section  2 describes the proposed image-based magnification calibration framework for electron microscopes. Section  3 describes the fundamentals of POC, the phase-based correspondence matching technique and the proposed scale estimation algorithm. Section  4 describes the use of Mandelbrot image for quantitative evaluation of scale estimation algorithms. Section  5 demonstrates a set of experiments for evaluating estimation accuracy of the proposed algorithm using Mandelbrot images and the images taken by an actual STEM. Section  6 ends with some concluding remarks. A novel framework of magnification calibration using a multi-stage scale estimation approach for electron microscopes is proposed. The proposed framework allows us to perform magnification calibration of electron microscopes within the magnification range for which no applicable standard materials exist. A scale estimation algorithm using the phase-based correspondence matching technique is proposed. The proposed algorithm can estimate the overall scale factor  with approximately 0.1&amp;nbsp;% scale error. A quantitative performance evaluation method using Mandelbrot images for scale estimation algorithms is proposed. Since the proposed method generates the image sequences whose scale factors are precisely controlled, we can correctly evaluate the accuracy of scale estimation algorithms even for the overall scale factor .    2 Image-based magnification calibration  In this section, we describe a novel framework of magnification calibration using a multi-stage scale estimation approach for electron microscopes. Figure   shows the flow of our proposed framework. We assume that the electron microscope has already been calibrated for a certain magnification setting \(m_{0}^{\mathrm{true}}\,(&amp;gt;\!\!1)\) using standard calibration samples, and hence the magnification parameter  has been adjusted correctly. The problem considered here is to calibrate the microscope for a higher magnification setting, say \(m\ (&amp;gt;\!\!m_0^{\mathrm{true}})\), for which no standard calibration materials exist. Our framework is to estimate the scale change  between the microscope image  captured with the correct magnification setting  and the target image  captured with the uncalibrated magnification setting . Let \(S\ (&amp;gt;\!\!1)\) be the scale factor of the microscope image  with respect to the image . We can estimate the true magnification for the setting  as . Thus, the goal of our framework is to perform calibration for magnification  on the basis of image analysis without using standard calibration materials. A major problem of this framework is the difficulty of calibration for higher magnification setting \(m \ (&amp;gt;&amp;gt;\!\! m_{0}^{\mathrm{true}})\). For example, when , we cannot estimate the image scale factor  directly in practice. Addressing this problem, we estimate the overall scale factor  through a multi-step approach as , where we employ a set of multiple images between  and  with reasonably smaller relative scale factors . Hence, high-accuracy scale estimation for the relative scale factor  between  and  is indispensable to reduce the multiplicative error accumulation including in the overall scale factor . In the following section, we describe a high-accuracy scale estimation algorithm using phase-based correspondence matching.    3 Scale estimation algorithm  A number of image matching algorithms to estimate transformation parameters have been proposed. One of the famous image matching algorithms is to use scale invariant feature transform (SIFT)-based matching [6]. In the case of computer vision applications, SIFT-like feature-based matching algorithms exhibit sufficient accuracy to estimate transformation parameters [8]. However, for the purpose of scale estimation in microscope images, SIFT-like feature-based matching algorithms are not always suitable, since only one-pixel error of keypoint localization may lead to a significant error in scale estimation. Hence, for the cases considered in this paper, area-based image matching algorithms are more suitable than the feature-based image matching algorithms. The area-based image matching algorithm using POC is one of the most accurate algorithms to estimate transformation parameters such as translational displacement, rotation angle and scale factor [5, 9]. POC is an image matching technique using the phase components of 2D Discrete Fourier Transforms (DFTs) of given images. To combine the POC technique and the Fourier–Mellin transform, we can estimate a scale factor between images [2, 9]. Since this approach estimates a scale factor from the amplitude components of 2D DFTs of images, the low-quality images due to blur or noise, e.g., microscope images in high-magnification settings, result in a reduction of the accuracy of scale estimation. To achieve accurate scale estimation of electron microscope images, the proposed scale estimation algorithm obtains dense correspondence between images using a correspondence matching technique, and then estimates the scale factor from the correspondence. The key idea of the proposed algorithm is to use phase-based correspondence matching [10], which employs (1) a coarse-to-fine strategy using image pyramids for robust correspondence search and (2) a POC-based sub-pixel image matching technique for finding a pair of corresponding points with sub-pixel displacement accuracy. In the following, we describe the details of the proposed scale estimation algorithm. First, we briefly introduce fundamentals of POC, which allows us to estimate a sub-pixel displacement and evaluate a similarity between two images. Next, we describe the correspondence matching technique using POC. Then, we describe the proposed scale estimation algorithm using phase-based correspondence matching. POC-based image matching employs a POC function between two images which is defined by a correlation function calculated only from phase components in 2D DFTs of the given images. Consider two   images,   and  , where we assume that the index ranges are  \(n_{1}=-M_{1},\ldots , M_{1}\, (M_{1}&amp;gt;0)\) and  \(n_{2}=-M_{2},\ldots , M_{2}\, (M_{2}&amp;gt;0)\), and hence   and  . Note that we assume here the sign symmetric index ranges   and   for mathematical simplicity. The discussion could be easily generalized to non-negative index ranges with power-of-two image size. Let   and   denote the 2D DFTs of the two images.   and   are given by                         where  , and  .   and   are amplitude components, and   and   are phase components. The normalized cross-power spectrum   between   and   is given by            where   denotes the complex conjugate of   and   denotes the phase difference  . The POC function   is the 2D Inverse DFT of   and is given by            When the two images are similar, their POC function gives a distinct sharp peak. When  , the POC function   becomes the Kronecker delta function. When the two images are not similar, the peak drops significantly. The height of the peak can be used as a good similarity measure for image matching, and the location of the peak shows the translational displacement between the two images. Other important properties of POC are that the POC-based image matching is not influenced by image shift or brightness change, and it is highly robust against noise. In our previous work [9], we have proposed techniques to improve the accuracy of POC-based image matching: (1) the function fitting technique for high-accuracy estimation of peak position, (2) the windowing technique to reduce boundary effects and (3) the spectral weighting technique to reduce aliasing and noise effects. Technique (1) is to find the location of the peak that may exist between pixels by fitting the closed-form peak model of the POC function. Technique (2) is to reduce the effects of discontinuity at the image border in the 2D DFT computation by applying a 2D window function to the image. Technique (3) is to eliminate the high frequency components having low reliability by applying a low-pass-type weighting function to  in the frequency domain. The use of POC-based image matching with the above three techniques makes it possible to achieve accurate and robust estimation of translational displacement between images. To perform accurate scale estimation, we employ the sub-pixel correspondence matching technique using POC, which employs (1) a coarse-to-fine strategy using image pyramids for robust correspondence search and (2) a sub-pixel translational displacement estimation method using POC for finding a pair of corresponding points with sub-pixel displacement accuracy. The flow of the phase-based correspondence matching technique is illustrated in Fig.  . Consider two images: the reference image   and the input image  . Let   be a coordinate vector of a reference pixel in  . The problem of sub-pixel correspondence search is to find a real-number coordinate vector   in   that corresponds to the reference pixel   in  . We briefly explain the procedure as follows. Step 1 For  , create the  th layer images   and  , i.e., coarser versions of   and  , recursively as follows:        In this paper, we employ  . Step 2 For every layer  , calculate the coordinate   corresponding to the original reference point   recursively as follows:            where   denotes the operation to round the element of   to the nearest integer towards minus infinity. Step 3 We assume that  in the coarsest layer. Let . Step 4 From the th layer images  and , extract two sub-images (or local image blocks)  and  with their centers on  and , respectively. The size of image blocks is  pixels. Note that  is determined depending on the image size. Consider the images with  pixels and . The image size in the th layer is  pixels.  should be smaller than 120 pixels so as not to extract the sub-images from outside of the image, and also be power of two for fast DFT computation. The candidates for  are 32 and 64 pixels to estimate accurate displacement between the two sub-images using POC-based image matching. From the viewpoint of computation time, we employ  in this paper. If the image size is over  pixels, it is better to select . Step 5 Estimate the displacement between   and   with pixel accuracy using POC-based image matching. Note that, to reduce the computational cost, POC-based image matching with pixel-accuracy is used to estimate the displacement in this step, since   and   are the coarser versions of the original images   and  . Let the estimated displacement vector be  . The  th layer correspondence   is determined by              Step 6 Decrement the counter by 1 as   and repeat the procedure from Step 4 to Step 6 while  . Step 7 From the original images   and  , extract two image blocks with their centers on   and  , respectively. Estimate the displacement between the two blocks with sub-pixel accuracy using POC-based image matching. Let the estimated displacement vector with sub-pixel accuracy be denoted by  . Update the corresponding point as            In general, it is important to set the reference point   on the area having rich texture to obtain its accurate corresponding point   from the input image  . Since the peak value of the POC function between the local block images is used as a measure of correspondence reliability [ 9], a lot of reference points   are placed on the reference image   and their corresponding points   on the input image   are found using the phase-based correspondence matching technique described in the above. When the peak value of the POC function between the local image blocks is below a threshold, the corresponding point pair is regarded as an outlier. From our preliminary investigation, the threshold is selected within 0.3–0.5 to obtain accurate corresponding point pairs. Hence, we employ the threshold as 0.4 in this paper. Also, we empirically determine that the reference points are set on the reference image with a spacing of 10 pixels. We present a scale estimation algorithm using phase-based correspondence matching for electron microscope images. As mentioned in Sect.  2, the problem considered here is to estimate the precise relative scale factor   between the two neighboring images   and   of the image sequence as shown in Fig.  . Let   and   be the high-magnification image with the magnification   and the low-magnification image with the magnification  , respectively. Note that the magnification   and   are read from the electron microscope. The relative scale factor   can be written as       where   indicates the initial scale factor   defined by   and   indicates the error between the true and initial relative scale factors, i.e., the error included in the magnification read from the electron microscope which may be a maximum of 5&amp;nbsp;% [ 11]. Hence, we need to estimate   with high accuracy to obtain the precise relative scale factor  . In the proposed algorithm, we consider that   is a displaced, rotated and scaled version of the image  , since the electron microscope image may be deformed in the image acquisition process. In high-magnification range over  , the geometric transformation between the images is represented by not only scaling but also translation, rotation and sometimes perspective distortion due to slight movement of the object during image acquisition, which is known as drift distortion. Hence, the proposed algorithm estimates the relative scale factor   in the light of the translational displacement   and rotation angle   between images. Also, we employ the iterative procedure to estimate the relative scale factor   between   and  . If the initial relative scale factor   includes a large error, one-time correspondence matching may not be enough to obtain the accurate relative scale factor  . In the proposed algorithm, the parameters   and   are iteratively estimated. The following is the detailed procedure of the proposed algorithm (Fig.  ). Input:     High-magnification image  with the magnification  read from the electron microscope   Low-magnification image  with the magnification  read from the electron microscope   Initial value  of the relative scale factor  between  and      Output:     Relative scale factor  between  and .    Step 1 Set the initial transformation parameters, i.e., the translational displacement  , the rotation angle   and the scale factor  , as follows:          Step 2 Transform   by   and  , and extract the common region   associated with  . High-magnification image  with the magnification  read from the electron microscope Low-magnification image  with the magnification  read from the electron microscope Initial value  of the relative scale factor  between  and Relative scale factor  between  and . Step 3 Obtain the correspondence between  and  using the phase-based correspondence matching technique described in Sect. 3.2. In this paper, we set the reference points on  with a spacing of 10 pixels. When the peak value of POC function between local block images is below a certain threshold, the point pair is regarded as an outlier, and is removed. In this paper, the threshold value is 0.4. Let the reference points be  for  and the corresponding points be  for , where  is the number of corresponding point pairs. Step 4 Estimate the scale error   using the correspondence between   and  . In this paper, we employ the similarity transformation model to estimate  . The model is defined as follows:            where   is a rotation angle and   is a translational displacement. Substituting   and   into Eq. ( 9), we can estimate the transformation parameters by solving a set of linear simultaneous equations as the linear least-squares problem (For more details, refer to Appendix A in Ref. [ 8]). Step 5 Update the transformation parameters as follows:        where “ ” indicates regular multiplication. Step 6 Repeat the procedure from Step 2 to Step 5 until  is not updated. In this paper, we repeat the procedure 3 times. Step 7 Obtain the relative scale factor . For every neighboring image pair, the precise relative scale factor  is estimated using the above scale estimation algorithm, and then the overall scale factor of the image sequence is obtained by . Finally, we can calibrate the magnification of the image  by . 
Springer.tar//Springer//Springer\10.1007-s00138-013-0512-2.xml:Hyperspectral imaging based on diffused laser light for prediction of astaxanthin coating concentration:Diffused laser Elastic net Hyperspectral imaging Multispectral imaging Spectral imaging Image analysis Pellet coating Partial least squares Astaxanthin:  1 Introduction  Astaxanthin is a naturally occurring carotenoid with a high antioxidant activity essential for growth and survival and it is important for the development of colour in salmonid fishes [35]. The primary use of astaxanthin within aquaculture is as a feed additive to ensure that farmed salmon and trout have similar appearance to their wild counterparts [38]. The colour appearance of fish products is important for customers as a quality indicator [16, 29, 34]. Astaxanthin is very expensive [2] and therefore optimisation of its use in fish feed production is of importance. Synthetic astaxanthin is more easily available and costs slightly less than natural astaxanthin and is therefore used more often in industry. However, there is a demand for natural astaxanthin for the organic salmonid fish market, where natural astaxanthin is mandatory. Today, chemical measurement of astaxanthin is done by high-pressure liquid chromatography (HPLC). HPLC is a well-established technique for measuring synthetic astaxanthin content in fish oil. However, when measuring astaxanthin concentration in fish feed pellets, an additional step for extraction of the oil is necessary, and this is why measurements of astaxanthin from fish feed pellets are less accurate and more labour intensive. An automatic vision system for at-line pigment quality control of astaxanthin coating concentration level would be of great benefit to the industry, in relation to both process control and process optimisation. The aim of this study is to investigate the possibility of predicting the concentration level of synthetic astaxanthin coating on aquaculture feed pellets by spectral imaging. Since HPLC is used for measuring astaxanthin coating content in the industry, we used this as a reference method. Spectral imaging is called multispectral when using a small number of spectral bands (e.g. less than about 50), and hyperspectral when using a large number of spectral bands (e.g. hundreds). Previous studies on feed pellet monitoring by spectral analysis include near infra-red (NIR) reflectance spectroscopy for classification of feed material and feed pellets by Fernández-Ahumada et al. [12], and predicting chemical information in pharmaceutical pellet core and coating using NIR imaging by Sabin et al. [31]. Previous work on multi- and hyperspectral image analysis of astaxanthin coating by Ljungqvist et al. [22, 23] has shown promising results for screening of the concentration level. However, those studies did not use hyperspectral imaging in the visual part of the spectrum. In [23] only the NIR range was analysed, and in [22] only 20 wavelengths were analysed in the visual and parts of the NIR range. Neither of them used chemical measurements for validation of the astaxanthin coating level. For the previous work using the visual part of the spectrum [22], the spectral bands were located at predefined wavelengths due to instrument setup, chosen without the knowledge of the particular problem. The few spectral bands used may not be the ones that give the greatest ability to quantify the contents of astaxanthin. Thus, a more detailed study is called for, and here we report the characteristics for a new imaging method based on diffused laser light with more spectral bands. The study is focused on the visual region of the electromagnetic spectrum due to function of astaxanthin as a pigment. Vision systems have previously been implemented for quantisation of chemical contents, and a number of light-source options exist. In this paper, we present the use of a super-continuum light source for full-field illumination. The super-continuum laser, combined with an acousto-optical tunable filter (AOTF) provides a broadband tunable light source. This form of light source is often used for confocal microscopy [32], fluorescence lifetime imaging [24], and measurement of subsurface laser scattering (SLS) [26, 27]; also known as diffuse reflectance. To apply the light-source for full-field illumination, the small beam from the AOTF box is simply expanded and made diffuse. A parallel study with a commercially available multispectral system called VideometerLab was also performed. The performance of this device has been validated for similar surface chemistry applications [6–9, 11, 15, 17, 21]. Since the hyperspectral imaging device records more spectral bands than a multispectral device, it would give more detailed information for measuring the astaxanthin coating concentration. The presented work thus investigates both the possibility of examining astaxanthin contents by hyperspectral image analysis, and a comparison of the two modalities (multi- and hyperspectral imaging) for astaxanthin prediction.    2 Materials and methods  The pellets used in this study were produced for the purpose of this experiment, and the recipe was based on normal commercial fish feed for salmonid fish. The pellets had the approximate production diameter of 4.5&amp;nbsp;mm, and were coated with fish oil. An extruder machine was used for the pellet production. The feed material was extruded through a die plate with holes of a specific diameter which determined the diameter of the pellets. On the other side of the disk, there was a set of rotating knives that cut the material into shorter, cylinder-shaped pellets. The synthetic astaxanthin used was cold water dispersible (BASF SE, Germany), and in total seven different levels of synthetic astaxanthin concentration were added to the fish oil coating. The highest synthetic astaxanthin level in fish oil was 100 parts per million (ppm), and then the oil was diluted so that the concentration level became half of the original. This was repeated to achieve the seven nominal levels of synthetic astaxanthin concentration in the fish oil coating, where the last level was 0&amp;nbsp;ppm, see Table  1. Fish oil in itself typically contains a small amount of natural astaxanthin, however, this is assumed to be less than 1&amp;nbsp;ppm and is here referred to as a coating of 0&amp;nbsp;ppm concentration. Astaxanthin is commonly measured in ppm, and it is measured in mass so here ppm corresponds to mg/kg. Between production and image acquisition, the pellets were stored at C in a dark environment for months. They were stored in plastic bags where the oxygen had been pumped out and the bags were filled with nitrogen to minimise the oxidation process and quality reduction during storage. The spectral reflection of the pellets is a mix of the pellet compound (recipe) and the reflection of astaxanthin. The light captured by each pixel is thus assumed to be a linear combination of two main components; the pellet compound and the astaxanthin coating. Due to the production method, the coating is neither evenly distributed among the pellets, nor uniformly on each pellet. In the extrusion process, parts of the astaxanthin coating will go inside the pellet, while we only measure reflection from the surface. However, the industry is interested in the total amount of astaxanthin in the pellets. Therefore, the surface reflection is assumed to be linearly related to the total amount of astaxanthin in the pellets. It is assumed that most of the quantity of synthetic astaxanthin on each pellet can be estimated by measuring the coating surface of each pellet. For practical use, however, it is not interesting to estimate the quantity on each single pellet, but rather on a larger amount of pellets, and calculate an ensemble average. The hyperspectral imaging system consists of four parts: light source, spectral filter, diffuse filter and camera. The illumination system is based on a SuperK Extreme (NKT Photonics A/S, Denmark) super-continuum white light laser producing a quasi-continuous output. The super-continuum light is filtered using a SuperK Select (NKT Photonics A/S, Denmark), where an AOTF is used for spectral filtering of the beam. The combined light source and filtering box provides a wavelength tunable laser beam delivered in a photonic crystal fibre (PCF) by a Fiber Delivery System (NKT Photonics A/S, Denmark). The combined system provides 0.5–6.5&amp;nbsp;mW. In combination, this system provides light in the visual and NIR region, ranging from 455 to 1,015&amp;nbsp;nm. A step size of 5&amp;nbsp;nm was used as the spectral resolution, resulting in 113 spectral bands. The bandwidth grows linearly as a function of wavelength; at 500&amp;nbsp;nm it is 3.5&amp;nbsp;nm, at 900&amp;nbsp;nm it is 14&amp;nbsp;nm. The spectrally filtered light from the Fiber Delivery System forms a Gaussian fundamental transverse electromagnetic (TEM) beam. This is transformed to illuminate a square area below the camera using an engineered diffuser (ED1-C20-MD, Thorlabs, Sweden). This diffusing method has a high power transmission onto the sample. The illumination does not form a perfectly uniform intensity distribution; it produces a gradient due to the projection caused by the oblique incidence of the beam onto the camera field of view. It also produces a short distance intensity fluctuation. To minimise the latter effect, the Gaussian beam delivered after the AOTF box is expanded using a &amp;nbsp;mm focal length negative lens (LC1439, Thorlabs, Sweden). By illuminating the engineered diffuser, the short distance intensity fluctuations are minimised and become insignificant for this application. Image capturing is done using a Grasshopper GRAS-20S4M grey-scale charge-coupled device (CCD) camera (Point Grey Research Inc, Canada), which uses a 12-bit analogue to digital converter (ADC) with a 16-bit output. The image resolution is 1,600   1,200 pixels with a pixel size of approximately 0.028   0.028 millimetres. For an overview of the SuperK setup see Fig.  . To compare the result of the SuperK setup, the commercially available multispectral VideometerLab (Videometer A/S, Hørsholm, Denmark) was also used for image acquisition. It uses 20 wavelengths distributed over the ultra-violet A (UVA), visual and NIR region: 385, 430, 450, 470, 505, 565, 590, 630, 645, 660, 700, 850, 870, 890, 910, 920, 940, 950, 970, 1,050&amp;nbsp;nm. This system uses a Point Grey Scorpion SCOR-20SOM grey-scale CCD camera. The objects of interest are placed inside an integrating sphere (Ulbricht sphere) with uniform diffuse lighting from light sources placed around the rim of the sphere [5]. All light sources are light-emitting diodes (LED) except for 1,050&amp;nbsp;nm, which is a diffused laser diode. The curvature of the sphere and its matt-white coating ensure a uniform diffuse light so that specular effects are avoided and the amount of shadow is minimised. The device is calibrated radiometrically with a following light and exposure calibration according to the National Institute of Standards and Technology (NIST). The system is also geometrically calibrated to ensure pixel correspondence for all spectral bands [ 13]. For an overview of the VideometerLab setup see Fig.  . VideometerLab has the advantage that the intensity is calibrated with respect to the sensitivity of the CCD and the intensity of the light sources, which means that the resulting reflection spectrum can be compared with, e.g. a spectroscopy spectrum. The image resolution is  pixels. In this situation, one pixel represents approximately  millimetres. The Scorpion camera has a 12-bit ADC, and the system uses 8-bit data output from the camera. After calibration correction, the reflectance intensity output is at 32-bit precision. To explore the spectral properties of astaxanthin further, and to assist in the interpretation of the spectral image results, a spectrometer was used in the visual and NIR range. Absorption spectra of synthetic astaxanthin in a solution of fish oil along with plain fish oil were recorded using a NIRSystems 6500 absorption spectrometer (Foss NIRSystems Inc, USA) with a spectral resolution of 2&amp;nbsp;nm. The absorption spectra were transformed to reflection values using the standard relation , where  is the absorption value and  is the reflection value. To calibrate the hyperspectral imaging prediction method, an HPLC analysis of the synthetic astaxanthin concentration in pellets was performed. By analysing samples from each concentration level, we could estimate the average astaxanthin content in the pellets, which can be used to validate the nominal levels used in the production. To reduce the effect of the analysis method, we used two independent HPLC measurements from different parties. The HPLC analysis was done at the National Food Institute, Division of Industrial Food Research at the Technical University of Denmark (Lyngby, Denmark) using an Agilent 1100 series HPLC (Agilent Technologies, Palo Alto, CA, USA), equipped with a UV diode array detector. The oil was extracted from the pellets using acetone and homogenised to a concentrate, which was analysed for synthetic astaxanthin content, according to the modified protocol of Bligh and Dyer [3]. A fraction of the lipid extract was evaporated under nitrogen and redissolved in 2 mL of -heptane before injection. Astaxanthin content was determined after injection of an aliquot () of the -heptane fraction into a LiChrosorb Si60-5 column (100&amp;nbsp;mm  3&amp;nbsp;mm, 5 ) equipped with a Cromsep Silica (S2) guard column (10&amp;nbsp;mm  2&amp;nbsp;mm; Chrompack, Middelburg, The Netherlands) and eluted with a flow of 1.2&amp;nbsp;mL/min using -heptane/acetone (86:14, v/v) and detection at 470&amp;nbsp;nm. Concentrations of astaxanthin were calculated using authentic standards from Dr. Ehrenstorfer GmbH (Augsburg, Germany). Furthermore, HPLC analysis was also made at Eurofins A/S (Galten, Denmark), which is a commercial laboratory. Images of petri dishes (plastic, diameter of 9 cm) filled with pellets were captured using both the SuperK setup and the VideometerLab. For each concentration level, 10 images of different pellets were captured using both SuperK and VideometerLab, then an additional 20 images were captured with VideometerLab, see Table 1. In total, 70 SuperK pellet images and 210 VideometerLab pellet images were captured over two consecutive days. The concentration level sequence was randomised, and samples were interleaved two at a time. The pellets were at normal room temperature during image acquisition. A filled petri dish resembles the rapid inspection that the industry would desire for this application. The pellet cluster inside the petri dish in each image was segmented from the light-grey background using a grey-scale threshold, using the band of 500&amp;nbsp;nm for the SuperK images and the band of 470&amp;nbsp;nm for the VideometerLab images. The threshold segmentation was complemented with the morphological methods erosion and dilation using a disk as structuring element [33]. Furthermore, the topmost layer of pellets was segmented to remove parts with less light, and in this way also avoid some of the granulometry information in the image. Since the SuperK images contained some specular reflections due to the direct lighting, the strongest specular effects were also removed by a threshold for both the VideometerLab and SuperK images. The mean spectrum of the pixels in each segmented image was used as samples. In this way, the impact of the pattern from the diffuse filter used by the SuperK setup was reduced and was assumed not to impact the results of the image analysis.
Springer.tar//Springer//Springer\10.1007-s00138-013-0513-1.xml:Gaussian-weighted Jensen–Shannon divergence as a robust fitness function for multi-model fitting:Residual consensus Model fitting Multiple structure estimation Robust regression:  1 Introduction and related work  Model fitting techniques are widely used in computer vision applications, in which the data to be modeled usually contains a significant number of outliers. RANSAC [1] is commonly applied to estimate a model with noisy data and numerous extensions and modifications have been derived from it (see [2, 3] for a good overview of different extensions of RANSAC). These RANSAC-based algorithms evaluate every hypothesis using a cost function  Then the best hypothesis is selected as The truncation cost functions used by RANSAC and its extensions facilitate toleration of more than 50&amp;nbsp;% outlier rate. However, this attractive capability also brings the requirement of a application-dependent parameter, i.e. the inlier noise scale (also referred to as bandwidth or truncation threshold). Since this prior information usually is not available for many practical computer vision applications, the development of nonparametric cost functions for robust estimators has received widespread attention in recent literature [4–9]. They analyze the distribution of residuals of individual data points w.r.t. the hypotheses, either with a histogram or with kernel methods. A problem for histogram analysis-based approaches is that the small pseudo-models and severe outliers can easily generate many false peaks and valleys which obscure the genuine modes. The kernel function-based approaches estimate the residual histogram density using a smoothness parameter, and they therefore can tackle small pseudo-structures, but instead they have the well-known density problem, i.e. over- or under-estimation of the proportion of inliers. Both of these two categories of hypothesis evaluation function still require parameters (kurtosis or skewness threshold for histogram approaches and bandwidth of the kernel approaches), although these parameters are less sensitive than the inliner noise scale used in RANSAC. RANSAC-based approaches apply a simple and straightforward optimization for searching the best hypothesis. This optimization can be represented as  with , where  are the model hypotheses. This straightforward optimization mechanism facilitates preemptive capability [10] while confining the data to a single structure only. Several extensions of RANSAC are capable of tackling the problem of multi-model estimation and remaining preemptive [11, 12], but they introduce more sophisticated user-dependent parameters and therefore are less robust in real applications. Recently, the holistic multi-model estimators combine the hypotheses evaluation and optimization search strategy into a complete framework to reveal the number and the parameters of the models simultaneously. All these approaches discover the inherent model number using residual distribution analysis [4, 5, 13–15]. However, these holistic multi-model estimators lose the preemptive capability and are too slow to be employed in real-time (at least soft real-time) tasks, such as scene understanding for mobile robot or homography detection for autonomous vehicle navigation. In this paper, we present a novel framework for multi-model estimation, which facilitates both  and  characteristics, and can deal with  estimation. We make the assumption that data are constrained to a specific known input space, such as a 3D volume of space several meters across for estimating planar structures in indoor scenes. With this assumption, we distribute data points into the input space uniformly to form an ideal uniform dataset. By comparing the residual distribution of data from the ideal uniform dataset and from the real dataset using Gaussian-weighted Jensen–Shannon divergence (GJSD), an effective and nonparametric cost function for hypothesis evaluation is generated. An optimization framework based on evaluating every hypotheses with GJSD is then introduced, and uses the ring-topological particle swarm optimization [16] to find multiple structures simultaneously. The algorithm presented in this paper obtains faster convergence while being robust to the presence of large amounts of noise/outliers. Furthermore, it can handle data imbalance more effectively than conventional approaches. In summary, the algorithm presented provides a simple, generalizable framework for multi-model estimation that can be used in many practical tasks which need to estimate models in a limited time. The most related work to our approach is the paper [17], which also applies PSO for plane fitting. However, it does not clearly consider the model fitting problem as two separate subtasks. It uses the same cost function as RANSAC which needs the prior estimation of inlier scale. The use of GJSD-based evaluation as alternative in this paper makes the prior inlier scale unnecessary. Also a “deflation” function is used in [17] to make the found optimum disappear in the subsequent PSO iterations; this requires more iterations and it is also difficult to control the invisible areas without any overlaps. Instead we adopt the simple but powerful  PSO method to select hypotheses and reveal the number of models simultaneously.    2 Gaussian-weighted Jensen–Shannon divergence  This section describes the details of the proposed evaluation function using Gaussian-weighted Jensen–Shannon divergence [18]. Jensen–Shannon divergence, which is based on Kullback–Leibler divergence (KLD) [19], is popularly used in probability theory and statistics for measuring the similarity between two distributions. The superiority of JSD over KLD on handling zero values in distributions and removal of nuisance in the use of KLD arising from its asymmetry has been demonstrated in [20]. We use JSD to measure the divergence between the residual distributions of data from the ideal uniform distribution dataset and from real dataset, which clearly differ for hypotheses near to and far from an actual optimum (see last column of Fig.&amp;nbsp; ). When the hypothesis is geometrically close to an optimum, the residual distribution of this hypothesis for the real dataset has a peak at zero (first row in Fig.&amp;nbsp; ). Conversely, the residual distribution of a hypothesis that is not close to the optimum, is similar to the residual distribution that is generated using ideal uniform dataset (second row in Fig.&amp;nbsp; ). Using a Gaussian-weighted variant, we could emphasize the divergence at the specific place (e.g. the peak position) and also depreciate the influence of the parallel models in the residual distribution (last figure of row 1 in Fig.&amp;nbsp; ). Given input data  and model hypotheses  which are generated by randomly sampling from , we calculate the distance  of all the points to each hypothesis  in , where  is the distance of point  to hypothesis . Then the residual histogram of each hypothesis denoted as  can be computed using . The right-column images in Fig.&amp;nbsp; illustrate the obtained residual histogram with respect to the correct and arbitrary hypotheses (marked in red in left-column images of Fig.&amp;nbsp;). Note that the evaluation function we present here is based on a signed distance metric. In other words, instead of using an absolute distance metric like the Euclidean, as in the case of RANSAC-based approaches, we use a signed distance metric that is weighted by the location of the candidate point whose distance is to be calculated, with respect to the reference hypothesis. Thus, the distances of candidate points from the reference line are negative for points left of the line and positive for points right of the line. Conventional residual distribution-based fitness measures [8, 13, 15] do not suppress non-maxima effectively, due to the use of unsigned metrics. Observing the computed residual distribution histogram , it is obvious that there is a peak near  for correct hypotheses. Instead of using user-dependent parameters such as kurtosis [21], bandwidth for kernel density estimation [7] or the number of points per structure (the height of the peak) [15], we detect this peak by comparing the residual histogram with an ideal distribution. Since the region where the data lie are available prior information, we can calculate the ideal distribution of the hypothesis to the uniformly distributed data. This histogram, denoted as , is used as a criterion to calculate the divergence to the real histogram of distances, for the scenario at hand. For instance, the middle-column images in Fig.&amp;nbsp; display the histogram of corresponding hypotheses (marked in red) to the assumed uniformly distributed dataset in a bounding box. Assuming a uniform distribution of candidate points, the histogram of distances obtained should be roughly flat for an isometric distribution of points in the original space. However, in practical applications, candidate points are distributed in a non-isometric space bounded by hyper-planes based on possible values the underlying parameters representing the space can take. Hence, it is necessary to estimate ideal histograms of distances based on these boundary considerations. For the example shown in the figure, for an isometric distribution, we would have a rectangular distribution for the histogram. But since the space of data points are bounded by the axes  and , we obtain distributions with slopes at either end. For the case of row 1, the hypothesis line is closer in inclination to one of the axes, the flat portion of the histogram is large while the slopes are small, corresponding to small regions beyond the boundary required to complete the isometric distribution. On the other hand, since in row 2, the inclination of the hypothesis line is farther away from both axes, the regions outside the boundary, required to complete the isometric distribution are much larger, resulting in smaller flat region of the histogram and larger slopes. Note that the ideal distribution  for each hypothesis  is only determined by the given input space, therefore, we can calculate this function and store it before we process the robust estimation. We normalize   and   and represent them as   and  , where   is the number of the bins in each histogram. The Jensen–Shannon divergence (JSD) of   and   is defined by            where  . The JSD values of the hypotheses in Fig.&amp;nbsp;  are 0.0511 for the correct one and 0.0175 for the arbitrary one. Thus, it is necessary to select hypotheses that produce larger differences between the ideal and test distributions, due to the occurrence of peaks, while discarding hypotheses that produce distributions that are similar to the ideal distribution. Thus maximization of the divergence or JSD is a necessary criteria for hypothesis validation. It can also be seen from the given example that the JSD resulting from correct and arbitrary hypotheses are significantly different. However, several parallel hypotheses (parallel with the correct one) can also produce a high JSD value because the divergence of the whole distribution is evenly taken into account. Hence, we weight the residual distribution using a standard Gaussian function   to generate the Gaussian-weighted JSD. The Gaussian function reinforces the influence of JSD for data points close to the hypothesis, and weakens the effect caused by the parallel model. In our tests,   is robust enough to distinguish parallel line models with distance between each other larger than 0.2 in range of whole dataset [0 1]. If two parallel lines are closer, the inlier scale noise makes them even hard to be distinguished by a human and we will not consider these extreme cases in this paper. The GJSD is formulated as,            Using GJSD to evaluate the hypotheses in Fig.&amp;nbsp; , we get 0.1612 for the correct one and 0.0075 for the arbitrary one. In fact, GJSD is a powerful evaluation tool for estimation of both a single model and multiple models, as demonstrated with the experiments in Sect.&amp;nbsp; 5.1. Figure&amp;nbsp;  demonstrates when variance of Gaussian function varies, the average fitness value of all the particles can always converge to a stable value which means all the particles are clustered into several niches. Figure&amp;nbsp;  depicts the independence of Gaussian parameter with respect to the algorithm performance.    3 PSO using ring topology  This section describes the details of the proposed solution to the model estimation problem using a particle swarm optimization based framework. Given the model can be formulated using  parameters, i.e. each optimal model can be estimated by searching in a  dimensional space. A cost function  where  is used as the function to be minimized. This is similar to the cost function used with RANSAC. The goal here is to choose the hypothesis that maximizes the difference between the residue distribution  and the ideal distribution , which is given by the GJSD. Therefore, translating the maximization into a minimization function and choosing  produce the required hypothesis selection. Thus  is used to evaluate the current position of model  in this space. The problem of model estimation is to find . Particle swarm optimization (PSO) is a meta-heuristic optimization method which was first intended for simulating social behaviour of birds [ 22]. The ring topology for PSO algorithm (known as   PSO) was actually not new and described in one of the first papers on PSO [ 22]. It was successfully applied in [ 16] with an aim to locate multiple optima. The stable and effective results displayed in [ 16] also inspire us to apply the ring topological PSO for the multiple structure selection problem.   PSO maintains a swarm of candidate solutions (called particles), and makes these particles fly around in the search-space according to their own personal best-found position and the best-known position in a neighbourhood niche. Each particle has a velocity that directs its movement and is adjusted iteratively. The   PSO is demonstrated in Fig.&amp;nbsp; , the found optima are capable of attracting neighbouring particles to form a niche, and once two niches are overlapped, the “better” niche (better fitness value) will entice the connecting particles from another niche. Since the movement of a particle is affected not only by its neighbors but also by its inertia, personal best position and random parameters, the particles at the border of two niches moves slowly and the niches can stay stably and arrive at convergence [ 16].            where   denotes the velocity of particle   in   dimensions  ,   is the   particle’s own best position found so far, and   is the best position in the neighbourhood of the  th particle. For all the particles in   dimensions,   are the updated velocity and new position derived from this new velocity, respectively. As in [23],  is the “constriction factor” which is introduced to limit the maximum velocity and the dynamic search area in each dimension.  are inertia weight and two acceleration constants, where  is the factor that influences the “cognitive” behaviour, i.e., how much the particle will follow its own best solution, and  is the factor for “social” behaviour, i.e. how much the particle will follow the swarm’s best solution.  and  are set to 2.1 and 2.0, respectively, as it was proved to be the best set [23].  is a function of  and  as reflected in (Eq.&amp;nbsp;4).  and  are uniformly distributed random numbers in the interval .    4 Complexity  In this section, we will discuss the computational complexity associated to the proposed algorithm as well as the realization of preemptive model estimation. As the complexities of both our algorithm and RANSAC are dominated by two major factors, which are generation of hypothesis by drawing the minimal sets and evaluating the fitness of every candidate model, here we will only compare the complexities of these two algorithms. The superior performance of our algorithm over more state-of-the-art approaches will be illustrated by the experimental results in Sect.&amp;nbsp;5.1. Given the cost of hypothesis generation   and evaluation  , the overall complexities of the standard RANSAC and our approach in the worst case are:            where   and   are iteration number of RANSAC, number of the particles and iteration number of PSO. Note that the number of the particles   instead of the iterations of PSO   plays the similar role as iterations of RANSAC   in the algorithm since both these two parameters impact the runtime of hypothesis generation step. As displayed in Fig.&amp;nbsp; ,   PSO can quickly (after 15–30 iterations) cluster hypotheses into different niches and maintain a reasonable number of members in each niche for a long time (see results in Fig.&amp;nbsp;  after 50 iterations). These stable niches facilitate using classical clustering technologies to obtain the parameters and the number of the models. We also notice that even after the small numbers of iterations ( 15), the particles have been already clustered into several niches, the best solution of the model estimation can be provided by the niches’ means roughly. This is desirable that the system can provide the current best solution given the time constrains, hence the preemption can be achieved in the case of real time systems which require a solution in a specific time. Algorithm&amp;nbsp;1 lists the scheme of preemptive   PSO. In all the following tests with synthetic data, the parameter  and  for PSO are set to 100 and 30. The required iteration of RANSAC (parameter ) is calculated using formula , where  is the inlier rate and  is the desired possibility of successful estimation ( is set to 99&amp;nbsp;% in all the test, i.e. typical value of  is 750 if inlier rate is 5&amp;nbsp;%, therefore we can expect that in our tests RANSAC is approximately four times faster than the proposed algorithm). For a more detailed demonstration that  PSO can effectively induce stable and robust niching for multiple global optima localization, we refer readers to paper [16, 24].    5 Experiments  We start to test our GJSD estimator with synthetic data for both single model and multi-model estimation, and then demonstrate the application of our methods in a real robotic task, indoor plane estimation. All the results of synthetic data processing are obtained on an Intel Quad Core 2.4&amp;nbsp;GHz CPU, 4&amp;nbsp;GB RAM and the real robotic task is operated on an Intel Mobile Core i7 2720QM CPU, 8&amp;nbsp;GB RAM laptop connected with a Microsoft Kinect. We first compare the speed of GJSD, RANSAC [ 1], MDPE [ 6], QMDPE [ 6] and pbM [ 7] to demonstrate the time complexity. Therefore, in this experiment only a single model will be estimated from a synthetic dataset. Note that we do not consider the RHA [ 8], J-linkage [ 15] and KF [ 13] methods in this experiment because all of them are originally designed for estimating multiple models. Note that each line in this experiment contains 100 inliers corrupted with Gaussian noise of standard deviation  , which is high relative to the range of the whole dataset (i.e. [0 1] [0 1]). The outlier rate considers both gross outliers as well as pseudo outliers which actually are inliers for the other models. The times in the test do not take the random sampling procedure into account because all methods apply the same random sampling with replication and have same number (1,000) of sampling. It costs 0.174(s) in average (measured using Matlab clock function). All the methods are coded in Matlab except for pbM which is coded in C++. Every method is tested 20 times and average processing time is recorded. From Table&amp;nbsp; 1, we notice that GJSD is slower than RANSAC but significantly faster than kernel-based methods since we do not run any kernel smoothing or residual sorting algorithms which have high computational cost. The comparison of breakdown points among various methods is also interesting. From Fig.&amp;nbsp;  we can observe that, given the correct scale of inlier error, RANSAC can provide correct estimation results and begins to break down at approximate 92&amp;nbsp;% outliers. Meanwhile, GJSD almost has the same breakdown points without the requirement of inlier scale. LMedS can only tolerate approximate 50&amp;nbsp;% outliers because it does not truncate the loss function. We also notice that the performance of RANSAC drops quickly when the outlier rate is more than 92&amp;nbsp;%, in contrast, even at relatively high outlier rate ( \(&amp;gt;\)95&amp;nbsp;% ), GJSD is still, loosely speaking, able to correctly estimate 80&amp;nbsp;% of the models out of 20 times. Although RANSAC and GJSD start to break down at the same place, the incorrect estimations after the breakdown points of these two methods are significantly different as displayed in Fig.&amp;nbsp; c. We see that RANSAC obtains completely wrong estimation of the model and meanwhile GJSD gives an incorrect estimation closely and parallel to the true model. This “slightly” wrong estimation result somehow is still useful in many practical applications and might be refined using some explicit restrictions (e.g. a plane estimation result can be refined with recognized objects which are supported by the plane). In this section, all the input data are contaminated with more than 90&amp;nbsp;% outliers and relatively high inlier noise ( ). Figure&amp;nbsp;  illustrates the convergence of   PSO in different line fitting problems. For each line estimation result, we define  , respectively, to be the true and estimated lines’ parameters, where   and  . Thus the error metrics between the estimated and true model is calculated as  . Also note that this error is measured only when the numbers of the model are correctly estimated, in other words, the incorrect estimation of the number of structures is not taken into account since we believe that this type of the error makes the system to produce the meaningless results thus the error measurement in this case is not necessary anymore. To measure the error of multiple line estimation results, the average error of   is computed as,            Results of multi-model fitting for the presented scheme in comparison to the state of th art Kernel Fitting (KF) [ 13] and J-Linkage [ 15] are shown in Figs.&amp;nbsp; ,  . Figure&amp;nbsp; shows the results of line fitting using KF, J-Linkage and the proposed approach. The results have been obtained for estimating an inclined line and a ‘W’ shape case (4 models); the outlier rates are 91.7 and 93.7&amp;nbsp;% for the single and multi-model cases. The inlier noise rate is 0.02. For KF the parameter step size  is fixed at 100.
Springer.tar//Springer//Springer\10.1007-s00138-013-0514-0.xml:Selection of negative samples and two-stage combination of multiple features for action detection in thousands of videos:Sparse representation Spatiotemporal features Support vector machines Tracking of humans Human action detection Interactions between people Pose estimation STIP Person detection Event recognition Random forest:  1 Introduction  The amount of image and video data being recorded and stored is increasing daily, both on the internet (e.g. YouTube) and for surveillance applications. This yields practical problems when analysts want to extract information from the huge data collection. What is needed are tools for automatic analysis of the data. One of the key technological capabilities to aid search for the relevant data is automated detection of specific events in videos. In many applications, the relevancy of events is determined by the actions that are being performed by the humans in the scene. A typical event is that one person approaches the other, walks up to the other person, and gives him something. These actions, ‘walk’, ‘approach’ and ‘give’, occur in a particular order and are partially overlapping. This complicates the search for particular events. Therefore, in this paper, we address the challenge of detecting many actions, occurring in isolation, or simultaneously. We consider the technological capability to analyze single- and multi-action events, where actions may involve a single actor (‘jump’), multiple actors (‘approach’), items (‘give’), and interaction with the environment (‘arrive’). An excellent overview of datasets related to action recognition was presented by Liu [36], however, the datasets discussed therein are not sufficient to test such a capability. Some datasets are single-actor, e.g. KTH [1] and Weizmann [2], and the performance on these datasets has almost been saturated [3] by methods with key components that serve as the basic pipeline in this paper (see Sect.&amp;nbsp;2). The MSR action dataset [37] contains only three actions but is very challenging because it requires spatio-temporal localization of the action, which is not a problem addressed by this paper. The UCF Sports [4], Hollywood2 [5], YouTube [6], Olympic sports [38], and the Columbia Consumer Video [46] datasets are more challenging (see e.g. [7]) as they involve interactions with other people and items and the recording conditions are hard. These actions in these datasets are all very context-dependent, which makes it a concept search rather than purely an action recognition task. The IXMAS [39] and UT-interaction [20] datasets contain subtle actions with 12 action types [39] and two-person interactions with six action types [20], respectively. A very challenging dataset is the visint.org [8], where multiple actions may happen at the same time, or just before or after each other. This dataset includes 4,774 videos of a wide range of events that involve a large number (48) of human actions, in various forms including humans inside a car or on a motor bike or bicycle. Due to its realism and complexity, we select the visint.org dataset for the study in this paper on human action detection. In addition, we will include IXMAS [39] and UT-interaction [20] datasets in our experiments, which allows a comparison to state-of-the-art action recognition methods. Together these two datasets represent a broad scope of possible actions. The events in the visint.org dataset are realistic and each event is represented by multiple actions: on average seven actions per movie clip. While each video scripts a single event, this event can be represented by several actions. For example, a transaction between persons can contain the actions “move”, “give”, and “receive”. The actions vary from a single person (e.g. walk) to two or more persons (e.g. follow). Some of these actions are defined by the involvement of some object (e.g. give), or an interaction with the environment (e.g. leave). The most complex actions involve two persons and an object (e.g. exchange, throw-catch combination). Figure&amp;nbsp;  depicts a few illustrations of this dataset. The dataset contains a test set of 1,294 realistic videos with highly varying recording conditions and on average 195 variations of each of the 48 actions. A complicating property of this dataset is that the actions are highly unbalanced: e.g. 1,947 positive learning samples for “move” to only 58 samples for “bury”, see Fig.&amp;nbsp; . The improvement in human action detection performance as proposed in this paper follows from the observation that particular human actions are highly correlated, see Fig.&amp;nbsp; . One category of actions shows correlations due to similarity, e.g. chase and follow. Another category is the sequential relations, e.g. leave and exit. There is also a category of compound actions, e.g. to exchange something requires both to give and to receive something. Indeed, all of the actions from these examples, and many others (see Fig.&amp;nbsp; ), are highly correlated in the human annotations of the visint.org dataset. We propose a method for the human-action recognition application that gives a major contribution in the performance on this visint.org dataset with a gain factor of 2.37 relative to previously published results on this dataset [ 9]. The reason for this major improvement is the exploitation of the correlations between the 48 actions and it is related to a different approach on three themes:          The outline of this paper is as follows. Our method will be motivated and detailed in Sect.&amp;nbsp; 2, where we also compare to related work. The experimental setup is defined in Sect.&amp;nbsp; 3. The experiments and results on the visint.org dataset are shown in Sect.&amp;nbsp; 4, and a comparison to state-of-the-art results on the IXMAS and UT-interaction datasets is provided in Sect.&amp;nbsp; 5. A discussion of the results is provided in Sects.&amp;nbsp; 6, and Sect.&amp;nbsp; 7 concludes the paper with our main findings. Better selection of negative training samples, by choosing those that are most similar to positives. Exploitation of the posterior probabilities of all action detectors in a two-stage SVM classifier. Combination of multiple features that are based on low-level motion and high-level object features.    2 Method  The basic action recognition framework is similar to the bag-of-words model used in a recent performance evaluation [10]. The general scheme is that features are computed, quantized by a codebook, stored in histograms, and finally the action is detected by an action-specific classifier. More specifically, in [10], space-time interest points (STIPs) features were used, quantized by a -means codebook, and videos were classified by a SVM. Due to its simplicity and reasonable performance in various detector-feature configurations on the KTH dataset and also on the more challenging UCF Sports and Hollywood2 datasets [10], we select this framework as a baseline pipeline to which we can add our proposed improvements. We replace the -means codebook by a random forest as a quantizer [11], because it worked better in our experiments (data not shown). Further we consider two types of features: the low-level STIP features and high-level event property (EP) features (Sect.&amp;nbsp;2.2), which we both compute on the data in our method and experimentation, we will compare and combine them. The basic pipeline and the advanced pipeline that we use in our experiments are shown in Fig.&amp;nbsp; . Each of the components in the advanced pipeline will be described and compared to the basic pipeline in the following subsections: feature computation (Sect.&amp;nbsp; 2.2); sampling for, and creation of, the random forest (Sect.&amp;nbsp; 2.3); the classification, by a single classifier (one stage) and a two-stage classifier (Sect.&amp;nbsp; 2.4); and finally combining features (Sect.&amp;nbsp; 2.5). In our ICPR ’12 paper [12], we performed initial experiments by comparing a STIP-based two-stage classifier to its one-stage counterpart. In this paper, we extend this setup to include also a different video feature, i.e. the complementary EP feature. Thereby we generalize to the advanced pipeline shown in Fig.&amp;nbsp;, extending it further by exploring two schemes to combine the two features. In this paper, we show experimentally the impact of the several choices inside the pipelines. We demonstrate how an optimized pipeline design provides a major improvement for the detection of 48 human actions. Many interesting features have been proposed for action recognition, which can be distinguished into two types of features: low-level motion features and high-level object features. The low-level features aim at capturing properties of the actions such as motion. A well-known example is the motion template [13], which captures the motion of the full body. A drawback is that it depends on the silhouette, whose detection is known to be error-prone. Local motions have been studied extensively, e.g. the representation of actions in terms of a PCA basis of optical flow [14]. The segmentation is still a critical element here, which has been a motivation to consider motion in several feature point configurations. That proved to be a promising direction due to its success for action recognition, see e.g. [15]. Several motion-based descriptors have been proposed since. The STIP detector and descriptor [16] is an example for capturing the local motions and shapes around detected Harris points in (x,y,t) space. It is very popular, because it has proven to be a very discriminative and robust feature for actions [17]. The high-level features are a different category and include, among others, detection of persons in the image and estimation of their poses. Poses are interesting because they relate very directly to human actions, as opposed to the low-level features such as STIP. Pose estimation has been done by matching exemplars [18] and searching the image for poses by evaluating deformable part models in spatial windows [19]. The window-based approach has also been successfully applied to the detection of persons, which potentially can be applied as a first step before analyzing the pose. The representation of a person’s trajectory has enabled storyline generation [20]. In [21] the layout of the scene is exploited to interpret trajectories and actions. The event property (EP) features as proposed in [9] aim to combine the ideas that were proposed in previous work. Persons are detected by a dedicated detector [22] where other objects are detected by a generic moving object detector [23]. All entities are tracked with a standard search-and-update tracker, and described by seven canonical poses that are classified from outputs of [19], and by additional event properties such as “two entities approach each other”. Our choice for the low-level feature is STIP due to its robustness, no need for segmentation, and its proven value for action recognition. Our choice for the high-level features is EP, because it combines several methods that each have proven their merit for representing meaningful intermediate-level aspects of human actions. In Sect.&amp;nbsp;2.5, we will show how these two can be combined. We note that there are other more specific features that can also be used for action recognition, such as frequency domain-based features [45] that are informative of (quasi) periodic motion patterns such as walking and digging. It is our goal to include features that are common to a wide range of human activities. Therefore, we choose not to include such more specific features as they are representative of a subset of activities only. During the training phase, two components are learned for an action detector: the random forest to create bag-of-feature histograms [11] and a classifier to detect the presence of actions. For the classifiers considered this learning is no problem: after the random forest the amount of data is limited and it is capable of learning from unbalanced classes. However, for the learning of the random forest, both the size of the data and the unbalanced classes are an issue [24]. Sampling is needed to reduce the amount of data and to obtain a proper balance in the training set between positive and negative examples. On the visint.org dataset, on average we have 500 STIPs and EP feature vectors per video, resulting in approximately 2-M feature vectors. Considering all these feature vectors in intractable: in recent applications of random forests typically 500-K feature vectors are used for learning, see e.g. [25]. The other issue is the unbalanced presence of actions in this dataset. An extreme example is “bury” being present in 2&amp;nbsp;% of the samples only. In fact, only one action, “move” occurs in more than 50&amp;nbsp;% of the clips. For all other actions, we are faced with the challenge of selecting proper negative samples for training. It has been shown previously that significant improvements can be achieved by doing such sampling carefully [26]. In previous work, the rationale has been proposed to select negatives that are similar to positives, social tags, i.e. free textual strings, have been exploited in [27]. However, social tags are unbound as a user can input any textual string. That makes the search for good negatives hard and therefore an iterative scheme is applied to find a good subset of negatives. In our case, the tags are bound: the annotations are fixed, as 48 binary judgements have been made by the annotators whether each action is present in a video. This opens up the opportunity to exploit the other 47 tags on whether other actions are present/absent in order to select the negatives for the current action. Each sample consists of one video clip, which typically has several action labels, and hundreds of feature vectors. For the positive class of a particular human action, the positive videos can be sampled randomly to obtain enough positive feature vectors. The most common approach to obtain the negatives is to do the same. However, the set of negative samples is much more heterogeneous: these can consist of any combination of the other 47 human actions. Therefore, we propose a selective sampling scheme. For discriminative learning, the rationale is that good negative samples are similar to the positive samples. To select negatives that are similar to positives, we consider all remaining 47 binary judgements by the annotators whether each action is present. Due to the correlations between actions, we expect to be able to determine the similarity between videos based on correspondences between the remaining 47 annotations. Similarity is determined by comparing the 47 annotations based on the  distance metric, where we normalize these vectors before comparison, such that they sum to one. The random forest quantizes the equally balanced positive and negative feature vectors into histograms (or visual words). Furthermore, we investigate for each action which type of sampling (random or selective) works best. Selecting the best sampling strategy for each action allows further optimization of the performance. Several classifiers have been used in previous work to estimate action probabilities, such as the tag propagation (TP) [9] or the support vector machine (SVM), e.g. [10]. For classification we chose an SVM, which is currently the most popular classifier in bag-of-feature models due to its robustness against large feature vectors and sparse labels (see ‘Classifier’ in Fig.&amp;nbsp;). In preliminary experiments, the SVM proved indeed to be better than standard alternatives, e.g. K nearest neighbors, nearest mean, logistic linear, quadratic, and Fisher classifiers (data not shown). In this paper, we additionally investigate the merit of a second-stage classifier, where all 48 posterior probabilities of the first classifier bank are used as new features for a second stage. Both the first and the second stages consist of 48 single action classifiers. The rationale behind using a second stage is that actions are correlated (Fig.&amp;nbsp;), so they are informative of each other. For instance, the likelihood of walking is very predictive of moving. This is a source of information that we want to exploit, by a two-stage setup. In Fig.&amp;nbsp; this is depicted by the ‘Classifier-2’ which is applied to ‘Classifier-1’ outputs. Note that the basic pipeline (Fig.&amp;nbsp;, left) does not exhibit this property. In machine learning, this is commonly known as sequential stacked learning (SSL), see e.g. [49]. In video concept detection, others have investigated methods to combine class-specific detectors in a second-stage classification. For instance, in the TRECVID challenge, the posterior probabilities of many concept detectors were taken as feature values and fed to a higher level classifier [28]. Another technique for the second-stage analysis is a weighted linear combination of all posteriors, which yielded a better estimate of each of the classes in [29]. Typically, such techniques have been used to combine different feature modalities, like image and text features [30], and different concepts [28]. Very recently, for action detection, first-stage action detectors were used as an ‘action bank’ to represent activities in video and to be classified by a second-stage classifier [7]. In a first stage, action detectors were applied to the video frames, and in the second stage, the posteriors were max-pooled over volumes in the video and represented by a histogram. This advanced scheme involves sampling over viewpoints and scales, dividing the video into volumes, and accumulating the evidence from the first-stage detectors into a well-engineered histogram representation. This two-stage scheme proved to be very effective on multiple datasets, yet also computationally demanding. The authors indicate that on average 204&amp;nbsp;min are consumed to process a video of the UCF50 dataset, with a maximum of 34&amp;nbsp;h [7]. Given that these are short video clips, we consider such a method currently computationally intractable for detection of action in thousands of realistic videos, which is the case that is investigated in this paper. In this paper, we answer the question how much performance can be gained by exploiting the information in the correlated actions. To that end, we want to assess the immediate advantage of exploiting the information in the first-stage detectors. Hence, we use their continuous posterior probabilities directly without complex and computationally demanding operations that are needed to make advanced representations such as the action bank [7], although we acknowledge the potential that can be gained by such advanced representations. The added value of our paper is that we assess the merit of combining multiple action detectors in a simple two-stage setup, where each detector has been composed of the standard state-of-the-art components STIP features, a random forest, and a SVM (see Sect.&amp;nbsp;2.1) that have proven to perform well (see e.g. [1, 5, 10, 17]) and are very efficient (STIPs run at approximately 10&amp;nbsp;fps and the random forest quantization and SVM classification are real-time). Combining the first-stage detector outputs by a weighted linear combiner as used in [29] proved not to be effective for our purpose (data not shown). Rather we consider the simple and computationally efficient scheme where the posterior probabilities from stage one are used as features in stage 2. In the last step of the pipeline, we improve the action detection by combining low-level (STIP) and high-level (EP) features. The most straightforward approach is to directly concatenate the EP and STIP histograms and feed them in a single classifier and create a detector for each action (dashed arrows in Fig.&amp;nbsp;). As an alternative, we also consider the concatenation of the posteriors of the first classifiers for both EP and STIP features and the creation of a final-stage combined detector for each action. Other combiner schemes are available, such as multiple kernel learning [31], but we will show that this simple two-stage combiner is effectively taking advantage of the complementarity both feature types.    3 Experimental setup  The visint.org dataset [8] includes 48 human actions in a train set of 3,480 videos of 10–30&amp;nbsp;s, and a test set of 1,294 similar video. This dataset is novel and contributed by the DARPA Mind’s Eye program. The annotation is as follows: for each of the 48 human actions, a human has assessed whether the action is present in each video or not (“Is action X present?”). Typically, multiple actions are reported for every video, on average the number of reported actions is seven. For each action we create a random forest [32] with 10 trees and 32 leafs, based on 200-K feature vectors, 100&amp;nbsp;K from randomly selected positive videos, and 100&amp;nbsp;K from either randomly selected or selective sampled negative videos. The random forest quantizes the features into histograms [11] and a SVM classifier with a  kernel [33] is trained that serves as a detector for each action. For the random forest we use Breiman and Cutler’s implementation [32], with the -parameter equal to the total number of features (162 for STIP and 78 for EP features). For the SVM we use the libSVM implementation [34], where the  kernel is normalized by the mean distance across the full training set [33], with the SVM’s slack parameter default . The weight of the positive class (i.e. the samples of a particular action) is set to  and the weight of the negative class (i.e. samples that do not contain the action) to , where  and  are the amount of positive and negative class samples [35]. Although detection of actions will simultaneously involve spatial localization in the video, the performance of action detection will not be measured in terms of localization accuracy in this paper. The reason for this is that not all datasets contain annotations of action location. The performance will be measured by the MCC measure,    with T&amp;nbsp;=&amp;nbsp;true, F&amp;nbsp;=&amp;nbsp;false, P&amp;nbsp;=&amp;nbsp;positive and N&amp;nbsp;=&amp;nbsp;negative. The MCC measure has the advantage of its independence of the sizes of the positive and negative classes. This is important for our evaluation purpose, as the human actions are highly unbalanced (see Fig.&amp;nbsp; ). This evaluation metric was also used in the evaluation of the DARPA’s mind’s eye program and it allows comparison to previous results on this dataset [ 9,  12]. In this paper we report the average and the standard deviation of the MCC per verbs (denoted as  ) where in [ 9] we reported the MCC calculated using TP, TN, FP, and FN averaged over all verbs.    4 Experiments and results  The parts of the pipeline that we vary are the sampling of feature vectors for the random forest (Sect.&amp;nbsp;4.1), the usage of two-stage classification (Sect.&amp;nbsp;4.2), the combination of STIP and EP features in a joint pipeline (Sect.&amp;nbsp;4.3), and analysis of the best approach for each action (Sect.&amp;nbsp;4.4). The other parts remain fixed during the experiments. Optimization of the parameters, training of the random forest and classifiers, and selection of the best sampling approach for each action have been performed on the training set. The results in these sections are based on applying the trained method to the test set. A combined overview of these results is given in Sect.&amp;nbsp;4.5. Table&amp;nbsp; 1 shows the results of random vs. selective sampling, for both EP and STIP features separately and one-stage classification. For completeness, we also inserted the state-of-the-art results [ 9] on the visint.org dataset (experiment number 00). For each table of the results, we indicate in bold the best performance of the tested conditions.
Springer.tar//Springer//Springer\10.1007-s00138-013-0515-z.xml:Harmony search-based hybrid stable adaptive fuzzy tracking controllers for vision-based mobile robot navigation:Vision-based navigation Harmony search algorithm Tracking control Hybrid optimization approaches Adaptive fuzzy controller Autonomous mobile robot:  1 Introduction  Autonomous mobile robot navigation has been regarded as a popular research area for the last few decades. Some earlier attempts to solve the problem of path planning were presented in [1, 2]. Various classical theoretical approaches such as Dijkstra’s algorithm [3] and A* algorithm [4], designed originally for some other purposes, have also been suitably extended to solve navigation problems. The potential field method [5, 6] and probabilistic roadmap methods [7], have been extensively utilized by many researchers for navigation in static environments only. On the contrary, for the moving obstacle environments, other potential solutions based on dynamic potential field method [8, 9], and sensor-based path planning methods [10–12] have been extensively used. A robot typically calculates and estimates the motion of the moving obstacles based on sensory data form infra red (IR), sonar sensors, laser range finder, optical sensor and RFID [12, 13]. The chief problem associated with most of the sensor-based techniques is that the data fusion algorithm, used to synchronize the sensory data, cannot process the data at the same time stamp due to the out-of-sequence measurements (OOSM) problem. In order to make precise navigation, this OOSM problem should be solved and thus, vision has become a popular alternative as a sensing mechanism [14]. In some vision-based approaches, the navigation system is used to construct a high-level topological representation of the world and the robot learns to recognize rooms or spaces and to navigate between them by building models of those spaces and their connections [14, 15]. Many robot navigation systems also focus on producing a detailed metric map of the world using expensive hardware, such as a laser scanner, telefocus camera, etc. After this map is built, the robot then has to solve a complicated path planning algorithm [16, 46]. These methods pose significant, additional computational burden to the processing unit and create difficulties during real-life applications. The present paper uses a novel idea of formulating a mobile robot navigation problem as a tracking control problem, where vision is used as the sensing element for navigation [17]. The problem has been solved using hybrid stable adaptive fuzzy controllers. Fuzzy systems and fuzzy control philosophies have been successfully employed in many problems in the area of robotics [18–22] and other interesting engineering applications [23–26]. In this present scheme, single camera-based vision is used to generate the reference path and adaptive state feedback fuzzy controllers are utilized to track that path. A simple path planning algorithm is devised to create the reference path for the mobile robot navigation purpose. The planned path is then utilized to create the reference signals for -direction and -direction movements and two previously tuned adaptive fuzzy controllers are implemented, for -direction and -direction movement of the mobile robot. For the actual navigation of the mobile robot following the planned path, the controllers have to generate suitable control signals to produce the requisite drive signals for left and right wheel actuators of the robot. The major contribution of this work is that each tracking controller is separately tuned with some arbitrary reference signal, generated from a similar environment to that in which the robot will actually navigate, and, once the tuning part is successfully concluded, these controllers are readily applied to the real-life applications. The system is so designed that once the reference path is generated, the robot is commanded to track a specified fraction of this reference path. Then a new image of the environment is acquired and the whole process of new path planning, control actuation generation and robot navigation for a specified fraction of the reference path is repeated again and again. This is done with the objective of equipping the system with the flexibility of handling dynamically varying environments, so that the robot navigation can still be carried out with dynamic variations in positions of obstacles during its navigation. In the present paper, a hybridization of locally operative Lyapunov theory and a globally adaptive harmony search algorithm-based stochastic approach [27, 28] has been utilized to design the hybrid stable adaptive state feedback fuzzy controllers for autonomous mobile robot navigation, utilizing the concept of tracking control. The relationship between music and mathematics dates back to ancient civilization. In 2001, Z. W. Geem developed an optimization method, namely harmony search, for function optimization and engineering applications, inspired by the musical phenomena of harmony improvisations. It is a meta-heuristic algorithm based on music [29]. Many more meta-heuristics are found in literature, most of them mimicking some natural or artificial phenomena. The most commonly used class of algorithms is that of genetic algorithms (GAs), which are based on natural selection and mechanism of population genetics [30]. Other such computational algorithms, similar to GA, include evolutionary strategies, genetic programming, particle swarm optimization, ant colony optimization, simulated annealing and tabu search [31, 32]. In contrast to the other metaheuristic search techniques, HS algorithm is based on the musical process of searching for perfect state of harmony, utilizing the esthetic and acoustic criteria, which impose fewer mathematical requirements [29, 33] and can be easily adopted for various types of engineering optimization problems. In recent years, HS method has been successfully applied in several fields including function optimization [34, 35] mechanical structure design [34, 36], and pipe network optimization [37]. Some exciting applications of HS algorithm, specifically in the domain of robotics, have also been recently reported in [38], where Tangpattanakul et al. proposed a method for optimal trajectory planning of robot manipulators with six degrees of freedom. In their work, Tangpattanakul et al. proposed a hybrid methodology, combining the HS and the sequential quadratic programming (SQP), to develop a globally optimal trajectory for a manipulator movement. The works of Fourie et al. are also included in [38], where a visual tracking system has been developed employing a proposed formulation of the HS algorithm-based harmony filter. The method has been successfully applied to accurately track a poorly modeled target under challenging environments. The Lyapunov theory and harmony search algorithm have been hybridized to create a superior method by combining the strong points of both the methods. In our proposed hybrid methodologies, the Lyapunov theory-based adaptation and HS algorithm-based stochastic global search are operated concurrently or preferentially, to optimize both the structure and the free parameters of the fuzzy logic controllers (FLCs) over the solution space, with the objective of controlling the plant. Vision-based path planning and the design of tracking controllers for controlling the -direction and -direction movements are performed with the objective of autonomous mobile robot navigation without using any IR or other proximity sensors. In this design methodology, the number of membership functions (MFs) for each input variable, the supports of input MFs, the number of rules, the values of the scaling gains and the positions of output singletons are optimized to evaluate a candidate controller setting for optimal performance, in terms of the integral absolute error (IAE) value between the path planned using vision sensor and the actual path traversed by the mobile robot. The obtained results illustrate, on the whole, the superiority of the preferential hybrid approach over the other approaches. The results obtained in both simulation and experimental environments are compared with similar strategies developed using GA- and PSO-based fuzzy logic tracking controllers for vision-based robot navigation [17]. The rest of the paper is organized as follows: Sect.&amp;nbsp;2 details the design of stable adaptive fuzzy controllers. Section&amp;nbsp;3 describes the HS algorithm-based tracking controller design technique as well as the hybrid controller design strategies. Section&amp;nbsp;4 discusses in detail the implementation of the proposed stable adaptive fuzzy tracking controller-based mobile robot navigation scheme in an indigenously developed mobile robot platform and also the comparative study among different design strategies. Section&amp;nbsp;5 concludes the paper.    2 Design of stable adaptive fuzzy tracking controller  Let us consider the motion of the mobile robot in a plane which can be described as [ 39]:                         where  \(\underline{p}=\left[ {\begin{array}{lll} x&amp;amp;{} y&amp;amp;{} \varphi \\ \end{array}}\right] ^{\mathrm{T}}\) is the present pose of the robot in the world Cartesian coordinate system, the coordinate   denotes the centre of mass of the robot, the heading direction   is taken counterclockwise from the  -axis,  \(\underline{m}=\left[ {\begin{array}{ll} v&amp;amp;{} \omega \\ \end{array}}\right] ^{\mathrm{T}}\) is the vector containing linear velocity   and angular velocity  , is the input vector,   is a symmetric and positive definite inertia matrix,   is the centripetal and Coriolis matrix, and   is the gravitational vector. In this present work, our control objective is that for a given reference trajectory   and orientation of mobile robot, we must design two controllers that can generate suitable torque   such that the current robot position   achieves the desired reference position  :       To achieve the control objective, the system should be so designed that a   is derived based on the differentially steered drive velocities to the left and right wheels of the robot according to the steering system in ( 1). This objective is satisfied by designing two stable adaptive fuzzy logic controllers, for  -direction and  -direction movements, that can ensure asymptotic stability, achieve satisfactory transient performance and can suitably configure this mobile robot navigation problem as a tracking control problem, where the controllers guide the robot to track or navigate a desired tracking or navigation path. Now to design such  -direction and  -direction tracking controllers, let us consider that our objective is to design an adaptive strategy for an  th order SISO plant in general, and is given as [ 27,  40– 42]:       Here, the state vector is given as   where   denotes the   derivative of state   is the first state where  , both plant input and plant output are scalar quantities, given as   and  , respectively, and   is an unknown continuous function. The control objective is to force the plant output   to follow a given bounded reference signal   under the constraints that all closed-loop variables involved must be bounded to guarantee the closed loop stability of the system. Thus, the tracking error is  . Let the parameter vector of the output singletons of the fuzzy controller be denoted as  . Hence our control objective is to find the structure of the fuzzy controller as well as a feedback control strategy  , using fuzzy logic system and an adaptive law for adjusting   such that the following conditions are satisfied:        The closed loop system must be globally stable in the sense that all variables,  and  must be uniformly bounded, and         &amp;nbsp;           The tracking error  should be as small as possible under the constraints in i) and the  space should be stable in the large for the system [40, 41].         &amp;nbsp;      To accomplish these control objectives with a direct adaptive fuzzy controller, let the error vector be   and   be such that all the roots of the Hurwitz polynomial   are in the open left half of s-plane [ 40– 42]. Now the ideal control law for the system in ( 4) is given as [ 40,  43]:       where   is the  th derivative of the output of the reference model. The closed loop system must be globally stable in the sense that all variables,  and  must be uniformly bounded, and The tracking error  should be as small as possible under the constraints in i) and the  space should be stable in the large for the system [40, 41]. This definition implies that  guarantees perfect tracking, i.e.  [41, 42]. Here the vector  describes the desired closed-loop dynamics for the error. In practical situations, since  and  are not known precisely, the ideal  of (5) cannot be implemented in practice. Thus a suitable solution can be to design a fuzzy logic system to approximate this optimal control. Now, to ensure stability, we assume that the control   is given by the summation of a fuzzy control,  , and an additional supervisory control strategy,  , given as:       Let us assume that the adaptive fuzzy logic controller (AFLC) is constructed using a zero order Takagi–Sugeno (TS) fuzzy system. Then   for the AFLC is given in the form [ 27,  40,  42]       where   is the vector of the output singletons,   is the firing degree of rule&amp;nbsp; ‘ ’&amp;nbsp;=&amp;nbsp;  is the total number of rules,   is the membership value of the  th input membership function (MF) in the activated  th rule,   is the vector containing normalized firing strength of all fuzzy IF-THEN rules&amp;nbsp;=&amp;nbsp;  and  . Let us define a quadratic form of tracking error as   where   is a symmetric positive definite matrix satisfying the Lyapunov equation. The supervisory control action,  , should be so designed that   should be negative semidefinite, i.e.  . Here   is constructed as given in [ 40,  43] and can be presented as       where  \(\left\{ \begin{array}{l} I_1^{*} =1 \text{ if } V_e &amp;gt;\overline{V} \\ I_1^{*} =0 \text{ if } V_e \le \overline{V} \\ \end{array}\right. , \overline{V}\) is a constant specified by the designer,  \(f^{U}\ge | {f(\underline{q})}| \text{ and } 0&amp;lt;b_{\mathrm{L}} &amp;lt;b\). With this   it can be shown that  , where   is a positive definite matrix. Thus, as  \(P &amp;gt; 0\), boundedness of   implies the boundedness in  . Hence, the closed loop stability is guaranteed. The zero order TS-type fuzzy control   can be so constructed that it will produce a linear weighted combination of adapted parameter vector  . Thus a simple singleton-based adaptation law as proposed in [ 40– 43], can be given as       where  \(\nu &amp;gt; 0\) is the adaptation gain or learning rate and   is the last column of  .    3 Design of harmony search algorithm-based AFLC  The HS algorithm is inspired by musicians’ behavior where a combination of pitches determine the quality of harmony generated, here a set of values of the decision variables is judged by the corresponding value of the objective function. As in musical harmony, if a combination of decision variables can produce good result, then this solution vector is stored in memory and this helps to produce an improved solution in near future. In Lyapunov theory-based design of stable AFLC’s [40], only the output singletons of the fuzzy controllers were adapted and the other free parameters e.g. supports of the input MFs, input and output scaling gains and also the structural parameters of the controllers, i.e. the number of MFs in which each input is fuzzified, the number of rules, etc. were chosen a priori. Furthermore, the AFLC was implemented for a fixed controller structure. However, while designing the HS algorithm-based optimal controller, all these parameters are obtained automatically by encoding them as part of the solution vector, i.e. each of these parameters forms a decision variable. This helps to determine the optimal structure of the FLC as well as its complete settings. In HS algorithm-based design, a harmony in solution space is formed as [ 27,  28]:            The structural flags are implemented to determine whether a particular MF will be present for an input to the FLC or not. It can only take binary values where 0 indicates the non-existence and 1 indicates the existence of the corresponding MF. However, HS is an algorithm where each entry in the harmony vector can take continuous values. Hence, for each structural flag in the vector, the universe of discourse is set as [0, 1] and the flag is set to 0 if the continuous value of the variable is  \({&amp;lt;}0.5\) and the flag is set to 1 if the variable is  . The total number of 1s in the structural flags keeps changing in each iteration and hence the total number of MFs in which an input variable is fuzzified also changes in each iteration. This changes the structure of the AFLC in each iteration as it changes the total number of MFs for each input variable, which changes the total number of rules of the fuzzy rule base and hence changes the total number of active output singletons. Each input is fuzzified in the range [ , 1], with two fixed triangular MFs having their peaks fixed at   and 1 respectively and the centre locations of all other triangular MFs are stored in the particle vector. All intermediate MFs for that input variable are flexible in nature. They can be either active or inactive during an iteration and their peaks are also adapted by HS algorithm in each iteration. For each MF the peaks of the immediate adjacent active MFs on either side of its own peak forms the left and right base support of it. Whether the immediate adjacent MF is an active MF or not is determined by the content of its corresponding structural flag. Hence, some of the centre locations of MFs are ignored while evaluating the AFLC output, because their corresponding entries in the structural flags are zero. A similar logic holds true for the output singletons in a harmony vector. If a singleton has one or more antecedent MF that is inactive, then it becomes inactive itself. HSBA Algorithm The outline of the HS [ 29] based controller design algorithm employed in this paper is given as follows [ 27]              The optimization problem can be stated as:                                    is a candidate solution vector,        and        is the universe of discourse of        and       , where PST is the plant simulation time and       .              &amp;nbsp;                 Generate HMS number of randomly generated candidate solution vectors and store them in the HM matrix (stored by the values of the objective function       ):                                                         &amp;nbsp;           Determine the structure of the candidate controller based on structural flags as incorporated in the harmony .         &amp;nbsp;           Calculate the fitness value (e.g. integral absolute error (IAE)) of the controller for each harmony that acts as a candidate controller in this work, by using candidate controller simulation (CCS) algorithm as shown in Algorithm 1.         &amp;nbsp;                 Improvise a new harmony vector        either by choosing a value for a decision variable from the past history, stored in HM matrix, or by choosing any value in its permissible universe of discourse, with probability HMCR        [      33,       34,       44]. Hence a new decision variable        in        can be determined as:                                            Each decision variable of the new vector       , if generated from HM matrix, is further examined for a potential pitch-adjustment possibility with a probability PAR. In pitch-adjustment, the decision variable adjusts its value according to the formula:                             where bw is an arbitrary distance bandwidth and        is a uniform distribution.              &amp;nbsp;           Evaluate the objective function  for this new harmony. If , then replace the solution vector in existing HM matrix that produced the worst harmony, with this new harmony. Then rearrange the HM matrix by sorting all decision vectors according to their objective function values.         &amp;nbsp;           The termination criterion can be set in two ways, either by choosing the maximum number of HS improvisation/iteration  or by using a minimum error criteria, i.e. continue the HS improvisation until a maximum pre-specified allowable error is attained by the controller.         &amp;nbsp;      Further, in 2007 Mahdavi et al. [ 34] suggested an improvement to the original HS algorithm as presented in [ 29], where the PAR and bw in step-v are varied throughout the generations as linearly increasing and logarithmically decreasing manner respectively, but the HMCR parameter is kept constant. In [ 34] the values of PAR and bw change dynamically with the generation of harmony improvisation as:       where   is minimum pitch adjusting rate,   is the maximum pitch adjusting rate,   is the present generation number, and            where   is the minimum bandwidth,   is the maximum bandwidth. The optimization problem can be stated as:           is a candidate solution vector,   and   is the universe of discourse of   and  , where PST is the plant simulation time and  . Generate HMS number of randomly generated candidate solution vectors and store them in the HM matrix (stored by the values of the objective function  ): Determine the structure of the candidate controller based on structural flags as incorporated in the harmony . Calculate the fitness value (e.g. integral absolute error (IAE)) of the controller for each harmony that acts as a candidate controller in this work, by using candidate controller simulation (CCS) algorithm as shown in Algorithm 1. Improvise a new harmony vector   either by choosing a value for a decision variable from the past history, stored in HM matrix, or by choosing any value in its permissible universe of discourse, with probability HMCR   [ 33,  34,  44]. Hence a new decision variable   in   can be determined as:              Each decision variable of the new vector  , if generated from HM matrix, is further examined for a potential pitch-adjustment possibility with a probability PAR. In pitch-adjustment, the decision variable adjusts its value according to the formula:         where bw is an arbitrary distance bandwidth and   is a uniform distribution. Evaluate the objective function  for this new harmony. If , then replace the solution vector in existing HM matrix that produced the worst harmony, with this new harmony. Then rearrange the HM matrix by sorting all decision vectors according to their objective function values. The termination criterion can be set in two ways, either by choosing the maximum number of HS improvisation/iteration  or by using a minimum error criteria, i.e. continue the HS improvisation until a maximum pre-specified allowable error is attained by the controller. This modification showed successful results for several constrained and unconstrained optimization problems. Another modification of the original HS algorithm has been proposed in [ 27], where HMCR and PAR parameters are varied simultaneously but the bw parameter is kept constant throughout the generation of harmony improvisations to obtain the optimal solution vector. In this proposed modification of the harmony improvisation of the basic HS algorithm, the parameters HMCR and PAR both are increased linearly from   to   and   to   respectively and bw is kept fixed as:       The physical implication of this modification is that as the generation of harmony improvisation increases the optimization approach relies more on the harmony memory because the harmony memory would be better one due to the improvisations throughout the generations. The PAR parameter is also increased because the fine tuning is required as the optimization algorithm relies more on the past values of the harmonies stored in the memory. That signifies the modifications of the HMCR and PAR parameter of the basic HS algorithm. Along with this modification, another modification in the improvisation stage is also proposed in [27]. In the original HS algorithm during the harmony improvisation, only one harmony is improvised in each iteration and consequently updates the harmony memory if the newly improvised harmony performs better in terms of the objective function. Thus only one harmony is replaced and remaining (HMS - 1) number of harmonies remain unaffected in each iteration. Instead of using such a concept of harmony memory update, if the entire harmony memory is improvised in each iteration then it can be hoped that the convergence of the algorithm will be better as the algorithm will then potentially explore more solution space in each iteration. Therefore, in this modification, the harmony memory is improvised HMS number of times rather than single harmony improvisation in each iteration. It is also reported in [27] that the proposed modification with entire HM improvisation concept shows a superior result than the other HS variants. Therefore, from now onwards this modification is implemented for the design of different hybrid adaptation-based control strategies. In this design methodology, the Lyapunov theory-based local adaptation of output singletons and the modified HS algorithm-based global optimization technique (DyHSBA) are combined to design stable adaptive fuzzy controllers. In this paper this approach is referred as the hybrid adaptation strategy-based approach (HASBA). Two different modes of hybrid models such as concurrent and preferential, are proposed and developed to design the stable adaptive fuzzy controllers for vision-based mobile robot navigation purpose. These two hybrid techniques are described in the following sections. In this design process, the Lyapunov theory-based adaptation and the DyHSBA run concurrently to optimize the structure of the controller, i.e. the number of MFs required to fuzzify the input variables, center locations of the input MFs, positions of the output singletons etc. and the scaling gains of the input and output variables. In this method also, a solution vector   is divided into two sub-groups given as [ 27,  42,  43]:       where       The decision variables comprising   have a non-linear influence and the decision variables comprising   have a linear influence on the control signal   [ 27,  42]. For each harmony chosen as a candidate controller, it is first subjected to the Lyapunov theory-based adaptation of the   portion of the chosen harmony and the finally adapted values are then used to update the candidate controller. Then this updated harmony is subjected to a usual pass of the DyHSBA algorithm. In this process   is subjected to both local and global search experiences in every improvisation of a harmony. This process of concurrently employing the Lyapunov theory-based adaptation and DyHSBA algorithm for each candidate controller is continued in quest for the best solution. In this proposed hybridization scheme, the DyHSBA method and the HASBA-Con method are both used to achieve the minimum possible tracking error. Here, each harmony in the harmony memory, i.e. each candidate controller, evaluates the CCS algorithm and also the Lyapunov theory-based adaptation separately. Then, the algorithm for which the better fitness value is obtained is used as the guiding factor to explore the solution space. If the Lyapunov theory-based adaptation is better for that candidate solution vector in that iteration, then the process will follow the concurrent hybrid model, otherwise, the process will follow the DyHSBA method in that iteration. Hence each harmony in the HM matrix is subjected to different guiding rules of improvisation and the preference, or the choice of the guiding rule, is based on the evaluation process just described before. This process of evaluation is performed in each iteration afresh for each harmony and the whole harmony memory is improvised to get an improved convergence in the design process.   
Springer.tar//Springer//Springer\10.1007-s00138-013-0516-y.xml:Hierarchical abnormal event detection by real time and semi-real time multi-tasking video surveillance system:Human trajectory analysis Real-time abnormal event detection Video surveillance system:  1 Introduction  Video surveillance system has been used for various applications, such as traffic monitoring, security system, post incident analysis, etc. Originally these video surveillance systems were designed for human operator to watch concurrently or to record video data as archive for later analysis. Watching surveillance video is a labor-intensive task, as a large number of cameras need to be deployed and monitored; furthermore, it is a rather tedious task so it is easy for human observers to loose attention. Recently, computer vision researches have been heavily involved in intelligent video analysis applications. Automation can help overcome both cost and performance issues and free security personnel from routine tasks so that they can focus on higher-level cognitive tasks that better utilize their abilities. The goal of the proposed work is to detect and provide warnings about the occurrence of undesirable or suspicious events. It requires some models of the possible human activities as well as some model of what is normal in an examining area. One of the major requirements for the proposed system should be the capability of catching incoming streaming track data without loss and analyzing track data to check abnormal events without stopping. At the same time, finding whether the detected event is aberrant, or not, requires further analysis. In simple cases, this is indicated merely by the presence of a person, vehicle or another object in locations where they are not normally found. Such processing is computationally demanding and does not run in real-time on state-of-art desktop PC level computers, thus it would be used sparingly in an overall system, in an off-line mode. Recently, spatio-temporal primitive features, such as STIP (Spatial Temporal Interest Points) have been often used for activity recognition [2, 4, 6, 7, 9]. Statistics of primitive features are collected and classified using various machine learning classification techniques. But, they are usually suitable for segmented videos with known action boundaries. In other words, it is hard to handle unsegmented live streaming surveillance video data. Another group of human activity recognition research has explored towards the estimation of human pose&amp;nbsp;[1, 10, 12]. But, pose estimation requires expensive computation and it may not be suitable for the real-time video analysis application. In addition, it becomes very challenging problem under inter- or intra- occlusions. In this paper, we propose a hierarchical abnormal event detection system that takes care of real-time and semi-real time event analysis tasks. In low level task, we implemented a trajectory based abnormal event detection method that can process in real time. In high level task (semi-real time process), we request ‘Video On Demand’ data for the time of before-and-after the detected abnormal event from the video storage server. On receiving the requested video data, we apply an intensive video analysis algorithm to confirm whether the detected abnormal event is triggered by actual humans or not. The activities to be recognized may be belonging to a certain object (such as unexpected non-entry zone appearance), between two or more actors (such as collision, line formation).    2 Definitions  In this section, we explain a few important terminology definitions which will be often used in this paper. We define ‘ ’ to be something that happens and to be abnormal or normal. The purpose of our system is mainly the detection of pre-defined abnormal events. The scenarios of the target abnormal events for the proposed system are as follows:         : Someone walks in through the pre-defined illegal entry zone.         &amp;nbsp;            : Two people walk toward and collide with each other. One person stumbles and falls down to the ground. The stumbled person lies down for a while.         &amp;nbsp;            : A group of people suddenly stop and form a line in middle of the pathway. It indicates the unusual event of pedestrian traffic flow in the pathway.         &amp;nbsp; : Someone walks in through the pre-defined illegal entry zone. : Two people walk toward and collide with each other. One person stumbles and falls down to the ground. The stumbled person lies down for a while. : A group of people suddenly stop and form a line in middle of the pathway. It indicates the unusual event of pedestrian traffic flow in the pathway. We define ‘’ as human location information at each frame. Track contains  position in world ground plane, time stamp, and track ID. In other words, it captures the instant location of human at each frame. We also define ‘’ as the list of tracks with the same ID from its beginning to end. Trajectory represents spatial and temporal history of human’s moving. After analyzing the given trajectories and tracks, we create event information which is defined as ‘ ’. Event object has more information than the event itself. The highlighted attributes of the event object are as follows:     : Event types of abnormal events or undecided event as ‘unknown’.    : Start (birth) and end (termination) of the event    : Location of the event.    : List of associated trajectories from its birth to the current time.  Event object may represent an abnormal event or not. When it is undecided event, it simply implies human’s trajectory. We refer ‘event object’ without specifying ’abnormal’ as undecided event (i.e., human’s trajectory) in this paper. The event location attribute is used and updated differently according to the event type. For example, illegal entry saves one point location and requires continuous location update so that we can track down the intruder after he or she makes illegal entry. In case of line formation, it requires two location points and does not require location update. : Event types of abnormal events or undecided event as ‘unknown’. : Start (birth) and end (termination) of the event : Location of the event. : List of associated trajectories from its birth to the current time.    3 System overview  We assume that human trajectory is provided in real-time by a baseline surveillance system [ 5,  11]. Our goal is to detect abnormal events by processing the real-time track data from the baseline system. At the same time, we also want to improve the system reliability by reducing the number of false alarms (which is common problem in video surveillance system). For this purpose, we apply computationally expensive object detection algorithm to verify the initially detected events. In order to integrate between real-time baseline surveillance system and non-real-time proposed system, we implemented our system as a hierarchical level system that allows concurrent real-time and non-real-time operations as shown in Fig.&amp;nbsp; . We use a baseline video surveillance system that has capability of multi-camera human tracking system [5, 11]. An initial one-time calibration of all cameras is required during system installation. The foreground / background modeling is continuously performed for human detection algorithm. Once the foreground map has been computed, the human detection approach examines whether foreground patches using geometric shapes that have size and shape similar to people. Human detection responses from all cameras are projected onto the ground plane at each frame. A centralized tracker collects the time-ordered detections and forms detection response into tracks. However, due to real-time processing constrains, there might be false alarms generated by video image noise or non-human objects. In addition, there might be missing humans caused by lack of motion foreground data (e.g., standing still). We implemented two different processes inside the proposed system:  and .  is tightly coupled with the real-time baseline surveillance system and  is loosely coupled. The processes are programmed as multi-threads to support a heterogeneous coupling system. Each process has subcomponent modules as shown in Fig.&amp;nbsp;. receives all incoming tracks from the baseline system at every frame and checks if the received track is associated with the already existed event objects as will be described in Sect.&amp;nbsp;4.1.  checks if multiple event objects are related and merge them to remove redundancy (in Sect.&amp;nbsp;4.2). There are three  that correspond to each target abnormal event (in Sect.&amp;nbsp;4.3). When the system detects any abnormal event,  sends a VOD (Video-On-Demand) data request for time of before-and-after the detected suspicious event (in Sect.&amp;nbsp;4.4). When  in the high level process confirms the acknowledgment of receiving the requested VOD data, it calls  for all available views to verify the initially detected abnormal events (in Sect.&amp;nbsp;5.1).  collects all detection results and makes a decision whether the detected event is triggered by actual human and notifies the confirmed ’Abnormal Event’ to the baseline system (in Sect.&amp;nbsp;5.2). Experimental results and conclusion will be presented in Sect.&amp;nbsp;6.    4 Low level process  The baseline surveillance system has a capability of the scene calibration to extract the world ground coordinate for each track&amp;nbsp;[13]. The baseline system sends a list of tracks at each frame if any. The low level process handles incoming tracks, detects an initial abnormal event, and notifies to the high level process for verification. collects the tracks from the baseline system and associates them with the already existed event objects. The association at this stage is simply checking whether they have the same track ID or not. If they are associated, we update the associated event object with the current track information. Single track can be associated with multiple event objects. For example, a track can associate with illegal entry as well as line formation events. In this case, we add the track to all associated events. If a track does not find its associated event, then we create new event object for the unassociated track. Please note that new event object is undecided regarding being normal or abnormal at this stage. Once an event object is created, it should be continuously updated by incoming tracks. If it fails to be updated due to track loss from the baseline system, it becomes ‘Missing event object’ which will be handled by  in the next Section. Multiple event objects may represent the same actual event. For example, when two line formation events are close each other, they should be combined as one event. Upon merging events, trajectories with the same track ID in both events are also merged to become one trajectory. In addition, we also connect a missing event object to one of the other updated event objects. There are two possible cases in missing event object. The first case is human’s exit, i.e., human leaves the scene coverage of the camera. The second case is caused by track loss. Only the second case is required to connect to another existed event object. Assuming that the camera is static, we exploit scene knowledge regarding entrance and exit of the scene, such as at the left/right, the top/bottom fringes of images, or the pre-defined doorways in the middle of image. Under this assumption, when an event object disappears in non-exit areas, we consider the event object to be missed, not to be exited. In this case, we search for any nearby event objects. Especially, if the nearby event object is just created at its proximity, it is more likely for the missed event object to be reappeared. We connect the missed and the nearby event objects. Given the list of trajectories in an event object, the system makes decisions regarding whether suspicious event is taking place or not. The system examines the trajectory not only in spatial relationship, but it also considers the temporal constraints, such as the duration of abnormal event. In this section, we describe how to detect different types of abnormal events using the proposed trajectory analysis method. Please note that the following three abnormal detectors are simultaneously running and single event object can contribute to multiple abnormal events. People usually walk through the pathway. When the pedestrian traffic stops and forms a line, it indicates that an abnormal situation is happening. The system should notify to the human operator (security personnel) to pay more attention. There could be many false line formations when a group of people walk together or people walk separately, but accidentally form a line. It suggests that we should take into consideration not only the spatial relations, but also the temporal constraints to detect abnormal line formation event reliably. When it comes to spatial inference, at each frame we form a line hypothesis by choosing three tracks such that the distance among each other is less than a distance threshold (). We check any other human tracks by estimating the perpendicular distance error to the hypothesized line. When the distance error is less than another threshold (), we create an event object whose type is initial line formation. Here, the initial line formation event object is not abnormal event yet, since it needs more evidence. We add the human tracks that form a line, to the created event object. We control the spatial proximity constraint by using two distance thresholds  and . The threshold depends on the domain knowledge of defining the abnormal line formation. For example, it may become higher value when the target site covers larger area and vice versa. The proposed system allows the domain user to provide these values. We will show the actual parameter settings for our evaluation sites in Sect.&amp;nbsp;6.
Springer.tar//Springer//Springer\10.1007-s00138-013-0517-x.xml:Structured light self-calibration with vanishing points:3D reconstruction Self-calibration Structured light Vanishing points Pattern projection:  1 Introduction  Multiple view-points are needed in non-intrusive machine vision applications that deal with metric measurements of the scene such as automatic inspection, 3D reconstruction, object recognition, robot guidance for target or self-localization, reverse engineering, process control and others. The techniques used for solving the shape acquisition problem can be divided in two groups:  and . Passive techniques are used for obtaining depth in a scene without any physical interaction with the observed objects. The most common passive vision technique is the stereoscopy. A stereoscopic system, formed by two or more cameras, provides the necessary view-points and can be used for range estimation if two conditions are accomplished: (1) the correspondences between images are accurately determined and (2) the cameras are calibrated. A crucial problem in stereovision is finding the corresponding features between several views of the same object. Geometric constraints, such as the ones introduced by the epipolar geometry&amp;nbsp;[14], can be used for solving the correspondence problem yet at a high computational cost. Moreover, the resolution of stereovision systems is relatively low as it is dependent on the number of corresponding features which is ultimately determined by the texture of the scene objects or by the existence of singular points. On the other hand, active techniques have been introduced in order to enhance the capabilities of passive vision systems for finding correspondences. One of the most well-known techniques is the structured light(SL)&amp;nbsp;[10, 25] which is an active stereovision technique that alleviates the correspondence problem by illuminating the scene with a specially designed light pattern. The light reflected by the scene is measured and range estimation can be inferred. Concretely, the features projected by the pattern onto the scene objects are uniquely identified and the correspondences can be established at a high resolution. The first SL techniques were based on laser scanning&amp;nbsp;[1]. In this case, the obtained images are easy to segment and these methods provide a high resolution. However, the speed and the mechanical components of such configurations introduce unacceptable restrictions for some applications such as the measuring of free moving objects. A more evolved model was introduced by the group of coded SL techniques. These techniques are projecting bi-dimensional light patterns on the scene by replacing one of the cameras by a light projector. Such techniques use temporary or spatially encoded patterns that allow a unique identification of the observed points. The second condition that a stereoscopic system must fulfill is the calibration of its components. Many camera calibration techniques&amp;nbsp;[5, 11, 15, 24, 29, 32] have been developed and excellent surveys&amp;nbsp;[24, 27] can be found in literature as it is an extensively studied field since the first machine vision applications appeared. A camera can be calibrated without the need of a calibration pattern by using the image of the points at infinity, i.e., the  (VPs)&amp;nbsp;[4, 6, 12, 19]. These particular points can be accurately obtained especially when dealing with scenes that meet the Manhattan world assumption&amp;nbsp;[7]. The VPs extracted from mutually orthogonal directions can be used to calculate the intrinsic and the extrinsic camera parameters&amp;nbsp;[14], thus, to calibrate the camera. Since the VPs can be used to derive geometric information of the camera, there have been developed many methods for their detection&amp;nbsp;[2, 18, 23, 28]. Active SL techniques use a projector which has to be calibrated, as well. The projector models can be divided into three groups&amp;nbsp;[31]: the line model, the light-stripe model and the plane SL model. The last model is the most used in practice because it defines the projector as a reversed camera and, thus, camera calibration techniques can be applied. Even though the projector can be modeled using the perspective geometry, the approach to the projector calibration is different than in the case of the camera since it cannot offer an image of the scene. The lack of a scene image provided by the projector makes its calibration essentially different compared to the camera calibration. Thus, the existing camera calibration methods, relying on the image analysis and on establishing the correspondences between the image features and the scene structure, cannot be applied directly. The calibration of a SL system is usually performed in two steps: first, the camera is calibrated; second, the calibration of the projector is performed using the calibrated camera. The calibration procedures mainly rely on the use of specially manufactured 2D or 3D calibration objects. For example, Kimura et al.&amp;nbsp;[17] proposed a method for the calibration of a projector using a calibrated camera and several images of the calibration planes. Fernandez et al.&amp;nbsp;[9] presented a similar method that requires, however, only a two-sided plane and is independent of the SL algorithm used for the reconstruction. In both methods the coordinates of the 3D points used for the calibration of the projector were calculated using a calibrated camera. The main drawback of this approach is that the errors introduced at the camera calibration stage are propagated to the projector’s model; moreover, a previously calibrated camera is required. Martynov&amp;nbsp;[20] proposed a projector calibration method that uses an uncalibrated camera as a sensor to determine the projector’s image by calculating the camera–projector homography. The projector is eventually modeled as a reversed camera and is calibrated using a classical camera calibration approach. However, the method needs a series of iterations for determining the best fitted points for the projected image. Also, the method is based on the assumption that the points are accurately determined on the camera image which might be difficult if the projected pattern is distorted due to a possible misalignment between the projector and the screen. It is well known that, when the projector is not orthogonal with respect to the target screen, its projection is affected by a trapezoidal shape distortion known as . Moreover, the camera’s image is a perspective transformation of the scene. Thus, a new distortion is added when the scene containing the pattern is captured by the camera. Even though the pattern is distorted twice, a homography between the scene and the projector can be calculated&amp;nbsp;[26] using the image provided by camera. Consequently, the transformation that compensates the keystone effect can be determined and applied to the projected pattern. The resulting projection is identical to the one produced by a projector whose optical axis is normal to the screen plane. We use the vanishing points of the corrected pattern for calibrating the projector, independently of the camera parameters. This paper proposes a calibration method for a SL configuration and brings in the following contributions:            The calibrated configuration is eventually used for shape acquisition using coded SL. The calibration object is simplified becoming a white planar surface with a known size and orthogonal edges. The error propagation is eliminated as the camera and the projector are calibrated independently. The method requires very low human interaction and does not depend on the SL algorithm used. The corresponding VPs of the camera and of the projector are used for the first time in SL systems. The remaining of the paper is structured as follows: Sect.&amp;nbsp;2 introduces the mathematical fundamentals for the calibration of the pinhole model using VPs and presents the calibration methodology for the camera and the projector of a SL system. The calibrated model can be used for 3D reconstruction as shown in Sect.&amp;nbsp;3. The accuracy of the reconstruction is studied through experimental results presented in Sect.&amp;nbsp;4. The paper ends with the conclusions that are detailed in Sect.&amp;nbsp;5.    2 Calibration methodology  The camera and the projector are projective devices and can be represented using the pinhole model. A projective transformation consists of a non-singular linear transformation of homogeneous coordinates from a point   in the world coordinate system to a point of the image   up to a scale factor  , as shown in Eq.&amp;nbsp;( 1):            The projection matrix,  , can be decomposed and written as the product of the camera matrix and the transformation matrix from the world to the camera coordinate system:       The pinhole model considers the skew coefficient between the two image axes, denoted by  , the aspect ratios, denoted by   and   and the focal distance  . Thus, the camera matrix   has the form:            The six extrinsic parameters of   are the three rotations and three translations corresponding to each orthogonal axis. The pinhole model is calibrated when the intrinsic and extrinsic parameters are determined. Two VPs, determined from a projection screen having the orthogonal edges visible in the camera image, can be used for the calibration of a pinhole device using a method inspired by Guillou et al.&amp;nbsp;[12]. For simplicity, this section explains the calibration for the camera case but the method is also applied for the projector after the image rectification, as it is demonstrated latter on. We assume, without loss of generality, that the principal point is located at the center of the image, the skewness is equal to zero  and the aspect ratio is equal to one, i.e., . Hence, the intrinsic and extrinsic camera parameters can be obtained by means of geometric relations using only two vanishing points. Let us consider two coordinate systems: the world coordinate system, centered at   and having the orthogonal axes   and the camera coordinate system, centered at   with the axes  . Let the camera projection center be placed at   and the center of the image, denoted by  , be the orthographic projection of   on the image plane. Let the two vanishing points   and   be the vanishing points of two axes   and   of the world coordinate system, as shown in Fig.&amp;nbsp; . The coordinates of the vanishing points in the image plane are   and  . The projection of   on the line   is denoted by  . The principal point is located at the intersection of the optical axis with the image plane, thus, its coordinates  are immediately obtained. Its position is crucial&amp;nbsp;[13] for further calculations implied in the calibration process. The focal distance   can be calculated by considering that   and   are placed along the optical axis, as shown in Fig.&amp;nbsp; , which means that:       The distance   from the center of the image to the line at infinity, joining the two VPs, is calculated &amp;nbsp;[ 12] as:       The matrix   models the world-to-camera rotation. Since the two vanishing points   and   correspond to orthogonal axes and considering the fact that all parallel lines meet at the same VP, a new coordinate system can be determined. Therefore, the three orthogonal axes are also centered at   and have the same orientation as the world’s axes since   and  . Hence, the rotation between the camera and the new coordinate systems is identical with the rotation between the camera and the world coordinate systems. The vectors   are:       Obtaining the rotation matrix   as:            Finally, the translation vector   must be calculated. A segment with a known size in the scene is used considering, without loss of generality, that one of its two ends is located in the origin of the world coordinate system. The segment is determined by the world points   and  , expressed in metric units, as shown in Fig.&amp;nbsp; . The segment can be aligned with its image in the system of coordinates of the camera using the rotation matrix  :            The original segment is imaged by the camera through a projective transformation and two image points   and  , represented in pixels, are obtained. In the pinhole model, the metric coordinates of any point in the image can be calculated by undoing the pixel transformation, the third coordinate being the focal distance:       We can now translate the segment on the image plane by setting its first point on its image   and calculating the position of the second point. Thus, the translated segment is represented by the points   and  :       The obtained segment is parallel to the original one thus forming two similar triangles   and  , as shown in Fig.&amp;nbsp; . Taking advantage of the properties of similar triangles, we can write:       Therefore, the distance   from the camera center to the world center can be calculated as:       Hence, the translation vector is:       Our method automatically determines the rotation about the   and   axes of the calibration plane such that the VPs of the world’s   axes are aligned with the camera   axes. Then, the intrinsic and extrinsic camera parameters can be obtained by means of the geometric relations presented above. The VPs are invariant to translation, therefore, the camera translation is refined through a Levenberg–Marquardt error minimization algorithm with the initial solution given by Eq.&amp;nbsp;( 13). The projector is calibrated by projecting the image of a checkerboard onto the same white plane used for the camera calibration. Generally, the optical axis of the projector is not perpendicular to the screen so the pattern is affected by a double distortion introduced by the keystone effect and by the camera perspective transformation. The pattern is rectified by eliminating the two distortions. Figure&amp;nbsp;  presents the four points   that bound the projected pattern. We consider a distortion-free projected pattern as a rectangle having the same VPs as the screen. Consequently, geometric relations can be used to calculate the positions of the points   that bound such a projection. Let   and   be the coordinates of the VPs formed in the camera image along the   and   world axes, respectively. The keystone effect is removed by enforcing the co-linearity between the points   and   along the   direction and between the points   and   along the   direction. If the point  , then the positions of the other three points are calculated from the intersections of the following lines:       The effect of the image rectification is illustrated in Fig.&amp;nbsp; . The keystone effect, clearly visible in Fig.&amp;nbsp; a, appears when projecting a checkerboard with orthogonal edges on a flat surface. After removing the distortion, the corrected image is projected and the distortion-free pattern appears as shown in Fig.&amp;nbsp; b. Note that the VPs of the projector appear in the projector image. Let us consider that the camera and the projector reference systems, see Fig.  a, are placed at   and  , respectively. Both devices are randomly oriented and point towards a planar surface aligned with the   plane of the world coordinate system  . The homography   between the world screen plane and the camera image plane, containing the points   and  , respectively, can be calculated from the system of Eq.&amp;nbsp;( 15) using at least four points.            Similarly, the homography   between the projector and the camera image is determined. Thus, estimating the homography between the screen and the projector becomes straight forward, as shown in relation&amp;nbsp;( 16): Figure b shows the camera image containing the camera calibration plane, defined by the points  and the projected pattern, bounded by the points . The projector’s keystone effect is removed, independently of the camera’s parameters, by applying a transformation to the original pattern, shown in Fig. c. After the transformation, the resulting projection has the same VPs as the target screen. Considering the constraints imposed by the screen’s VPs, the points  are calculated on the camera image, see Fig. d. Using the homography , the new points  on the projector are determined. Figure e shows the projector image containing also the estimated position of the points  and . Thus, the pattern can be rectified by a homography derived for a given projector–screen configuration. However, applying the homography directly will produce oversampled or subsampled images. New patterns can be designed when structured patterns, as the checkerboard, are projected. Using the homography  to relate the edges of a rectified projection on the screen with the projector image, a new pattern can be generated as shown in the Fig.&amp;nbsp;d, e. Besides, in the case of a non-structured pattern, high-quality images can be obtained by adding an interpolation method to the transformation. The distortion of the projector’s view, shown in Fig.&amp;nbsp;e, strongly binds with the projector–camera–screen configuration. Thus, the VPs of the projector’s distorted image can be used for calibrating the pinhole model of the projector. Briefly, the calibration algorithm has the following steps:        Determine the VPs of the camera using the target screen,         &amp;nbsp;           Generate the rectified pattern,         &amp;nbsp;           Determine the projector’s VPs,         &amp;nbsp;           Calibrate the camera and the projector using their respective VPs by using the method explained in Sect.&amp;nbsp;2.1.         &amp;nbsp; Determine the VPs of the camera using the target screen, Generate the rectified pattern, Determine the projector’s VPs, Calibrate the camera and the projector using their respective VPs by using the method explained in Sect.&amp;nbsp;2.1.    3 Structured light for 3D reconstruction  SL techniques are used for overcoming the limitations of passive stereovision due to the lack of correspondences in the case of scenes with non-textured objects. In this work, we project a coded pattern built using a spatial neighborhood coding technique based on a De Bruijn sequence. We used the optimized De Bruijn pattern designed by Pages et al.&amp;nbsp;[22] using 4 hue levels and 64 colored vertical slits separated by black bands. This technique permits the identification of the correspondences in one-shot and is suitable for the reconstruction of both static and moving objects. A De Bruijn sequence of order  in conjunction with  different symbols forms a single dimensional structure of length  containing unique instances of substrings of length . The background is removed by using a threshold for the low values of the luminance. In the existing SL configuration the decoding process uses horizontal scan lines. The intensity peaks resulting from the center of each stripe are located by applying a second derivative filter to the signal obtained from the scan line. The intensity differences are enhanced by the filter and the detection is obtained with sub-pixel accuracy. The segmentation is performed using a binary rule: the regions where the second derivative is  \({&amp;lt;}1\) are set to 0 and the others are set to 1. The result of the segmentation is illustrated in Fig.&amp;nbsp; . Finally, the observed stripes are matched with the projected pattern, operation known as  . The color of a stripe can be precisely classified among the 4 levels of Hue composing the projected pattern. The key to the decoding strategy lies in identifying the hue value of each stripe in a given scan-line. The matching between the projected and the perceived stripes is solved using the colors of two neighbor stripes, i.e., with a window of size equal to 3 (sequence of order 3). A large number of correspondences can be detected by applying the process iteratively for all the image rows that contain the object. The ideal case occurs when all the stripes in the scan-line are correctly identified and the sequence is decoded. However, in reality, not all the stripes are visible or some of them are incorrectly labeled. Such classification errors produce outliers that eventually decrease the global accuracy of the reconstruction. The correspondence between the projected stripes and the detected ones is obtained through a dynamic programming algorithm&amp;nbsp;[ 30], using the RGB components of the stripes.
Springer.tar//Springer//Springer\10.1007-s00138-013-0518-9.xml:Accurate ball detection in soccer images using probabilistic analysis of salient regions:Naive Bayes classification Soccer ball pattern recognition Sparse feature representation Ball detection Interest point detection Dictionary learning:  1 Introduction  Automatic sport video analysis has become one of the most attractive research fields in the areas of computer vision and multimedia technologies [1]. This has led to opportunities to develop applications dealing with the analysis of different sports such as tennis, golf, American football, baseball, basketball, hockey, etc. However, due to its worldwide viewership and commercial value, there has been a boom in soccer video analysis research [2, 3] and a wide spectrum of possible applications are being considered [4–6]. Some applications (automatic highlight identification, video annotation and browsing, content-based video compression, automatic summarization of play, customized advertisement insertion) require only the extraction of low-level visual features (dominant color, camera motion,image texture,playing field line orientation, change of camera view, text recognition) and thus are more reliable [7, 8]. In other applications (such as verifying the referee’s decision, tactics analysis, player and team statistic evaluations, etc.), more complex evaluations and high-level domain analysis are required. Unfortunately, the methods currently available have not yet reached a satisfactory level of accuracy and new solutions need to be investigated [9]. In particular, ball detection and localization in each frame is an issue that still requires more investigation. The ball is invariably the focus of attention during the game; however, its automatic detection and localization in images is challenging as a great number of problems have to be resolved: occlusions, shadowing, the presence of very similar objects near the field lines and/or on parts of the players’ body, appearance modifications (for example, when the ball is inside the goal it is distorted by the net and it experiences a significant amount of deformation during collisions) and unpredictable motion (for example, when the ball is shot by players). In the last decade, different approaches have been proposed to face the problem of ball detection in soccer images. The most successful approaches in literature consist of two separate processing phases: the first one aims at selecting the regions which most probably contain the ball in each image (). These candidate regions are then analyzed in depth to recognize which of them really contains the ball (). Candidate ball extraction can be performed using global information such as size, color and shape or a combination of them. In particular, the circular Hough transform (CHT) and several modified versions have long been recognized as robust techniques for curve detection and have been largely applied by the scientific community for candidate ball detection purposes [10]. Which methodology is best for validating ball candidates is more controversial. In [11], a candidate verification procedure based on the Kalman filter is presented. In [12], size, color, velocity and longevity features are used to discriminate the ball from other objects. These approaches experience difficulties in ball candidate validation when many moving entities are simultaneously in the scene (the ball is not isolated) or when the ball abruptly changes its trajectory (for example, in the case of rebounds or shots). To overcome this drawback, other approaches focus on ball pattern extraction and recognition: for example, in [13] neural architecture discriminating the wavelet coefficients extracted from ball and no-ball examples is proposed. This approach fails to validate ball candidates in the case of textured (invariance to rotation, scaling and illumination changes is limited) and occluded balls (wavelet description needs a large visible ball portion to supply significant coefficients). In [14], a preliminary comparative study of the most popular strategies to describe and classify ball patterns was made. The study demonstrated that none of the approaches in literature can efficiently cope with all the possible variations of the ball’s appearance and the authors concluded that new solutions have to be introduced to increase detection performance. To this end, this paper presents a new approach to recognizing soccer ball instances in images acquired from static cameras. Edge circularity is at first computed on the whole image to extract the regions most likely to contain the ball. Then some points of interest are localized in the selected regions by measuring self-similarity. A feature vector for each detected point is built by integrating the gradient magnitude and orientation in a surrounding region and finally building the eight-bin orientation histograms. These vectors are clustered to build a visual codebook and finally a model that best represents the distribution of these codewords for each object class is built in a probabilistic way. This framework assures robustness to intra-class variation as well as to different photometric and geometric transformations. This has been proven through an extensive testing phase conducted on a number of real images containing different instances of the ball also in the presence of translation, scaling, rotation, illumination changes, local geometric distortion, clutter and partial and heavy occlusion. The proposed framework was also favorably compared with some of the leading approaches from the literature demonstrating its capability to better face changes in ball appearance. In the rest of the paper, Sect.&amp;nbsp;2 gives an overview of the proposed approach, whereas Sects.&amp;nbsp;4, 5 and 6 detail its fundamental steps. Section&amp;nbsp;7 presents the experimental results, a comparison with other related approaches and an extensive discussion. Finally, in Sect.&amp;nbsp;8, conclusions are drawn.    2 Overview of the proposed approach  The proposed ball recognition approach consists of seven main subtasks:        ball candidate extraction by analyzing the input image’s edge circularity;         &amp;nbsp;           detection of interest points in the ball candidate regions;         &amp;nbsp;           encoding the local information around the interest points in a vectorial form;         &amp;nbsp;           learning a dictionary that identifies the most important building blocks (atoms) in ball images and non-ball images;         &amp;nbsp;           representing ball candidates in the learned dictionary space;         &amp;nbsp;           estimating class-conditional distributions in the new representation;         &amp;nbsp;           classifying the input ball candidate on the basis of the learned class probability.         &amp;nbsp; ball candidate extraction by analyzing the input image’s edge circularity; detection of interest points in the ball candidate regions; encoding the local information around the interest points in a vectorial form; learning a dictionary that identifies the most important building blocks (atoms) in ball images and non-ball images; representing ball candidates in the learned dictionary space; estimating class-conditional distributions in the new representation; classifying the input ball candidate on the basis of the learned class probability. Figure   schematizes the proposed approach, whereas in the following sections, a detailed description of the involved algorithmic procedures is given.    3 Ball candidate extraction  Ball candidates are identified in two phases: at first all the moving regions are detected making use of a background subtraction algorithm. The procedure consists of a number of steps. At the beginning of the image acquisition, a background model has to be generated and later continuously updated to include lighting variations in the model. Then, a background subtraction algorithm distinguishes moving points from static ones. Finally, a connected components analysis detects the blobs in the image. The implemented algorithm uses the mean ( ) and standard deviation   to give a statistical model of the background. Formally, for each frame the algorithm evaluates:                         where   is a parameter that allows the model to dynamically update itself to variations in light conditions. It should be noted also that Eq.&amp;nbsp;( 2) is not the correct statistical evaluation of standard deviation, but represents a good approximation of it, allowing a simpler and faster incremental algorithm that works in real time. The background model described above is the starting point of the motion detection step. The current image is compared to the reference model, and points that differ from the model by at least two times the correspondent standard deviation are marked. Formally, the resulting motion image can be described as:            where   is the binary output of the subtraction procedure. An updating procedure is necessary to have a consistent reference image at each frame, a requirement of all motion detection approaches based on background. The particular context of application imposed some constraints. First of all, it is necessary to quickly adapt the model to the variations of light conditions, which can rapidly and significantly modify the reference image, especially in cases of natural illumination. In addition, it is necessary to avoid including players who remain in the same position for a certain period of time (goalkeepers are a particular problem for goal detection as they can remain relatively still when play is elsewhere on the field) in the background model. To obtain these two opposite requirements, we chose to use two different values for   in the updating Eqs.&amp;nbsp;( 1) and ( 2). The binary mask   allows us to switch between these two values, permits us to quickly update static points   and to slowly update moving ones  . Let   and   be the two updating values for static and dynamic points, respectively:            In our experiments, we used   and  . The choice of a small value for   owes to the consideration that very sudden changes in light conditions can produce artifacts in binary mask in Eq.&amp;nbsp;( 3): in such cases these artifacts would be slowly absorbed into the background, while they would have remained permanent if we had used  . The binary image of moving points is the input of the circle detection algorithm based on the circle Hough transform (CHT) which has been formulated as convolutions applied to the edge magnitude image. The convolution kernels were properly defined to detect the most completed circle in the image, being independent of the edge magnitude, and according to different shapes of the ball. Further details are reported in Sect.&amp;nbsp;3.1. The candidate ball regions are selected starting from the result of the convolution operations. Candidate regions are squared portions of the initial image: the side length is equal to  where  is the maximum expected radius of the ball. In each image, the number of candidate regions is not set in advance. In fact, a region is labeled as a candidate if:        the point in its center has a Hough value greater than a weak selected threshold. For example, a threshold could be set to . This way, only regions containing at least a number of edge points equal to the points on a quarter of the circumference were assumed as candidates to contain the ball. This assures that, when the ball is in the image, the pertinent region has been extracted but, at the same time, the number of regions to be further investigated is not excessive;         &amp;nbsp;           the distance between its center and the centers of other previously selected regions in the same image is greater than .         &amp;nbsp; the point in its center has a Hough value greater than a weak selected threshold. For example, a threshold could be set to . This way, only regions containing at least a number of edge points equal to the points on a quarter of the circumference were assumed as candidates to contain the ball. This assures that, when the ball is in the image, the pertinent region has been extracted but, at the same time, the number of regions to be further investigated is not excessive; the distance between its center and the centers of other previously selected regions in the same image is greater than . In Fig.&amp;nbsp; a, one image containing the ball is shown, whereas Fig.&amp;nbsp; b displays the corresponding Hough space (highest values are white and the lowest ones are black). Figure  c shows two candidate ball regions extracted in correspondence to two peaks in the Hough accumulation space: the region on the left is relative to a ball, whereas the region on the right contains a non-ball object. The circle Hough transform (CHT) aims to find circular patterns of a given radius   within an image. Each edge point contributes a circle of radius   to an output accumulator space. The peak in the output accumulator space is detected where these contributed circles overlap at the center of the original circle. To reduce the computational burden and the number of false positives typical of the CHT, a number of modifications have been widely implemented in the last decade. The use of edge orientation information limits the possible positions of the center for each edge point. This way, only an arc perpendicular to the edge orientation at a distance   from the edge point needs to be plotted. The CHT along with its modifications can be formulated as convolutions applied to an edge magnitude image (after suitable edge detection). We have defined a circle detection operator that is applied over all the image pixels, which produces a maximal value when a circle is detected with a radius in the range [ ]:       where the domain D  is defined as:               is the normalized gradient vector:       and   is the kernel vector       The use of the normalized gradient vector in ( 5) is necessary to have an operator whose results are independent from the intensity of the gradient in each point: we want to be sure that the circle detected in the image is the most complete in terms of contours and not the most contrasted in the image. Indeed, it is possible that a circle that is not well contrasted in the image gives a convolution result lower than another object that is not exactly circular but has a greater gradient. The kernel vector contains a normalization factor (the division by the distance of each point from the center of the kernel) which is fundamental to ensuring we have the same values in the accumulation space when circles with different radii in the admissible range are found. Moreover, the normalization ensures that the peak in the convolution result is obtained for the most complete circle and not for the greatest in the annulus. As a final consideration, in Eq.&amp;nbsp;( 1) the division by   guarantees the final result of our operator in the range [ 1, 1] regardless of the radius value considered in the procedure. The masks implementing the kernel vector have a dimension of   and represent the direction of the radial vector scaled by the distance from the center in each point. The convolution between the gradient versor images and these masks evaluates how many points in the image have a gradient direction concordant with the gradient direction of a range of circles. Then the peak in the accumulator array gives the candidate the center of the circle in the image.    4 Detection of interest points  At first, the ball candidate regions are processed to detect interest points. In the past few years, many techniques have been proposed to try to locate image regions that are distinctive, repeatable, robust to occlusion, viewpoint changing, geometric and photometric deformations. The literature on point detector methods is extensive and good reviews can be found in [15–17], where the performance of different approaches is measured against changes in viewpoint, scale, illumination, defocus and image compression. Comparison results show that there is not one detector outperforming others, but all feature detectors experience a weak robustness to various transformations. Moreover, all the methods are very unstable for small regions: they produce stable scale and shape estimates for large support regions, but then other effects like occlusion and background clutter start to affect the results. To overcome these drawbacks, a new approach for interest point detection in this paper is used. Its main advantage is that different types of features in images can be detected by using a common computational concept. The key idea is to find regions with high self-similarity, i.e., regions that are exactly or approximately similar to a region fragment [ 18]. A self-similar region is detected by computing a normalized correlation coefficient between the intensity values of a local region and the intensity values of the same geometrically transformed local region. A local region is self-similar if a linear relationship exists:       where   is a circular region of radius   and   is a point located in  .   denotes the intensity value at location  , and   represents a geometric transformation defined on  . Considering   as a reflection and rotation the strength of the linear relationship in ( 9) can be measured by the normalized correlation coefficient. These coefficients, which can be computed for different mirror line orientation or different angles of rotation, give information about region self-similarity. Local saliency is measured by the average normalized correlation coefficient computed over all orientations of the mirror line at a given location radial saliency map ( ), or over all angles of rotation tangential saliency map ( ). Saliency maps   and   are computed over a range of different scales for regions with different radii  . Interest points are finally extracted from   and   considering the local scale space maximum and minimum. This approach gives a rich set of highly distinctive local regions: it enhances the context-free attentional operators based on radial symmetry and it is not gradient based, thus avoiding threshold tuning.
Springer.tar//Springer//Springer\10.1007-s00138-013-0519-8.xml:Key observation selection-based effective video synopsis for camera network:Video synopsis Camera network Graph matching Video surveillance:  1 Introduction  Everyone is familiar with the time-consuming and inefficient task in browsing and finding anything of interest from a collection of raw video. And in security applications, there are great demands for efficient technologies for video browsing and retrieval. When facing the tremendous amount of unedited video, capturing endlessly from millions of video surveillance camera distributed around the world, how to obtain a compressed video abstraction from raw video becomes a beneficial and significant task for video browsing and retrieval. Video synopsis is a compact representation of video that enables efficient video browsing and retrieval. It reduces the spatio-temporal redundancy in video by displaying the moving objects from different periods simultaneously. Video synopsis can achieve high compression rate, while keeping the dynamic character of moving object in video. For example, 5&amp;nbsp;min synopsis video may be generated from 10&amp;nbsp;h raw video. The existing video synopsis approaches [24–27] always concentrate on generating synopsis video for individual camera. However, the view scope of a single camera is finite and limited. To monitor a wide area, video streams from multiple cameras have to be utilized. However, checking the activity of the same object across different cameras is a tough task. Video synopsis for camera network is a good solution, which can generate synopsis video in large scope of field, and provide adequate coverage of the overall environment. To conduct video synopsis for camera network, the global activity (trajectory) of moving objects across multiple cameras should be obtained in advance. However, various possible disturbances exist in this task, such as false moving objects extraction, object re-entrances and occlusion. Therefore, how to perform trajectory association robustly and efficiently under complex scenarios is a challenging and tough task. Previous methods [1, 2, 11, 14] often model trajectory association as a point-wise matching problem. They ignored the structural or context information among trajectories and could not obtain an optimal association sets from global view. In computer vision, it is widely known that the fundamental problem of establishing correspondences between two sets of visual feature can be effectively solved by graph matching method. Graph matching is used in various tasks, such as feature tracking, image retrieval, object recognition, and shape matching [5]. It had been adopted in performing trajectory association for multiple cameras [10], in which an undirected bipartite graph was constructed for two cameras, and the maximum matching of the bipartite graph was found. However, the existing graph matching methods, adopted in association, focused on exploiting relatively weak unary or pair-wise attribute and did not aim at optimizing a well-defined objective function [6]. Instead, based on Boolean Quadratic Programming, graph matching formulation can take into consideration both unary and pair-wise terms. Since Quadratic Programming is known to be NP-hard, approximate solutions are required. Graph matching by random walk in an association graph is a good approximate solution. Video synopsis [ 24– 27] can generate synopsis video that displays the moving objects at a non-chronological order and can greatly eliminate redundancy in spatial and temporal domain. However, existing video synopsis methods neglect the redundancy in the content domain. Too many observations can degrade the efficiency and compression rate in video synopsis. In this article, we propose a novel video synopsis method, which is based on key observation. The intuitive analysis of key observation selection can be seen in Fig.&amp;nbsp; . We refer to the space–time sequences (observations) of an object as a tube or trajectory. And we will use tube and trajectory interchangeably throughout the paper. As we can see in Fig.&amp;nbsp; a, numerous observations exist in tubes, causing great content redundancy. Through key observation selection, we can obtain a more compact and comprehensive synopsis video than conventional methods, as shown in Fig.&amp;nbsp; b. The framework of our approach is summarized as follows and shown in Fig.&amp;nbsp; . First, necessary pre-processes including background generation and trajectory extraction are performed. Second, trajectory association based on reweighted random walk is adopted to recover the optimal association sets (fusion is a sub-step of association). At last, we use key observation selection method to reduce the content redundancy of the original trajectories, and develop an improved video synopsis. The rest of the paper is organized as follows. Section 2 overviews the related works. Section 3 elaborate the methodology of our work. The experimental evaluation is given in Sect. 4, and we conclude this paper in Sect. 5.    2 Related work  Our work is mainly related to two groups of research domain. One is trajectory association for moving objects across cameras. The other is video synopsis. In this section, we briefly review some related works from both sides. Trajectory association between cameras can mainly be categorized into three classes, namely appearance-based, geometry-based and hybrid approaches. Appearance-based approaches [9, 10, 13, 17, 22, 38] mainly adopt color information individually to match trajectories across cameras. Kuo et al. [17] adopted the multiple instance learning (MIL) to learn a discriminative appearance model in online mode. The spatio-temporal constraints of trajectories observed in two camera views can provide some weakly labeled training samples which include some potentially associated pairs of trajectories and exclude impossible associations. The selected potentially associated pairs have false positives as noise [35]. The appearance relationships between camera views may change dynamically, e.g., the lighting conditions change throughout the day. Therefore, their models need to be updated adaptively. Geometry-based approaches [1, 11, 13, 20] establish association generally by exploiting epipolar geometry, homography correlation and camera calibration. Amato et al. [1] proposed an trajectory association approach using spatial information from multiple cameras, which are represented as consecutive points in a joint ground plane under the world coordinate system. Methods based on pure geometric constraints heavily rely on the accuracy during the correspondence process. Hybrid approaches [2, 7, 10, 29] combine appearance and geometry approaches in association process. Sheikh et al. [29] associated trajectories across multiple airborne cameras using a statistical approach. They made two basic assumptions: (1) cameras are significantly high with respect to the ground and (2) at least one object is covered by two cameras simultaneously at a minimum duration. They posed the problem of maximizing the likelihood function as a &amp;nbsp;-dimensional matching and used the matching result as an approximate association. Du et&amp;nbsp;al. [7] proposed a framework that combines particle filtering and belief propagation to track athletes in team sports between multiple cameras, although some existing trajectory association approaches based on hybrid methods achieve good performance. However, none of previous works had explicitly tackled the noise and outlier during association. Our trajectory association work is inspired by the approaches of [2, 10]. Anjum et&amp;nbsp;al. [2] adopted multiple features, including appearance, motion, spatial distance, etc., in performing trajectory association for partially overlapping cameras. But they neglected the temporal information between trajectories from two cameras. Javed et&amp;nbsp;al. [10] established object correspondence across non-overlapping cameras using motion trend and object appearance. In their work, bipartite graph matching method is adopted to find corresponding trajectories between two non-overlapping cameras. Hamid et&amp;nbsp;al. [9] proposed to use various K-partite graph matching algorithms to solve association problem. And Wu et al. [36] formulated the problem of finding correspondences across multiple camera views as a multidimensional assignment problem and solved it with a greedy randomized adaptive search procedure. However, the existing algorithms belonging to this category are not robust to noise and outlier in association. One main factor is that previous methods [10, 14] always model trajectory association with relatively weak unary and pair-wise attributes, while ignoring the structural or context information among trajectories and cannot obtain an optimal association sets from global view. Our association algorithm is formulated as a Boolean Quadratic Programming, which is a generalization of the classical graph matching problems. Since Quadratic Programming is known to be NP-hard, approximate solutions are required. Our work adopts a novel graph matching in random walk view, related to the Quadratic Programming formulation. The main advantage of quadratic programming is the ability to represent context information between pairs of trajectories and couple them to encourage similar motion. In addition, the reweighted step in our algorithm can strengthen the effect of reliable nodes in random walk by adopting a jump or teleport. Consequently, it can be more robust to noise and outliers. There are mainly two kinds of techniques in video abstraction, namely, key frame extraction [16, 41] and video skimming. Key frame extraction could largely compress the original video, but it loses not only the dynamic nature of original video but also video contents. Video skimming [18, 21, 30], aims at extracting video segments from the original video to obtain a shorter video, which is more coherent and expressive compared with those derived from the key frame-based technique. However, it is likely for people to spend large amounts of time-browsing video segments with little useful information. As an alternative, the space–time video montage [12] analyzes the spatial–temporal information of the input video. However, it achieves relatively low compression rate, not suitable for storage or indexing for large amount of video. Video synopsis [24–28] breaks the previous framework and generates a synopsis video that displays the moving objects at a non-chronological order, while keeping the dynamic aspect of video. Video synopsis can greatly eliminate redundancy in the spatial–temporal domain. Similar to previous work, Peleg et&amp;nbsp;al. [24] generated a more coherent video synopsis based on clustering of similar activities [40]. Following the video synopsis framework, [37] proposed a theoretical method to determine the play time of objects in synopsis video. In existing video synopsis methods, redundancy in content domain is always neglected, which will cause collision in synopsis video. Collisions in synopsis video can lead to the loss of important contents and thus make the synopsis video chaotic. And it will also degrade the quality of synopsis video.    3 Methodology  In this section, we firstly will introduce the trajectory extraction in Sect. 3.1. Then we elaborate the trajectory association and fusion in Sect.&amp;nbsp;3.2. The key observation selection method is given in Sect.&amp;nbsp;3.3, and finally video synopsis for camera network is introduced in Sect.&amp;nbsp;3.4. Let   be a set of   partially semi-overlapping cameras in a typical camera network for large area surveillance shown in Fig. . For clarity, in this paper, we only focus on the trajectory association between two or three semi-overlapping cameras, whose solution is easy to extend to the case of multiple cameras. Let  and  stand for two overlapping cameras, for extracting trajectories from individual camera, we adopt background subtraction [31] followed by a graph-based tracking [32]. Let  represents the resulting  trajectories in camera , and  represents the resulting  trajectories in camera . Each trajectory consists of a set of observations, e.g., , where  represents the time of the observation and  represents the total observation number of the trajectory. We perform trajectory association on the mosaic plane (virtual ground plane). The matrix for transforming image plane to mosaic plane is obtained during panoramic image (mosaic) construction. For each camera, we obtain a representative background image by computing a temporal median over the related background images. Based on the representative background images from two cameras, we construct a panoramic image (mosaic plane) [ 4] as the virtual ground plane. The two homography matrices (  and  ) are estimated by performing bundle adjustment during panoramic image construction. With the homography matrix at hand, trajectories projecting from image plane to mosaic plane are estimated by, eg.:       where   is the projection of trajectory on mosaic plane. However, the projection is prone to cause the corresponding trajectories unaligned in the overlapping region () on the mosaic plane. Additionally, because of noise and errors in the trajectory extraction, existing trajectory association algorithms often fail to correctly associate. To solve this problem, we introduce our robust and efficient graph matching method based on reweighted random walk to conduct trajectories association for partially overlapping cameras. Each camera captures a set of trajectories belonging to different objects. Trajectory association across different cameras tends to solve the correspondence of trajectories among multiple sets of candidates. Given the similarities between pairs of trajectories obtained in different cameras, a most likely assignment problem remains to be solved under the constraint that a trajectory in one camera can match with at most of one trajectory in another camera. In computer vision, graph matching method had been adopted in performing trajectory association for multiple cameras, and it also achieved good performance. In the case of partially overlapping cameras, we need to establish the correspondence between the transformed trajectories   across the overlapping regions on the mosaic plane. Let   and   be two graphs standing for camera   and camera  , respectively. The nodes in each graph are corresponding to trajectories in each camera view across the overlapping regions, as shown in Fig.&amp;nbsp; a. Then a bipartite graph based on two graphs is constructed, and the association problem is to find the optimal matching set in the bipartite graph [ 8], as shown in Fig. b. However, some trajectories may be visible in one camera, but not visible in the other, i.e., trajectory 7. During the creation of bipartite graph (Fig.&amp;nbsp; b), the cardinality constraint must be satisfied [ 8]. This limitation can be overcome by adding virtual node “ ” to the smaller graph with maximum cost associated with their adjacent edges. The dotted edges are out-coming edges of node “ ”. The bold edges stand for the optimal matching set of two graphs. Let  ,   and   are the number trajectories in camera   and camera  .   implies that node   in camera   corresponds to node   in camera   camera  . The graph matching problem can be formulated as an Integer Quadratic Program, that is finding the indicator vector   that maximizes the quadratic score function as follows:            where   is the pair-wise affinity matrix. Since Quadratic Program is known to be NP-hard, approximate solutions are required. The existing graph matching algorithms, such as adopted in [ 10], focused on exploiting relatively weak unary and pair-wise attributes and did not aim at optimizing a well-defined objective function. Different from the graph matching algorithms mentioned above, our work adopts a novel graph matching in random walk view [ 5], related to the Quadratic Program formulation. We construct an association graph   from   and   as illustrated in Fig.&amp;nbsp; . Hence, the original graph matching problem between   and   is equivalent to node ranking or selection on  . To select the nodes in  , we adopt the statistics of Markov random walks. However, some nodes in   correspond to false candidate correspondence. In this case, the normalization of affinity matrix can strengthen the adverse effect of outliers and prevent random walkers. To avoid this case, we adopt a jump during the random walk, i.e., the random walker moves by traversing an edge with probability  . The probability   represents the bias between the two possible actions, e.g., following edge or jumping, as shown in Fig.&amp;nbsp; . This method can strengthen the effects of reliable nodes in random walks, thus is more robust to noise and outliers. For more detailed algorithm information, kindly refer to the algorithm in [ 5]. In our reweighted random walk-based graph matching algorithm for trajectory association, the critical issue is how to construct the pair-wise affinity matrix . In our algorithm, we adopt a novel view-invariant feature [3], appearance feature and spatio-temporal information [39], to construct the affinity matrix. color histograms are used to model the appearance of a trajectory. In our implementation, we use 8 bins for each channel to form a   histogram for each observation. Then we average appearance feature on all observations in a trajectory. So the final appearance feature for trajectory   is:       where   is color histogram feature and   is the total observation number in trajectory  . The view-invariant feature is borrowed from [ 3], whose representation of trajectories is independent of the view point. A trajectory can be modeled as a parametric curve representing the object’s locations over successive frames as  . The centroid distance function is an invariant representation of the raw shape data used in the affine invariant applications. The centroid distance function is expressed by the distance of each point in trajectory from the centroid of the trajectory:       where  ,  . DTW is adopted to compute the distance of trajectories that belong to two cameras in overlapping region. In our algorithm, we build connecting trajectories observed in multiple camera views based on their temporal extents. Let   and   be the starting and ending time of trajectory  . Let   be a positive temporal threshold, which is set at 25 empirically in our implementation. If trajectories   and   are in different camera views, and their temporal extents are close,       then   and   will be connected by an edge in the graph for matching. This means that trajectory   and trajectory   may be the same object since they are observed by cameras around the same time. An example can be found in Fig.&amp;nbsp; . As shown in Fig.&amp;nbsp; a, trajectories   and   observed by two cameras   and   have overlap in temporal extent, so they are connected by an edge graph. As shown in Fig.&amp;nbsp; b, trajectories   and   observed by two cameras   and  , have no overlap in temporal extent but the gap is smaller than  , so they are also connected. The combined distance on appearance feature and view-invariant feature of two trajectories is calculated as:       where   is a normalization term,   an empirically determined coefficient and is set to   in our experiments,   is the   distance of two trajectories in overlapping region. Let   and   be two trajectories in camera  ,   and   be two trajectories in camera  . The node in pair-wise affinity matrix   is computed with above distance as follows:       Taken temporal extent into consideration, if trajectory   and trajectory  , or trajectory   and   trajectory, have no overlap in temporal extent, then       After constructing the pair-wise affinity matrix, the quasi-stationary distribution of this reweighted random walk is efficiently computed using the power iteration method. In the final discretization step, the Hungarian algorithm is adopted to cover the optimal association sets. Once association is finished, the next procedure is to fuse pairs of corresponding trajectories in overlapping regions, and construct a global trajectory across the whole field on mosaic plane. To fuse two corresponding trajectories   and   captured by two cameras, we adopt an adaptive weighting algorithm, in which the trajectory that has more observations and longer length will be given higher weight than other. The weights are calculated as function of number of observations and length for each trajectory:               where   is the number of observations in the trajectory and   is the length of the trajectory. Traditional video synopsis methods [25–27] can reduce the spatial–temporal redundancy in raw video greatly by changing the spatial–temporal relationships among different objects. However, there may be numerous observations belonging to one object, as shown in Fig.&amp;nbsp;a. In typical scenarios, adjacent observations may be similar in action and appearance. In addition, too many observations will tend to cause collision, and negative impaction on subjective effect. Aiming at solving such problems, we propose a key observation-based video synopsis approach for promoting the efficiency of the synopsis, targeting at selecting the representative observations of objects, which can well represent the original activity. In our method, the concept “key observation” is defined as observations with remarkable changes or worthy of being reserved, according to the change of action and appearance. In [33], -means clustering method is adopted in selecting a pre-defined number of key actions. However, the number of key actions cannot be fixed for different objects even in the same scenario. So we adopt a data-driven method in selecting key observations. We try to sample the objects in each tube to maximize the representation of the object moving process. The observations which have significant action or shape change are deemed as key ones, according to our criteria. Different from [16], we extract key observations from every object instead of input video. To explain our algorithm explicitly and effectively, we adopt the definition described in [25]. To simplify discussion, every tube (trajectory) can be denoted as temporal duration , observations are included in this duration, as . Let  and  denote two observations belonging to one tube. We adopt a multiple kernel-based similarity measurement to select key observations. Distance kernel This kernel is used to measure the spatial uniformity between observations, and is defined as:       where dist  can be set as the simple   distance in Euclidean space for the   position of two observations. Motion kernel The simple intuition is, if two observations have similar action direction, they should be similar in motion space. The motion kernel is defined as:       where   measures the motion angle between the two observations. Appearance kernel This kernel is used to measure the change of appearance between observations, and is defined as:       where   measures the KL-divergence between two appearance distributions of observations (  is the color histogram in HSV space). There are multiple ways to associate these kernels together, but which is best is an open problem in machine learning field. In our setting, we adopt the linear combination of the three and found it effective.            The   is the selection criteria in our method.  ,   and   ( \({{\lambda }_1}+{{\lambda }_2}+{{\lambda }_3}=1,{{\lambda }_1},{{\lambda }_2},{{\lambda }_3}&amp;gt;0\)) are three weight parameters, which are learned from our ground-truth data for particular scenario in advance by cross validation. First of all, observations of objects’s entering in and leaving out of camera scope should be regarded as key ones. Then we compute the distance   between observations   belonging to last defined key observation and current observation in one tube. If the similarity is smaller than a pre-determined threshold, then we select the current observation as the key observation. We define key observations as   and non-key ones   as in every tube. And observations with   are extracted to form a new tube, and the time of observations in new tube is organized in a continuous mode. The detailed algorithm is summarized in  . Different from traditional methods of abstraction, the temporal relationship of objects in synopsis will be changed to obtain higher compressive video abstraction, which can display objects appearing in different periods of original video simultaneously. It eliminates the spatial–temporal redundancy of original video. In [27], Rav-Acha et al. proposed the object-based synopsis method for surveillance video. Afterwards, Pritch et al. [25] constructed a framework of webcam video synopsis. First, it extracts the object from tubes (the 3D space–time representation of each object), then formulates an energy function composed of activity loss cost , background consistency cost , time consistency cost , and occlusion cost . Following the above method, we also introduce concepts of collision and time consistency cost. In addition, we combine the key observation selection with video synopsis generation, looking for a temporal mapping   and   that minimize the object function in ( 15). The energy function we formulate is as follows:                         where   and   represent two key observation-based new tubes selected according to   with threshold  ,   and   are two key observation-based new tubes mapped into video synopsis.   is tube and background consistency cost, measuring the cost of stitching objects to the time-lapsed background.   is time consistency cost, preserving the chronological order of objects.   is collision cost, penalizing for the spatial–temporal overlaps among objects.   and   are two empirical parameters set by user.   is observation selection cost, penalizing for the loss of observations in key observation selection.       where   denotes the observations discarded during key observation selection for the tubes appearing in synopsis video, and   are characteristic functions representing the appearance of tube, and defined as:            where   is a pixel in the background image,   the respective pixel in the image, and   is the time in which this object exists.   is activity loss cost, penalizing for the loss of observations during key observation selection. If key observation-based new tubes have not mapped to synopsis video, then   is activity loss cost for original tube. If key observation-based new tubes appear in synopsis video, then   equals  , and only loss of observations in key observation selection exists. In our method,   is set from  , with a step  . Finally, the energy function is minimized using simulated annealing algorithm for every  . And we select the smallest energy loss as best arrangement for synopsis. After we achieve the best arrangement of tubes, we stitch projected tubes into background image using Poisson editing [ 23] to generate final synopsis video.   
Springer.tar//Springer//Springer\10.1007-s00138-013-0520-2.xml:Active tracking and pursuit under different levels of occlusion: a two-layer approach:Reinforcement learning Bio-inspired computer vision Tracking with occlusion Active pursuit Hybrid system:  1 Introduction  Active tracking and pursuit of targets by autonomous systems is a rapidly developing research area. In this paper we present a real-time algorithm for such a system, with a focus on robust reactions to occlusion. We address the problem of tracking targets with occlusions. This problem can be roughly divided into two sub-problems arising from difference in occlusion’s duration. The first sub-problem is overcoming short-term occlusions, i.e., those where the target’s characteristics as observed before the occlusion are still relevant. The second sub-problem is selecting a policy for reobtaining a view of the target when the target had enough time to change its motion pattern behind the occlusion with high probability. Tracking systems must be able to deal with the disappearance and reappearance of targets. Single hypothesis trackers such as Kalman filter-based trackers [1] cope poorly with the disappearance of a target. In [2], for example, a location in the frame that resembles the target is chosen by the mean-shift process. This can cause the tracker to drift away from the target’s real location and prevent it from being reidentified when it next appears. Particle filter-based trackers [3] and trackers that incorporate Sequential Monte Carlo and Kalman filter, e.g. UKF, [4], robustly cope with the problem of target reappearance. However, these multi-hypothesis trackers best suit situations where the target reappears in the current scene. Moreover, these trackers require that the image be large enough to be identified without zooming in. But these requirements are not met in many scenarios, such as when the target moves fast, is far from the camera or when an active tracking system needs to physically pursue the target. The algorithm presented here is designed to cope with these difficulties. We use two layers of reaction to estimate bearing to an occluded target. The first layer, the hybrid filter for active pursuit (HAP), is designed to cope with medium term occlusions. This would include, for example, those for which a known method—such as mean shift combined with a Kalman filter—fails, while the properties of the target’s motion (e.g., velocity and direction) before it disappeared can be considered unchanged. Our design of HAP is a variation on the general concept of the novel approach in [5], where the authors generalized the classic Kalman filter by modeling the arrival of a new observation as a random process. They also found a critical value on the arrival rate beyond which a transition to unbounded error covariance occurs, and they used a random process that indicates, at every time instance, whether the measurement of the Kalman filter is available. However, this indicator has a similar binomial distribution for every . HAP differs from [5] in that we introduce a new random process indicating that the target has been identified and use a binomial distribution with a parameter that can vary over time. This modification arises from the fact that the probability of identifying a target while in pursuit can change over time. The second layer is a decision algorithm, embedded in a learning procedure and based on game theory-related reinforcement which is designed to address long occlusions. It is pursuit-evasion oriented [ 10]. The system decides on reacting by one of the following possible robot reaction policies:        Continue its motion towards the place where the target was last spotted.         &amp;nbsp;           Move towards the predicted target location, using the target’s observed motions prior to occlusion.         &amp;nbsp;           Search for a new target.         &amp;nbsp;      The learning process is based on “trial and error” and is designed to perform adequately with a small number of samples. Thus, we require that        A wrong choice will not reoccur on a given scenario in the future.         &amp;nbsp;           A robot showing poor results (i.e., a low rate of success) will not attempt to pursue and will perform mainly the search policy.         &amp;nbsp;           Each trial should have an effect on the learning process.         &amp;nbsp;      The algorithm will have a data structure that can be easily shared among agents, or sent to a central control of a multi-agent system. Moreover, the learning process is designed with an agent performing tasks according to its skills, so that a successful pursuer should pursue while a unsuccessful pursuer should search for entering targets. These capacities make this system suitable for embedding in a multi-agent system [ 10]. Continue its motion towards the place where the target was last spotted. Move towards the predicted target location, using the target’s observed motions prior to occlusion. Search for a new target. A wrong choice will not reoccur on a given scenario in the future. A robot showing poor results (i.e., a low rate of success) will not attempt to pursue and will perform mainly the search policy. Each trial should have an effect on the learning process. Our method, which is related to a game theoretic approach, is termed “Randomized Prediction with Expert Advice” [6]. “Prediction with Expert Advice” describes a problem where a forecaster attempts to iteratively predict a state so as to minimize a function called “regret” by evaluating the quality of each expert’s advice in comparison to the actual state achieved. In the system, when a target is lost, the recent history of its observed data is saved to a parameter vector, which is considered as a new expert. Its  is the latter vector with an addition of one of the three robot strategies mentioned above. After a policy has been applied and its success evaluated, the policy is reconsidered and the quality of all the advice involved in the decision is reevaluated. The algorithm iteratively creates a mapping from the space of all possible target parameter vectors to the set of policies. The learning algorithm is suitable for systems where hidden Markov models (HMM) [7] are difficult to construct, for example systems that randomly generate a high noise level from sensors and communication. The data structure built by the learning process is simple and suitable for sharing in a multi-agent system. To validate the robustness of the method we implement an algorithm based on this method on  NEMALA, a low-cost, miniature robotic platform designed to function as an agent in a multi-agent system. The possible robot strategies are inspired by the reactions of praying mantises to occlusions. Praying mantises were presented with simulations of prey disappearing on a computer screen. The mantises reacted in one of the following manners:        Fixated on the place where the target had been last observed.         &amp;nbsp;           Continued performing saccadic movements, as if the target was visible and in motion. This behaviour resembles extrapolation.         &amp;nbsp;           Performed peering movements.         &amp;nbsp;      These behaviours were “translated” to the pursuing behaviours of the robot. In this paper we focus on robust estimation of the bearing to the target. We assume that correct estimation of the range to the target is less important because, if high enough velocity is maintained in the direction of the target, it should eventually be intercepted. Fixated on the place where the target had been last observed. Continued performing saccadic movements, as if the target was visible and in motion. This behaviour resembles extrapolation. Performed peering movements.    2 The first layer: a hybrid Kalman filter for bearing estimation  Our control process comprises two parallel sub-processes, one for controlling direction and one for controlling velocity. The system state is represented by the following state vector:       where   is the range from the robot to the target (in cm) and   is the angle (in radians) between the line connecting the location of the robot with the target’s location and the principle axis of the camera. The camera cannot move independently. We assume that   and   are controlled by two independent processes that aim to minimize   and  . We assume that the pursuer’s velocity is higher than the velocity of the target such that if bearing to the target is evaluated correctly, a simple guidance law will lead to its interception. In the simulations, the velocity of the pursuer was constant and higher than the velocity of the evader. We assume that the robot steers to minimize . Control issues are not discussed in this paper as we consider the main challenge of our system to be the estimation of . We follow the approach presented in [5]. The system performs vision-based pursuit as follows: for every new frame we assume that an image-processing stage is performed and that its output   is the evaluated location of the target’s center of mass in the frame (in pixels). We further assume that the principal point of the camera is at  . In our experiments we used the mean-shift tracker for the image-processing stage. We assume that if the estimation of   is correct, then       when FL is the focal length and PS is pixel size, both in centimeters. As mentioned above, we introduce a new random process indicating that the target had been identified; we term this process  . For each time instance  , this process is defined by a random variable we termed  , denoted by  . For example, if obtained data indicate that the target is partially occluded, then the probability of   should be smaller than if data indicate that the target is currently in full view. In the case that HAP fails to reidentify the target, i.e., if the estimated probability of   is smaller than a certain threshold  , the system switches to the second layer. The     is defined as follows: For every  ,            Target identification is evaluated by setting a threshold on a resemblance function   that considers the target’s expected parameters, obtained from a known target model, and the model of the area of  . Let us denote   where   is also a function of  . We assume that   is distributed s.t  . We compute   as the resemblance of the target’s histogram to an initial model colour histogram. In most simulations we used a function of the inner product of the normalized histograms. The robot turns at every iteration to obtain  . We model the dynamics of the   as         .   is a constant that is in direct proportion to the system’s efficiency in keeping the target’s image within the captured frame close to the principal point.  , is a white noise caused by the tracking process:          is a Gaussian distribution with 0 mean and variance  . We assume that   and   are related as follows:       We introduce a dynamic bimodal measurement noise covariance            where  \(r(\lambda _{t})&amp;gt;R\), the variance of the measurement noise in the case that the target is unidentified, monotonically decreases as a function of  . This definition of the measurement noise covariance suits the different characteristics of the two modes (identification and non-identification of a target) in which our system performs. We begin with the following definitions:                         From Eqs.&amp;nbsp;( 4) and ( 6),   and   are jointly Gaussian conditioned on previous measurements and   with            and            The Kalman filter equations will be                         Correction:                         where       As mentioned earlier in this section,if the value of   falls beneath the threshold  , measurements are considered unreliable and the HAP—as ineffective. In the following section we present a decision process for selecting a policy for re-obtaining a view of the target:    3 The second layer: decision using a game theory approach  As noted in the previous section, when the value  decreases below , a decision needs to be made regarding one of the three robot policies described in the introduction. We call this stage the “decision stage,” and the algorithm described in this section  (DEA). The system’s decision depends on a learning process, based on the game theoretic approach known as “prediction with expert advice” [6]. In our algorithm, we keep an evolving set of learned data in a data structure, consisting of a matrix denoted as , and additional variables. In the decision stage, two processes occur: (i) an update of the data structure and (ii) a policy decision. Selected parameters of the available data, such as the target’s last  s and recent history of   are kept as a new row in the advisors’ matrix. We call such a set of parameters  , denoted by  . In addition we add to each row   three variables:     —weight, assigned an initial value 1, indicating the quality of the advice.    —the line number of the advice followed (Eq.&amp;nbsp;18).    —The chosen policy number (1,2, or 3, as defined in Sect.&amp;nbsp;1).  The last two variables are updated after a policy decision has been made. An  ,  , is the set  . —weight, assigned an initial value 1, indicating the quality of the advice. —the line number of the advice followed (Eq.&amp;nbsp;18). —The chosen policy number (1,2, or 3, as defined in Sect.&amp;nbsp;1). Define the advice distance       where   is a positive semidefinite matrix. We define, for the last row, a   ( ), as            The   is the advice index we use to set the current policy, Following the decision stage, the system carries out the chosen policy. The evaluation stage takes place only if one of the active policies (1 or 2) was chosen. Let  denote the frame when the decision was made. A policy choice is called “successful” if the target is reidentified. The advisors’ matrix is updated at this stage only in case of failure. In this case an additional row  added. If the target is successfully identified, the weights are updated as follows:            The latter equations express our wish to strengthen correct advisors. In the case of failure, the weights are updated as follows:            In this case, the policies of   and   are also updated as follows:                         where  \(\alpha \le \beta \le \delta ,\,\epsilon &amp;lt;1\) and  \(\psi &amp;lt;1\). is the “increment” in the weight of a correct policy decision and thus it is the highest.  is relatively large, to ensure that an error will result in a trial of a different policy if a target with similar parameters appears.  is the increment in a weight of a correct advisor.  is a “penalty” for a mistaken advisor.  is small due to the fact that the algorithm produces policy 3 when both active policies have produced failures. If the value of  is smaller, the system will be more likely to choose active pursuit policies 1 and 2. The correct values for these parameters should be set through experiments. The algorithm described above enables rapid decision making upon the arrival of a new target. The obtained advisors’ matrix, via Sect.&amp;nbsp; 3.1, implies the mapping of the possible parameters of the target vector   to the possible reaction policies. We term this mapping the  . We first assume that the size of the set   is finite and second, that a real mapping, maximizing the probability of regaining a target for each set of target parameters, is available. We term this mapping the   ( GT). We define the correctness   of an output mapping   in relation to the ground-truth mapping   as follows:       Then,       The correctness is the optimization function we try to maximize. If the real mapping has a few connected regions, then the algorithm above may produce a mapping that contains too many connected regions, resulting in a decrease in its correctness with time. We address this problem by segmenting the learning process: we pre-define the segment size  . For each   arriving targets, the obtained advisors’ matrix   is stored. Each such advisors’ matrix is weighted as 1 upon its creation and its weight is updated through time in accordance with its success in evaluating policies for each target. A decision as to the policy for a newly arrived target is now made by a weighted voting of the different advisors’ matrices. We term this procedure weighted temporal mappings (WTM).   
Springer.tar//Springer//Springer\10.1007-s00138-013-0521-1.xml:Human interaction categorization by using audio-visual cues:Human interactions Video BOW Audio:  1 Introduction  Given a video clip where there are people interacting between them, the goal of this work is to automatically assign a single category label—from a set of predefined ones—to such human interaction. We address this problem by considering human interactions as an  , i.e., sequence of image frames plus sound (see Fig.&amp;nbsp; ). In Fig.&amp;nbsp;  we can see four scenes of people interacting. In such scenes, there are two people very close with the arms holding the other person. Two of such scenes—extracted from TV Human Interactions Dataset (TVHID) [ 20]—have the label   and the other ones the label  . In spite of the fact that, nowadays, recorded video clips contain not only image but also sound, the current approaches for distinguishing such kind of human interactions only make use of the video pixels, discarding the rich information encoded in the audio signal. The previously presented cases can be clearly ambiguous for a computer if we only take into account the visual information. However, if we focus on the audio signals represented in Fig.&amp;nbsp; , we notice that   and   have different audio patterns. Furthermore, many human interactions have associated very well defined audio-visual patterns—words as   or   are very common during a  —introducing a very clear discrimination with other interactions. Therefore, in this paper we introduce a new approach to deal with the categorization of human interactions by using audio-visual information. Our contribution is twofold: (1) we introduce the use of the audio signal in the challenging problem of human interaction categorization; and (2) we carry out a thorough experimental study on TVHID where it is shown that the combination of visual and audio information offers better results than only using the visual one—as done up to this moment. The rest of the paper is organized as follows. Section&amp;nbsp;1.1 explains some of the more relevant works related to ours. In Sect.&amp;nbsp;2, we introduce the audio-visual model used in our proposal, which is based on the successful Bag of Words model. The experiments and results are presented in Sect.&amp;nbsp;3. The paper is concluded in Sect.&amp;nbsp;4. In recent years, an increasing number of research papers have been published in the context of Human Action Recognition (HAR) in videos. For example, [30] compiled published works within a period of 20&amp;nbsp;years devoted to human actions and activities. In the early years the proposed approaches were tested on  datasets [7, 25, 37], where a single person performed a target action (e.g., walk, jump, hand-wave,...) in controlled scenarios. Soon,  datasets were compiled from Hollywood movies [11, 12, 18], where one or more persons perform a named action in an uncontrolled, and usually cluttered, scenario. A particular case of human actions is . We can distinguish between the interactions performed by a person with an object—as &amp;nbsp;[12] or &amp;nbsp;[6, 39]—or between two or more persons, as  or &amp;nbsp;[20, 24]. Human Interaction Recognition (HIR) in video sequences [17, 20, 23, 24] is a very difficult problem due to several reasons: (a) action performance and camera viewpoint—the different velocities and manners of performing the interaction by the persons in combination with diverse camera viewpoints; (b) imaging conditions—the ever-present difficulties found when working with images from real scenarios (i.e., uncontrolled imaging conditions); (c) non-stationary noise—cluttered and different backgrounds, partial occlusions or diverse person clothing; and (d) relative volume occupied by the interaction—only a very small region of the pixels along with a short number of video frames are related to the event of interest (e.g., the involved hands in ). The latter reason is the one that mostly differentiates realistic human interactions in video with regard to still images or simulated human actions (e.g.,  in Weizmann dataset&amp;nbsp;[7] is very repetitive). In comparison with the task of object/concept categorization on still images, where the area of interest is a large percentage of the image, the HIR problem is clearly much more challenging. Also if we compare with the HAR problem, we see HIR more challenging not only due to higher complexity, but also due to the difficulty of getting large training databases from real scenes. Up to our knowledge, the only existing dataset devoted to human interactions in realistic situations is TV Human Interactions Dataset (TVHID), introduced by Patron-Perez et al. [20]. In [20], the problem of HIR on this dataset is addressed by first detecting and tracking people and, then, by combining head pose estimators with visual local context descriptors [i.e., Histogram of Oriented Gradients (HOG) and Histogram of Optical Flow (HOF) features]. There are papers where the problem of semantic video retrieval is addressed by using only audio features; for example, Bakker and Lew&amp;nbsp;[1] combine local and global audio features to classify sound samples from video into several classes as, for example, , ,  or . Tzanetakis and Chen&amp;nbsp;[31] build audio classifiers to distinguish between , , ,  and  from videos. Bredin et al. [2] approach the problem of content-based video retrieval by combining multiple audio classifiers in a HMM-based framework. In McCowan et al. [19], it is shown how the use of audio-visual events can improve the recognition of group actions in meetings within controlled scenarios. However, we approach the uncontrolled case in this paper. On the other hand, in recent years  (e.g., news, commercials, sports,...) are assigned to videos by using the combination of visual, audio and even textual information [28]. For example, in [8, 14, 21], image (e.g., SIFT, HOG, Gist) and audio (e.g., MFCC, WASF) descriptors are combined by using different approaches for the task of multimedia event detection. Amitha et al. [21] propose and evaluate two types of fusion: (a) training high-level classifiers on the output of previously trained feature-specific classifiers; and (b) learning a linear combination of low-level classifiers. In order to represent multimedia events, Inoue et al. [8] use Gaussian Mixture Models and Support Vector Machines (SVM) to combine audio and visual features. Sidiropoulos et al. [26] introduce the usage of audio in the problem of video scene segmentation. Recently, in Jiang&amp;nbsp;et al. [9] a new challenge for multimedia video classification is proposed. However, the focus is not on human interaction but on event classes. In addition, and as mentioned above, in our case only a short and small part of the signal helps to classify the whole video sequence. Despite these related works, audio has not been exploited by HIR yet, which is one of the main novelties of this paper.    2 Representation model  Inspired in the models used by the  community, Sivic and Zisserman&amp;nbsp;[27] proposed an analogy between the textual words and the visual words (i.e., image region descriptors) with the idea of representing an image (i.e., the ) as an orderless collection of visual words: a  (BOW). In its simplest way, a BOW is equivalent to a histogram  with  bins (i.e., as much as words in the dictionary ), where each bin represents how many times a visual word is present in the target image. In general, the histogram is L1 normalized. The operation of assigning a word to a bin histogram, implies the process of finding the word  that makes minimum the distance between the current word and all the words included in the dictionary. Euclidean distance is a common choice to carry out the word assignment. Although this representation was originally used on images, it was generalized in the recent years to describe video sequences&amp;nbsp;[ 11]. Figure&amp;nbsp;  shows the classical pipeline used to learn representations of human actions: (1) compute Spatio-Temporal Interest Points (STIP) on input video; (2) compute descriptors from STIP (e.g., HOG/HOF); (3) learn a dictionary of visual words from the set of STIP extracted from the training videos; (4) describe the videos by using the STIP descriptors and the previously learnt dictionary; and (5) train a discriminative classifier (e.g., SVM). For a given video sequence, we build different BOWs depending on the kind of  used: visual or audio descriptors. We use the popular -means algorithm&amp;nbsp;[15] to build a dictionary . The goal of -means clustering is to find a partition of the descriptor space in  regions. Each region will be represented by the mean vector of its components. We have chosen the implementation of this algorithm included in VLFeat library&amp;nbsp;[33]. The resulting audio-visual video descriptor will be used as input for a classifier. In our case, we have chosen a Support Vector Machine (SVM) with  kernel, which has shown to be very effective when working with histogram-based representations&amp;nbsp;[34]. Spatio-Temporal Interest Points (STIP) were first introduced by [10] and applied to the problem of recognizing individual human actions (e.g., walk) in video. They propose a  operator to detect  points in the space–time volume. In addition to the  coordinates, each STIP has associated a spatial and a temporal scale  that delimit the video volume where the event of interest happens. An effective alternative to Harris3D operator is a simple . It consists of extracting video blocks at regular locations and scales in space and time, usually, with overlapping. In several problems, this approach has shown state-of-the-art results [36]. The most popular volume descriptors&amp;nbsp;[11] for STIP are Histograms of Oriented Gradients&amp;nbsp;[3] (HOG) and Histograms of Optical Flow&amp;nbsp;[4] (HOF). HOG encodes local appearance, whereas HOF encodes local motion. In order to compute a HOG descriptor, the image volume is divided into a  dense grid of , where each cell will contain a local histogram over orientation bins. Then, at each pixel, the image gradient vector is computed. Each pixel votes into the corresponding orientation bin with a vote weighted by the gradient magnitude. The votes are accumulated over the pixels of each cell. Afterwards, in order to provide illumination invariance, a normalization stage is performed over each  (group of cells). The normalized histograms of all of the blocks are concatenated to build the final HOG descriptor. A similar procedure is used for computing HOF descriptor, but replacing image gradient (spatial) by optical flow. For our experiments, four orientation bins will be used for HOG and five for HOF. In order to use the audio signal in the BOW framework, first we split the audio signal into overlapping   of   seconds (i.e.,  ). An example over an audio signal extracted from a   example is represented in Fig.&amp;nbsp; . Then, we compute on each audio frame a set of descriptors. The simplest descriptor is the  signal per se (i.e., the actual values), which will be used in the experimental section as baseline on audio features. The use of Mel-frequency cepstral coefficients (MFCC) as audio descriptor is a popular choice specially in the fields of speech or music recognition&amp;nbsp;[ 5,  13]. It offers a description of the spectral shape of the audio in a given interval of time. It is computed as follows&amp;nbsp;[ 13]:        compute the Fourier Transform (FT) of the signal;         &amp;nbsp;           map the powers of the spectrum obtained with FT onto the  (i.e., perceptual scale of pitches&amp;nbsp;[29]);         &amp;nbsp;           compute the logs of the powers at each of the mel frequencies;         &amp;nbsp;           compute the Discrete Cosine Transform of the list of mel log powers, as if it were a signal.         &amp;nbsp;      The amplitudes of the resulting spectrum define the MFCC. compute the Fourier Transform (FT) of the signal; map the powers of the spectrum obtained with FT onto the  (i.e., perceptual scale of pitches&amp;nbsp;[29]); compute the logs of the powers at each of the mel frequencies; compute the Discrete Cosine Transform of the list of mel log powers, as if it were a signal. In addition to MFCC, and for comparison purposes, we extract the following set of simple features in the time domain&amp;nbsp;[ 13]:    zero-cross: the number of times the signal changes sign (i.e., crosses -axis);        coefficient of skewness:                  where       is the third-order moment of the data and       is its standard deviation;            excess kurtosis:                  where       is the fourth-order moment of the data and       is its standard deviation;       flatness: the flatness of the data results from the ratio between the geometric mean and the arithmetic mean;   entropy: the relative Shannon’s entropy of the data (i.e., it is divided by the length of the signal).  For each audio frame, the previous features are concatenated into a single feature vector which will describe such audio frame. zero-cross: the number of times the signal changes sign (i.e., crosses -axis); coefficient of skewness:      where   is the third-order moment of the data and   is its standard deviation; excess kurtosis:      where   is the fourth-order moment of the data and   is its standard deviation; flatness: the flatness of the data results from the ratio between the geometric mean and the arithmetic mean; entropy: the relative Shannon’s entropy of the data (i.e., it is divided by the length of the signal). The combination of audio and video information has been previously employed in video categorization&amp;nbsp;[9]; however, its use in the problem of interaction recognition has not been yet deeply explored. The action recognition problem normally involves a single person, and people do not usually speak to themselves while performing actions. On the other hand, interaction involves two or more people, and both visual and audio information plays important role in communication. This work aims at showing that the combination of both sources of information (see Fig.&amp;nbsp;) can yield better results in this problem, than their stand-alone use. Two main approaches in data fusion can be considered,  and . In the first approach, fusion is performed before the classification process takes place. Normally, it consists in joining all the features into a single feature vector. Late fusion, on the other hand, performs first classification of all sources of information separately, and then, fuses the results. Most often, another classifier is trained on the output of the individual classifiers. This work tests both approaches to analyze their performance.    3 Experiments and results  This section explains the experiments performed to validate our proposal. Our goal is to demonstrate that audio information can be employed to improve the classification performance in the HIR problem. To do so, we have first tested the performance of video features. In our work, HOG and HOF features have been tested both separately and together. Statistical tests have been run on the results so as to analyze which combination performs better. Then, we have tested performance of the audio features previously explained. Finally, we have tested the combined use of audio and visual features. Again, statistical tests have been run to analyze the impact of the combination. With regard to the feature combination method, early and late fusion approaches are evaluated, with special emphasis in early fusion. Experimentation has been carried our in the TV Human Interactions Dataset (TVHID) [ 20] which consists of 200 videos from TV shows grouped in four categories: 50  , 50  , 50   and 50  . In addition, a set of 100   videos (i.e., none of the other interaction categories) is included. Figure&amp;nbsp;  contains examples of the four interactions included in TVHID. Note the different imaging conditions (e.g., illumination, scale, background clutter,...) where the interactions happen. Each video clip is labeled with a single interaction class from the possible ones. The dataset provides information about the frame intervals where the interaction happens within each video plus additional information such as the coordinates of upper-body bounding boxes and an approximation of the head orientations.
Springer.tar//Springer//Springer\10.1007-s00138-013-0522-0.xml:Accurate and robust localization of duplicated region in copy–move image forgery:MIFT Matching SIFT Copy–move image forgery Blind image forensics:  1 Introduction  Recent advances in imaging technologies, both in hardware (e.g., digital cameras) and software (e.g., image editing applications), have enabled manipulating digital image contents easily in order to hide or create misleading images with no observable trace [1]. Establishing the authenticity of images, however, is of essence in many applications such as criminal investigation, medical imaging, journalism, intelligence services, and surveillance systems [7, 24]. Recently, the field of digital forgery detection has been introduced to address this issue and has become a very important field in image processing. Digital altering has already appeared in many disturbing forms [1] and there have been several research studies on improving image forgery techniques [25]. These techniques usually include deleting or hiding a region in the image, adding a new object to the image or representing the image information in an incorrect way. Based on the operation used to create a tampered image, techniques can be categorized into three main groups: image retouching, copy–paste (i.e., splicing), and copy–move (i.e., cloning) [2]. Image retouching manipulates an image by enhancing or reducing certain features of the image without making significant changes on image content [ 30]. Image splicing on the other hand utilizes two or more images to create a tampered one. This technique adds a part of an image into another image in order to hide or change the content of the second image [ 2]. Finally, image cloning creates a forged image by copying a certain portion of an image and moving it to another location of the same image in order to conceal or duplicate some part of the image [ 4]. The key characteristic of image cloning is that, since the duplicated region is picked from the image itself, the noise components, texture and color patterns are compatible with the rest of the image. Thus, it is not easy to detect the forgery parts. Moreover, there might be post-processing operations that can even make the exposing procedure harder [ 3]. Figure&amp;nbsp;  shows an example of copy–move forgery. Developing reliable methods for image forgery detection has become an active research topic [7]. Approaches in the literatures can be divided into two main categories:  and  [2, 6]. Active approaches, like watermarking, try to expose digital tampering by adding prior information to the images (e.g., a signature) [2]. Passive or blind approaches, on the other hand, attempt to detect forgeries in images without assuming any knowledge of the original images or adding any prior information to the images. The aim of these approaches is to demonstrate the possibility of detecting forgeries in the absence of any watermark [7]. In this study, our focus is on detecting copy–move (i.e., cloning) image forgery. Among the blind image forgery detection methods proposed in the literature, pixel-based approaches are the most popular; the key idea is exposing image tampering by analyzing pixel level correlations [2]. In general, pixel-based approaches for copy–move forgery detection can be classified into categories:  and  [2]. The key idea behind these methods is discovering and clustering similar parts in an image. In the first category of methods, block matching is used to detect duplicated regions. The simplest approach is using exhaustive search to find and match similar image regions [4]. The main drawback of this approach is high computational complexity and inefficiency. To cope with this challenge, several other approaches have been proposed. In an early approach [4], quantized discrete cosine transform (DCT) coefficients were used assuming overlapped blocks. DCT blocks were stringed in vectors and sorted lexicographically; copied blocks were then detected by finding similar block pair vectors that had an equal offset as well. The method proposed in [5] employed principal component analysis (PCA) to reduce data dimensionality and improve robustness to additive noise. In [1], seven different features were extracted to describe blocks. The first three features were extracted using the average values in the three-color channels. The rest of the features were computed by dividing the block into two equal parts in four directions and finding the proportion of the first part to the sum of the two parts. Sorting the vectors and finding similar blocks were carried out next, similar to other methods. In [6], nine normalized features in overlapped blocks were extracted. These features were obtained by computing the ratio of sub-averages and total average in each block. Radix sort was applied instead of lexicographic sort. In [7], a blur invariant representation for each overlapped block was employed to extract the feature vectors of each block. Besides, PCA was used to reduce the number of features and kd-trees to find similar blocks. In [8], discrete wavelet transform (DWT) and singular value decomposition (SVD) were used in order to reduce the dimensionality of images before sorting the vectors lexicographically and checking for duplicates. A similar approach was presented in [3] and then completed in [9] where DWT was employed to reduce image dimension and the phase correlation was used to detect duplication zones. In particular, the copied blocks were distinguished at the coarsest level of DWT and verified in finer levels. In [10], a new technique was introduced using one-level DWT. The low-frequency sub-band was selected as a low-dimensional image and the diagonal detail coefficients were considered as a resource to estimate noise in each part of the image. It was assumed that interesting blocks have similarities in the low-frequency band and dissimilarities in the diagonal detail band, which are in fact noise. An extension of this method can be found in [39]. The methods described above perform block matching to detect copy parts in forged images; however, they rarely consider large variations in scaling, rotation and illumination, very common operations in image manipulation. To overcome this issue, a different category of approaches in copy–move forgery detection try to emphasize the use of feature matching for detecting forged regions in images. The method presented in [ 11] employed local statistical features, known as scale invariant feature transform (SIFT) [ 14]. Since, SIFT features are invariant to changes in illumination, rotation, and scaling, looking for similar features in an image could reveal potential image forgery [ 12]. Huang et al. [ 11] adopted this idea to detect image forgery. Using SIFT features for image forgery detection has been adopted in several other studies including Pan and Lyu [ 12] and Amerini et al. [ 13] where the authors used almost similar techniques to find similar features and potentially interesting areas. An affine transformation between matching regions was estimated using Random Sample Consensus (RANSAC). The method proposed by Pan and Lyu [ 12] includes a verification step which tries to locate the duplicated regions using the normalized correlation map and thresholding. The steps of this algorithm are summarized in Fig.&amp;nbsp; . As shown in our experimental results, a weakness of Pan’s method, as well as similar methods [11, 13], is that they cannot localize the forged region very accurately. Moreover, these methods were evaluated on a relatively small number of real forged images. In this study, we improve on copy–move forgery detection using keypoint-based features (e.g., SIFT) by focusing on the issue of accurate detection and localization of duplicated regions. Specifically, we have made several contributions in this work. First, we employ mirror reflection invariant feature transform (MIFT) features [20] instead of SIFT features for finding similar regions in images. MIFT features share all good properties of SIFT features but are also invariant to mirror reflection transformations. Like in other approaches, we find similar regions by finding corresponding MIFT features and estimate an affine transformation between them using RANSAC [26]. Corresponding MIFT features define the initial detection window. Second, since the quality of the affine transformation computed is critical in localizing the duplicated region accurately, we refine the parameters of the affine transformation iteratively by increasing the detection window slowly and computing new MIFT features. Third, to extract the duplicated region, we use dense MIFT features and apply hysteresis thresholding [12] instead of standard thresholding, to reduce false positives and negatives. To further reduce false positives and negatives, we apply morphological operations on the low and high hysteresis thresholded images and combine the results. We have evaluated the performance of the proposed methodology by performing a comprehensive set of experiments using a large database of real images (i.e., CASIA v2.0). To better understand the strengths and weaknesses of the proposed method, we have analyzed independently the effects of different types of transformations (e.g., rotation, scale, reflection, blur, deformation) as well as combinations of them. Comparisons with competitive approaches show that the proposed method can detect duplicated regions in copy–move image forgery more accurately, especially when the size of the duplicated regions is small. The rest of this paper is organized as follows: Sect.&amp;nbsp;2 briefly reviews the problem of local feature extraction. Section&amp;nbsp;3 describes the steps of the proposed approach in detail. Section&amp;nbsp;4 presents our experimental results and comparisons. Finally, Sect.&amp;nbsp;5 concludes our work and discusses directions for future research.    2 Local feature extraction  Recently, significant research has been performed on extracting local invariant features with application to object recognition and categorization [14], image matching and retrieval [15] and video mining. The goal of local feature extraction methods is to find interest points (or keypoints) and to define a distinctive descriptor for the each of them which is invariant to transformations such as scale, rotation, or affine. More precisely, the key characteristics of these methods are their distinctiveness, robustness to occlusion and clutter, and light invariance [16]. Methods for finding local features can be classified as sparse or dense. Sparse methods compute a descriptor for each keypoint by selecting a small patch around it. Dense methods, on the other hand, do not extract any keypoints explicitly but select a small patch around each pixel and compute a descriptor [17]. The descriptors typically are defined in the form of a vector of measured values inside the patches. These image measurements can emphasize different image properties like pixel intensity changes in a region or curvatures. Different algorithms have been proposed for keypoint extraction. Among them, the Harris corner detector is one of the most popular algorithms for extracting keypoints invariant to translation, rotation, and partially to illumination [18]. The SIFT, proposed by Lowe [14], extracts a sparse set of keypoints using a similar algorithm. The method involves four main stages: scale-space extrema detection, keypoint localization, orientation assignment, and keypoint descriptor [14]. The descriptor produced is a normalized 128-element vector for each extracted keypoint [14, 18]. Due to the success of SIFT, many studies have attempted to improve its performance both in terms of accuracy and time complexity. PCA-SIFT [23] is a variation of SIFT, which projects gradient images to a lower dimension using PCA. Histogram of oriented gradients (HOG) [19] employs normalized local histograms to build the descriptor. RIFT [35] is a rotation-invariant extension of SIFT which is constructed using circular normalized patches divided into concentric rings of equal width. Rotation invariance is achieved by measuring orientation at each point relative to the direction pointing outward from the center. Speeded up robust features (SURF) [36] speeds-up computations using a fast approximation of the Hessian matrix and “integral images”. These methods, however, cannot handle reflection. Mirror reflection invariant feature [ 20] generalizes SIFT by producing mirror reflection invariant descriptors. The resulted descriptor is invariant to mirror reflection as well as to other transformations such as affine. In general, mirror reflection can be defined in the horizontal or vertical direction as well as a combination of both directions. As explained in [ 20], to handle mirror reflection, it is adequate to deal with the horizontally or vertically reflected images. While the traditional SIFT approach uses a fixed order to organize the cells, making the descriptor sensitive to mirror reflection, MIFT reorganizes the order of the cells and restructures the order of orientation bins in each cell. We have adopted MIFT descriptors in this work to find duplicated regions with or without mirror reflection. Figure&amp;nbsp;  shows an example using SIFT and MIFT in the case of mirror reflection. As mentioned earlier, dense descriptors are extracted at each pixel location and are usually used in texture and background classification [21]. Local binary pattern (LPB) [33, 34], HOG-LBP [22], and Weber local descriptors (WLD) [17] are some popular methods in this category. In this paper, we use dense MIFT features for extracting the duplicated region more accurately.    3 Method overview  The key objectives of the proposed approach are (1) to recognize copy–move manipulated images, (2) to classify images as forged or non-forged, and (3) to accurately locate the duplicated region in the tampered images. Since in copy–move image forgery a part of the image is copied and pasted on another part of the same image, finding similar parts in an image is the key idea explored here as well as in other studies. This is accomplished using feature extraction methods (e.g., SIFT) to extract and match local features from various regions of the image in order to find similar regions. Figure&amp;nbsp;  illustrates the main steps of our approach. Keypoint extraction and matching is the first step of our method; we have experimented both with SIFT and MIFT features for comparison purposes. The next step involves finding corresponding features which allow us to find similar regions and estimate an affine transformation between them. The affine transformation parameters are later refined which allows for localizing the duplicated regions more accurately. To extract the duplicated regions, we employ dense MIFT features, instead of standard correlation, along with hysteresis thresholding and morphological operations for reducing false positives and negatives. Copy–move image forgery detection requires detecting the duplicated region in a single image. Feature extraction algorithms find correspondences between similar regions in the same image. As described in Sect.&amp;nbsp;2, SIFT is a powerful technique, which extracts features invariant to scale, rotation, and brightness. However, SIFT descriptors are not invariant to mirror reflection. To account for this issue, previous approaches proposed extracting SIFT descriptors from horizontally and vertically reflected versions of the original image [12, 13]. In this paper, we have adopted MIFT features which are invariant to reflection. As described earlier, each keypoint is characterized by a feature vector that consists of a set of image statistics collected at a local neighborhood around the keypoint. In general, to find matching keypoints between images, the keypoints from one of the images are first indexed using a kd-tree [ 32]; then, the matching keypoints from the other image are identified. Due to the high dimensionality of the feature vectors, the search within the kd-tree is performed using the “best bin first” search algorithm [ 31,  32]. In our case, since we search for duplicated regions in a single image, we divide the image into smaller parts and compare the descriptors among them. The search is performed outside a small window centered at the detected keypoint to avoid finding nearest neighbors of a keypoint from the same region [ 12]. Once a matching candidate has been found, it is accepted as a distinctive matched point if the ratio of the distances from the first and second nearest neighbors is smaller than the threshold [ 14]. This threshold can vary from zero to one; a threshold closer to zero yields more accurate but fewer matches. Here, a low threshold is utilized since it reduces false matches. Figure&amp;nbsp;  shows an image and the extracted keypoints. Using the keypoint correspondences from the previous step, an affine transformation is estimated. The transformation can be used to verify whether two regions correspond by mapping one region to the other. To eliminate incorrectly matched keypoints before estimating the affine transformation parameters, a pre-processing step is applied using some simple geometric constraints. To further remove incorrect matches, the affine transformation parameters are estimated using RANSAC [26] which can estimate the model parameters with a high degree of accuracy even when a significant number of errors are present. Corresponding keypoints extracted during matching can lie in different parts of the image. Since in copy–move image forgery a region of an image is duplicated, matching keypoints should lie within two regions; we refer to this as the “location” constraint. Moreover, if we were to connect with lines corresponding keypoints in those regions, then the lines formed should have similar slopes; we refer to this as the “slope” constraint. We take advantage of these two geometric constraints to eliminate incorrect matches between keypoints. To apply the “slope” constraint, we find the slope of all lines connecting corresponding keypoints and cluster them in different groups. The group with the largest number of keypoints is selected as the main group. Then, we compare all other groups to the main group and eliminate any group having a different slope (i.e., within a threshold) from the slope of the main group. Next, we apply the “location” constraint on the remaining groups by eliminating groups containing a small number of correspondences as well as removing corresponding keypoints from groups if the keypoint locations are rather far (i.e., within a threshold) from the average keypoint location of the group. Figure&amp;nbsp;  shows an example of corresponding keypoints before and after removing mismatched keypoints. Although the geometric constraints described in the previous section can be used to eliminate many incorrect matches, they cannot eliminate all of them as it is evident from Fig.&amp;nbsp;b. To further remove incorrect matches, we apply the RANSAC algorithm [26]. RANSAC is a simple, yet powerful parameter estimation approach designed to cope with a large proportion of outliers in the input data. In essence, RANSAC is a resampling technique that generates candidate solutions using the minimum number data points required to estimate the underlying model parameters. This algorithm estimates a global relation that fits the data, while simultaneously classifying the data into inliers (points consistent with the relation) and outliers (points not consistent with the relation). Due to its ability to tolerate a large fraction of outliers, RANSAC is a popular choice for a variety of robust estimation problems [27, 28]. Using RANSAC, the affine transformation is calculated iteratively by selecting three or more non-collinear keypoints from all possible pairs. The affine transformation is then estimated based on these nominated points. The accuracy of the parameters is examined by classifying all available corresponding keypoints into inliers and outliers. Considering the fact that the estimated transformation should map more keypoints to their correspondences with a smaller error, the accuracy of the transformation is estimated by finding how well the keypoints map to their correspondences. If the difference of a mapped point and its correspondence is less than the threshold, then it will be selected as inlier; otherwise, it will become an outlier. As mentioned earlier, the algorithm calculates the transformation iteratively and selects the parameters that yield the largest set of inliers. Figure&amp;nbsp;  shows an example of applying RANSAC on the matches found from the previous step. As it can be observed, RANSAC was able to find a highly accurate set of correspondences. The affine transformation matrix computed by RANSAC can be used to locate the duplicated region in the image. However, to estimate the duplicated region more accurately, we apply one more step to further refine the affine transformation parameters. The purpose of this step is to refine the affine transformation parameters estimated from the previous step. As Fig.&amp;nbsp;  shows, there are cases where the correspondences selected as inliers do not cover well the region of duplication. Thus, the estimated affine transformation is not precise enough to map the whole duplicated region to the copied region. In this step, we refine the affine transformation parameters iteratively, by slowly increasing the search window around the corresponding regions. Figure&amp;nbsp;  shows the main steps of the refinement process. Given a pair of corresponding regions, first we define a detection window for each region using the inliers found by RANSAC (see Fig.&amp;nbsp; ). The detection windows are then slowly resized (i.e., horizontally and vertically). Then, keypoints are detected inside the resized windows and RANSAC is applied to find a new set of inliers. The new inliers are used to re-estimate the affine transformation parameters. Repeating these steps, the affine transformation parameters are refined iteratively until the number of inliers does not increase anymore. Figure&amp;nbsp;  shows an example with five iterations. The number of correspondences and inliers at each iteration are shown in Table&amp;nbsp; 1. As it is evident from the example, the iterative process yields more correspondences, covering a larger area inside the original and duplicated regions; this yields a more accurate affine transformation. It should be mentioned that the threshold used for finding corresponding keypoints during the iterative process is greater than the one used in the initial step. This allows finding more correspondences compared to the initial stage. The last step of our algorithm attempts to accurately locate the duplicated region. Cross-correlation has been used before to locate the duplicated region and verify similarity with the original region [12]. In this study, we detect the duplicated region using dense MIFT features. To detect as many pixels as possible inside the duplicated region, we employ dense MIFT features. The key idea is computing a MIFT descriptor at each pixel location inside the detection window instead of at the keypoint locations only. This is on contrast to traditional methods employing pixel correlation for finding the duplicated region. Since MIFT descriptors can be matched more accurately than using pixel correlation, the duplicated region can be detected more precisely. Other dense feature descriptors, such as LBP or WLD, could be employed at this stage. Using the estimated affine transformation, the correspondences between the original and forged regions can be computed for each pixel location. The similarity between corresponding locations is then calculated using dense MIFT descriptors. Thresholding the distance between corresponding MIFT descriptors can then reveal the duplicated region. Figure&amp;nbsp;  shows an example using this process. Using a single threshold to determine the similarity between corresponding MIFT descriptors in the original and duplicated regions might compromise detection results. In this work, we have opted for using hysteresis thresholding [37], a process based on two thresholds, one low and one high, which takes into consideration spatial information. Hysteresis thresholding has been used before in the context of edge detection [37]. The high threshold is used to detect “strong” edges while the low threshold is used to fill in gaps between “strong” edges using “weak” edges. The key idea is to include edge points whose strength exceeds the low threshold but are also adjacent to “strong” edge points. In a similar manner, we use the high threshold to detect “strong” corresponding pixels, that is, corresponding pixels from the original and duplicated region having very similar MIFT descriptors (i.e., very likely to belong to the duplicated region). Additional pixels (i.e., “weak” pixels”) are detected if they are adjacent to “strong” pixels and the distance between the corresponding MIFT descriptors in the original and duplicated regions exceeds the low threshold. In our experiments, the low threshold is chosen to be  times lower than the high one, where  is a parameter. The output of the previous step is a group of pixels, which might still include holes or contain isolated pixels. To deal with these issues, we apply morphological operations (i.e., dilation and erosion) to remove small holes and eliminate isolated pixels. These operations are applied separately on the images obtained using the high and low thresholds described in the previous section. Then, we simply combine the results to obtain the final duplicated region.    4 Experimental results  In this section, the performance of the proposed approach is analyzed through a comprehensive set of experiments. For comparison purposes, we have also compared our method with the method of Pan and Lyu [12].
Springer.tar//Springer//Springer\10.1007-s00138-013-0523-z.xml:Analysis of object description methods in a video object tracking environment:Tracking assessment Descriptors Computer vision Appearance models Video object tracking:  1 Introduction  The automatic tracking of objects has been gaining importance in recent years. However, everyday situations still present complex problems within the scope of research activities. There are innumerous research efforts targeting different aspects of the problem and a vast number of published work in international journals, conferences and workshops. Tracking multiple objects, and in particular humans, is a difficult problem presenting many challenges, especially if it occurs in non-controlled environments as in everyday scenarios. In these situations, algorithms or tracking systems must deal with factors, such as coverage of large areas, group movement, partial or total occlusion, shape deformation, fast changes in direction, illumination variations and shadows, among others. The main steps of a video object tracking (VOT) framework can be summarised as: object detection, often based on background/foreground segmentation; object description, where different cues (e.g., appearance, shape) are extracted to uniquely characterise each object; definition of correspondences for each tracked object throughout the video sequence. Consequently, the representation of objects and how effectively it discriminates between different instances is a key issue [16]. Several techniques have been proposed, each with its strengths and weaknesses, but without a generally accepted method. The appearance model receives and provides information from and to other modules of a VOT framework. As a result, its performance will depend on previous processing stages and will affect those that succeed it. For example, by estimating the position of an object in the next frame it is possible to reduce the uncertainty that the appearance model must resolve; lower discriminative capabilities of the appearance model may cause track drift or even track loss (inability to detect an object being tracked or erroneous identity assignment to the tracked object). The stand-alone comparison of descriptors has been the subject of several studies&amp;nbsp;[7, 24, 25, 31, 39] and is not within the scope of this paper to make an exhaustive review or benchmarking. Rather, recognising the importance of these techniques to visual tracking we also found the necessity, and a logic evolution, to perform this analysis from a tracking solution point of view. This will enable a better understanding of the impact on the overall tracking. To validate our proposal, we compare a set of widely used description methods in a common object tracking solution. Video object tracking strategies are still highly related to the application scenario. Even though the concepts and strategy proposed in this paper can be applied to different tracking scenarios, we present a proof of concept by assessing state-of-the-art descriptors in a people tracking solution. Furthermore, we also put forward some ideas for appearance models. The remaining of this paper is organised as follows. A brief overview of state-of-the-art description methods and their use in video object tracking algorithms is presented in Sect.&amp;nbsp;2.1; in Sect.&amp;nbsp;2.2 we present a brief summary of the results from a stand-alone comparison of the descriptors. Our proposal is described in Sect.&amp;nbsp;3. It includes the main concepts, suggested metrics and datasets. In Sect.&amp;nbsp;4, we describe a set of experiments intended to serve as proof of concept for the proposed assessment approach. The results and conclusions are presented in Sects.&amp;nbsp;5 and&amp;nbsp;6, respectively.    2 Object descriptors  Ideally, an object’s description should enable its discrimination from other objects during the tracking process. However, this is hard to achieve in a real scenario. Towards this goal, two main approaches can be found on the literature: (1) based on the analysis of histogram responses; and (2) based on the extraction and matching of local feature descriptors. Following the formulation that objects’ appearance and shape, within an image, can be described by its distribution of colour, intensity gradients, or edge directions, histogram analysis has been widely studied. Interesting results have been reported using colour appearance and colour histograms in single camera tracking, but poor performance was achieved in multi-view scenarios. As a result, increasing attention has been given to the research of edge or gradient based features as an alternative or complement to colour based descriptors&amp;nbsp;[1]. Local feature descriptors have been widely used in different areas including video object tracking and image retrieval. These descriptors are commonly computed at key points of an image, which are salient patches that contain rich local information about the image [17]. High repeatability of the key points is desirable since it expresses the reliability of a detector for finding the same physical key points under different viewing conditions. The feature vector used to represent the neighbourhood of the key point should be robust (invariant) to noise, detection displacements, and geometric and photometric deformations. It has been recognised that the dimension of the descriptors has a direct impact on the computation time [4]. Although descriptors of lower dimensions are desirable for fast key point matching, they are in general less distinctive than their high-dimensional counterparts. The Scale Invariant Feature Transform (SIFT) key point detector and descriptor proposed by Lowe&amp;nbsp;[23] is one of the most well known methods to determine local descriptors that are invariant to changes in scale, rotation and translation. The detector generates a great number of key points compared to other detectors, but the extraction process tends to be slower. Furthermore, the high dimensionality of SIFT descriptors has significant impact on the matching step. Ke and Sukthankar [19] applied Principal Component Analysis (PCA) to decrease the dimension of the vector. Although the resulting vector enabled a faster matching, Mikolajczyk and Schmid&amp;nbsp;[25] proved that it is less distinctive than SIFT and the PCA slows down the feature computation. Gradient location-orientation histogram (GLOH)&amp;nbsp;[34] is another variant of SIFT using a log-polar binning structure instead of four quadrants. In the study by Mikolajczyk and Schmid&amp;nbsp;[25], GLOH slightly outperforms SIFT, but the use of PCA has a negative impact on the computational weight&amp;nbsp;[19]. Bay et al. [4] described a fast scale and rotation invariant key point detector and descriptor which they named SURF (Speeded-Up Robust Features). It shares many similarities with SIFT, but with performance gains due to the approach followed on the detection of key points and on the matching process. Fast Invariant to Rotation and Scale feature Transform (FIRST), proposed by Bastos et al.&amp;nbsp;[3] in the context of Augmented Reality and Computer Vision, relies on the detection of local maxima Shi and Tomasi corners to define key points location. Each feature’s intrinsic scale factor is determined by selecting the multiplication factor that maximises the luminance average around the key point, using integral images for fast indexation. Similar to SIFT, FIRST uses an orientation histogram to make the descriptor rotation invariant. However, instead of recording the histogram itself, the final patch is rotated by the maximum bin value found. Since FIRST uses  non-normalised patches, matching is performed through Normalised Cross Correlation, which formulation is largely simplified by describing features through a DoG (Difference of Gaussians). In order to accelerate the matching step on large feature databases, DoG patches are clustered based on its sum of all pixels sign (positive or negative). Each patch is evaluated in 6 distinct regions (3 vertical regions and 3 horizontal regions) resulting in a 6 bits string which enables fast cluster creation and matching problem reduction. Reported results indicate that FIRST algorithm is faster than SURF, although yielding slightly less repeatability. Methods using key point selection and local descriptors have produced interesting results. However, in some cases, such as their application to objects with large smooth regions, the number of selected points may be insufficient for a successful matching process&amp;nbsp;[1]. Dalal et al.&amp;nbsp;[12] proposed a new human detector based on a grid of Histograms of Oriented Gradient (HOG). Unlike descriptors such as SIFT, these are computed on a dense grid of uniformly spaced cells. It has been shown that HOG is insensitive to colour variation. The HOG descriptor has a high dimensionality, thus requiring a large amount of memory storage&amp;nbsp;[18]. In addition, it has difficulties with the effective representation of objects or backgrounds with large smooth regions since the contours are indistinctive. Usually HOG presents high sensitivity to rotation transformations. Most of the base invariant descriptors presented, being local based key point descriptors such as SIFT, SURF and FIRST, or being based on histogram analysis such as colour histograms and HOG, fall into the same problem formulation: the need to identify corresponding properties between different images taken from the same scene under different conditions, such as presence of affine transformations (rotation, scale), changes in noise, image blurring, and luminance conditions variation. This ability is usually denoted as repeatability. The above-mentioned techniques share some shortcomings. It has been recognised that in image patches of reduced dimension or with little texture it may not be possible to extract an effective description of the objects&amp;nbsp;[1]. Moreover, these descriptors disregard spatial information, which has been identified as an important feature to increase the object identification rate&amp;nbsp;[28, 33, 38, 43]. Histogram approaches are typically faster than common invariant descriptors, but ignore spatial and shape information. Han et al.&amp;nbsp;[16] proposed the combination of colour histograms and HOG descriptors arguing that the computation is efficient and the combination of these two features can effectively represent an object because they complement each other. Jiang et al. [18] used the HOG people detector to initialise the bounding box (BB) of pedestrians and colour histograms to describe each object. Rather, than extracting a single histogram, the BB was divided into a lower and upper part. A colour histogram was computed for each part, thus incorporating spatial information. During the tracking process, different weights were used for the upper and lower parts based on the argument that the lower part is more likely to become occluded. Tang and Tao&amp;nbsp;[35] used SIFT descriptors in conjunction with a graph-based model to increase the robustness to occlusion. However, the matching efficiency tended to deteriorate with the increase of the number of objects or the complexity of the model (number of key points). In&amp;nbsp;[36] machine learning methods, the bag-of-visterms (BoV) technique and SIFT to classify object instances in multi-camera scenarios. Other methods have been proposed to represent objects namely human objects. In&amp;nbsp;[41], the authors propose modelling the human shape using skeleton decomposition. The human shape is subdivided iteratively into elementary disks. Following the same approach, in [42] it was presented an inter-frame interpolation method. The shape model was updated with partial changes to the skeleton decomposition model, which was built based on the interpolation of the input and output frames. The authors claimed that better performances could be achieved with only a small complexity overhead. The mean-shift algorithm has been widely used for visual tracking [6, 22, 46] and colour-based mean-shift has been identified as a fast and effective algorithm for tracking colour blobs, but sensitive to large “distracters”. Zhou et al.&amp;nbsp;[46] used mean-shift in combination with the SIFT technique to improve tracking. The SIFT features were used to match a region of interest across frames and mean-shift applied for a similarity search through the use of colour histograms. Shahed et al. [32] tracked a deformable object using rectangles with a minimum overlap and colour information extracted for each box. In [1], SIFT and SURF were tested in a cascade of region descriptors. Many different studies have performed the independent evaluation of descriptor methods&amp;nbsp;[24, 25]. In Bastos et al.&amp;nbsp;[3] a framework simulating the complete pipeline of images 3D viewing and projection was proposed following the same evaluation metrics. As a result, it was possible to generate and render images under different viewing variations, namely scale and rotation, as well as under different perturbations, specifically, luminance, blurriness and random noise changes. The authors also analysed the extraction and matching times per key point for the descriptors. The studies with this benchmarking framework showed that SIFT is typically superior to FIRST and SURF in terms of repeatability and distinctiveness. Exceptions occurred in the presence of noise and luminance variations, which suggests that the DoH (Determinant of Hessian) adopted in SURF is less sensitive to these perturbations than key points extracted in SIFT via DoG (Difference of Gaussians). Considering perspective transformations tested, the computation of Sobel derivatives to find key points’ dominant orientation in SIFT and FIRST proved more robust than the Haar wavelet responses used in SURF. Since FIRST key points do not have a scale-space representation, the repeatability across scale changes is considerable smaller than in SIFT, even though is similar to SURF. Regarding extraction and matching times per key point, SURF outperformed SIFT in both measures, while FIRST presented the best computational performance. In terms of quality, SIFT key points are highly distinctive and accurate, while SURF is the most error-prone algorithm, but has the highest matching rate due to the large number of key point extracted. The authors also performed the tests for colour histogram and HOG algorithms, which are not based on local key points. Due to the nature of these methods, the evaluation was made by measuring the average chi-squared () distance between the histogram created from an original image, and the histogram created from the transformed or perturbed image. Colour histogram proved to be more robust than HOG to all invariance tests, but at a higher computational cost. A noticeable result of these stand-alone tests was the need for different evaluation approaches due to the nature of the methods.    3 The proposed integrated analysis strategy  The main concepts of the proposed assessment approach are described in Sect.&amp;nbsp;3.1. While the proposed concepts are horizontal to different tracking contexts, in this paper we focus on people tracking scenarios. In Sects.&amp;nbsp;3.2 and 3.3, we describe a set of suggested metrics and datasets applicable to this context. In the specific context of video object tracking, the performance of the appearance model and the corresponding descriptor technique is not independent of other modules. Rather, it is influenced by the preceding modules and affects those that succeed it. In Fig.&amp;nbsp;. This architecture focuses on common tracking modules. Upon receiving images of a stream, some pre-processing can be performed, for example to reduce noise in the image or change the colour space. An initial step of the tracking solution consists of the detection of the objects of interest, in this case people, and the initialisation of the corresponding models (in our particular case the focus is on the appearance model). Once the objects are detected in the current frame it is necessary to define correspondences with previous detected objects; this process makes use of the information stored in the appearance model. The final step consists of the update of the models with the new observed information. Modules for higher level processing using the output of tracking may exist, but they are not relevant to this proposal. In this paper we propose a new paradigm for the assessment of object description techniques for video object tracking, with the individual description methods being integrated in a common tracking solution and the evaluation performed upon the tracking results. This approach takes into account the interdependencies among the modules of the system and is performed from the tracking point of view, i.e., it uses the tracking output. To our knowledge, such approach has never been attempted. The underlying framework must be prepared to accommodate the integration and test of different appearance models. Throughout the experiments, only the extraction of the object appearance and the matching between two individual instances should be model specific. All other stages of the algorithm must be common. The appearance models are to be encompassed in modules with a common and generic application programming interface (API). Each of these modules should filter and store the required information and be responsible for performing all model specific information, namely initialisation, comparison and update. The evaluation of tracking results is by itself a research problem with several open issues, such as: what factors to evaluate? What information is available? What metrics to use? The criteria used in the evaluation of the tracking algorithms should be appropriate to the application scenario. This has resulted in the use of different features (e.g., as objects’ trajectory, silhouette or assigned identifier) and application of different metrics (e.g., trajectory root mean square error, detected and reference region overlap, identity consistency). Proposals for evaluation frameworks and metrics already exist, but they have not been generally adopted by the research community&amp;nbsp;[2, 5, 8, 13, 15, 27, 30]. Some of these approaches focus on evaluation without comparing with a reference (commonly known as ground truth, GT), but the results provided typically lack sufficient discriminative information. Consequently, evaluations based on comparisons with GT are commonly favoured, but test sequences are not accompanied by this information. To objectively evaluate the impact of the different representation models in the tracking solution we propose two complementary strategies, intended to provide greater flexibility. The first consists of a set of metrics proposed by Bashir et al.&amp;nbsp;[ 2] to summarise evaluation results for a complete sequence. Specifically, the object paradigm was used, which implies a previous alignment of reference and detected tracks. These will be referred to as ‘object metrics’. The use of these metrics was motivated by its use, or of slight variations, in other papers of the literature such as&amp;nbsp;[ 5,  14,  20,  40]. From the proposed set of metrics in&amp;nbsp;[ 2] we selected:                                      TG is the total number of ground truth tracks; true positives (TP) consists of the number of tracked objects mapped to a GT track; similarly, false positives (FP) are the number of tracked objects without a correspondence in the GT and false negatives (FN) the number of GT track without a tracked object mapped to it. The tracker detection rate (TRDR) and false alarm rate (FAR) characterise the tracking performance of the algorithm, while the detection rate (DR) measures the sensitivity of the algorithm. The fragmentation error rate (FER) was also considered to indicate how often a track label changes; it consists of the average number of detected tracks per GT track (tracks paired with a GT track). Ideally, the FER value should be one, with larger values reflecting poor tracking and trajectory maintenance. Note that for the first two metrics, TRDR and DR, we wish to obtain high values, while for metric FAR low values are desirable. The computation of these metrics requires the existence of reference information for the sequences. Specifically, the metrics are calculated using information of the bounding boxes enclosing each object. This requirement limits the test sequences that can be used. The second evaluation strategy consists of the hybrid framework described in&amp;nbsp;[9, 10], which enables the computation of an error metric for every frame of the sequence, i.e., it can also capture the temporal evolution of the error. Moreover, it has been demonstrated&amp;nbsp;[8–10] that this framework: (1) enables the use of different types of GT; (2) this information is not required for the complete sequence. The output of this framework will be referred to as ‘hybrid metric’. Due to the described properties of these metrics, we suggest their use, particularly in a context of people tracking. Nevertheless, other metrics can be selected and, for different application contexts, that can be a requirement. The important point is that the evaluation is done using the tracking output. Moreover, different application scenarios may require the maximisation/minimisaton of a specific metric or subset of metrics. In addition, we propose the computation of several time measures in each experiment to assess the impact of the descriptor and appearance representation in the algorithm execution and the corresponding computational weight. These consist of: the average frame processing time, which we will refer to as SPT (sequence processing time), in seconds per frame; the average descriptor extraction time (DET), in seconds per track; the average descriptor matching time (DMT), in seconds per track. For each track, the values for DET and DMT were averaged over the number of iterations performed to define object correspondences. The evaluation should be performed using datasets representative of the scenario and accessible to researchers, thus favouring replication and comparison of results. For our particular demonstration scenario we selected sequences of two widely used datasets: CAVIAR project [ 11]; PETS 2006&amp;nbsp;[ 29] workshop. Specifically, we used the sequences: OneShopOneWait1 (OSOW1) and OneShopOneWait2 (OSOW2) from the CAVIAR project; ST1-C1/cam3 (ST1C3) and ST1-C1/cam4 (ST1C4) from the PETS 2006 workshop. These sequences are representative of monitoring and surveillance scenarios depicting commonly observed problems: group movement; shape deformation; appearance similarity; occlusion; object crossing. Also, they were captured with dissimilar cameras and offer different perspectives over the scene. Figure&amp;nbsp;  depicts illustrative frames of the sequences. The sequences of the CAVIAR project have half-resolution of the PAL (Phase Alternating Line) standard (  pixels, 25 frames per second) and were compressed using MPEG-2; the resolution of the PETS sequences are PAL standard (  pixels, 25 frames per second) and were compressed as JPEG image sequences (approx. 90&amp;nbsp;% quality). The sequences have associated different types of ground truth (GT). The CAVIAR project provided reference information, in the form of bounding boxes (BB), for every sequence of the dataset; this was provided in an XML file following the Computer Vision Markup Language (CVML) syntax&amp;nbsp;[21]. This BB GT information was also mapped to the corresponding image mask&amp;nbsp;[10]. In addition, for the sequences OSOW1 and OSOW2 reference segmentations were manually generated for a subset of the frames. Sequences ST1C3 and ST1C4 are not provided with reference information applicable to this context (provided GT consists of a list of events), thus reference segmentations were manually produced for a subset of frames.    4 Proof of concept 
Springer.tar//Springer//Springer\10.1007-s00138-013-0524-y.xml:Dynamic random regression forests for real-time head pose estimation:Random forests Classification Head pose estimation Real time Regression Head localization:  1 Introduction  Recently, estimation of the human head pose has become an intriguing and actively addressed research topic, inspired by the increasing demands of many human-head-related computer vision applications, such as security systems and human–computer interaction. Over the last decade, great effort has been dedicated to develop efficient, accurate, and robust algorithms for this problem. Nonetheless, most of the approaches suffer from the flaws in terms of efficiency, accuracy, or robustness against partial occlusion and the variations of head pose, illumination, and facial expression. It is with the aim to seek an approach for head localization and pose estimation that we propose a novel solution, the dynamic random regression forests (DRRF). As one of the recent developments of the conventional random forests (RF) [ 1], the DRRF is a real-time algorithm for locating the human head and estimating its pose simultaneously. Figure   shows the overview of this algorithm, which consists of an offline training stage and an online testing stage. In the offline training stage,   tree-shaped classifiers (the forest) are hierarchically constructed using a set of training data. The forest is able to classify the data and estimate the head pose. In the online testing stage, for each depth data, all the   patches acquired by a sliding window are fed into each of the classifiers to give a total number of   intermediate outcomes, which are finally integrated to generate the final result of head position and orientation. The contributions of the proposed approach can be summarized as follows.        A boosting strategy is proposed for data induction, where the prediction ability of existing trees is evaluated and the data with larger prediction error has more possibility of being sampled for building the next tree. Due to this strategy, the learning quality is clearly upgraded.         &amp;nbsp;           The key parameters of the binary test of each node are optimized using a dynamic method, which strikes the balance between computational cost and accuracy. The resulting optimum binary test improves the regression and classification performance of the DRRF.         &amp;nbsp;           A stem operator is integrated into the conventional tree-shaped classifier to avoid the premature termination of node split and hence increase the possibility of optimum data split.         &amp;nbsp;           A weighted voting scheme is applied to estimate the head location and pose in a more comprehensive manner by giving more weight to intermediate outcome with higher quality for the final integration of head location and pose.         &amp;nbsp;      The remaining of this paper is organized as follows. Previous works regarding to the head pose estimation are presented in Sect.  2. After that, the proposed DRRF is described in detail in Sect.  3, which is followed by Sect.  4, where a number of comparative experiments are performed to demonstrate the DRRF’s efficiency, accuracy, and robustness. Finally, the paper concludes in Sect.  5. A boosting strategy is proposed for data induction, where the prediction ability of existing trees is evaluated and the data with larger prediction error has more possibility of being sampled for building the next tree. Due to this strategy, the learning quality is clearly upgraded. The key parameters of the binary test of each node are optimized using a dynamic method, which strikes the balance between computational cost and accuracy. The resulting optimum binary test improves the regression and classification performance of the DRRF. A stem operator is integrated into the conventional tree-shaped classifier to avoid the premature termination of node split and hence increase the possibility of optimum data split. A weighted voting scheme is applied to estimate the head location and pose in a more comprehensive manner by giving more weight to intermediate outcome with higher quality for the final integration of head location and pose.    2 Related work  The methods for head pose estimation using 2D image, as elaborated in [2], can be categorized into two groups, the feature-based methods and the appearance-based methods. In the feature-based methods, the head pose is inferred from the extracted features, which include the common feature visible in all poses [3], the pose-dependent feature [4], and the discriminant feature together with the appearance information [5]. In the appearance-based methods, the entire face region is analyzed. The representative methods of this type include the manifold embedding method [6], the flexible-model-based method [7], and the machine-learning-based method [8, 9]. The performance of both kinds of methods may deteriorate as a consequence of feature occlusion and the variation of illumination, owing to the intrinsic shortcoming of 2D data. Generally, the appearance-based methods outperform the feature-based methods, because the latter rely on the error-prone facial feature extraction. Since the depth image provides the information on the distance between the viewpoint and the surface of scene object, head pose estimation using depth image theoretically gives better results than the approaches using 2D image. On the other hand, nowadays, the 3D image sensors which can provide high quality depth image at affordable cost have become prevalently available, such as Microsoft Kinect and ASUS Xtion, and therefore find increasingly widespread applications pertaining to head pose estimation. The methods to estimate head pose using depth image include the reference-based methods and the frame-by-frame methods. The reference-based methods [10, 11, 28] start with initializing head position and orientation information in a reference frame. After that, two processes, updating the values of the tracking parameters and re-estimating head position and orientation, are alternately carried out. The loop terminates when the propagated error exceeds a tolerance value, which is followed by a re-initialization step. Processing only the region of interest is prevalently employed in the reference-based method, because it is more efficient than processing the entire region. By taking advantage of the small pose shift between two consecutive frames, the reference-based methods usually display good accuracy. Nonetheless, for the initialization step or re-initialization step, these methods require the information of frontal or near frontal face which may not be always available; and the propagated error may degrade the performance. The frame-by-frame methods are of two types, the feature-based methods and the appearance-based methods. In the feature-based methods, head pose is evaluated by utilizing the 3D information of facial features, which are determined according to the facial landmarks, such as nose tip, eye corners, etc. Naturally, the performance of these methods largely relies on the availability and quality of facial feature information. Specifically, if the facial features are extracted accurately, the constraints fairly designed for these methods can be adopted to identify the head pose successfully; otherwise, if the facial features are unfortunately invisible as a result of partial occlusion, or if the extracted facial feature information is not accurate enough, a failure in head pose estimation will occur as a consequence. For a brief historical review, a succinct summary of important works are worth mentioning. Malassiotis and Strintzis [12] used the 3D head blob to build the principal curvature maps to locate local minima as facial feature candidates. Subsequently the knowledge of facial symmetry is used to select nose tip position and estimate head orientation by approximating the nose ridge with a 3D line segment. Chang et al. [13] and Sun et al. [14] located eye cavities, nose saddle, and nose tip mainly in accordance with curvature information, and then estimated head pose by making use of the knowledge of facial symmetry. The high performance computing power of GPU was employed by Breitenstein et al. [15] to realize head pose estimation in real time. They first compute shape signatures according to the geometrical knowledge to generate nose candidates and pose hypotheses. Afterwards, each pose hypothesis is evaluated by comparing a set of pre-computed generic face templates and choosing the orientation with the minimum error. The design of their system takes account of how to handle large pose variation, partial occlusion, and the variation of facial expression. Their later step was to extend their work to a system [16] in a similar working manner with low quality depth data acquired by passive stereo image. The appearance-based methods use facial appearance as representation and do not rely on the information of facial features. Bleiweiss and Werman [17] proposed model-based method to evaluate head pose by iteratively fusing color and depth data; this method, however, is only applicable to frontal face images. To estimate head pose from multiple views, Zabulis et al. proposed a method in [18]. In this method, the estimation procedure after face detection consists of computing the textured visual hull of the subject and unfolding it on a hypothetical sphere, with the parameterization being iteratively rotated to move the face eventually to the equator. The system presented by Cai et al. [10] is founded on the linear deformable model which provides information on both static and action deformations. In this system, head pose is calculated by using regularized deformable model fitting algorithm with emphasis on handling noisy depth data. In the two algorithms proposed by Fanelli et al. [19, 20], the problem of head pose estimation is formulated as a regression problem owing to the capabilities of high generalization power, fast execution speed, and easy implementation of the RF theory [1]. The common principle of these two algorithms is to first build a non-linear mapping from various inputs to continuous outputs based on the result of depth image analysis, and then head pose is integrated by multiple intermediate outcomes which are generated by regional image patches. Although the testing using high quality scanner data verifies the good performance of the algorithm in [19], this method is applicable in the case where it is unrealistically assumed that the head is the only object in the field of view. In contrast, a more reasonable consideration, which can distinguish the face from other body parts or background, is presented in [20], and experiments are conducted to test the method using low quality depth data. These two methods are robust to illumination variation, partial occlusion, and facial expression, and exhibit real-time performance with normal CPU without recourse to GPU.    3 Dynamic random regression forests  The DRRF algorithm comprises two stages, i.e., the offline training and online testing. The offline training is basically to construct multiple tree-shaped classifiers, which involves data induction, construction of tree-shaped structure, and optimization of parameters. In the online testing, the intermediate outcomes generated by the trees are integrated to provide the final result. The essence of the DRRF is to model the problem of head pose estimation as a regression problem, and to build a non-linear mapping of a set of depth images into a set of data regarding the location and orientation of head. The mapping is accomplished by a set of tree-shaped classifiers (the forest), which, as reported in [1], shows better performance and is less prone to over-fit than an individual classifier. The whole procedure of the offline training is described as follows. The construction of the tree-shaped classifiers is characterized by the rule that each tree is built independently and is arbitrarily added to the existing trees. Nonetheless, the study in [21] reports that redundant trees in the existing forest may deteriorate the performance and a well-selected subset of forest is able to outperform the original forest. In order to avoid this situation, study on data induction has been made. The tree-shaped classifiers are built using data selected form the data pool. Usually, the bagging strategy [1] is adopted for data selection; in this strategy, data is sampled randomly from the entire data pool with replacement until enough samples are collected. In other words, data has uniform probability of being sampled in bagging strategy. As supported by the noticeable success of work [22], the boosting strategy is considered as a better way of data induction than the bagging strategy. The underlying idea of the boosting strategy is that the prediction error of data can be acquired by evaluating the existing forest, and that the data with larger prediction error should be identified and assigned with more likelihood of being sampled when building the next classifier. This strategy associates the tree-shaped classifiers, which would have been independent of each other if the bagging strategy were applied, and thereby enhances the quality of data induction and the performance of classifiers. Specifically, except for using the bagging strategy to build the first tree, an evaluation of the current forest is undertaken prior to data induction, so as to estimate the prediction error of data. As a matter of fact, in this evaluation, the regression ability is estimated by the present forest composed by all the trained trees, and more attention is paid to samples with significant prediction errors which are more likely to be sampled for future training. After the evaluation, data with re-weighted probabilities is sampled with replacement to build new tree-shaped classifiers. The algorithm of the boosting strategy is shown in Algorithm&amp;nbsp;1. As regards the boosting strategy for data induction, three facts are particularly worth emphasizing.        The out-of-bag data (the data in the training dataset but not sampled for training current classifier [1]) is selected to be the internal evaluation data for the on-growing forest. This choice makes efficient use of the data pool in that it averts the need of assigning an exclusive subset of the data pool for validation; otherwise, datasets with shrunk size are used for training and testing purposes. Furthermore, since the set of out-of-bag data is different for each classifier, the diversity of data for prediction is improved, which results in a more comprehensive assessment of the current forest and thus better performance of the classifiers.         &amp;nbsp;           The object of assessment should be the present forest rather than the latest tree built, partly because that the tree to be built should fit as well as possible the current forest instead of one specified tree, and partly because that iterative estimation of the on-going forest keeps abreast of forest construction and controls the growth of trees in a global view.         &amp;nbsp;           The weight of data is proportional to its prediction error. As an instance of the extreme case, if one data has been inducted for the construction of every tree-shaped classifier and the prediction error estimated by the uninvolved trees of current forest is invalid, the new weight for such data must be assigned with the minimum probability among all the updated weights.         &amp;nbsp; The out-of-bag data (the data in the training dataset but not sampled for training current classifier [1]) is selected to be the internal evaluation data for the on-growing forest. This choice makes efficient use of the data pool in that it averts the need of assigning an exclusive subset of the data pool for validation; otherwise, datasets with shrunk size are used for training and testing purposes. Furthermore, since the set of out-of-bag data is different for each classifier, the diversity of data for prediction is improved, which results in a more comprehensive assessment of the current forest and thus better performance of the classifiers. The object of assessment should be the present forest rather than the latest tree built, partly because that the tree to be built should fit as well as possible the current forest instead of one specified tree, and partly because that iterative estimation of the on-going forest keeps abreast of forest construction and controls the growth of trees in a global view. The weight of data is proportional to its prediction error. As an instance of the extreme case, if one data has been inducted for the construction of every tree-shaped classifier and the prediction error estimated by the uninvolved trees of current forest is invalid, the new weight for such data must be assigned with the minimum probability among all the updated weights. The head region is defined as the region of the depth image where the head is present, and the non-head region is defined as the remaining portion of the depth image. The positive patches are the patches whose centers are located in the head region, and the negative patches are those whose centers are located in the non-head region. As illustrated in Fig.  , after data induction, the positive patches with fixed dimensions are randomly extracted from the head region, and labeled with the corresponding real-valued information  , where   represents the 3D offset from the ground truth of head position to the center location of extracted patch, and   signifies the ground truth of head orientation. Negative patches with the same dimensions are extracted similarly from the non-head region without labeling. Figure  b shows the possible overlaps between positive and negative patches. These overlaps are in fact useful for the distinction between head region and non-head region, because patches with overlapped parts will force the forest to identify the boundary of the head. The basic structure of a classifier is a tree consisting of node operator and leaf operator, as diagrammatically illustrated in Fig.  a. The node operator in a tree-shaped classifier acts as a binary tester, where data is passed to the left or the right child of the node according to the result of the test; the leaf operator can be interpreted as informative storage describing a cluster of annotated data. The binary test in each node involves feature channel selection and value comparison, which acts the important role in the construction of classifier. In our binary test, only the depth data is analyzed, which is computationally economic and practically effective, as reported in the work [ 24]. In the binary test described in Inequality  1, a threshold value is compared with the difference between the averages of the two subpatches, which is better than the values of two single pixels [ 25],            where   represents the patches in current node;   and   are the subpatches (see Fig.  c);   and   are the pixel numbers of subpatches with valid depth information; and   denotes the threshold value. If a patch satisfies Inequality  1, it is passed to the right child; otherwise, to the left. At each node, the binary test is optimized by choosing the values for the test parameters from a pool of candidates randomly generated. To this end, an objective function must be designed to quantify the performance of the test in terms of classification and regression, and the optimal parameter values which can maximize the objective function are chosen for the binary test at the node. Similar to the works [ 20,  29], the accuracy of classification, viz. the ability to discriminate between head and non-head patches, is expressed as the classification objective function,       where   is the probability of patches   belonging to class   and   represents the number of patches. The uncertainty of classification is minimized, when the information gain formulated as Eq.  2 is maximized. The accuracy of regression, which can be interpreted as the ability to get continuous outputs from various inputs, is expressed as the regression objective function [ 20,  30],            where   and   are the covariance matrices of location offset and pose orientation, respectively. The uncertainty of regression is minimized, when the regression information gain shown in Eq.  3 is maximized. A number of objective functions for the binary test have been proposed to take account of the accuracy of both classification and regression, for example, a weighted linear or exponential combination of the classification objective function and regression objective function. Among all these solutions for building the objective function, the interleaved method is chosen, because it is easy to implement, requires less effort for parameter tuning, and gives accuracy comparable to that of other methods [20]. According to the interleaved method, the object function is randomly chosen from the classification objective function and the regression objective function with equal probability, except for at the last two depth levels where the accuracy of regression needs to be emphasized and therefore the regression objective function is used for the binary test. The remaining problem pertains to the efficiency of the dynamic binary test. Extremely randomized trees [ 23] boosts performance by picking a node split with very thorough trials. This approach, however, may cause overwhelming computation during the process of the tree construction, particularly at the first several depth levels, where the quantity of patches to be split is huge, and binary test with fixed trial number requires a huge computational cost of optimization process. Instead, the dynamic binary test aims to achieve a more efficient data split by increasing the number of iterations for parameter generation, including location and size of subpatches and threshold, in proportion to the tree depth with different coefficients. By this strategy, the patches are split roughly at the beginning levels, and are divided more finely at deeper levels. The detailed dynamic binary test is shown in Algorithm&amp;nbsp;2. The flow of patches moves from the root down to the leaves. At each node, they are separated according to the optimized binary test. Finally, when either minimum number of patches or maximum tree depth is satisfied, a leaf is created, and the flow of patches stops there. The leaf stores the class information which is the ratio of the number of positive patches to that of all patches, and the mean and variance of location offset and head pose of only positive patches. As mentioned earlier, the optimal parameters of a binary test are obtained by comparing only a subset of the collection of all the possible candidates. This might, especially when building the first several levels of the tree, give rise to the potential risk that the process of node split is likely to terminate prematurely, which means that abnormal failure of data split with patch number less than the minimum requirement happens due to the limited trial of binary tests, and is followed by distorted patch flow and inappropriate data distribution over the leaves. As one variant of the conventional RF, the fern-shaped classifier proposed in [26] outperforms the RF only for the keypoint recognition application. Nonetheless, the regression ability of the fern-shaped classifier is clearly insufficient to solve this problem due to the lack of optimizing the binary test. In order to resolve this problem, a solution, which is capable of improving the accuracy of regression, is proposed. As shown in Fig.  , the solution consists of inserting a new kind of operator, i.e., stem operator, into the tree-shaped classifier, which is conventionally made up of only two types of operators, i.e., node and leaf. The stem operator provides more binary test trails to further improve the performance of the node split. Specifically, when there is split failure, the stem operator is repeatedly applied on the same bunch of patches for no more than a predefined number of iterations until the optimal split is found; otherwise, this node is terminated by a leaf. The pseudo code of the proposed solution is given in Algorithm&amp;nbsp;3. The additional trial for optimizing the binary test is more essential for the nodes close to the root, because they are more liable to suffer abnormal split failure. For the nodes at deeper levels, however, a node split ended with a leaf is reasonable, and does not incur much computational cost because of the relatively smaller number of patches. Hence, the stem operators at deeper levels act as an intermediate unit which merely transfer the data. The classifiers with stem operator are of various maximum depths ranging from  to , so they are able to classify data in a more discriminative and regressive manner. The DRRF derived from the offline training is utilized in the online testing stage, where the classification and regression are executed in parallel for a given depth image within the duration of a single image scan. In the first place, the pending patches are densely extracted by a sliding window, as illustrated in Fig. b. Subsequently, all these patches are input into the DRRF, where every patch will have to travel through every tree of the DRRF. A patch’s journey through a tree begins at the root of the tree and ends at a leaf, and inherits the information stored in this leaf. At any node, the patch is evaluated by the optimal binary test of the node and then passed to either the left or the right child of the node in accordance to the result of evaluation. The information in the leaves needs to go through a two-stage integration (coarse integration followed by fine integration), for which only the qualified leaves are permitted to vote. The coarse stage aims at obtaining the initial central positions of partitioned votes. In the fine stage, the iterative mean shift technique [27] using a spherical kernel is applied to generate better centric locations of clusters. In order to go a step further to distinguish head from the outliers or noises, only the cluster containing more than a certain number of votes is declared as head, and all the votes of this cluster is integrated to generate the final result, where the means of  and  are the outputs of head location and pose respectively. To procure better performance of the probabilistic prediction, the quality of the information in leaves must be evaluated and filtered according to two criteria, i.e., data variance and class ratio. Since the leaves with high variance are not informative, they should be discarded. The class ratio measures the usability of leaf information. If the ratio is higher than 0.5, positive patches are more than negative ones; otherwise negative patches dominate. In [ 20], only leaves with class ratio being 1.0 are preserved, which implies that only the leaves containing exclusively positive patches are counted. This setting is so excessively stringent that some useful leaves are unnecessarily removed and wasted, and that the regression ability is therefore weakened. To avoid these consequences, a weighted voting scheme is proposed to replace the voting method in [ 20]. In the light of the new voting scheme, besides the requirement of small data variance, a leaf is classified as a qualified vote, as long as the positive patches occupy a dominant proportion of all the patches; the proportion is used as the voting weight. The proposed weighted voting scheme not only makes fuller use of the trained forest but also estimates the head location and pose in a more comprehensive manner. In the following procedures of integration, the centroid of a cluster   is determined using the following equation,       where   is the weight coefficient;   is the vote distribution; and   is the vote amount in cluster  .   
Springer.tar//Springer//Springer\10.1007-s00138-013-0525-x.xml:Multimedia event detection with multimodal feature fusion and temporal concept localization:Fusion Classification Multimedia Machine learning:  1 Introduction  Multimedia content is being produced and shared through the Internet (e.g., YouTube) at an unprecedented pace in recent years. For example, just on YouTube, video data is currently being uploaded at the rate of approximately 30 million hours a year. Accordingly, the need for automatic tools that organize and retrieve videos has become crucial more thanever. In this paper, we examine the task of multimedia event detection (MED), where the goal is detecting or classifying video clips by the main event occurring in the clip. In particular, we focus on high-level events such as ‘making a sandwich’, ‘parkour’, and ‘grooming an animal’, where the event is defined by complex collection of elements including people, objects, actions, sounds, scenes, and their spatial/temporal relationships. Automatic understanding of visual content in unconstrained Internet videos is a very challenging task, particularly because they contain very diverse content. Even videos from the same event class (e.g., ‘making a sandwich’) exhibit significant intra-class variation: they are captured under a variety of camera conditions (e.g., pixel quality and motion); they are of highly variable duration and often heavily edited (e.g., shot stitching and added captions); they are typically not choreographed and do not follow a particular structure; the evidence indicating the presence of a particular high-level event typically occurs during specific segments of a video and is cluttered amid extraneous content introduced by diverse editing. Due to the importance of the MED task, many state-of-the-art systems have been developed and reported by world-class researchers recently [18, 21, 23, 30, 39, 48–50, 59]. The main observations drawn from the success of these systems are consistent. Most systems extract a large number of multimodal features from both visual and audio signals. The features include low-level features such as SIFT, HoG, and MFCCs which provide significant portion of performance for MED. In addition, the use of pre-defined concept detectors for objects (e.g., person and cars) and actions (e.g., jump and punch) are also frequently incorporated. Then, the videos are classified by fusing features where score fusion is often used to combine scores independently computed from different subsets of features. In this paper, we present a video event classification system which addresses various core challenges for MED task. Overall, our system is designed to incorporate many of the above-mentioned success principles in its architecture. Figure&amp;nbsp; a shows our system architecture: it extracts large array of multimodal features at multiple granularities; multiple base classifiers are built from different subsets of features; finally, score fusion combines multiple scores and improves the MED performance. In particular, our work incorporates novel developments into the system, which can be summarized into three major contributions (highlighted in red boxes in Fig.&amp;nbsp; a). First, our work developed and incorporated novel features at diverse granularities, aiming to provide more semantic understanding capability into our system. The hierarchy of granularities and their relationships towards the modeling of complex events is illustrated in Fig.&amp;nbsp; For example, our work explores the use of  concept features, which are detected based on low-level features. The mid-level features include (1) visual Object Bank (OB)&amp;nbsp;[34] which provides 177 object detectors for pre-defined classes, and (2) a large array of audio event detectors [6] based on low-level MFCC features. Furthermore, this work developed novel  visual scene concepts which are discovered from the OB response distributions in an  manner, which are effective in discovering semantically consistent visual scenes. Note that this is distinct from more conventional approaches based on using pre-defined concept classes. Finally, all these features are  used to model complex events, where the strengths and limitations of each are exploited. Second, we present a novel approach for weakly supervised semantic concept selection based on a Latent SVM (LSVM) formulation [13, 56], which not only improves classification but also provides a novel scheme for . This approach addresses the temporal clutter issue mentioned earlier, where salient audio-visual evidence is buried amid less relevant content. In particular, this approach effectively utilizes “high-level” concepts mentioned earlier. For example, mid-level concepts correspond to general objects (e.g., tree, hat, computer, people) that may appear across a wide range of events. In contrast, “high-level” concepts capture a higher level of semantic information that is more specific to the event occurring in a particular video (e.g., skateboarding in garage, surfing on water). In detail, under the proposed approach, high-level concept classifiers are trained in an unsupervised fashion, without requiring expensive manual annotations. Then, our approach automatically identifies the classes and temporal locations of discriminative high-level concepts for a given target event class, which are both treated as hidden variables, effectively solving a weakly-supervised learning problem. Specifically, we show how the model parameters can be learned using Latent SVMs [13, 56], a max-margin discriminative method that provides the ability to model unobserved variables during training. Through evaluation, the proposed method shows substantially superior performance beyond conventional approaches. Furthermore, we show that the capability to localize salient video segments provide a novel and effective summary of retrieval results. Third, we present two novel approaches [26, 35] to  score fusion functions which combine scores from different base classifiers. These fusion learning algorithms provide strengths under different circumstances and potentially boost performance beyond existing approaches. In this work, they are thoroughly evaluated to combine scores from a large array of 21 base classifiers, and their performance compared with existing approaches. In addition, we present our methodology to generate cross-validation splits to learn fusion classifiers, which is designed to maximize the utility of limited training data during fusion learning. Finally, we present the rationale for the  fusion methodology incorporated in our system. For example, our evaluations indicate that there is no clear winner among the compared fusion methods and performance gaps between different methods can be more than trivial. Accordingly, multiple fusion approaches are tried per target event class during training, and our system selects the best approach case by case. The proposed event classification system has been extensively applied to the challenging TRECVID MED 2011 dataset [42], which consists of training exemplars for 10 high-level event classes (listed in Table&amp;nbsp;2) and provides a large test data archive of about 32&amp;nbsp;K video clips. The evaluation results of our full system as well as the results from modular evaluation of key algorithmic components highlight the benefits of the proposed system. The rest of the paper is organized as follows: Sect.&amp;nbsp;2 reviews the related work in multimedia event detection. Section&amp;nbsp;3 describes the overall system architecture. In Sect.&amp;nbsp;4, a set of features incorporated in our system are introduced. In Sect.&amp;nbsp;5, diverse approaches to build base classifiers are described, where Sect.&amp;nbsp;5.2 particularly focuses on the newly developed Latent SVM formulation. Sect.&amp;nbsp;6 presents our framework for fusion learning, along with two new score fusion learning algorithms. Finally, Sect.&amp;nbsp;7 presents the evaluation results along with various discussions on various interesting observations.    2 Related work  Previous work in the area of video detection and retrieval primarily considers the task of query-based video retrieval using concept detectors. The problem of video classification can be viewed as a subproblem of query-based video retrieval, where the query is pre-defined. A recent and thorough survey of content-based video retrieval techniques can be found in [19]. We discuss some of the closely related video retrieval/classification methods here. In the past, research on video retrieval has largely focused on the use of pre-specified lexicons of concepts such as LSCOM [18] and MediaMill [49]. These lexicons typically cover a wide range of concepts including scenes, people, objects, activities, and events at different levels of abstraction. These lexicons, however, do not account for the fact that the appearance of a concept can vary from one event class to another. For example, people may dress differently depending on the environment they are in (e.g., snow vs. beach). These differences become more noticeable when using concepts at higher levels of abstraction because of an increased semantic gap [14]. This implies that concepts which represent a higher-level of abstraction depend more on the event category and consequently have limited reusability among different events. A second challenge when using human specified lexicons is that the concepts may not reflect the true corpus found in the data. To address this challenge, Bao et al.&amp;nbsp;[4] proposed the use of a bipartite graph propagation model to incorporate both human specified concepts as well as implicit concepts extracted by Latent Dirichlet Allocation for video retrieval. Another approach taken by Feng et al.&amp;nbsp;[14] constructed a so-called “universal” object detector using a combination of concept detectors with small semantic gaps. In [21], Jiang et al. demonstrated the benefit of leveraging both high and low-level features for multimedia event detection. In the present work, we model the events using various low-level features which are extracted from the given set of videos itself. We also include a set of object models independent of the events, which can help in capturing some semantic information. In addition, a weakly supervised semantic concept detection is employed to retrieve semantic information specific to the target events. Along with video retrieval/classification tasks, the problem of providing a description of the important evidence that a video clip or an image contains an instance of a semantic category has been steadily gaining prominence in the computer vision community. For example, a number of papers have been proposed to leverage latent topic models on low-level features [5, 8, 44, 55]. To date, the most common approach to such lingual description of images has been to model the joint distribution over low-level image features and language, typically nouns. Early work on multimodal topic models by Blei et al.&amp;nbsp;[5] and subsequent extensions [8, 15, 44, 55] jointly model image features (predominantly SIFT and HOG derivatives) and language words as mixed memberships over latent topics with considerable success. Other non-parametric nearest-neighbor and label transfer methods, such as Makadia et al. [38] and TagProp [17], rely on large annotated sets to generate descriptions from similar samples. Alternatively, a second class of approaches directly seeks a set of high-level concepts, typically objects but possibly others such as scene categories. Prominent among object detectors are Object Bank (OB) [34] and the related deformable parts model (DPM) [13] which have been successful in the task of annotating natural images. While this paper focuses on the problem of multimedia event detection, our LSVM method also additionally enables discriminative summary by temporal evidence localization for the target events. It is noted that the proposed scheme of temporal evidence localization, represented with discriminative video segments for an event class of interest, are different from those considered for the TRECVID MER tasks [42, 43], which are based on lingual description of key evidence. When discriminative clusters of LSVM model are manually named, the temporal evidence localization can be further extended to provide semantic textual description as well. When addressing the problem of event detection, we can benefit from exploiting different modalities including video, audio, and textual channels. In [22], Jiang and Loui extended upon a method that considered concept detection in 10-second video segments to construct an event recognition system that operates on an entire video using a global bag of “audio-visual grouplets” representation. In our work, we compute an array of features to represent visual as well as audio channels. However, unlike [22], we do not group the features together at an early stage, but adopt a late fusion scheme to combine the scores from individual base classifiers, which is called . There exist numerous score fusion methods, which can be grouped into three main categories. The first category can be understood as  fusion where fixed rules are applied regardless of actual base classifier score distributions, prior to simple score summation. As one of the pioneering works, multiple classifier combination rules are studied in [28], where extensive experiments showed that Sum and Product are top two best performing methods. In recent work [50], the geometric mean is reported to be highly effective despite its simplicity. Both product and geometric mean can still be understood to belong to the first category where a logarithm transformation is used prior to summation. While simplicity is the main advantage, as also reported in [50], more sophisticated fusion methods can often outperform them at the expense of additional computation. The second category of late fusion methods [20, 40, 47] are formulated within a score normalization framework, where particular assumptions are made on score distributions and used to align base classifier scores. However, most of these methods require the normalization transforms to be determined manually, based on expert knowledge. Our work differs in that it is more focused on building a robust fusion model from a large set of black-box classifiers. The third category aims to learn a score function to combine scores from multiple base classifiers. In [51], weights are learned by minimizing different target error metrics with different regularizations. In [36], a linear dependency between features is proposed to address the independent assumption issue in fusion process. Smith et al.&amp;nbsp;[48] treat the confidence scores from multiple models as a feature vector, and then learn a classifier for different classes using a sample-based approach. Lan et al.&amp;nbsp;[30] introduced a double fusion technique, which unifies both feature level fusion and score level fusion. Our two fusion learning algorithms [26, 35] introduced in Sect.&amp;nbsp;6 belong to the third category with the following distinctive properties: MFoM method [26] can optimize learning process for a wide array of custom metrics [26]; Local Expert Forest [35] learns multiple localized fusion functions across multi-dimensional score space rather than relying on a single fusion function.    3 Overview  Our system consists of three major building blocks: (1) feature extraction, (2) base classifiers, and (3) score fusion, as illustrated in Fig.&amp;nbsp;. Real-world videos exhibit significant variations in salient sensory information across different event categories. For example, ‘parkour’ event is distinctively characterized by motion in videos, while ‘birthday party’ event exhibits unique auditory patterns from birthday songs. Therefore, our feature extraction module (shown in orange in Fig.&amp;nbsp;) computes a large array of multimodal features (both visual and audio) from input videos, which improves its capability to capture salient information across diverse event classes. The features incorporated in our system are described in Sect.&amp;nbsp;4. Next, multiple  independently compute detection scores based on available features. As a result, each video clip is associated with multiple base classifier scores. In our framework, each base classifier can incorporate an arbitrary subset of the features (i.e., anywhere from one to all features), and the same features can be re-used across base classifiers. For each base classifier, its parameters are learned and tuned via cross-validation on training data. Most of the base classifiers incorporated in our current system are based on SVMs or variations of SVMs such as Multiple Kernel SVMs or Latent SVMs (LSVM). In particular, we use LSVMs in a novel high-level scene concept detection and localization framework, which enables unique summary descriptions of retrieval results (see Sect.&amp;nbsp;7.2.2 for video description results). More details of the base classifier types and incorporated kernels are described in Sect.&amp;nbsp;5. The major rationale for incorporating multiple base classifiers stems from two considerations: computational demand and open expandable architecture. In terms of computational demand, simply, multiple base classifiers operating across different subsets of features is more parallelizable and less memory intensive, compared to the alternative of loading and using entire features at the same time, such as early fusion [3, 23]. Furthermore, different base classifiers may use different modeling schemes, e.g., SVMs or logistic regression, and provide multiple instantiations of classifiers that are designed to be fused through the next step of score fusion results in being more open and expandable, which can incorporate additional off-the-shelf classifiers in a flexible manner as needed. Finally, the  module combines multiple base classifier scores through diverse fusion methods, and computes a single final detection score per video clip. We incorporate multiple fusion methods in our system, both untrained (average and geometric mean fusion) and learning-based (Local Expert Forest [35] and Maximal Figure-of-Merit [26]). The parameters of the learning-based fusion methods are estimated through cross-validation. Interestingly, our study indicates that there is no clear winner among different fusion methods. Accordingly, our system incorporates a  framework, where different fusion approaches are tried on each event class, and the best approach per class is selected. More details of the fusion learning scheme and novel fusion methods are described in Sect.&amp;nbsp;6.    4 Features  The proposed system incorporates a large array of audio-visual features to improves its capability of capturing salient information across diverse event classes. In the following sections, incorporated visual (Sect.&amp;nbsp;4.1) and audio (Sect.&amp;nbsp;4.2) features are described. For each modality, features are categorized to be either “low-level” or “mid-level”. In this work, “high-level” features are exclusively utilized by Latent SVM in Sect.&amp;nbsp;5.2. By low-level features, we mean features such as Color SIFT (CSIFT)&amp;nbsp;[46] which do not directly deliver semantic information on its own. On the other hand, mid-level features directly detect semantic categories such as visual object classes (e.g., people and vehicles) or semantic auditory patterns (e.g., drilling sound or laughter). In this paper, most low-level features are used in the form of standard bag-of-words (BoW). That is, the raw visual and audio features are first computed over small spatial/temporal volumes densely sampled across a given video clip. A large, unlabeled collection of these features are clustered to build a codebook, and each computed feature is quantized by the cluster index in this codebook. Finally, the quantized features are pooled spatially over each frame and/or temporally across the entire clip, producing a summary BoW feature vector.
Springer.tar//Springer//Springer\10.1007-s00138-013-0526-9.xml:Fuzzy logic-based pre-classifier for tropical wood species recognition system:Pattern recognition Wood pores Fuzzy logic Texture Wood species recognition system:  1 Introduction  Tropical countries are blessed with abundance of wood supply. With more demands in timber industries and more tightly controlled international requirements, many of these countries are required to meet tighter security requirements as well as higher technical demands such as more accurate recognition of the correct timber species, prevention of fraud and illegal logging, and Environmental Investigation Agency (EIA) requirements, to name a few. In many timber industries one of the major problems is to find good wood graders. Currently, very few certified officers are involved in the traditional wood identification process. The process of training up experienced officers in performing the job is difficult since the job is no longer considered lucrative and rather laborious. Moreover, the possibility of biasness and mistakes by human wood graders has to be considered. Besides that, it is impractical and cost effective for a human to analyze and identify large number of timber species. Currently, the examination of the wood sample is done by the experts using naked eye based on the weight, color, odor, feel, texture and surface. The experts must be familiar with features of all types of wood species. A dichotomous key (table for wood texture analysis) is provided as a guideline for the experts to determine the wood species [1]. Several automatic wood species recognition systems that overcome the errors caused by traditional wood identification system which based solely on human expertise have been developed. Previous wood species recognition systems are designed based on two approaches: spectrum-based processing system and image-based processing system. In the former category, 21 temperate wood species are classified using fluorescence spectra [2]. In the latter category, image processing techniques are used to classify 52 tropical wood species as proposed in [3, 4] and 112 wood species that belong to hardwoods and softwoods in [5]. In recent study of texture recognition, various techniques have been applied to identify the features of an image for example [6] proposed an approach to represent and recognize objects with a massive number of local image patches while [7] presented a generative hierarchical model to represent and recognize compositional object categories with large intra-category variance. As shown in Fig.&amp;nbsp; , the most prevailing features that were used by the experts to identify the wood species are pore sizes and pore quantities. Figure&amp;nbsp; a represents a wood species with medium size of pores and numerous quantity of pores while Fig.&amp;nbsp; b represents another wood species with larger size of pores and fewer quantity of pores. The wood texture is considered essentially fuzzy because there exists no precise boundary between categories since the wood texture varies within similar species due to few factors such as location of growth, weather, age, etc. as shown in Fig.&amp;nbsp; , notwithstanding the fact that the wood textures such as pore sizes and quantities are used to distinctively define the wood species, for example the quantity of pores of the same wood species in Fig.&amp;nbsp; a is more compared to the one in Fig.&amp;nbsp; b. It can be seen that nonlinearity in wood features also exists within the same wood species as shown in Fig.&amp;nbsp; . Figure&amp;nbsp;  represents several wood samples of the same species that have been categorized into three groups based on their pores size based on pores measurement defined in [ 1]. Another issue faced by the tropical wood species recognition systems is the ability to efficiently manage the large wood database which require a longer processing time for training and testing. In this paper, we propose to use a fuzzy logic-based pre-classifier to overcome the nonlinearities of the wood species as well as to manage the large wood database. The fuzzy logic-based classifier categorizes the wood samples based on the quantity of pores and the size of pores prior to the classification stage. The proposed pre-classifier also aims to reduce the processing time in the training and testing stages of the wood species classification, albeit the existence of the bottleneck in building online wood species recognition system. Fuzzy rule is capable of handling approximate data for sharply defined problems. As such, it adds nonlinearity to associative processing, where large data sets are checked in parallel based on the linear metrics. The human-decision-based classification in real-time wood recognition system is basically based on visual inspection of the wood anatomy textures, which can actually be presented as image data processing using statistical parameters representing the texture and the gray values. Due to its simplicity and similarity to human reasoning, fuzzy logic algorithms that were proposed by [8] have been successfully applied in solving variety of problems including pattern classification problem [9], medical image processing [10], recognizing rose variety [11], wood color [12], and noise [13], fabric texture recognition system [14], control systems [15], robotics system [16] and human-error-tolerant interface [17]. Hence, the linguistic interpretation of the human behavior provided by a fuzzy model could be useful to experts in determining the wood species. Moreover, it could improve the man–machine interface in environments for computer-aided training of human operators. The advantage for using fuzzy if–then rules for wood classification problem is that knowledge acquisition can be achieved for users by carefully checking these rules discovered from the training patterns. This paper is structured as follows. Section 2 describes the materials and data acquisition while Sect.&amp;nbsp;3 focuses on the proposed methodology of the proposed tropical wood species recognition system. Finally, Sect.&amp;nbsp;4 presented the experimental results and discussion of the performances of the overall system.    2 Materials and data acquisition  The wood samples for this research are obtained from the Forest Research Institute of Malaysia (FRIM). The wood samples are in cubic form (approximately 1&amp;nbsp;in. by 1&amp;nbsp;in. in size). A specially designed portable camera with 10 times magnification is used to capture the wood texture images. The camera is equipped with a systematic focusing function whereby the distance between the camera and the wood sample is fixed to 10&amp;nbsp;cm and need not be adjusted. The housing of the camera is made of a tube based on the theoretical optimal object distance and therefore the object is just required to be laid against the camera housing. The “tube function” is made in opaque material in order to cut all ambient light and its fluctuations. The camera is attached with six white LEDs to provide sufficient lighting homogeneity. The colors of the LEDs are chosen to be white in order to have “normal” black and white image including evolution towards color measurement. The LED lighting system provides compact light source which is easy to integrate, resistance to vibration, cost effective and immediate lights with no raising delay. The size of the each image is  pixels. The wood images are pre-processed using homomorphic filters in order to enhance the image presentation. Homomorphic filtering sharpens features and flattens lighting variations in an image. Hence, illumination and reflectance on the image can be removed. All images are pre-processed using homomorphic filters to enhance image presentation. Homomorphic filtering uses a linear filter to do nonlinear mapping to a different domain and later it was mapped back to the original domain. The algorithm removes all illumination and reflectance on the image. An example of wood image of species   that has been processed by homomorphic filters is shown in Fig.&amp;nbsp; b. Then, a binary picture where only black pores are present is created in Fig.&amp;nbsp; c. Texture is characterized not only by gray value at a given pixel but also by the gray value ‘pattern’ in a neighborhood surrounding pixel where the brightness level at a point depends on the brightness levels of neighboring point. Texture analysis methods have been utilized in a variety of application domains such as surface inspection, medical imaging and remote sensing [18]. The image classification task requires characterization of different images with patterns of values, transforming the problem into pattern recognition one (called feature extraction process). Subsequently, the patterns are fed into a classifier for solving the task. A set of  training wood images:  is available. It is assumed that each image belongs to one of  class . The wood features are extracted from each wood image which results to a set of wood features A. In this study, the pores quantities and sizes are measured using the statistical properties of pores distribution (SPPD) method developed by [ 4]. The categories of pore size is defined based on [ 1] and is shown in Table  1. However, the pores’ features are only suitable for pre-classifying purpose where the species are divided into groups based on pore sizes and quantities. In order to determine the wood species, more distinctive features are needed which will be used in the final classification stage. Therefore, we used the Basic Gray Level Aura Matrix (BGLAM) feature extractor to extract the features due to the simplicity as well as the capability to characterize the co-occurrence probability distributions of gray levels at all possible displacements configurations of an image. It is proven that BGLAMs can give the necessary and sufficient information to differentiate between images. In addition, BGLAM features are directly calculated from the images without using any filter thus providing a more accurate representation of an image. The framework of BGLAM feature extractor is explained in detail in [19, 20]. Hence, BGLAM features are used to classify the wood species in the final classification. The image is segmented into 16 gray level with each pixel size . Total number of BGLAM features is calculated based on . Hence, for 16 gray level (GL), the total number of BGLAM features  is 136.    3 Proposed methodology  The flowchart of the proposed tropical wood species recognition wood system consists of image acquisition, fuzzy pre-classifier and neural network classification (Fig.&amp;nbsp; ). The fuzzy logic pre-classifier serves as the clustering mechanism dividing the wood images into different groups based on pores sizes and quantities which actually mimics the human way of categorizing the wood species. The boundaries for the categorization of the pore sizes and quantities are somewhat fuzzy because there are no precise boundaries among similar species. The use of the fuzzy clusters breaks down the wood images database into smaller databases which are easier to manage. In this method, the test image will be pre-classified and assigned to its respective group. The classification will be performed based on the smaller trained database. In this way, the overlapping of features among the wood species is reduced making the task of classification easier. Moreover, the use of smaller databases is particularly useful as it will reduce the processing complexity which in turn will reduce the classification processing time. During the training stage, the training wood samples are grouped into different databases namely group S, group SM, group MM and group L representing ‘small’, ‘smaller medium’, ‘medium medium’ and ‘large’ respectively. Prior to the grouping, the features of each of the training and testing images using BGLAM technique and the pore sizes and quantities are obtained. The pore sizes and quantitatives are the inputs to the fuzzy logic pre-classifier. Each database consists of several wood species of within the same range of size and quantity of pores. The feedforward multilayer neural network uses the BGLAM features and will only classify the test sample based on its corresponding wood database, making the process easier and less complex. Suppose we have a set of input images from a wood database with  -feature variables and   image samples. The input data (wood sample features)   on the pattern space is represented by the following pattern matrix:       These training patterns can be formed into   classes. In this paper, the wood data has 2 feature variables with 3,640 training wood data and 4 groups, that is,   and  . The algorithm is as follows:        For every wood image sample, the pore sizes are categorized into three categories based on [1] which are small, medium and large pores as shown in Table 1. The pores counting are performed by calculating the number of pores for each category of pore size.         &amp;nbsp;      As shown in Table  2, the results of pores counting process are   where   represents the quantity of small pores,   represents the quantity of medium pores and   represents the quantity of large pores. Since a wood sample consists of small, medium and large pores, we will only choose the maximum pores (maxPores) to be used as fuzzy inputs. The maximum pores (maxPores) is identified by choosing the highest quantity of pores that belong to either small, medium or large pores. For example, if  , it can be concluded that the maxPores size,   is   and maxPores quantity,  . For every wood image sample, the pore sizes are categorized into three categories based on [1] which are small, medium and large pores as shown in Table 1. The pores counting are performed by calculating the number of pores for each category of pore size. The maxPores size and quantity are the inputs to the fuzzy logic pre-classifier and the group of wood species represents the output. Input size maxPores, . Input quantity maxPores, . Output, . The grouping model is obtained using the fuzzy IF–THEN rules with   feature variables and can be written as below :            Where all variables are defined as follows:        Now the fuzzy rule base can be defined in tabular form (Table  3). The membership function (MF) deals with mapping an input space to a membership value between 0 and 1. The membership function   describes the membership of the elements   of the base set   in the fuzzy set  , whereby for   a large class of functions can be taken. Trapezoidal membership function is applied in this paper as shown in Fig.&amp;nbsp; . For example: the linguistic label “medium” is associated to a possibility distribution defined by four parameters ( ). The following represents the trapezoidal membership function used in this paper:            In this work, let the fuzzy input be the size and quantity of maxPores of each wood samples. The parameters of membership function for input size and quantity are defined as   and   respectively. The parameter for input size is determined based on the measurement as shown in Table  1. The parameter for input quantity is determined by judiciously adjusting the width of membership function to obtain the best result. The defuzzification of the data into a crisp output is accomplished by combining the results of the inference process and then computing the “fuzzy centroid” of the area. The weighted strengths of each output member function are multiplied by their respective output membership function center points and summed. Finally, this area is divided by the sum of the weighted member function strengths and the result is taken as the crisp output. It is defuzzified into a crisp output by Center of Area (Centroid) method. The centroid defuzzification technique can be expressed as: , where  is the defuzzified output,  is the aggregated membership function and  is the output variable. In this work, the output is computed using Matlab fuzzy tool. In this study, we used three types of classifiers namely the LDA classifier, KNN classifier and multilayer feedforward multilayer neural network for comparison purposes. Neural networks “acquire knowledge” from examples through the training process. Neural networks have been successfully applied in solving various classification tasks in medical, industry, business and science such as authentication of coins [21], sign language recognition [22], face recognition [23], EKG pattern recognition [24], product inspection [25], wood veneer classification [26] and wood species classification [27]. The neural network module uses a three-layer (input, hidden and output) fully connected feedforward neural network architecture. The network is trained on a set of training wood data. The performance of the developed recognition system is evaluated based on its ability to correctly classify test samples to their corresponding timber genus. The metric used to accomplish this job selected to be the percentage recognition rate, , defined as Eq. 5. The -nearest-neighbor (NN) algorithm assigns an input data to the dominant class among its  nearest neighbors within the training set. These nearest neighbors are determined using Euclidean distance from the input data. The NN algorithm has been applied widely to solve classification problems such as rock image classification [28], fingerprint recognition [29], phoneme recognition [30] and gait classification [31]. In this study, the value of  is selected as&amp;nbsp;7. The LDA classifier algorithm is explained in detail in [ 4]. To classify the testing sample, the following discriminant function is used:       where   is the feature array of the test sample   is the mean for each features in each species,   is the pooled within group covariance matrix and   is the prior probabilities of each species. Sample   will be assigned to species   that has maximum  . The classification accuracy is calculated based on percentage of correctly classified test samples over number of test samples shown in Eq.  5.       In order to evaluate the classification results of the proposed wood species recognition system, the precision and recall evaluation measures are computed. Precision and recall evaluation metrics have been widely used to evaluate the quality of a classifier such as implemented in Chinese text classification system [ 32], information mining in remote sensing image archives [ 33] and vehicles for urban traffic scenes classification system [ 34]. Precision is a measure of accuracy provided that a specific class has been predicted while recall is a measure of the ability of a prediction model to select instances of a certain class from a dataset. Precision and recall can be calculated from the confusion matrix. Table&amp;nbsp; 4 shows an example of a confusion matrix used in this work. A confusion matrix summarizes the types of errors that a classification model is likely to make. The confusion matrix is calculated by applying the classification model to test data in which the target values are already known. These target values are compared with the predicted (classified) target values. Precision and recall can be calculated from the based on Eqs.  6 and  7:                         where TP (true positive) is the number of samples Correctly classified, FP (false positive) is the number of samples incorrectly labeled as belonging to the class and FN (false negative) is the number of samples which were not labeled as belonging to the positive class but should have been. For example, the false positive and false negative for species A are calculated based on Eqs.  8 and  9                            : samples that belong to species B that are classified as species A. : samples that belong to species C that are misclassified as species A. : samples that belong to species A that are misclassified as species B.
Springer.tar//Springer//Springer\10.1007-s00138-013-0527-8.xml:Evaluating multimedia features and fusion for example-based event detection:Multimedia event detection Difference coding Content extraction Video retrieval Late fusion:  1 Introduction  The goal of multimedia event detection (MED) is to detect user-defined events of interest in massive, continuously growing video collections, such as those found on the Internet. This is an extremely challenging problem because the contents of the videos in these collections are completely unconstrained, and the collections include varying qualities of user-generated videos, often made with handheld cameras, and may have jerky motions, wildly varying fields of view, and poor lighting. The audio in these videos is recorded in a variety of acoustic environments, often with a single camera-mounted microphone, with no attempt to prevent background sounds from masking speech. For purposes of this research, an event, as defined in the TRECVID MED evaluation task sponsored by the National Institute of Standards and Technology (NIST) [ 1], has the following characteristics:            Figure&amp;nbsp;  shows some sample video imagery from events in the TRECVID MED evaluation task. Events are more complex and may include actions (hammering, pouring liquid) and activities (dancing) occurring in different scenes (street, kitchen). Some events may be process-oriented, with an expected sequence of stages, actions, or activities (making a sandwich or repairing an appliance); other events may be a set of ongoing activities with no particular beginning or end (birthday party or parade). An event may be observed in only a portion of the video clip, and relevant clips may contain extraneous content. It includes a complex activity occurring at a specific place and time. It involves people interacting with other people and/or objects. It consists of a number of human actions, processes, and activities that are loosely or tightly organized and have significant temporal and semantic relationships to the overarching activity. It is directly observable. Multimedia event detection can be considered as a search problem with a query-retrieval paradigm. Currently, videos in online collections, such as YouTube, are retrieved based on text-based search. Text labels are either manually assigned when the video is added to the collection or derived from text already associated with the video, such as text content that occurs near the video in a multimedia blog or web page. Videos are searched and retrieved by matching a text-based user query to videos’ text labels, but performance will depend on the quality and availability of such labels. Highly accurate text-based video retrieval requires the text-based queries to be comprehensive and specific. In the TRECVID MED evaluation, each event is defined by an “event kit,” which includes a 150–400 word text description consisting of an event name, definition, explication (textual exposition of the terms and concepts), and lists of scenes, objects, people, activities, and sounds that would indicate the presence of the event. Figure&amp;nbsp;  shows an example for the event working on a woodworking project. The user might also have to specify how similar events are distinguished from the event of interest (e.g., not construction in Fig.&amp;nbsp; ), and may have to estimate the frequency with which various entities occur in the event (e.g., often indoors). Subcategories and variations of the event may also have to be considered (e.g., operating a lathe in a factory). Another approach to detect events is to define the event in terms of a set of example videos, which we call an example-based approach. Example videos are matched to videos in the collection using the same internal representation for each. In this approach, the system automatically learns a model of the event based on a set of positive and negative examples, taking advantage of well-established capabilities in machine learning and computer vision. This paper considers an example-based approach with both non-semantic and semantic representations. Current approaches for MED [2–7] rely heavily on kernel-based classifier methods that use low-level features computed directly from the multimedia data. These classifiers learn a mapping between the computed features and the category of event that occurs in the video. Videos and events are typically represented as “bag-of-words” (BOW) models composed of histograms of descriptors for each feature type, including visual, motion, and audio features. Although the performance of these models is quite effective, individual low-level features do not correspond directly to terms with semantic meaning, and therefore cannot provide human-understandable evidence of why a video was selected by the MED system as a positive instance of a specific event. A second representation is in terms of higher-level semantic concepts, which are automatically detected in the video content [8–11]. The detectors are related to objects, like a flag; scenes, like a beach; people, like female; and actions, like dancing. The presence of concepts such as these creates an understanding of the content. However, except for a few entities such as faces, most individual concept detectors are not yet reliable [12]. In addition, training detectors for each concept requires annotated data, which usually involves significant manual effort to generate. In the future, it is expected that more annotated datasets will be available, and weakly supervised learning methods will help improve the efficiency of generating them. Event representations based on high-level concepts have started to appear in the literature [13–16]. For an example-based approach, the central research issue is to find an event representation in terms of the elements of the video that permits the accurate detection of the events. In our approach, an event is modeled as a set of multiple bags-of-words, each based on a single data type. Partitioning the representation by data type permits the descriptors for each data type to be optimized independently (specific multimodal combinations of features, such as bimodal audiovisual features [3], can be considered a single data type within this architecture). The data types we used included both low-level features (visual appearance, motion, and audio) and higher-level semantic concepts (visual concepts). We also used automatic speech recognition (ASR) to generate a BOW model in which semantic concepts were expressed directly by words in the recognized speech. The resulting event model combined multiple sources of information from multiple data types and multiple levels of information. As part of the optimization process for the low-level features, we investigated the use of difference coding techniques in addition to conventional coding methods. Because the information captured by difference coding is somewhat complementary to the information produced by the traditional BOW, we anticipated an improvement in performance. We conducted experiments to compare the performance of difference coding techniques with conventional feature coding techniques. The remaining challenge is finding the best method for combining the multiple bags-of-words in the event-detection decision process. The most common approach is to apply late fusion methods [3, 5, 17] in which the results for each data type are combined by fusing the decision scores from multiple event classifiers. This is a straightforward way of using the information from all data types in proportion to their relative contribution to event detection on videos with widely diverse content. We evaluated the performance of several fusion methods. The work described in this paper focused on evaluating the various data types and fusion methods for MED. Our approach for example-based MED, including methods for content extraction and fusion, is described in Sect.&amp;nbsp;2. Experimental results are described in Sect.&amp;nbsp;3, and Sect.&amp;nbsp;4 contains a summary and discussion. All the experiments for evaluating the performance of the MED capability were performed using the data provided in the TRECVID MED evaluation task. The MED evaluation uses the Heterogeneous Audio Visual Internet Collection (HAVIC) video data collection [18], which is a large corpus of Internet multimedia files collected by the Linguistic Data Consortium.    2 Approach for example-based MED  The work in this paper focuses on SEarch with Speed and Accuracy for Multimedia Events (SESAME), an MED system in which an event is specified as a set of video clip examples. A supervised learning process trains an event model from positive and negative examples, and an event classifier uses the event model to detect the targeted event. An event classifier was built for each data type. The results of all the event classifiers were then combined by fusing their decision scores. An overview of the SESAME system and methods for event classification and fusion are described in the following sections. The major components of the SESAME system are shown in Fig.&amp;nbsp; . A total of nine event classifiers generate event detection decision scores: two based on low-level visual features, three based on low-level motion features, one based on low-level audio features, two based on visual concepts, and one based on ASR. The outputs of the event classifiers are combined by the fusion process. Figure&amp;nbsp;  shows the processing blocks within each event classifier. Each event classifier operates on a single type of data and includes both training and event classification. Content is extracted from positive and negative video examples, and the event classifier is trained, resulting in an event model. The event model produces event detection scores when it is applied to a test set of videos. Figure&amp;nbsp;  does not show off-line training and testing to optimize the parameter settings for the content extraction processes. This section describes the feature coding and aggregation methods that were common to the low-level features and the content extraction methods for the different data types: low-level visual features, low-level motion features, low-level audio features, high-level visual features, and ASR. The coding and aggregation of low-level features share common elements that we describe here. We extracted local features and aggregated them by using three approaches: conventional BOW, vector of locally aggregated descriptors (VLAD), and Fisher vectors (FV). The conventional BOW approach partitions low-level features into -means clusters to generate a codebook. Given a set of features from a video, a histogram is generated by assigning each feature from the set to one or several nearest code words. Several modifications to this approach are possible. One variation uses soft coding, where instead of assigning each feature to a single code word, distances from the code words are used to weigh the histogram terms for the code words. Another variation describes code words by a Gaussian mixture model (GMM), rather than just by the center of a cluster. While conventional BOW aggregation has been successfully used for many applications, it does not maintain any information about the distribution of features in the feature space. FV has been introduced in previous work [19] to capture more detailed statistics, and has been applied to image classification and retrieval [20, 21]. The basic idea is to represent a set of data by a gradient of its log-likelihood to model parameters and to measure the distance between instances with the Fisher kernel. For local features extracted from videos, it becomes natural to model their distribution as GMMs, forming a soft codebook. With GMM, the dimension of FV is linear in the number of mixtures and local feature dimensions. Finally, VLAD [20] is proposed as a non-probabilistic version of FV. It uses -means instead of GMM, and accumulates the relative positions of feature points to their single nearest neighbors in the codebook. Compared with conventional BOW, FV and VLAD have the following benefits:        None of the above aggregation methods consider feature localization in space or in time. We introduced a limited amount of this information by dividing the video into temporal segments (for time localization) and spatial pyramids (for spatial localization). We then compute the features in each segment or block separately and concatenate the resulting features. The spatial pooling and temporal segmentation parameters that yielded the best performance were determined through experimentation. FV takes GMM as the underlying generative model. Both FV and VLAD are derivatives, so feature points with the same distribution as the general model have no overall impact on the video-level descriptors; as a result, FV and VLAD can suppress noisy and redundant signals. Two event classifiers were developed based on low-level visual features [22]. They both follow a pipeline consisting of four stages: spatiotemporal sampling of points of interest, visual description of those points, encoding the descriptors into visual words, and supervised learning with kernel machines. The visual appearance of an event in video may have a dependency on the spatiotemporal viewpoint under which it is recorded. Salient point methods [23] introduce robustness against viewpoint changes by selecting points, which can be recovered under different perspectives. To determine salient points, Harris–Laplace relies on a Harris corner detector; applying it on multiple scales makes it possible to select the characteristic scale of a local corner using the Laplacian operator. For each corner, the Harris–Laplace detector selects a scale invariant point if the local image structure under a Laplacian operator has a stable maximum. Another solution is to use many points by dense sampling. For imagery with many homogenous areas, such as outdoor snow scenes, corners may be rare, therefore relying on a Harris–Laplace detector can be suboptimal. To counter the shortcomings of Harris–Laplace, we used dense sampling, which samples an image grid in a uniform fashion, using a fixed pixel interval between regions. In our experiments, we used an interval distance of six pixels and sampled at multiple scales. Appearance variations caused by temporal effects were addressed by analyzing video beyond the key-frame level [24]. Taking more frames into account during analysis allowed us to recognize events that were visible during the video, but not necessarily in a single key frame. We sampled one frame every 2&amp;nbsp;s. Both Harris–Laplace and dense sampling give an equal weight to all keypoints, regardless of their spatial location in the image frame. To overcome this limitation, Lazebnik et al. [25] suggested repeated sampling of fixed subregions of an image, e.g., 1&amp;nbsp;&amp;nbsp;1, 2&amp;nbsp;&amp;nbsp;2, 4&amp;nbsp;&amp;nbsp;4, etc., and then aggregating the different resolutions into a spatial pyramid, which allows for region-specific weighting. Since every region is an image in itself, the spatial pyramid can be combined with both the Harris–Laplace point detector and dense point sampling. We used a spatial pyramid of 1&amp;nbsp;&amp;nbsp;1 and 1&amp;nbsp;&amp;nbsp;3 regions in our experiments. In addition to the visual appearance of events in the spatiotemporal viewpoint under which they are recorded, the lighting conditions during recording also play an important role in MED. Properties of color features under classes of illumination and viewing features, such as viewpoint, light intensity, light direction, and light color, can change, specifically for real-world datasets as considered within TRECVID [26]. We followed [22] and used a mixture of SIFT, OpponentSIFT, and C-SIFT descriptors. The SIFT feature proposed by Lowe [27] describes the local shape of a region using edge-orientation histograms. Because the SIFT feature is normalized, the gradient magnitude changes have no effect on the final feature. OpponentSIFT describes all the channels in the opponent color space using SIFT features. The information in the O3 channel is equal to the intensity information, while the other channels describe the color information in the image. The feature normalization, as effective in SIFT, cancels out any local changes in light intensity. In the opponent color space, the O1 and O2 channels still contain some intensity information. To add invariance to shadow and shading effects, the C-invariant [28] eliminates the remaining intensity information from these channels. The C-SIFT feature uses the C-invariant, which can be seen as the gradient (or derivative) for the normalized opponent color space O1/I and O2/I. The I intensity channel remains unchanged. C-SIFT is known to be scale-invariant with respect to light intensity. We computed the SIFT and C-SIFT descriptors around salient points obtained from the Harris–Laplace detector and dense sampling. We then reduced all descriptors to 80 dimensions with principal component analysis (PCA). To avoid using all low-level visual features from a video, we followed the well-known codebook approach. We first assigned the features to discrete codewords from a predefined codebook. Then, we used the frequency distribution of the codewords as a compact feature vector representing an image frame. Based on [22], we employed codebook construction using -means clustering in combination with average codeword assignment and a maximum of 4,096 codewords. The traditional hard assignment can be improved using soft assignment through kernel codebooks [29]. A kernel codebook uses a kernel function to smooth the hard assignment of (image) features to codewords by assigning descriptors to multiple clusters weighted by their distance to the center. We also used difference coding, with VLAD performing -means clustering of the PCA-reduced descriptor space with 1,024 components. The output of the word encoding is a BOW vector using either hard average coding or soft VLAD coding. The BOW vector forms the foundation for event detection. Kernel-based learning methods are typically used to develop robust event detectors from audiovisual features. As described in [22], we relied predominantly on the support vector machine (SVM) framework for supervised learning of events: specifically, the LIBSVM1 implementation with probabilistic output. To handle imbalance in the number of positive versus negative training examples, we fixed the weights of the positive and negative classes by estimating the prior probabilities of the classes on training data. We used the histogram intersection kernel and its efficient approximation as suggested by Maji et al. [30]. For difference coded BOWs, we used a linear kernel [19]. We evaluated the performance of these two event classifiers on a set of 12,862 drawn from the training and development data from the TRECVID MED evaluation. This SESAME Evaluation dataset consisted of a training set of 8,428 videos and a test set of 4,434 videos sampled from 20 event classes and other classes that did not belong to any of the 20 events. To make good use of the limited number of available positive instances of events, the positives were distributed so that, for each event, there were approximately twice as many positives in the training set as there were in the test set. Separate classifiers were trained for each event based on a one-versus-all paradigm. Table&amp;nbsp; 1 shows the performance of the two event classifiers measured by mean average precision (MAP). Color-average coding with a histogram intersection kernel (HIK) SVM slightly outperformed color-difference soft coding with a linear SVM. For events, such as changing a vehicle tire and town hall meeting, the average HIK was the best event representation. However, for some events, such as flash mob gathering and dog show, the difference coding was more effective. To study whether the representations complement each other, we also performed a simple average fusion; the results indicate a further increase in event detection performance, improving mean average precision from 0.342 to 0.358 and giving the best overall performance for the majority of events. Many motion features for activity recognition have been suggested in previous work; [4] provides a nice evaluation of motion features for classifying web videos on the NIST MED 2011 dataset. Based on our analysis of previous work and some small-scale experiments, we decided to use three features: spatiotemporal interest points (STIPs) and dense trajectories (DTs) [31], and MoSIFT [32]. STIP features are computed at corner-like locations in the 3D spatiotemporal volume. Descriptors consist of histograms of gradient and optical flow at these points. This is a very commonly used descriptor; more details may be found in [33]. Dense trajectory features are computed on a dense set of local trajectories (typically computed over 15 frames). Each trajectory is described by its shape and by histograms of intensity gradient, optical flow, and motion boundaries around it. Motion boundary features are somewhat invariant to camera motion. MoSIFT, as its name suggests, uses SIFT feature descriptors; its feature detector is built on motion saliency. STIP and DT were extracted using the default parameters as provided2; the MoSIFT features were obtained in the form of coded BOW features.3 After the extraction of low-level motion features, we generated a fixed-length video-level descriptor for each video. We experimented with the coding schemes described in Sect.&amp;nbsp;2.2.1 for the STIP and DT features; for MoSIFT, we were able to use BOW features only. We used the training and test sets described above. We trained separate SVM classifiers for each event and each feature type. Training was based on a one-versus-all paradigm. For conventional BOW features, we used the  kernel. We used the Gaussian kernel for VLAD and FV. To select classifier-independent parameters (such as the codebook size), we conducted fivefold cross validation of 2,062 videos from 15 event classes. We conducted fivefold cross validation on the training set to select classifier-dependent parameters. For BOW features, we used 1,000 codewords; for FV and VLAD, we used 64 cluster centers. More details of the procedure are found in [34]. We compared the performance of conventional BOW, FV, and VLAD for STIP features; BOW and FV for DT features; and BOW for MoSIFT, using the SESAME Evaluation dataset. Table&amp;nbsp; 2 shows the results. We can see that FV gave the best MAP for both STIP and DT. VLAD also improved MAP for STIP, but was not as effective as the FV features. We were not able to perform VLAD and FV experiments for MoSIFT features, but would expect to have seen similar improvements there. The audio is modeled as a bag-of-audio-words (BOAW). The BOAW has recently been used for audio document retrieval [35] and copy detection [36], as well as MED tasks [37]. Our recent work [38] describes the basic BOAW approach. We extracted the audio data from the video files and converted them to a 16&amp;nbsp;kHz sampling rate. We extracted Mel frequency cepstral coefficients (MFCCs) for every 10&amp;nbsp;ms interval using a hamming window with 50&amp;nbsp;% overlap. The features consist of 13 values (12 coefficients and the log-energy), along with their delta and delta–delta values. We used a randomized sample of the videos from the TRECVID 2011 MED evaluation development set to generate the codebook. We performed -means clustering on the MFCC features to generate 1,000 clusters. The centroid for each cluster is taken as a code word. The soft quantization process used the codebook to map the MFCCs to code words. We trained an SVM classifier with a histogram intersection kernel on the soft quantization histogram vectors of the video examples, and used the classifier to detect the events. Evaluation with the SESAME Evaluation dataset showed that the audio features achieved a MAP of 0.112.
Springer.tar//Springer//Springer\10.1007-s00138-013-0528-7.xml:On hierarchical modelling of motion for workflow analysis from overhead view:Video feature extraction Hierarchical motion modelling Human behaviour recognition Workflow analysis Probabilistic sequence analysis:  1 Introduction  Recently, there has been an increased interest in the area of human activity and behaviour recognition. While many different subsets of activities are investigated, the conditions and overall specifications of the problem vary largely. One of the applications of this is in visual surveillance. The goal here is to detect a set of pre-defined tasks which occur within a workflow of a complex industrial environment. Bobick [4] defined different human motion types and an understanding of these motions. In this, Bobick proposed a three-level motion-understanding scheme based on the order of the implied knowledge required to understand the motion. In increasing order of knowledge, these motion levels are: ‘movement’, ‘activity’, and ‘action’. The majority of work in the area of human motion analysis is focused on the recognition of human ‘activities’. There we have ‘activities’ such as jumping, boxing, and waving. However, the study of what Bobick described as ‘action’, the most knowledge intensive form of motion understanding, in which contextual or causal relations play a critical role, appears in a smaller number of works and it is a less definitive problem. This high contextual dependency also encourages diverse terminology, while the approaches and techniques to understand them do not lend themselves to direct comparison either. It is also worth noting that, while this may not always be the case, often a knowledge-based hierarchy of motion results in a hierarchical structure for the perceived motion itself; ‘activities’ are made up of ‘movements’ and ‘actions’ are made up of ‘activities’. Manufacturing [7, 9, 19, 20] and office [3, 10, 17], environments, medical operating rooms [12], TV studios [13] and smart homes for the elderly [8] are amongst the settings wherein ‘action’ perception and understanding from video streams had been studied. Pinhanez and Bobick [13] were among the first to analyse the actions within a video stream. They used this information to build an ‘intelligent studio’ wherein the cameras were controlled automatically based on a predefined script and the visual data from the cameras themselves. Considering the sequential nature of the patterns, using a hidden Markov model is a justified and popular choice. Padoy et al.&amp;nbsp;[12] studied the workflows in a simulated operating theater. They introduced the Workflow-HMM which is a form of two-layered hierarchical HMM. Nguyen et al. [10], who experimented in an office environment, also used hierarchical HMMs, however, movement trajectories are used there to learn and recognize actions. Behera et al. [3] have also exploited qualitative spatial information to inform action recognition. In this, all possible pair-wise spatial relations among objects are represented in a relational state space and a probabilistic Latent Semantic Analysis is used to model workflow from this relational state space. Voulodimos et al. [20] and Oliver et al. [11] used HMMs as a means for low level sensory data or high level decision fusion. Criticizing HMMs as requiring well-defined states and robust features, Veres et al.&amp;nbsp;[19] opt for a neural network: they used an echo state network to learn the patterns within an annotated global scene descriptor. Shi et al.&amp;nbsp;[17] proposed the use of propagation networks to handle parallel tracks within one action. Kosmopoulos et al.&amp;nbsp;[7] incorporated user’s feedback into the learning process using a neural networks based method to dynamically correct erroneous classifications. While on a slightly different note, Nater et al.&amp;nbsp;[9] have proposed a novel approach for unsupervised discovery of tasks within a workflow. In this, assuming a temporal consistency and cyclic repeated patterns, they have used Slow Feature Analysis method to learn and extract invariant components in the temporal signal. In these works, the modelling of workflows and the recognition of actions are addressed as well as temporal and spatial segmentation of the sequences. However, the recognition of actions is often addressed separately from the problem of sequence segmentation. Temporal segmentation, as in detecting the start and the end of a sequence which makes up an action, is also the focus of a related line of work which is devoted to unusual activity or unusual scene detection with the aim of detecting sequences that have not been previously observed [1, 5, 8]. In many approaches the sequence to be classified starts with a person entering the field of view and it ends when they exit this field. Thereby every movement is interpreted toward a meaningful behaviour. While such examples largely simplify the problem, this is not normally the case in real scenarios. In this paper, the imagery derived from a camera monitoring a working cell in a car manufacturing plant is analysed. In this, the human operators may be involved in multiple consecutive actions. On the other hand they might perform no predefined actions at all. The other point of concern is the spatial segmentation of actions. The complex and cluttered environment which is analysed in this paper, poses an overwhelming detection and tracking problem as was experiences by [7, 19, 20]. They have thus opted for a global scene descriptor and abandoned the problem of spatial segmentation. In this paper, Bobick et al.’s hierarchy of human motion approach is adopted, and the problem is split into activity and task recognition. The tasks are the specific actions of this environment and our goal is to detect these tasks. These tasks, however, are composed of lower level activities which are not specific to this environment and can be observed in other environments and in other workflows. Therefore by describing our environment-specific tasks in terms of generic activities we move toward a more general solution which can be used in other applications. This approach also helps to divide the problem into smaller, more manageable and better defined problems which can be tuned, tested and improved separately. In terms of functionality, it enables us to determine what stage the task is in at each point in time which can be very useful when analysing complex or long duration tasks. The activities are classified using a set of motion and pose features in a small time window. A probabilistic model examines the sequences of activities for instances of a task while the remainder will be labelled as background. In this, two separate HMMs for task and background compete for a sequence. The sequence of activities is also subjected to a probabilistic analysis by activity HMMs which remove the irregularities in the activity sequence prior to searching for task instances. Briefly, the various levels of our hierarchical approach are: (i) detection and tracking; (ii) activity classification; (iii) boosted activity sequence (via activity HMMs); (iv) task extraction; and (v) task label assignment. Using a surfeit of informative cues and handling the uncertainty in the input from preceding layers of the hierarchical process, the error which would otherwise accumulate through the stages of the hierarchical approach is reduced. We address both problems of temporal and spatial segmentation in identifying a series of predefined tasks in imagery monitoring a car manufacturing plant. We will show that the overhead view poses a more manageable tracking problem while providing a full visual access to the environment which is viable for a human motion analysis. We are faced with problems such as noise, occlusion and overlapping tasks, which are inevitable in a loosely constrained environment such as this. Also, there is a series of background actions which are not within our predefined tasks, these include: idle or random actions of the human operators; maintenance; cleaning; and replacement of empty racks. The main contribution of this work is to use computer vision to enable the analysis of the patterns of workflow in complex industrial environments. Our hierarchical approach serves to successfully divide this complex problem into manageable parts. Although applied to a specific problem for detecting a set of specific tasks, we will discuss that the distinction between the concepts of activity and task helps to improve the reusability and the generality of this approach. We will outline the conditions which would need to be satisfied for this approach to be applicable to a new problem and provide guidelines for how to do so. In addition to what has been offered in [19, 20] our approach enables us to (i) localize the tasks in image frame and (ii) offer a stage by stage commentary-like report of the task as it happens. The dataset and the taxonomy of the tasks and activities are described in Sect.&amp;nbsp;2. In Sect.&amp;nbsp;3, the analysis of workflow patterns including the task and activity recognition are described in detail. The experimental results on activity classification and task modelling, segmentation and recognition are presented in Sect.&amp;nbsp;4. Finally, overall conclusions are reviewed and future works are discussed.    2 Workflow patterns and the dataset  The work here has been developed within a European Union project, SCOVIS (Self-Configurable Cognitive Video Supervision). The environment we aim to monitor is a working cell within a car manufacturing plant which contains a welding cell and various racks of parts. In this, parts are individually handled and carried to the welding cell by human operators. They are then welded together, assembling a frame which is then moved to the next stage of the manufacturing process. The same process is thereafter repeated in the work cell so a repeating pattern of activity can be observed. In this context, a series of tasks needs to be performed before the frame is ready to be transported to the next stage. Six tasks are identified within this environment:                An additional task, ‘welding’, also exists in this environment. This task has significantly different definition and is not considered in this work. Figure&amp;nbsp; a shows the position of the stationary components (the welding cell and the racks), and Fig.&amp;nbsp; b shows tasks 3 and 4 being performed. Task 1: A part from Rack 1 (upper compartment) is placed on the welding cell by a worker(s). Task 2: A part from Rack 2 is placed on the welding cell by worker(s). Task 3: A part from Rack 3 is placed on the welding cell by worker(s). Task 4: Two parts from Rack 4 are placed on the welding cell by worker(s). Task 5: A part from Rack 1 (lower compartment) is placed on the welding cell by worker(s). Task 6: A part from Rack 5 is placed on the welding cell by worker(s). These six tasks can all be described generally as picking up a part from a rack and placing it on the welding cell. However, these tasks are different in terms of their visual appearance—the parts are of different shapes and sizes, they are handled in different manners by one or two human operators, while the position of the racks offers a vital cue. The sequential order of the required tasks is flexible, and they may also be performed concurrently. Four activity categories: walking, carrying, handling and standing are analysed. These are taken together from the basic building blocks of the higher level task analysis. We use images derived from an overhead view to recognize the tasks in the workflow described above. The dataset includes synchronized video captured from four oblique views of the same cell. Veres et al.&amp;nbsp;[ 19] and Voulodimos et al.&amp;nbsp;[ 20] have analysed the workflow patterns from these oblique views. The overhead view analysis of the workspace might not be the optimal view angle to observe individual activities which include motion perpendicular to this viewing plane, also an estimate of height is not acquirable. However, it offers a much lesser occluded scene and a manageable detection and tracking problem. In the oblique views, Veres et al.&amp;nbsp;[ 19] have reported a mere recall of 24&amp;nbsp;% with a precision of 9&amp;nbsp;% in detection using two different state-of-the-art person detection and tracking methods. The difficulty of the tracking problem in the oblique views compelled Veres et al. [ 19] and Voulodimos et al. [ 20] to opt for a global scene description and thus abandon the problem of spatially localizing a task. The other benefits of using the overhead view include: a potentially larger field of view through the use of a panoramic camera; and better estimation of position on the ground plane. Figure&amp;nbsp;  shows sample images derived from an oblique and the top view, concurrently. Three workers are partially occluded from an oblique view angle, while two of them are fully visible in the image derived from the overhead view, and the third worker is outside the active area of this working cell.    3 Workflow analysis  The aim of this work is to detect and identify a set of predefined tasks of a workflow. For this, we adopt the hierarchical structure of human motion by Bobick. The tasks are broken down into smaller entities: the activities and modelled using the workflow definitions inherent to our data. The set of activity categories are the distinguishable parts within a task sequence. Albeit, note that we have opted for a generic set of activities. Figure   demonstrates the overall approach. Tracking is performed using a simple blob tracker based on motion [ 2]. A set of shape and motion features are extracted from a 10-frame window for each moving blob, and their fluctuations are analysed using the Fourier transform. A simple KNN classifier then classifies the activity which is being performed based on a binary tree structure. Feature selection is performed at each node of the tree to select the most relevant features. This initial prediction of activity is further subjected to a probabilistic analysis of transitions between the activities in different activity zones using HMMs; the initial predictions of the activities are fed into a set of localized HMMs as observations while the hidden states represent the ‘true’ activities. This amended sequence of activities is analysed by probabilistic models for task and background, which compete for the incoming sequence. Although each step of this process is a difficult problem and there is a tendency for errors to accumulate towards the higher levels of the hierarchical process, it is worth noting that there is often a surfeit of information available which could be used to reduce error. This includes: a surfeit of frames to classify activities—as noted by [16] we need approximately 5–7 frames to identify a simple activity; more than required number of individual activities to recognize a task—often, only the key activities need to be observed to recognize a task; the use of location information or activity zones; and sequential constrains within activities, as exploited by the activity HMMs. The uncertainties are handled via different levels of HMM processing. However, clearly if a subject is not detected we cannot proceed with the classification of its activities and detect possible tasks. Thus we aim for a low false negative rate in detection and tracking. In the remainder of this section, starting with the activity classification we work through the layers of the hierarchical process depicted in Fig.&amp;nbsp;. A simple blob tracker is used to detect the main moving parts i.e. human beings. This is described in our earlier work [2]. Various shape-based and motion-based features are then extracted for the activity classification. Four activity categories: walking; carrying; handling and stand are considered. A binary decision tree which uses the features selected via the ASFFS (Adaptive Sequential Forward Floating Search) [18] provides an initial prediction for the activity. Exploiting our continuous video data, we can then analyse the plausibility of a predicted sequence of activities using HMMs. Both the motion and the shape of the moving blob contain cues as to the activity which is being performed. We build a composite feature set by combining a set of shape-based and motion-based features. These features, which are extracted for each detected blob, are:    motion-based features: average speed; instantaneous speed; and changes in the direction of motion. We shall collectively refer to these features as dynamic features, denoted by .        shape-based features: Hu Invariant Moments [     6], which are seven moments providing a global description of the shape. These are translation, scale and rotation invariant. Hu moments are computed using the normalised central moments,      , which are in turn computed from centralised moments,      :                         where                         and       is the image intensity at location      . The first Hu moment,       is defined as:                         For all seven Hu invariant moments see [     6]. Let       denote the collection of these moments:                         Some heuristic shape related properties are also considered. These are the area, diameter and the pixel density of the detected blob. In addition, considering that the parts which are being handled are metallic and they often have a strong projection in saturation and value axes of the HSV colour space, the sum of values in these two axes are also considered individually.       denotes the collection of these heuristic features.      A 10-frame window is considered for activity classification. For this, the 7-frame window suggested by [ 16] for classifying basic activities is used as a guideline. Clearly, for calculating the motion related features at least one previous frame is required. For estimating the features related to the change in the direction of motion two previous frames are needed, and thus these features can only be computed from the third frame onwards of the window. Thus a larger window of 10 frames is used here. motion-based features: average speed; instantaneous speed; and changes in the direction of motion. We shall collectively refer to these features as dynamic features, denoted by . shape-based features: Hu Invariant Moments [ 6], which are seven moments providing a global description of the shape. These are translation, scale and rotation invariant. Hu moments are computed using the normalised central moments,  , which are in turn computed from centralised moments,  :         where         and   is the image intensity at location  . The first Hu moment,   is defined as:         For all seven Hu invariant moments see [ 6]. Let   denote the collection of these moments:         Some heuristic shape related properties are also considered. These are the area, diameter and the pixel density of the detected blob. In addition, considering that the parts which are being handled are metallic and they often have a strong projection in saturation and value axes of the HSV colour space, the sum of values in these two axes are also considered individually.   denotes the collection of these heuristic features. Apart from the average speed, which has a single value representing the average speed of the moving blob during these 10 frames, all the features listed above will have one value per frame. The mean value for each feature and a measure of its changes within these 10 frames are considered. Let   be the set of all the shape and motion-based features; Let   be a feature where   and   is the average speed. Let the series of   values for blob   in a 10-frame window centered at time   be  :       A frequency-based analysis of   provides insights as to the changes in the feature value in this window. Let   denote discrete Fourier transform.                         where   and   are the magnitude and phase in different frequencies. Thereby the feature vector   is generated for each sample as:            where   and   denote the mean and the standard deviation of the feature values. The first six frequency components are considered since these are the most significant of the frequency responses. The mirror-symmetrical (negative frequency) components are not considered as they add no extra information. A binary decision tree approach has been adopted for the classification of activities. This splits the classes into still: (handling and standing); and moving: (walking and carrying). Due to the composite nature of our feature vector and that various feature types are susceptible to different levels of corruption in noise, a feature subset selection method, the ASFFS, is employed at each node of the tree to derive the discriminative cues whilst removing corrupt, irrelevant or redundant features. A KNN classifier is used to obtain a classification. The logical and structural patterns within a sequence of activities can be exploited to evaluate the plausibility of a sequence of predictions. Hidden Markov models (HMMs) are used to learn these patterns. HMMs are probabilistic finite-state machines which model distributions over sets of possible sequences. The activity HMM is shown in Fig.&amp;nbsp; . In this, the hidden states are the activities: walking; carrying; handling and standing, and the observations are the predictions obtained by the binary tree classifier. Let a set of predictions,  , by the decision tree be:  , and let   be the set of hidden states for the HMM. Given the set of predictions  , the probability of being in state   at time   is:       where       The activity HMMs are trained using the Baum–Welch algorithm and Viterbi algorithm is used to predict the most likely sequence of activities. Our data imposes that there is a spatial dependency regarding the expectation of various activities. Three distinct activity zones are identified and shown in Fig.&amp;nbsp;. Thereby for each activity zone a separate activity HMM is learnt. Further details on the activity classification as well as the initial experimental results can be found in our earlier work [2]. There, it has been shown that the activity HMMs improve the performance significantly. Noting that in a sequence of activities a transition to a different activity is a rare occurrence, instead, the same activity tends to be observed in adjacent frames, and considering that the activity HMMs model this structure, one of the immediately visible effects of employing the activity HMMs is stabilizing or smoothing the sequence of predictions. However, it has been shown in [2] that the gained improvement is much more than a simple stabilizing effect. Further results are discussed in the next section. HMMs are used to identify a specific sequence of activities which is referred to as task, within a bigger, less severely constrained sequence, which we refer to as background. For this purpose, a task can be simply described as picking up a part from a rack and placing it on the welding cell. Two HMMs, one to model a task sequence; and one for background activity, are used to make a decision as to the occurrence of a task by comparing the posterior densities:            Equation ( 12) presumes that the sequences,  , are segmented, where each sequence is either a task or background. However, the input sequence of activities is not temporally segmented. If the ratio,  , is calculated for the entire sequence up to the current point in time, the background HMM will almost always win against the task HMM, since between the two models the background HMM is the only one which can describe the whole data, even the task sections, albeit with a small probability, and in that sense it is inclusive of the task sequences. The background HMM is purposely general so that it can handle the other different actions, other than the tasks, which can happen in this environment. As a result and although it has not been trained for it, the background HMM can also accept the task sequences with some small probability. Also, unlike the task HMM it does not have an absorption state, and therefore it does not terminate. Due to this inclusive and continuous nature of the background model, a background sequence can, at any time, be interrupted if a task model obtains the starting conditions, as denoted by:            where   is the sequence that only contains the current observed activity at time  . Once a possible task sequence is started it is terminated only if the entire sequence fits the background model better, taking into account the sensitivity of the detection process;            A special case of Markov models sometimes referred to as left-to-right models are used for identifying a task sequence. In these, a sequence progresses through the states of the model from left to right and it will be absorbed in the final states. A task is detected if a sequence arrives at the absorbing or final state of the task HMM. The task HMM and background HMM are trained using the Baum–Welch reestimation method. Since the task HMM is a special case of HMM (left-to-right model) for which training cannot be performed using a single observation sequence, modified reestimation formulas for multiple observation sequences are given in [ 14]. In the test phase Viterbi algorithm is used to predict the probability of a sequence being generated by the task and background models. As mentioned before, a task can be generally described as a sequence of activities which corresponds to picking up a part from one of the racks and placing it on the welding cell. Since the initiation and completion of a task depends directly on the location of the detected activities as well as the activity itself, the observation at each state of the HMM is determined based on both the activity and the location. Four activity categories have been considered in Sect.&amp;nbsp; 3.1, while three activity zones are also distinguishable. These four activity categories are: walking; carrying; handling; and standing, and the three activity zones are: racks; welding cell; and walk ways. These three zones are highlighted in Fig.&amp;nbsp; . Thereby, the alphabet of the observed symbols in the task and background HMMs includes: 12 area-based activities; a start symbol; and a null symbol. Prior to learning, task HMM is set up as shown in Fig.&amp;nbsp;  by adjusting the observation and transition probabilities. For example, intuitively the task HMM starts when a person is detected in a rack zone either handling or carrying a load. The observation probabilities are further adjusted to account for the rate of correct activity classification in each zone. The sequence then progresses from left to right with no backward steps or hopping. A small noise value has been introduced into the transition probabilities to prevent instabilities at the learning stage. Without which, the process will be too sensitive to erroneous activity classifications. Also, note that the probability of observing a sequence decreases exponentially with duration. Thus, downsampling is advisable. Downsampling is achieved by resampling with a lower frequency from a smoothed sequence of area-based activities which also serves to reduce the noise level. Each observation in the smoothed sequence is derived by taking the maximum vote in a neighbourhood around the corresponding point in the initial area-based activity sequence. The background actions are represented by the background HMM shown in Fig.&amp;nbsp; . The background HMM is a fully connected HMM with three states corresponding to the three activity zones. It provides a general model for the motion patterns of the human workers between these activity zones. The initialization of the background HMM, prior to training, allows for equal transition and observation probabilities. The sections of the training data which do not include a task sequence are used when training the background HMM. Once an occurrence of a task sequence has been distinguished from the background, the trajectory of the segmented task sequence will be compared against the task map (see Fig.&amp;nbsp; ). The task map is the accumulated trajectories for each task over different occurrences in the training set. The distance between the trajectory of the segmented sequence and a specific task trajectory is the mean of the closest point distances of all the points in the detected sequence. It is easy to imagine that the description, appearance, components and peculiarities of tasks would be different from one application to another. However, the distinction between the concepts of activity and task can help to improve the reusability of this approach. For example, while task 1 in our application is a very specific task which will only be observed in this scenario, an activity such as walking is very generic and will appear in many other applications and can be detected and identified with the same method which was described here. To arrive at detecting tasks from activities, the overall methodology as was shown in Fig.&amp;nbsp;  can be used, while only the task HMMs need to be redesigned from scratch. Another advantage of this approach is in providing better situation awareness, by breaking down the tasks into activities as opposed to using the raw video input for task detection. Thereby we can provide a real-time commentary-like report of the stages of tasks as they happen. We can also provide a probabilistic identification for different tasks on partial data which can be very useful if we are dealing with long duration tasks. Also in terms of algorithm design and parameter tuning, this approach provides a more intuitive and self-contained parts which can be tuned, tested and improved separately. In order to be able to use this approach the problem should satisfy these conditions:        The overall objective should be to detect a set of well-defined repetitive tasks.         &amp;nbsp;           The tasks should be performed by physical activities of humans.         &amp;nbsp;           The image resolution should allow for the perception of the activities which make up the tasks.         &amp;nbsp;           A stable detection and tracking should be obtainable.         &amp;nbsp;      We have encountered an example of a task for which our approach did not provide a suitable solution. This task is welding. Once all the parts are carried and placed on the welding cell one to three workers will weld the pieces together. While welding, for the most part the human workers appear as standing motionless while they are holding the welding device. Welding also poses a challenging problem for our motion-based detection and tracking, where flares which are caused by welding fly about randomly. We shall demonstrate later in Sect.&amp;nbsp; 4.4 that other approaches can be used to handle these cases. The overall objective should be to detect a set of well-defined repetitive tasks. The tasks should be performed by physical activities of humans. The image resolution should allow for the perception of the activities which make up the tasks. A stable detection and tracking should be obtainable. Our approach can be suitable for other factory and manufacturing environments where certain well defined tasks are expected. To apply this approach, the system designer would first need to understand the tasks. The definitions of the tasks require knowledge of the environment and are to be outlined by the expert. The system designer will then need to: (i) identify the lower level activities which make up these tasks and identify the key areas of the visual field of view (if applicable); (ii) design the task HMMs; (iii) label ground truth samples for tasks and activities. Thereafter the feature extraction and feature selection for activity recognition parts can be re-used and activity HMMs and background HMM can be re-used with small adjustments. The overall hierarchical procedure would be the same as shown in Fig.&amp;nbsp;.
Springer.tar//Springer//Springer\10.1007-s00138-013-0529-6.xml:E-LAMP: integration of innovative ideas for multimedia event detection:Multimedia content analysis Multimedia event detection:  1 Introduction  With ever-expanding multimedia collections, multimedia content analysis is becoming a fundamental research issue for many applications such as indexing and retrieval. One of the interesting problem of multimedia content analysis is to automatically detect some predefined events in a video collection. An event is a complex activity occurring at a specific place and time which involves people interacting with other people and/or object(s)&amp;nbsp;[ 31]. In general, an event consists of a number of human actions, processes, and activities that are loosely or tightly organized and that have temporal and semantic relationships to the overarching activity. Given a collection of test videos and a list of test events, the task of event detection is to indicate whether each of the test events is present anywhere in each of the test videos. Compared with traditional concept analysis&amp;nbsp;[ 23,  36,  38], event detection is a more challenging task due to its dynamic attributes and semantic richness. For example, the event of “making a cake” consists of a combination of several concepts such as “cake”, “people” and “kitchen” together with the action “baking” within a longer video sequence. Figure&amp;nbsp;  shows a couple of example snapshots of the videos from the event “Changing a vehicle tire” and the event “ Grooming an animal” which are defined by TRECVID Multimedia Event Detection 2011 task. The study of the multimedia event detection first emerged in structured scenarios, e.g., surveillance videos, sports videos and news videos&amp;nbsp;[1, 33, 44, 48]. Recently, people started to focus more on general unconstrained videos such as those obtained from internet video sharing web sites like YouTube. To facilitate and encourage the research of new technologies and algorithms for multimedia event detection, the ACM Multimedia society has launched three international workshops on events in multimedia (EiMM’09-’11) and the National Institute of Standards and Technology (NIST) launched the task of “Event detection in Internet multimedia (MED)” in 2010 TREC Video Retrieval Evaluation (TRECVID) workshop&amp;nbsp;[31]. In general, there are three core challenges in detecting multimedia events: The first is that to detect an event one has to extract a comprehensive set of features from the raw video. In general, the procedure of extracting those features is computationally expensive and time-consuming. This is particularly a serious problem for a large collection of videos. For example, in the TRECVID MED’12 task, the testing video set consists of about 4,000 h of videos, and on YouTube there are about 30 million hours of videos uploaded each year. Even with the help of powerful computer clusters, how to efficiently extract a comprehensive set of features over large video collections is still a big challenge. The second challenge is that with extracted features from the videos, what representations should be used so that different aspects of the information conveyed by the features can be effectively utilized to model an event. The third challenge is how to model an event by jointly exploring the multiple modalities provided by either different features or/and different representations of the same features. In addition to these three challenges of general multimedia event detection, a new challenging task was defined by TRECVID MED’12 which is the event detection with few positive exemplars. In this challenge, only a very limited number of positive example videos of an event are provided, specifically,10 positive videos, thus the traditional classification scheme which works well for event detection with relatively large number of positive training examples might not be suitable anymore for the event detection with limited positive examples. To tackle these challenges of multimedia event detection, we have developed a framework&amp;nbsp;[ 5,  11] within which we implemented the Event Labeling through Analytic Media Processing (E-LAMP) system&amp;nbsp;[ 5]. Figure&amp;nbsp;  shows the overview of the framework. In this paper, instead of describing the whole framework, we focus us on a couple of key components (highlighted by red boxes in Fig.&amp;nbsp; ) which are novel and essential in helping us to achieve both effective and efficient event detection. More specifically, for the challenge of efficiency in feature extraction, we conduct comprehensive studies which reveal that using features extracted from the videos with reduced resolution may not degrade the performance of event detection. However, this can dramatically improve the efficiency of feature extraction which could be very important for processing large scale video datasets. For the feature representation, the spatial bag-of-words model achieves good performance by extending the classic bag-of-word model with spatial layout information. However, the commonly used spatial layout is very arbitrary and may not be effective to capture the complex spatial information embedded in different videos. In this paper, we systematically test a large number of different spatial layouts, i.e., tilings, to select the best one for each feature and each event. Our results show that the selected tilings can capture the spatial information more effectively than the commonly used tilings, thus improve event detection performance. To jointly explore the multiple modalities provided by the features and the associated representations, we have developed an algorithm which learns a more robust and discriminative intermediate representation from multiple features so that better event models can be built. Finally, to tackle the more challenging problem of event detection with few positive exemplars, an innovative algorithm is developed which is able to effectively adapt the knowledge learnt from auxiliary sources to assist in event detection. The rest of the paper is organized as follows. In Sect.&amp;nbsp;3, we introduce our studies on how video resolution can effect the efficiency and the effectiveness in event detection. In Sect.&amp;nbsp;4, we introduce our work on selecting the better tiling so that the spatial layout information can be encoded more effectively, and thus improve the performance in event detection. In Sects.&amp;nbsp;5 and &amp;nbsp;6, we introduce our novel methods developed to model the events with respect to different number of positive examples and the last section is our conclusion.    2 Related work  The study of multimedia event detection first emerged in structured scenarios, e.g., surveillance videos, sports and news videos. For example, in&amp;nbsp;[1], a robust real-time detection method using multiple fixed-location monitors was introduced to detect unusual events in surveillance videos. In&amp;nbsp;[48], an unsupervised online algorithm is developed for detecting unusual events in surveillance videos via dynamic sparse coding. Sadlier and O’Connor &amp;nbsp;[33] proposed to use audio-visual features and support vector machine to detect events in field sports videos. Xu et al. [44] presented a novel approach for event detection from the live sports game using web-casting text and broadcast videos. Wang et al.&amp;nbsp;[41] developed an multi-resolution bootstrapping framework framework for concept detection in news videos by exploring knowledge of sub-domain. With the success of event detection in those structured videos, people started to focus more on the general unconstrained videos such as those obtained from internet video sharing web sites like YouTube. Since 2010, the ACM Multimedia society has launched three international workshops on events in multimedia and a couple of interesting works have been reported&amp;nbsp;[28]. For example, Makkonen et al.&amp;nbsp;[26] try to detect events by clustering videos from large media databases. In&amp;nbsp;[4], build on recent work on local feature trajectories, the authors investigate the impact of a new trajectory filtering scheme and two new trajectory descriptors to the detection of such video events. Mertens et al. &amp;nbsp;[27] exploit non-speech audio features for building acoustic super-models that detect complex events from low-level audio features, and show that even using audio alone they can achieve high recognition rates. To facilitate and encourage the research of new technologies and algorithms for multimedia event detection, the TREC Video Retrieval Evaluation (TRECVID) workshop supported by National Institute of Standards and Technology (NIST) launched the task of “Event detection in Internet multimedia (MED)” in 2010&amp;nbsp;[ 31] which is often referred to as pre-specified multimedia event detection. In the MED task, a participant needs to detect the occurrence of an event within a video clip in the archive based on an event kit. According to the definition from NIST&amp;nbsp;[ 31], the event kit defines an event which consists of:            Since launched, the TRECVID MED task has quickly attracted many top research groups in both academic and industry and the TRECVID MED datasets have become the popular testing bed for multimedia event detection. Among the available datasets, the MED’11 training dataset is often used because of its moderate size and complexity. More specifically, there are 9,746 videos in the MED’11 training dataset which are about 300 h and belong to 18 events. Among the 18 events 3 events, e.g., P001 to P003 are from the MED’10 and the rest 15 events E001 to E015 are newly defined in MED’11. Table&amp;nbsp; 1 list the name of the 18 events. There are totally 1,543 positive videos for the 18 events and the rest videos are background videos which do not belong to any of the 18 events. “An event name which is an mnemonic title for the event An event definition which is a textual definition of the event An evidential description is a textual listing of attributes that are indicative of an event instance. The evidential description provides a notion of some potential types of visual and acoustic evidence indicating the event’s existence but it is not an exhaustive list nor is it to be interpreted as required evidence A set of illustrative video examples each containing an instance of the event. The examples are illustrative in the sense that they help to form the definition of the event but they do not demonstrate all possible variability or potential realizations”. To evaluate the performance in event detection, a couple of metrics are adopted by NIST for TRECVID MED tasks. Among them, the minimum normalized detection cost (minNDC) was used in the TRECVID MED’11 evaluation. The normalized detection cost (NDC) is computed as:    where   is the missed detection probability and   is the false positive probability for the system of a given event.  ,   and   are predefined constants which are  ,   and  , respectively. The minNDC is the minimum NDC a system can achieve on an event and the smaller value of minNDC indicates better performance. The main observation from recent successful MED systems &amp;nbsp;[2, 9, 14, 17, 20, 29, 31, 32, 39] is that the following components are in general important in achieving good performance in multimedia event detection: The first important component is that a comprehensive set of features are required to be extracted from both video and audio channels so that different aspects of the information conveyed in the videos can be captured. Table&amp;nbsp; 2 lists the features used by the E-LAMP system. Second, those extracted raw features need to be converted to appropriate representations which can be utilized in modeling the multimedia event. The most widely used non-parametric representations are the bag-of-words model (BoW)&amp;nbsp;[10] and its extension spatial bag-of-words model&amp;nbsp;[19] which incorporates the spatial layout information in to the bag-of-words representation. For the parametric representation, the Gaussian Mixture Model&amp;nbsp;[15] is the classical one and also shows good performance in multimedia event detection. The third important component is how to model an event. Typically, the classical classification scheme is employed to model and detect the events when a relatively large number of positive training examples of an event are available. For example, the support vector machine with  kernel has shown good performance. A more challenging situation in modeling an multimedia event is when there are only very few positive examples because in practise precisely labeled training data are difficult to obtain. In this situation, the traditional classification scheme which works well using relatively large number of positive training examples might not be suitable anymore. Ma et al.&amp;nbsp;[25] present a pioneer work to tackle this challenge using the knowledge adaptation. The fourth component is how to fuse the multiple modalities to achieve good detection performance. The multiple modalities can come from different sources. For example, different features, same feature with different representations, different models built from different features, etc. Many fusion methods&amp;nbsp;[3, 12, 30, 35] have been proposed and in general they can be categorized into early fusion which fuses the feature representations or late fusion schemes which fuses the detection scores&amp;nbsp;[37]. Recently, in&amp;nbsp;[18] the authors propose a double fusion method which combines the early fusion and late fusion together so that the overall performance can be further improved.    3 Improve efficiency of feature extraction by reducing video resolution  It has been shown that extracting a comprehensive set of features from the videos is an effective way to achieve good performance in multimedia event detection&amp;nbsp;[ 2,  9,  14,  17,  20,  29,  31,  32,  39]. However, the feature extraction is in general computationally expensive and time consuming, especially for those motion features, e.g., MoSIFT, STIP and Dense Trajectory. This problem becomes more serious when the total hours of videos to be processed is large. For example, the TRECVID MED’12 testing dataset contains about 4,000 h of videos. In Table&amp;nbsp; 3, we demonstrate this problem by showing the time spent on extracting MoSIFT feature over the MED’11 training dataset which is introduced in Sect.&amp;nbsp; 2. There are 9,746 videos in this dataset and the total length of the video is about 300 h. From the table, we can see that it will take about 16,200 h to generate the MoSIFT feature using a common single CPU core which is more than 50 times realtime of the videos. To improve the efficiency of the feature extraction, we take a simple strategy which is to reduce the resolution of videos and then extract those motion features on the resized videos. More specifically, we reduce the resolution of videos according to the following criteria: If the width of the video is greater than 320 pixels, resize the video width to 320 pixels. The height of the video is resized according to the aspect ratio of the video. Otherwise, skip this video. There are two reasons why we use 320 pixels as the target video width. The first ones is that our experiments show that this resolution preserves the vast majority of the features and further reducing the resolution significantly degrades the performance. The second reason is that a relatively large portion of the videos in the dataset will be resized. Table&amp;nbsp; 4 shows the statistics of video resolution in the TRECVID MED’11 training dataset. In Table&amp;nbsp;3, we show the time spent on video resizing and the extraction of MoSIFT feature over the resized videos. Compared to the raw videos, the total time spent on extracting MoSIFT features is reduced from 16,200 to 4,920 h which is about three times faster. It clearly shows that on the resized videos the time spent on MoSIFT feature extraction is significantly less than that on the original videos. Furthermore, because fewer features are extracted on low resolution videos, the time spend on generating the bag-of-words model is also reduced dramatically. We test the performance of the MoSIFT feature extracted from the resized videos on the TRECVID MED’11 training dataset. More specifically, we randomly sample half of the positive videos and null videos to form the training set and the rest half are used as testing set. To evaluate the performance, we adopt the minNDC as our evaluation metric which is introduced in Sect.&amp;nbsp; 2. In general, smaller minNDC value represents better performance. Table&amp;nbsp; 5 shows the performance of event detection using MoSIFT features extracted from resized videos vs. original videos. From the results, we observe that on average the performance of the resized MoSift is even a little bit better than that of the original MoSift. However, a simple   test shows that at 95&amp;nbsp;% significant level, the difference between the results from two methods is not statistically significant. For other motion features used in our system, e.g., STIP and Dense Trajectory, similar efficiency results to MoSIFT can be obtained but we omit them here.    4 Tiling  The spatial bag-of-words model (Spatial BoW) is the most widely used representations for raw features which is an extension of the classic bag-of-words model by incorporating the spatial layout information. In the spatial BoW model, an image is geometrically partitioned into several grids or tiles. A sperate histogram is then generated to describe each tile and the whole image is finally described as the concatenation of the histograms of all tiles. One problem of spatial bag-of-words model is that the existing tilings, e.g., the spatial partition of the image, are very limited which may not able to capture the versatile spatial information from the videos. For example in the spatial pyramid matching, the  ,   and   tiling are used and in &amp;nbsp;[ 5] the  ,   and   tilings (shown in the first row in Fig.&amp;nbsp; ) are adopted. However, the use of those tilings is ad-hoc and some preliminary work has shown that other tilings might produce better performance&amp;nbsp;[ 40]. To find more representative ways to encode the spatial information, we systematically tested about 80 different tilings to select the best one for each feature and each event. More specifically, our candidate tilings consitst of individual and the combination of the basic tilings which are:  ,  ,  ,  ,  ,  ,  ,  ,   and   tilings. Table&amp;nbsp; 6 shows the performance of feature specific tiling vs. our standard tiling, e.g.,  ,   and   tilings, on MED’11 training dataset introduced in the Sect.&amp;nbsp; 2. From the table, we can see clearly that for all of the five features, the feature specific tiling performs consistently better than the standard tiling. Table&amp;nbsp; 7 shows an example of the performance of event-specific tiling vs. classic tilings on a difficult event identified from MED’12 training data which is the event E025 “Marriage Proposal”. It can be seen clearly that the event specific tiling can improve the performance over standard tiling.  
Springer.tar//Springer//Springer\10.1007-s00138-013-0530-0.xml:Efficient segmentation of leaves in semi-controlled conditions:Graph cut Expectation-Maximization Species identification Electronic field guide Image segmentation:  1 Introduction  Agarwal et al.&amp;nbsp;[1] and Belhumeur et al.&amp;nbsp;[7, 26] proposed mobile systems for plant species identification via leaf recognition. These systems used the shape of a leaf to identify its tree species, and required that the leaves be photographed against a plain light-colored background. Large datasets of these leaf images were collected, giving us the opportunity to study a useful semi-controlled segmentation problem, which at the same time allows for objective assessments of different methods. On the other hand, much research in image segmentation has been dedicated to the problem of general segmentation in uncontrolled settings. For example, Arbelaez et al. present a comparison of methods on images of several different types&amp;nbsp;[4], which constitute the Berkeley Segmentation Dataset and Benchmark&amp;nbsp;[3]. Similarly, Alpert et al.&amp;nbsp;[2] collected a database with a variety of different image types. Thus, much previous work has been dedicated to designing general-purpose methods that have high agreement with users’ subjectively defined hand-drawn regions. The leaf segmentation problem we address here is different in that the environment is much more constrained. Furthermore, there is very little subjectivity involved in defining the ground truth for these images. Finally, we are also concerned in precisely locating segmentation boundaries. Precise boundary detection is important for plant identification since the leaf shape will be used as the main recognition cue. Again, this is in contrast to previous work, as precise boundary detection is usually not a prerequisite for general-purpose segmentation methods. Though at first sight segmenting a leaf against a plain light-colored background seems easy due to the somewhat controlled conditions, in fact it poses significant challenges. The task requires that segmentations be produced in time that is suitable for an interactive application, and whose boundaries are faithful to the true leaf boundaries, sufficient enough to enable correct recognition. Figure&amp;nbsp;  presents examples of leaf images from the three datasets we experiment within this study, to illustrate the variety of leaves and acquisition conditions that must be dealt with. In Fig.&amp;nbsp; , we demonstrate the difficulty of traditional methods in this problem. A series of undesirable results are shown, resulting from the GrabCut method (see Sect.&amp;nbsp; 4.3) when applied in difficult scenarios. The GrabCut method was chosen here for illustration purposes since it is very well known and was one of the best performing on our datasets (see Sect.&amp;nbsp; 5). Below we more generally identify a series of significant practical challenges, several of which were previously noted [ 26].      The datasets present a large variety of leaf shapes, due to the diversity among the different species. In particular, compound leaves are especially difficult to segment for traditional methods, due to their complex segmentation boundaries, some of which include several concavities and holes. Examples are presented in Fig.&amp;nbsp;  Pine leaves were identified to present a special difficulty, since most of them occupy only a small fraction of the image. Many methods fail on these because they are biased towards producing larger segments. Figure&amp;nbsp; in Sect.&amp;nbsp;5 illustrates this situation well, by showing results of applying the GrabCut method to several photos of pines leaves.   The leaf images present natural variations in lighting. One of the most difficult problems on these datasets has been correctly segmenting out the shadows that the leaves cast on the background. Refer to Fig.&amp;nbsp;  The color of a leaf can vary, producing difficulties as presented in Fig.&amp;nbsp;h. In addition, some tree species have shiny leaves, occasionally producing specular highlights which can confuse segmentation methods, as in Fig.&amp;nbsp;  The venation patterns on leaves can be very light-colored, presenting a strong contrast and creating a strong edge with the rest of the leaf, as in Fig.&amp;nbsp; Speed is a major challenge for our application, since the segmentation method should work as part of an interactive system. Coupled with this, high-resolution images are required in order to capture fine-scale details, such as leaf serrations. The datasets present a large variety of leaf shapes, due to the diversity among the different species. In particular, compound leaves are especially difficult to segment for traditional methods, due to their complex segmentation boundaries, some of which include several concavities and holes. Examples are presented in Fig.&amp;nbsp;c, d. Pine leaves were identified to present a special difficulty, since most of them occupy only a small fraction of the image. Many methods fail on these because they are biased towards producing larger segments. Figure&amp;nbsp; in Sect.&amp;nbsp;5 illustrates this situation well, by showing results of applying the GrabCut method to several photos of pines leaves. The leaf images present natural variations in lighting. One of the most difficult problems on these datasets has been correctly segmenting out the shadows that the leaves cast on the background. Refer to Fig.&amp;nbsp;a–c. The color of a leaf can vary, producing difficulties as presented in Fig.&amp;nbsp;h. In addition, some tree species have shiny leaves, occasionally producing specular highlights which can confuse segmentation methods, as in Fig.&amp;nbsp;e, f. The venation patterns on leaves can be very light-colored, presenting a strong contrast and creating a strong edge with the rest of the leaf, as in Fig.&amp;nbsp;f, g. In this work, we experiment with several available implementations of popular segmentation methods. In order to provide an objective evaluation, we manually segment images from two leaf datasets. We also present qualitative observations on these two datasets, as well as an additional third dataset, composed of images uploaded by users of the mobile leaf identification system. We observe that the segmentation methods do not immediately work well for leaf segmentation. First of all, several methods were not developed with speed as a requirement and, for reasonably sized images, cannot produce results in sufficient time. Other methods have their own specific inherent biases, and would require the introduction of important modifications in order to work well throughout the different leaf species. We thus present extensions to our previously proposed segmentation method, which was described in the work of Kumar et al.&amp;nbsp;[26]. We add a graph cut step as well as learning of several parameters of the method, via a training a set of manually segmented images. The new approach is still fast, while producing improved segmentation results.    2 Related work  Several plant species identification approaches were presented for the ImageCLEF plant identification task&amp;nbsp;[22]. The task’s datasets are composed of leaf images, along with their respective metadata. Some of the identification approaches proposed have a leaf segmentation step, while others do not. The  and  datasets are photographed against a uniform background and usually segmented using some variant of Otsu’s thresholding method&amp;nbsp;[34]. For literature reviews of plant identification approaches using images of leaves, we refer the reader to&amp;nbsp;[7, 22]. A more sophisticated leaf segmentation approach was presented by Cerutti et al.&amp;nbsp;[13], in which polygonal shape models were used to constrain an active contour. Due to the variety of leaf shapes, modeling becomes a complex problem. The authors presented a model for leaves with multiple numbers of lobes, while a separate model was used for compound leaves. The approach seems to be especially beneficial when dealing with unconstrained environments (see below). Though it should work well on leaves with known shape, it could be prone to failure on leaves with shapes that were not modeled. This lead the authors to only consider Angiosperm species for segmentation in ImageCLEF&amp;nbsp;[13]. A similar approach (though somewhat more simple) was presented by Manh et al.&amp;nbsp;[29], in which a single shape model was used with the goal of segmenting leaves of a particular species of weed. In the present work, we do not adopt any kind of shape prior, due to the difficulty of the modeling problem. Leaves can be simple, compound, or found grouped into clusters. More direct strategies such as that of Cerutti et al.&amp;nbsp;[13], Nilsback and Zisserman&amp;nbsp;[31] for flowers or by Kumar et al. for other object classes&amp;nbsp;[25], do not appear to be sufficient for our problem. These kinds of shape priors involve a somewhat limited amount of flexibility, which makes it difficult to model the variety and complexity of leaf shapes. Leaf segmentation is much more difficult on the  ImageCLEF dataset, in which photographs are taken in unconstrained conditions. Some authors have attempted to use completely automatic approaches, but with limited success. Casanova et al.&amp;nbsp;[12] experimented with k-means to cluster the image pixels into  and . Yanikoglu et al.&amp;nbsp;[39] assume the central region of the image contains the leaf, so that the largest cluster found from this central region (when performing histogram clustering) defines the leaf’s color. With this representative color in hand, an over-segmentation is found using watershed, whose segments are assigned to leaf or background according to the distance of their color to the reference leaf cluster color. Camargo Neto et al.&amp;nbsp;[11] also approached the unconstrained problem, in the context of image analysis for weed control. Their approach begins by finding leaf pixel candidates based on a color index. The final segmentation is found by creating a fragmentation of the set of candidate leaf pixels, whose fragments are then merged in a procedure that favors the formation of convex shapes. Most works, however, applied traditional  segmentation techniques in order to obtain reasonable results on ImageCLEF’s  images. Casanova et al.&amp;nbsp;[12] used mean shift&amp;nbsp;[16] to produce an over-segmentation of the image. A user would then indicate some background and foreground segments, which were used in a merging phase as to label the remaining segments. Cerutti et al.&amp;nbsp;[13] used an interactive version of their polygonal model method, in which a user initially marks a region inside the leaf, providing a color model for leaf pixels. Yanikoglu et al.&amp;nbsp;[39] use the marker-based version of the watershed transform, while Arora et al.&amp;nbsp;[5] adopted the interactive GrabCut system&amp;nbsp;[35]. Though user interaction is interesting in certain situations, it becomes impractical for large datasets with thousands of images or more&amp;nbsp;[22]. In this study, we focused only on fully automatic approaches. A system capable of working with photos of leaves on uncontrolled backgrounds would be very appealing from the perspective of a user. However, it is not clear to what extent such a system should rely on image segmentation. As noted by Bakic et al.&amp;nbsp;[6], shape boundary features of segmented leaves become unreliable in this scenario, leading several authors to work with interactive segmentation techniques or avoid the problem altogether&amp;nbsp;[22]. It is important to note that dealing with uncontrolled conditions requires approaches that are different in nature to the ones we study here, which are more appropriate in our semi-controlled scenario. We should note that for the task of plant identification, leaf shape is an extremely important cue&amp;nbsp;[7]. In order to use the leaf’s shape for recognition, it is not sufficient to provide only a coarse description of where the leaf is, but it is required that segmentation boundaries faithfully represent the true leaf shape. Due to this challenge, the authors of&amp;nbsp;[7] collected datasets in which leaves were photographed against a plain light-colored background. We approach the leaf segmentation problem under this semi-controlled condition. The current problem is thus substantially different from previous work on segmentation in uncontrolled environments&amp;nbsp;[22, 32, 38], in which a precise segmentation boundary is not as crucial. In our experiments, we use images taken in semi-controlled conditions, either in a laboratory setting, or taken with mobile devices against plain light-colored backgrounds (see Sect.&amp;nbsp;4.1). Our datasets were collected from a real-world functioning system for plant species identification via mobile devices&amp;nbsp;[26]. Images from our  and  datasets are used in practice as labeled training data within the system’s recognition engine, while the  images were taken by users of the mobile system, presenting us with a series of real challenges. We would like to point out that the leaf datasets from the ImageCLEF plant identification task&amp;nbsp;[22] are also relevant for the segmentation task, but we did not experiment with them. However, the datasets we have used are comprehensive and allow us to arrive at relevant conclusions, since they include a wide variety of species and acquisition conditions, and were taken from a real-world functioning system. It is important to note that images from our  dataset, which contain pressed leaves taken under controlled conditions, are representative of those in ImageCLEF’s  photos. At the same time, our  and  images, which were taken with mobile devices with leaves placed against plain light-colored backgrounds, represent to a good extent those in the  photos from ImageCLEF. The segmentation method we present here extends upon our previous work&amp;nbsp;[26], in which color space clustering was used to define foreground and background regions. Here, we introduce two major extensions. First, we add a graph cut&amp;nbsp;[9] step, applied on the pixel probabilities provided by the clustering method. The graph cut technique is widely used and well established in image segmentation. Its attractiveness comes from its global optimization formulation along with the fast methods available to find its solution. The second extension we introduce consists of estimating model parameters via a learning phase, using manual segmentations. This removes the need for manual parameter tuning, though it requires the availability of a dataset of manually segmented images. GrabCut was initially developed by Rother et al. for interactive image segmentation&amp;nbsp;[35]. However, given an appropriate initialization, the method is shown to be very useful for image segmentation in general, side-stepping the need for user interaction. The GrabCut approach has some similarity to the approach we adopt here, in that we perform color space clustering, followed by a graph cut step. However, there are two important differences. First, GrabCut makes use of multiple iterations, which alternate between model estimation in pixel color space and graph cut in image space. Our approach is faster, defining the model in a single step, after which graph cut is applied. Second, GrabCut uses a Gaussian mixture to model each class, instead of single Gaussians as we do here, thus we require less computation. In Sect.&amp;nbsp;5, we present a comparative evaluation of GrabCut and our method, demonstrating these differences. In this work, we learn certain graph cut parameters by selecting a range of parameter values and choosing the one that produces the lowest error on a training set of manually segmented images. This training strategy is able to work in a reasonable amount of time, since we end up only having two parameters to estimate. It has been noted—among others by Kumar and Hebert&amp;nbsp;[27]—that training of random fields using pseudo-likelihood inference is known to produce unreliable parameter estimates. Using more precise inference techniques becomes very time-consuming, so that more recent work takes an alternative, two-phase approach, similar to what we adopt here. In this two-phase approach, first unary potential parameters are learned in isolation. Then, the parameters describing binary or higher order interactions are learned by cross-validation (or hold out validation). An illustrative example of this type of approach was presented by Kohli et al.&amp;nbsp;[24]. A problem somewhat related to leaf segmentation is that of flower segmentation for species recognition. Nilsback and Zisserman&amp;nbsp;[32] developed a flower recognition approach which relied on segmentation prior to feature extraction. The flower segmentation problem is particularly challenging because image backgrounds are uncontrolled, while at the same time the appearance of different classes of flowers varies widely. The problem was dealt with by learning foreground and background color distributions, and by defining a flexible shape prior to capture the structure of petals. Chai et al.&amp;nbsp;[14] later proposed a co-segmentation method for flower segmentation, which did not require manual segmentations or modeling of a shape prior, yet showed increased performance. In the current work, we make use of manually segmented leaf images, following a more traditional supervised line of work.    3 Proposed segmentation method  The segmentation method we propose here is an extension of our previous work&amp;nbsp;[ 26]. Our method consists of the following steps, illustrated in Fig.&amp;nbsp;  and described in more detail in the following sections. First, the image pixels are clustered into two groups in saturation-value color space, so that one group corresponds to leaf pixels, and the other, to background pixels. The clusters are found using the Expectation-Maximization (EM) algorithm to fit two normal distributions to the pixel data. This provides us with a probability indicating how likely each pixel is to belong to each cluster, according to the model from the normal distributions. The probabilities define the energies used in the graph cut step that follows. The graph cut formulation also allows us to incorporate edge information, encouraging the segmentation boundaries to follow strong image edges. Finally, false-positive regions are removed from the segmentation via a post-processing procedure. The first step of our segmentation method uses the modified Expectation-Maximization (EM) clustering procedure in saturation-value color space that was described in our previous work&amp;nbsp;[26]. For completeness, we summarize the approach here, then explain the improvement that we introduced for pixel weighting. A mixture model composed of two normal distributions is fit to the pixel data in saturation-value space. If we denote a pixel’s saturation and value as  , the mixture model is       where   and   are normal distributions.   represents the mean of the foreground (leaf) distribution, while   is the mean of the background distribution. A common shared covariance matrix   is used. The set of model parameters is  . Note that each normal distribution is assigned an equal weight of  . The model is fit using the EM algorithm. From an initial estimate of the model parameters , EM proceeds to find a local maximum of the data’s likelihood (see e.g.,&amp;nbsp;[8]). This can lead to undesirable solutions if the initial parameters are not set carefully. The means for the normal distributions are thus initialized near their expected values, so that they converge to the correct clusters when provided with pixel data from a new image. The covariance matrix is simply initialized as a multiple of the identity matrix. In&amp;nbsp;[26], a pixel weighting approach was used. Pixel weighting was introduced in order to correctly segment pine leaves in which only a small fraction of the total image pixels were leaf. A region in saturation-value space that was likely to contain leaf pixels was manually defined. Given a new leaf image, the pixels inside and outside the region would be weighted so that each set of pixels had equal total weight. These pixel weights were then used during the EM procedure. Here, we will adopt basically the same weighting procedure, except that we define the region that is likely to contain leaf pixels using a training set of manually labeled leaf pixels. For the manually segmented images, we are provided a ground truth label for each pixel, which can either be  or . For each of the leaf and background classes of pixels, this allows us to estimate a probability density function in saturation-value space, using kernel density estimation [8]. Given a new leaf image, we divide its pixels into two sets: pixels that are more likely to be leaf according to the kernel density estimate, and pixels that are more likely to be background. As in the previous approach, each set of pixels is then assigned a weight, so that each set has the same total weight during EM. After running the EM procedure, we are able to compute an estimated probability that each pixel belongs to either leaf or background. We then include a graph cut step that uses these probabilities to determine the image segmentation, following the work of Boykov and Jolly&amp;nbsp;[9]. For quickly finding the optimal graph cut, we employ the optimized algorithm by Boykov and Kolmogorov&amp;nbsp;[10], whose implementation is provided by the authors. Let   denote the binary segmentation we wish to solve for, with all  , and   the total number of image pixels. The graph cut minimizes the energy function (or cost function)               is the regional term defined by the unary potentials of the underlying Markov random field (MRF) formulation and   denotes the boundary term, which penalizes discontinuities in the cut, and corresponds to binary or higher-order potentials in the MRF formulation. First, for the regional term, we will use the posterior probabilities provided by the EM procedure. Let   and   denote, respectively, the probability that pixel   is background and leaf, given its features  . These probabilities are assigned according to the model estimated by EM. Then the regional term is written                                      Following&amp;nbsp;[ 9], the boundary term is given by                         and   denotes the set of neighboring pairs of pixels in the image, which in our case we take to be the pairs of four-neighbors. From a grayscale version   of the image, we adopt the commonly used boundary coefficients       where   and   are, respectively, the gray values for pixels   and  . We work with a set of manually segmented training images, which allow us to estimate the graph cut parameters  and  (defined, respectively, in Eqs.&amp;nbsp;2 and&amp;nbsp;8). This is done by selecting a grid of  pairs and choosing the one that produces the highest average accuracy when applied to the images in the training set. After the graph cut step is applied to an image, we follow a straightforward post-processing procedure&amp;nbsp;[26] to remove undesired false-positive regions. The procedure removes two type of false positives. The first type, which is very common to these images, consists of regions along the image boundaries which fall outside the light-colored sheet of paper or background used to create a contrast against the leaf. The second type consists of isolated regions present due to shadows, uneven backgrounds, or extraneous objects. Post-processing consists of first performing a morphological dilation, then computing the connected components of the result. A connected component that has a large intersection with the image border relative to its area is then excluded as a false positive. Of the remaining connected components, the largest one is chosen to be the leaf. 
Springer.tar//Springer//Springer\10.1007-s00138-013-0532-y.xml:Generation of new points for training set and feature-level fusion in multimodal biometric identification:Nearest neighbor rule Convex combination Feature reduction Principal components MST Training set:  1 Introduction  Biometric recognition systems are useful in many security-related areas including passport verification, and authentication of persons with the help of fingerprints [1, 2], face [3, 4], iris [5], etc. For a given query (query can be an image in case of iris, fingerprint, palm print and face recognition problems; it can also be a signal in case of speech data), the system should be able to conclude whether it matches with any in its database. The system should have the option of deciding that the query does not belong to the given dataset, and hence it is an imposter. The problems mentioned above are all recognition problems. In multimodal biometrics, at least two different biometrics are used for the purpose of recognition. Multimodal biometric systems have gained importance over unimodal biometric systems due to the availability of more information for processing. However, how to fuse information of two different biometrics of the same person remains a matter of research [6]. This fusion can be performed in raw level, feature level, match score level or decision level. Since the feature set of any biometric trait is supposed to carry the most significant and rich information about raw biometric data, the integration at feature level is expected to provide better recognition performance than the other levels of fusions. Intuitively, the most important features of two biometric traits are expected to provide good performance. Although possessing these advantages, fusion in feature level is relatively understudied in comparison to other fusion levels [7]. The reasons are basically threefold. Firstly, the extracted features from different biometric traits may become incompatible to each other; for example, extracted minutiae from a fingerprint and derived eigen coefficients from a face seem to be incompatible. Secondly, when different feature vectors of different biometrics are concatenated together, the resulting feature vector may suffer from curse of dimensionality and would become very difficult to handle. Thirdly, and most importantly, designing a matcher algorithm for a fused feature vector, containing features of different multimodal traits, is more difficult than generating separate matcher algorithms for different single biometrics. Fusion of information at this level also faces other challenges such as large inter-person similarity, small intra-person variability and unknown relationship between features [8]. The problems regarding multimodal biometrics are as follows: (1) need to consider those biometric measurements which are non-intrusive and which provide good recognition rate; (2) development of recognition algorithms providing accurate recognition; (3) combining the information from these important biometric traits and obtaining, hopefully, a better recognition rate. Apart from these main problems, some other problems have also been attempted recently like face morphing [9–11] or generation of new faces. These new faces are included, many times, in the training set for improving the recognition rate. In this article, we shall not discuss which biometric measurements are non-intrusive, and which provide good recognition rates. We have considered data on faces and iris only. Here, new face and iris images are generated and they are included in the training set. The information from the two biometrics is combined at feature level. The resultant recognition rates are found to be significantly better than the existing recognition rates. A brief review on information fusion is given. In recent times, the combined classifier [12] approach has been adopted to get better results, especially at match score-level fusion. That is, we may have different feature sets, training sets, classification methods or training sessions, all resulting in a set of classifiers whose outputs may be combined, with the hope of improving the overall classification accuracy. A classifier combination is especially useful if the individual classifiers are largely independent. Various re-sampling techniques like bootstrapping may be used. Examples are stacking [13], bagging [14] and boosting (Or ARCing) [15]. Some of the important works regarding multimodal dataset fusion strategies other than feature level are briefly mentioned here. Zhang et al. [16] performed a score-level fusion of near infrared face and iris images. Ryan et al. [17] used different fusion techniques at both score and rank level for fusion of face and iris from a video sensor. Shoa et al. [18] developed an iris–face fusion system using weighted score method of score-level fusion. Zhang et al. [19] utilized canonical correlation analysis in low-quality images and performed score-level fusion using min max normalization. Fakhar et al. [20] performed a fusion of face and iris in match score level using weighted sum scores. Kang et al. [21] performed a score-level fusion in multiunit iris authentication using SVM and successfully increased iris recognition accuracy, in comparison to previous works. In a brief review of multimodal biometrics, Ross and Jain [22] have presented the idea of various levels of fusion, various possible scenarios and different modes of operation, integration strategies and design issues. For homogeneous feature sets (e.g., multiple fingerprint impressions of a user finger), weighted average of the individual feature sets are often used to compute the resultant feature set (e.g., mosaicing of fingerprint minutiae [23] and fusion using multiple hand features by Michael et al. [24]). On the other hand, for non-homogeneous feature sets (e.g., feature sets of different biometric modalities like face and hand geometry), we can concatenate them to form a single resultant feature set. Examples of feature-level fusion schemes proposed in the literature can be found in [6]. Chibelushi et al. [25] (voice and lip shape), Son and Lee [26] (face and iris), Kumar et al. [27] (hand geometry and palm print) and Ross and Govindarajan [7] (face and hand geometry). The literature on feature-level fusion is smaller in comparison to other fusion strategies. Wang et al. [28] established a complex FDA approach for classification in feature-level fusion of iris and face. Kanade et al. [29] performed a feature-level fusion of iris and face to generate long cryptographic keys for secure user authentication. Rattani et al. [30] performed a feature-level fusion of face and iris. Their algorithm computes the scale invariant feature transform (SIFT) features from the biometric sources. However, the method requires segmentation of the captured images. The SIFT method can be experimentally combined with a newly developed color image segmentation strategy [31] for improved processing speed and better performance. SIFT possesses the disadvantage of higher computational complexity and it could be speeded up using FPGA-based architecture [32]. Chen et&amp;nbsp;al. [33] extracted face and iris features separately and fed them into a wavelet probabilistic neural network classifier. The results are calculated on the decision layer of the neural network. However, in the context of multimodal biometrics, we have not come across the task of incorporating new points in the training set (generated from the existing class information) and doing classification using the new training set with the help of feature-level fusion. In this article, we have attempted to do it. A multimodal system can operate in one of three different modes: serial mode, parallel mode or hierarchical mode [34]. In this paper, the system design is taken to be in parallel mode. Here, new data points are generated from training set in feature space and used for recognition. The synthesis is done on the basis of inheriting features from intraclasses. Note that once a face image of a person is concatenated to his left and right iris images, we term the concatenated image, usually represented as a column vector, faris (face+iris). From now onward we will be using this term to signify the combined presence of face and iris as a single data point. Synthetic face generation [35] for the purpose of face recognition has been explored in recent times. 2D to 2D reconstruction and generation of new faces with various shapes and appearances received the attention of biometric researchers [36, 37]. However, its application is relatively unexplored in multimodal biometric scenarios. Reconstruction of an image from its feature space carries extra importance as it showcases the correctness of the applied feature reduction scheme. Face reconstruction using AAM by Cootes et al. [38] and multiview face reconstruction by manifold analysis [39] are popular face reconstruction techniques. This article follows the method of Madhura et al. [40] of constructing new face images and applies it over here to construct new faris images from existing faris points. The face databases used here are FIA [41], YALE [42] and FERET database [43] and the iris databases are MMU [44], CASIA [45] and DOBES [46]. FIA and YALE datasets consist of frontal faces with varying facial expressions, while FERET contains faces with different angular directions. In case of the iris database, the number of snapshots of the left iris and that of the right iris, in each class, is chosen to be equal. The paper consists of five sections. Section&amp;nbsp;2 consists of intuition and mathematical preliminaries and the following section introduces the proposed algorithm for synthetic faris generation. Section&amp;nbsp;4 contains a brief description of the datasets used and the design of experiments. Section&amp;nbsp;5 consists of analysis of experimental results.    2 Intuition and mathematical preliminaries  The mathematical preliminaries of the proposed method are given below. The assumptions made on the images are also stated below. Let us assume that all the face images under consideration have the same number of rows and columns. That is, all the face images are, say,  images. Similarly, all the iris images are assumed to have  rows and  columns. Every face image has exactly one face in it. Additionally, for every face image, the face occupies almost all the area of that image. That is, the number of non-face pixels in any face image is very small. There is no background for iris images too. The face images are assumed to be taken under laboratory conditions (i.e, illumination is same and the images are noiseless.). Every image is represented as a column vector. The class (face class, iris class or faris class) of a person is defined to be the set consisting of all possible images (face images, iris images or faris images) of that person. That is, it is a set of vectors where the number of components in each vector is assumed to be, say, . Note that ALL possible images include ALL possible expressions of the person concerned. Since we are looking for ALL possible images of a person, this set is assumed to be an uncountable set. We can also assume it to be bounded and closed [35]. We assume that a class is path connected [35]. That is, between every two face points, there exists a path joining those two points which is completely contained in the class. The class of a person is not known to us. However, a few points (i.e., the given images of a person) from the class are known. Then the class is estimated [35] as  where  are the given  observations in the training set from the class,  is the estimated threshold for the class,  denotes Euclidean distance and  is the estimated class of the person.  is calculated as follows [35] : (1) find minimal spanning tree (MST) of  where the Euclidean distance between two points is taken as the edge weight of the edge joining those two points; (2) among the  edge weights, find the maximal edge weight and represent it by ; (3) . Intuition of the above method is briefly discussed here. We do not know the class of a person. But, we do know that some points, say,  belong to the class. A point  belongs to a class, means that every point in a disc of radius  belongs to that class. Note that, by making small changes or no changes in the values of each component of the vector , the disc will be obtained.  is a measure of the amount of variation incorporated in . The procedure for estimating set is valid since the considered  goes to zero as  goes to  in probability [47–49]. If the estimation procedure is valid, and the classes are disjoint, then for a large number of points in the training set, the estimation procedure will provide more or less the true classes, which ultimately results in either very small or zero misclassification. Note that a disc is a convex set. That is, for every two points  and  in the disc, and for every , the point  belongs to that disc. Here,  represents a measure of variation imposed on . Since every point corresponds to an image in our formulation, there will always exist an intermediary image between any two different images. In fact, there will be infinitely many images between any two images. Thus, for generating new images, we use MST of all the points in the training set corresponding to one class, where edge weight is taken to be the Euclidean distance between the corresponding points. Each point on any edge of MST represents a new image with a unique variation. Here, we adopted Prim’s MST algorithm [50] to build an MST. This algorithm is illustrated below: Given a weighted connected undirected graph G with n vertices, Prim’s algorithm finds the MST T of G. Step 1. Find a minimum weight edge T of G. Step 2. Choose minimum weight edge , incident to a vertex in T. Step 3. Add e to T, provided it does not form a cycle in T. Step 4. Repeat from step 2, unless T contains  edges. We assume that the dimensionality reduction technique has been already carried out using principal component analysis [51]. Using PCA, the image space is converted to feature space. The whole process of class formation and all the designed algorithms are carried out in the reduced dimensional space.    3 Proposed algorithm  To apply the aforesaid intuition in image analysis, several face and iris images for the same persons are considered. The number of classes is the same as the number of persons. Let us consider  persons and for each person we have  face images of the same size and same background and  iris snapshots each for the left and right iris. The object size within the image, for all the images, is chosen to be more or less the same. If we represent an image by a vector, then we are considering all possible such vectors corresponding to a person. It can be noted that we do not know completely how many points (images) are sufficient to represent all possible variations of a person. Only a few points representing different variations of an image are known to us. Step 1. Let there be  classes; each class has  face images representing different variations and  snapshots of the left iris and  snapshots of the right iris images. Also, let the face images be numbered as  ; the left iris and right iris images are numbered as  and  respectively. Step 2. Form image set by concatenating   and   where   , for each class. Therefore, in each class there will be   images and in all classes a total of   images. This concatenation can be illustrated by an abstract pseudocode snippet defined below: for  to &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;for  to &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;for  to &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;vertically concatenate Step 3. Divide the total image set into test (TS) set and training(TR) set containing  and  images, respectively, where Step 4. Perform PCA on TR to obtain  projection vectors, where .
Springer.tar//Springer//Springer\10.1007-s00138-013-0533-x.xml:Biometric template protection with DCT-based watermarking:Bimodal system Watermarking Template Robustness Biometric security:  1 Introduction  With the advent of digital media and technological advancement issues of copyright and unique identification have emerged in quite new forms, creating pressing need to challenge the contemporary problem of digital legitimacy as well as inventing new ways to address identification and verification. A reliable identity management system is urgently needed to combat the epidemic growth to identity theft and to meet the increased security requirements in a variety of applications ranging from international border crossing to accessing personal information like secure access to buildings, computer systems, laptops, cellular phones, and ATMs. Systems that have the ability to authenticate individual accurately, reliably, cost effectively, and in a user-friendly manner are desired. The traditional token base (ID card, pass port, etc.) and knowledge base (password, pin, etc.) personal authentication system are not able to meet all of these requirements. Biometrics is a rapidly expanding area and focuses on identifying people based on inherent physiological or behavioral characteristics and bound individual at much deeper level than password and ID cards [1]. However, a single physical characteristic or behavioral trait of an individual sometimes fails to be sufficient for his/her identification/verification; for this reason, systems that integrate two or more different biometrics are currently being developed to provide an acceptable performance, to increase the reliability of decisions, to increase robustness with regard to fraudulent technologies, and to reduce Failure to Enroll Rate (FER) or Failure to Capture Rate (FCR) [2]. Even though biometric systems offer several advantages over traditional system they are still vulnerable to attacks [3, 4]. In [5] author produces a generic biometric system with eight possible hierarchical positions of threats. These threats can be fake biometric (fake finger, a face mask, etc), an old recorded signal (old copy of fingerprint, recorded audio signal of a speaker, etc.), a feature extractor could be forced to produce feature generated value chosen by attacker than that of the actual one, synthetic feature set, artificially match score, template manipulation (e.g. addition, modification, or template removal), template tampering due to a non-secure communication channel between stored template and matcher and overridden of matcher result. The attacks that can be launched against biometric systems have the possibility of decreasing the integrity of a biometric system. In order to promote widespread utilization of biometric techniques, an increased security of biometric data is necessary [6]. Encryption, cryptography, and watermarking are among the possible techniques to achieve this, but encryption does not provide security once the data are decrypted. The goal of cryptography is to make information unreadable. The security of a cryptographic system is dependent on the secrecy of the cryptographic key and hence, the key management is a key issue in cryptography. Watermarking involves embedding information into the host data to provide greater security. It seems appropriate to fit in a multimodal biometric authentication framework, such as in access control sites, credit cards, e-commerce, biometric passports, etc, to deal with storage and security issues. Proposed work addresses two critical issues in the design of a multibiometric system, namely template security and fusion methodology. An ideal template protection scheme should not degrade the recognition performance of the biometric system. A number of watermarking techniques have been proposed to secure information in an image. These can be mainly classified as spatial domain techniques and transformed domain [7–10] techniques. Recently watermarking techniques are also used in conjunction with biometric [11–19] to enhance the security of biometric. Ratha et al.&amp;nbsp;[14] proposed a blind data hiding method, which is applicable to fingerprint images compressed with WSQ (Wavelet-packet Scalar Quantization) standard. The watermark message is assumed to be very small compared with the fingerprint image. The quantizer integer indices are randomly selected and each watermark bit replaces the LSB of the selected coefficient. At the decoder, the LSBs of these coefficients are collected in the same random order to construct the watermark. Jain et al. [15] used the facial information as watermark to authenticate the fingerprint image. A bit stream of eigen face coefficients is embedded into selected fingerprint image pixels using a randomly generated secret key. The embedding process is in spatial domain and does not required original image for extracting the watermark. Noore et al. [16] proposed multiple watermarking algorithm, in texture regions of fingerprint image using Discrete Wavelet Transform (DWT). They used face and text information as watermark. Their approach is resilient to common attacks such as compression, filtering, and noise. Komninos et al. [17] combined lattice and block-wise image watermarking technique to maintain image quality along with cryptographic technique to embed fingerprint templates into facial images. Al-Assam et al. [18] proposed a lightweight approach for securing biometric template, based on a simple efficient and stable procedure to generate random projections to meet the revocability property. Nagar et al. [19] proposed biohashing and cancelable fingerprint template transformation techniques based on six metrics to protect biometric trait which facilitates the security evaluation. Their approach is vulnerable to linkage attacks. The problems of biometric template security raise concerns with the widespread explosion and deployment of biometric systems both commercially and in government applications. So by keeping security and secrecy issues in concern for the template security enhancement, in this paper, a novel biometric watermarking algorithm to support the capacity demanded by the multimodal templates is proposed. Section 2 describes the approach of biometric feature extraction and matching algorithms in brief. Sections 3 and 4 explain the proposed watermarking technique and fusion model, respectively. The results obtained are illustrated in Sect. 5.The matching ability of different biometrics without and with watermarking technique is verified and the resilience to various attacks during transmission and processing of signal is checked.    2 Biometric feature extraction and matching approach  Fingerprints and iris are selected as biometric as they are easily acquired, socially accepted, and more or less invariant to individual aspects like culture, sex, education level, orientation, etc. This section briefly explains fingerprint minutiae (features) extraction, iris feature extraction, and matching technique. To employ fingerprint minutiae extraction step, sensed print undergoes few necessary steps. In this work the raw finger print image has been routed through steps like (a) pre-processing: to extract fingerprint area, to remove the boundary, morphological opening operation requires to remove peaks introduced by background noise and closing operation to eliminate small cavities generated by improper pressure of fingerprint, (b) thinning: required to remove erroneous pixels which destroy the integrity of spurious bridges and spurs, exchange the type of minutiae points and miss detect true bifurcations, and (c) false minutiae removal: required to remove false ridge breaks and ridge cross-connections which are generated due to insufficient amount of ink and over inking, respectively. After extracting minutia points special feature vector   is generated corresponding to single minutia point   which is rotation invariant. Feature vector   is generated by defining surface geometry consisting of   radial grids  , with origin at the minutia point and grid separation angle   as shown in Fig.  . Grid   is oriented along the orientation of   minutia  . Grid nodes (points on grid) are marked along each grid at an interval of   starting with the minutia point   as the origin. Larger value of   and smaller value of   make the size of feature vector large. This will give better accuracy at the cost of increased computational complexity. By defining the orientation of grid nodes as   (1   m   N) the relative orientation between minutia   and node ridges is calculated as       which is free from the rotation and translation of the fingerprint.   represents the orientation of the ridge, that passes through the  th node, of  th grid, and for the  th minutia. If a node falls at furrows, then   is assigned as 0. The final feature vector   of a minutia   that describes local structural characteristic is then define as       where   gives number of grid nodes along the direction metric   corresponding to  th minutiae. Considering three grids and five nodes per grid, the size of specified feature vector obtained is 1  15 for each minutiae point. These feature vectors are converted into binary stream denoted as . Each relative orientation is represented with four bits, one for sign and three for orientation value. Individual minutiae data sets contained between 20 and 30 minutiae points, with an average of 25 minutiae points. Thus the size of  is approximately  bit for single fingerprint template. A distortion-tolerant matching algorithm [ 20] is used here that defines a novel feature vector for each fingerprint minutia based on the global orientation field. These features are used to identify corresponding minutiae between two fingerprint impressions by computing the similarity between feature vectors that gives high verification accuracy. Suppose   and   are the structure feature vectors of minutia   from input fingerprint and minutia   from retrieved features of fingerprint, respectively, then a similarity level is defined as            where   is the Euclidean distance between feature vectors   and   and   is the predefined threshold. Here, the selection of the value of   is trade-off between False Acceptance Rate (FAR) and False Rejection Rate (FRR); high value of   increases FAR and opposite is true for FRR. The value of   is empirically selected as 0.7 for whole database. Here, the similarity level describes a matching assurance level of a structure pair. The general iris recognition system consists of four important steps: (1) iris segmentation which extracts iris portion from the localized eye image, (2) iris normalization which converts the iris portion into rectangular strip of fixed dimensions to compensate for the deformation of pupil due to change in environmental conditions, (3) iris feature extraction deals with extraction of core iris features from the iris texture patterns and generate bitwise biometric template, and (4) iris template matching compares the stored template with the query template and gives the decision of authentication of a person based on some predefined threshold [21]. Among these steps the iris segmentation plays very important role in the whole system as it has to deal with eyelids and eyelash occlusions, and specular highlights. If iris portion is not properly segmented, then it may lead to poor recognition rates. Iris segmentation is done using pupil circle region growing technique which uses binary integrated edge intensity curve approach to avoid eyelids and eyelashes. After locating the iris inner and outer boundaries, which contain eyelids and eyelashes, we grow the circle of the pupil gradually and generate its edge image using sobel horizontal edge detector. As eyelids are horizontally aligned, horizontal biased sobel operator gives prominent horizontal eyelid edges. This approach is specially used to detect the upper and lower eyelid regions and to restrict the Region of Interest (ROI). When the computed horizontal edge intensity curve is below threshold value T1, it indicates that the eyelids portion has not started as shown in Fig.  . The radius is required to be grown until it covers eyelids. When the horizontal edge intensity curve exceeds threshold value T1, then it indicates that either upper or lower eyelid region has started appearing in the ROI. Here the growth of pupil circle is stopped. Thus the pupil circle is grown gradually to achieve a new outer iris boundary such that the area between the pupil boundary and new outer boundary does not contain eyelids or eyelashes. The threshold values T1 for horizontal binary edge curve has been set to 107 which is taken after the analysis of around 50 images and seems to be best for all database images. The partial iris region between iris inner and restricted outer boundary is converted into rectangular strip of fixed dimensions  by Daughman’s [22] rubber sheet model. Core feature of rectangular strip are extracted as suggested in [23]. The size of iris core feature  is 348 bits which is used as watermark. The matching between input iris feature and retrieved iris feature is done by standard Hamming Distance (HD).    3 Proposed watermarking approach  A Discrete Cosine Transform (DCT) states a sequence of finite data points in terms of a sum of cosine functions which oscillates at different frequencies. The 2-D DCT of an image ‘ ’ is given by        where            and the inverse transform is given by        where            The basis functions (transformation kernel) for   = 8 are shown in Fig.  . Each basis matrix is characterized by a horizontal and a vertical spatial frequency. The matrices shown here are arranged left to right and top to bottom in order of increasing frequencies. The top left pixel is known as the DC term, with frequency {0, 0}. It is the average of the pixels in the input and is typically the largest coefficient in the DCT of “natural” images [ 24]. Furthermore, most of the signal energy of the block DCT is dense in the DC component and the remaining energy always has a spreading diminishingly in the AC components in zigzag scan order as shown in Fig.   where the block with black shade represents DC component of 8   8 DCT block while blocks with gray and white shade indicate low-frequency and high-frequency AC component, respectively. Thus, hiding of watermark bit in DC co-efficient gives more robustness, but perception of watermark is a major issue and vice versa is true for high-frequency AC coefficients. As a tradeoff, proposed technique embeds watermark in low-frequency AC coefficients of the selected 8  8 DCT blocks. The technique proposed does not required original image for extraction and hence it is blind watermarking technique. In the proposed approach original image X of arbitrary size  is divided into non-overlapping 8  8 blocks. Let  be a pixel values from the block, where 18. Each block is transformed into a two-dimensional DCT block and categorized into smoother block or edge block. The eye is more sensitive to noise near edges. The embedding of information in edge block results in the perceptual effect in watermarked image. Therefore, it is necessary to remove edge block from image in embedding process. There are a number of approaches for such classification like the mean of pixel intensities  in an image block and the standard deviation of pixel intensities  in an image block. The feature  is a good supplemental tool in classification, but non-white backgrounds and light pictures may cause misclassification. Therefore,  should not be taken as primary because of the diversity of background colors. The value of standard deviation of pixel intensities ( in an image block is very small for background area and large for edge images. But  cannot be used to effectively discriminate edges and texture area in an image. Furthermore, these parameters are illumination-dependent parameters. The proposed method considers the local block structure tensor features to identify the block structural activity to remove them from bit embedding processing. In recent years, the Hessian matrix and its eigenvalues have been considered as edge and corner detection tools for many feature detection applications. Elements of Hessian matrix approximate 2nd order derivatives with respect to geometry and, therefore, encode the shape information and eigenvalues correspond to frequencies of vibration. Hessian matrix ( ) of an image   is defined as:            Let   and   be the eigenvalues of Hessian matrix. The pixel is considered as edge pixel if either one or both eigenvalues are found greater than the predefined threshold. To determine if an 8   8 block contains significant edges, edge pixel density of 8   8 blocks is calculated by computing total edge pixels within the block. The block is classified as edge block if edge pixel density is found to be greater than the predefined threshold. For the proposed algorithm, blocks with more than 30&amp;nbsp;% of pixels having either or both significant eigenvalues are discarded from embedding. Embedding of watermark bit is done by modulating low-frequency AC coefficients of 8   8 DCT block based on their estimated values. Estimated value of an AC coefficient is computed using the DC coefficients from 8 neighboring DCT blocks as shown in Fig.  , in which   9 are DC coefficients of neighborhood 8   8 blocks. By considering such 3   3 overlapping neighborhood DCT blocks,  , where 1 5 coefficients of center DCT block are estimated using equations shown in Eq. ( 7)            The notions behind the selection of   9 coefficients to estimate particular   coefficient in Eq.( 7) are:        Horizontal variations in each 8  8 DCT block are characterized by AC components  and  as shown in Fig.  (top left 2nd and 3rd block). Hence, DC values of horizontal neighborhood blocks (and ) are considered in the objective function for estimating  and .         &amp;nbsp;           Vertical variations in each 8  8 DCT block are characterized by AC components  and  as shown in Fig.  (top down 2nd and 3rd block). Hence, DC values of vertical neighborhood blocks ( and ) are considered for estimating AC components  and .         &amp;nbsp;             represents the diagonal variations as shown in Fig.  (left right diagonal 2nd block). Hence,  and  are considered for estimating AC components .         &amp;nbsp;      Linear programming (LP)-based optimization technique [ 25] is considered for calculating optimal weights (decision variable)   based on image content. LP is a mathematical method to achieve the best result in a given mathematical model for some list of requirements represented as linear relationships. Any linear program consists of four parts: a set of decision variables, parameters, objective function, and a set of constraints. The objective function fulfills the decision-maker’s desire (objective), whereas the constraints which shape the feasible region usually come from the decision-maker’s environment putting some restrictions/conditions on achieving the objective. Horizontal variations in each 8  8 DCT block are characterized by AC components  and  as shown in Fig.  (top left 2nd and 3rd block). Hence, DC values of horizontal neighborhood blocks (and ) are considered in the objective function for estimating  and . Vertical variations in each 8  8 DCT block are characterized by AC components  and  as shown in Fig.  (top down 2nd and 3rd block). Hence, DC values of vertical neighborhood blocks ( and ) are considered for estimating AC components  and . represents the diagonal variations as shown in Fig.  (left right diagonal 2nd block). Hence,  and  are considered for estimating AC components . The mathematical formulation of the problem is given below.        Write down the decision variables (here .         &amp;nbsp;           Formulate the objective function to be optimized as a linear function of the decision variables (Eq.7).         &amp;nbsp;           Formulate the other conditions of the problem (\(k&amp;gt;~0\)              &amp;nbsp;      In this method known   coefficients of benchmark images are used. All weights calculated for a particular   coefficient are stored in different matrices and histogram for matrix elements is computed. The histogram of this matrix with weights as elements is a discrete function       where   is the  th weight and   is the number of weights in the matrix having  th value. From this set of weights, a weight whose frequency of occurrence obtained maximum is taken and accordingly multiplied with corresponding   coefficient for the AC coefficient estimation. Table&amp;nbsp; 1 shows the weights derived from the experiment. The edge blocks are neither considered for estimation nor for watermark embedding because it leads to artifact in resultant watermarked image. Figure  a shows artifact when considering edge blocks along with smooth blocks for embedding watermark. Write down the decision variables (here . Formulate the objective function to be optimized as a linear function of the decision variables (Eq.7). Formulate the other conditions of the problem (\(k&amp;gt;~0\)). In this approach low-frequency   co-efficient of each smoother and texture blocks are selected for hiding watermark (fingerprint and iris features). Iris feature   and fingerprint features   are sequentially embedded by modifying the amplitude of transform domain   coefficients of selected DCT block. Modification is done based on comparison between original   value and its estimated value   as in Eq. ( 9), where   is a positive fraction which controls tradeoff between robustness and perceptibility, and ‘w’ in Eq.( 9) represents watermark obtained from cascading of fingerprint feature ‘ ’ and iris feature ‘ ’.            Decoding of watermark bit requires estimated value   and original value   of coefficient to extract watermark bit. If  \(AC_i &amp;gt;\mathop {AC_i }\limits ^\Lambda \) then extracted bit is ‘1’; otherwise, it is ‘0’.    4 Fusion model  In order to check the performance of fingerprint and iris system, DB1, DB2, DB3, and DB4 database in FVC2004 [26] and CASIA database version-1 [27] are used, respectively. Fingerprint database comprises of 800 fingerprint images of size 300480 pixels captured at a resolution of 512 dpi, from 100 fingers (eight impressions per finger). Individual minutiae data sets contained between 20 and 30 minutiae points, with an average of 25 minutiae points. To establish the verification accuracy of our fingerprint representation and matching accuracy, each fingerprint image of the same finger was matched with all other images of the same finger which forms 6,400 matching processes. This measures the FRR while the cross matching between the fingerprint images of different fingers which forms 57,600 matching processes measures the FAR. This means a total of 64,000 matching processes were done during experiments. A matching was labeled correct if the matched pair is from one of the eight impressions of the same finger in the FVC database and is incorrect otherwise. We have four scenarios: the first scenario says that the print belongs to the database but the test says it is false and measures the FRR at various thresholds. The second says that the print belongs to the database and the test says it is true and measures the Genuine Acceptance Rate (GAR) at various thresholds which equals to 1-FRR. The third says that the print does not belong to the database but the test says it is false and measures the FAR at various thresholds, while the fourth says that the print does not belong to the database and the test says it is true and measures the Genuine Rejection Rate (GRR). Given the similarity measure between two matching pairs, the FRR is defined as the proportion of known fingerprints that are falsely rejected, while the FAR is defined as the proportion of unknown fingerprints that are falsely accepted. CASIA database version-1 contains 756 gray-scale eye images of 108 users with resolution of 320280. Each user has 7 images captured in two sessions. Each image is represented by 348 bits after feature extraction. We have chosen 100 users from CASIA database and randomly correlate it with fingerprint data base to check improvement due to fusion approach. In biometrics system there is a tradeoff between FAR and FRR. Different system requirements of error rates are different based on application. So, it is a general practice to report the performance of the system at different decision threshold. This is done by plotting ROC curve. ROC curve is a plot of FAR/FRR against different decision threshold.
Springer.tar//Springer//Springer\10.1007-s00138-013-0534-9.xml:Tracking-by-detection of multiple persons by a resample-move particle filter:Data fusion Multi person tracking Particle filtering MCMC Monocular color vision Video surveillance:  1 Introduction  Visual multiple object tracking (MOT) has received tremendous attention in the vision community due to its numerous applications such as video surveillance in public or private human-centered environments (see a survey in [ 18]). Deploying a network of ceiling-mounted cameras is challenging as it should be easy to install by a non-expert user while the cameras should include on-board CPU resources to exchange high level data such as positions and characteristics of the target persons over the network. Since no intelligent camera dedicated to human tracking, contrary to human detection, is currently available off-the-shelf, our tracker is devoted to an intelligent camera with an FPGA board to execute parts of the algorithm in parallel (Fig.&amp;nbsp; ). Besides this broader technological aim, the traditional challenge with MOT is to simultaneously track persons that can a priori enter, exit, pass close to one another or merge in the scene. The objectives are twofold: (1) to correctly detect entering, leaving, and temporarily occluded targets, that is, characterize the targets’ status and, (2) to obtain a record of trajectories corresponding to the observed targets, i.e. the targets’ positions over time—maintaining a correct and unique identification for each target throughout. To cope with these difficulties, approaches based on tracking-by-detection and sequential Monte Carlo have become increasingly popular as they improve robustness in MOT by coupling target detection and tracking in the well-known particle filtering framework. They are well suited for parallelization [12, 22] as the standard PF weighs particles independently based on a likelihood function and then propagates these weighted particles according to an importance stage with an underlying proposal distribution based on a classical motion model. Various researchers have attempted to extend to MOT using decentralized approaches (namely one particle filter per target) [9, 10, 35] or centralized approaches [11, 26]; the latter deals more appropriately with the problem of joint data association between multiple targets. Recently, the introduction of Markov Chain Monte Carlo (MCMC) in the importance sampling stage has proved: (1) to deal more efficiently with high- and trans-dimensional state spaces and, (2) to require far fewer samples to adequately track the joint target state devoted to MOT [6, 27, 37]. The resulting MCMC-PF centralized approach propagates a set of unweighted particles (over time) which are drawn iteratively through a first-order Markov process. The limitations therefore are (1) its non parallelizability on clusters as the framework remains sequential like pure MCMC strategies and, (2) the high number of required burn-in iterations, especially for handling the continuous parameters, i.e. the targets’ positions. Our filtering approach combines the strengths of the two afore-mentioned algorithms—centralized PF and MCMC sampling steps combined with detection routines. We design a new filtering strategy where an MCMC sampling step using a resample-move strategy handles the discrete variables of the system, namely the targets’ status, leading to a more relevant sample cloud diversity. This breaks the time consuming burn-in iteration phase into multiple short Markov chains which are easily parallelizable. The continuous parameters can be handled with the traditional particle weighting stage. Regarding the person detection routines, we propose an efficient proposal distribution based on saliency maps that combine several information sources like diffusion dynamics, learned HOG+SVM person detections, and adaptive background mixture models. Combining detector responses within saliency maps is also considered in [4, 42], but it is fed into single or multiple object meanshift-based tracker in which the prediction is solely based on dynamics. Choudhury et al. [14] also use probability maps produced by a face detection routine in the PF weighting stage while the sampling stage is based on a zero-order dynamic model. Such probability maps, as far as we are aware, have only been marginally used in the importance sampling stage. This choice of proposal distribution is crucial in PF as it dictates ways to draw/predict the particles in the relevant state space areas. Finally, we propose experiments that demonstrate how the proposed approach outperforms the conventional and most closely related MCMC-PF strategy and how it is more suited for parallelization. The rest of the paper is organized as follows: Sect.&amp;nbsp;2 presents a brief survey of the literature in the area of multiple human tracking putting our work in perspective. Section&amp;nbsp;4 recalls some basic concepts about PF and MCMC-based methods; it then depicts our hybrid MCMC-based particle filtering framework. Sections&amp;nbsp;5 and&amp;nbsp;6, respectively, detail the tracker implementation and a discussion on the relative performance of our approach and prior filtering strategies in the context of monocular color vision-based video-surveillance. Finally, a brief summary and future works are discussed in Sect.&amp;nbsp;7.    2 Related work  Literature review on visual tracking is beyond the scope of this paper. As the proposed approach mixes the ideas of MCMC-PF and detection driven multiple person tracking, we mainly discuss these contexts in visual tracking. Particle filtering [3] offers a framework for representing the tracking uncertainty in a Markovian manner by only considering information from the current and the previous frame. The strength of this stochastic formulation and its numerous variants ( 1 [24], Mixed-state &amp;nbsp;[25], Auxiliary [32], etc.) lies in their simplicity, flexibility, and systematic treatment of non-linearity and non-Gaussianness while being more suitable for time-critical, online applications. In the PF framework, the classical MOT literature proposes decentralized or centralized solutions which are based on independent filters (one per target) or a joint state-space state representation respectively. As pointed out in [41], the classical  based on multiple independent PF performs poorly when the posterior is multimodal as the result of multiple targets. To circumvent this problem, several extensions have been proposed. Some researchers [34, 41] introduce a mixture PF, where each component/mode is modeled with an individual PF that forms part of the mixture. This mixture strategy is shown to require fewer samples but necessitates re-clustering of particles at each time step. Another decentralized solution is to propose interactively distributed filters, i.e. one filter per target, which is computationally inexpensive [31, 35, 45]. Wu et al. [45] use multiple collaborative trackers for MOT modeled by a Markov random network but they do not deal with the false labeling problem. The decentralized approach is carried further in [35] which proposed an interactively distributed MOT (IDMOT) framework using a magnetic-inertia potential model. Schemes like [43] are shown to improve the efficiency but also face limitations. However, the decentralized strategy suffers from the “data association error” whenever targets pass close to one another [5]. Consequently, the target with the best likelihood score typically “hijacks” the filters of nearby targets. To overcome this problem, recent approaches [9, 44] combine tracking with detection in decentralized PFs to re-initialize in case of target loss. In contrast,  estimate a joint state which concatenates all of the targets’ states and so estimates both discrete (number of targets) and continuous variables (targets’ positions) [11, 19, 23, 26]. By characterizing all possible associations between targets and observations, this formulation deals more appropriately with the joint data association problem. Some variants like kernel PF [11] improve efficiency, but also face the intrinsic limitations of centralized methods. Indeed, centralized PF suffers from exponential complexity in the number of targets due to the inefficiency of importance sampling which classically draws the particles from the system dynamics, i.e. “blindly” w.r.t the measurements. A remedy would be to steer sampling towards the high likelihood state space regions by incorporating both the dynamics and the measurements in the proposal distribution. Going one step further, an alternative addressed in [6, 13, 27, 37] is to replace the traditional importance-based sampling by a  (MCMC) sampling step within the joint PF. An unweighted sample swarm is obtained by storing the samples after the initial burn-in iterations in the Markov chain. Yet, this filtering strategy is shown to outperform their pure PF or MCMC counterparts. The required iteration number is yet worsened by the MCMC sampling which usually draws the particles solely according to the dynamics and so is possibly subject to a very high rejection rate. A major issue is then to draw the particles in the relevant areas of the high dimensional state-space while limiting the notorious burst in terms of particles and MCMC iterations.    3 Our approach  A first step to reduce the MCMC complexity is to design a novel MCMC-based PF where the MCMC sampling step is devoted only to the targets’ status with respect to people leaving/entering the scene. This sampling step through a small number of Markov chain moves is applied to each individual particle. These moves increase particle diversity with respect to the number of targets to track. This filtering strategy, which remains well suited for parallelization (except for the short Markov chain), is inspired by the  formalized by Berzuini et al. [8]. But, we leverage this strategy to address the visual tracking of a variable number of interacting targets, while Berzuini’s work centers on single track synthesis for trajectory calculation and uses MCMC to switch between different dynamic models. A second step to reduce the MCMC complexity is to construct an efficient data driven proposal distribution; it is widely accepted that proposal distributions that incorporate the recent observation outperform naive transition proposals considerably [33, 34, 36]. We propose a novel  to overcome the tricky/computational problem of combining multiple detector outputs in the tracking loop. There are a few attempts to combine the strengths of detection and tracking recently, but we can notice that almost all tracking-by-detection approaches consider a single detector. This is a major limitation of tracking-by-detection approaches especially those based on a single motion detector which often integrate static objects into their dynamically updated background models. To detect and track both moving and static persons, the expression of our proposal is given by a mixture of saliency maps corresponding to persons’ appearances and motion, but it can easily be extended to a wider set of detectors. A last consideration about tracking-by-detection paradigm concerns detectors trained either online or off-line. Many recent approaches [2, 9, 20] consider, both in the detection and tracking routines, the same observation model which is adapted on-the-fly. It is widely accepted that such unsupervised adaptation is prone to jitter and model errors may accumulate gradually [40, 44]. Off-line trained detectors are ideal means to provide such supervision as it relates to absolute information. In the vein of [10, 34], our approach merges two distinct data sources from preceding frames in traditional recursive fashion2 with that provided by off-line detectors devoted to the proposal. In essence, we combine the strengths of methods that rely on absolute information with those based on chained transformations. The former do not drift but cannot provide enough precision for every frame and so result in jitter. The latter do not jitter but tend to drift or even lose track altogether. Another fundamental difference from our detection routine is that classical tracking-by-detection approaches [9, 14, 20, 29, 30] assume detectors with high recall rates provide sufficient detection results. In contrast, we assume detectors with high recall merely output reliable detection results occasionally. Hence, contrary to [9, 14], the detectors are not used in the particle weighting stage but in the importance sampling stage which also involves the dynamics. The above review highlights the contributions of our centralized PF-based approach. The work most closely related to ours is the well-known MCMC-PF proposed by Khan et al. [27] but two major extensions are exhibited here. First, we use saliency maps (produced by several off-line trained detection routines) in the construction of the proposal. Second, we propose a new resample-move particle filter based on a short Markov chain to increase the parallelization capabilities compared to MCMC-PF. The result of interleaving saliency maps with our resample-move filter is a powerful and fully automatic MOT which outperforms the traditional MCMC-PF [27] (annotated  subsequently) and its upgraded version with saliency maps (annotated ).    4 Monte carlo-based tracking strategies  Recall that we aim to fit a template relative to each target all along the video stream through the estimation of its status , its image coordinates  and its scale factor . These parameters are accounted for in the state vector  related to the th frame. With regard to the system dynamics, the unpredictable motion of humans leads to define the state vector  and to assume that its entries evolve according to a mutually independent random walk model,   where  is a Gaussian distribution with mean  and covariance . Finally, the global state vector is defined by  where  is the number of targets. Each target might be in one of the four states: , in which case the target is outside the camera FOV; , in which it is a new target that appeared in the current image; , in which the target has been tracked for at least one previous image, and therefore, is already known; and , in which the target has disappeared in the current frame. Logically,  targets turn into  targets after processing the current image, and  targets likewise turn into  targets and as such, are removed from the state vector. In this section, we will take the above as a base and examine which different tracking strategies might be used to reliably track people in the scene. Markov Chain Monte Carlo (MCMC) methods have been used in a number of  systems, e.g. in [38, 46]. This class of algorithms, of which the most well-known is the Metropolis–Hastings algorithm, attempts to approximate an unknown distribution. This is achieved by creating a Markov chain of samples that successively approach the target distribution; in tracking systems, this corresponds to the actual scene configuration. As mentioned, the renowned MCMC algorithm is the Metropolis-Hastings (M-H) algorithm, which is shown in Table&amp;nbsp; 1, applied to  . This algorithm starts with a starting configuration  , from which a new state   moving the state subspace   of a randomly drawn target   (step #5) according to a transition model (the so-called proposal density)  , with   the measurements set, is proposed. The acceptance ratio   of this   state is then calculated (step #7) where the   are the pairwise interaction potentials between targets. This ratio is then compared to a uniformly drawn threshold  , which determines if the new state is accepted as state   for the next iteration, or rejected. This process continues for a high enough number of iterations that the algorithm can be said to have converged to a point in which drawing any further states is approximately equivalent to drawing from the distribution the Markov chain is simulating. While an MCMC with an adequate state transition model and acceptance ratio calculation will eventually converge, the number of iterations necessary to do so heavily depends on the starting position  and the discriminative capabilities of the measures used in the calculation of . As a result, it becomes necessary to run the Markov chain for a number of burn-in iterations before any accurate sampling can be attempted. An MCMC algorithm can adequately handle variable dimension state spaces, which is one of the challenges of , as shown by the Reversible Jump algorithm [21]. The criteria for dimensionality changes are that if the drawing of a new state  includes a probability for a dimensionality change, there must exist a corresponding probability for the opposite operation. This ensures that the search for the convergence can properly navigate the state-space. A common technique that simplifies both the drawing of  and the evaluation of  is for the state transition model to consider only changes to a randomly chosen subset of the state (in the case of , this translates into changing a single target per iteration). As a result, the parts of the state vector that remain constant can be simplified from the  calculation. While this does increase the number of necessary iterations, the complexity of the system is greatly reduced as a consequence. Khan et al. [27] proposed a mixed Markov Chain and Particle Filtering (MCMC-PF) algorithm, which we will henceforth call . commences by running a number of burn-in MCMC iterations, so that the Markov chain converges before it is used for particle sampling (Step #3, Table&amp;nbsp; 2). The initial state   is chosen randomly from the set of particles of instant  . Once the requisite number of iterations   has passed, a particle is chosen every   particles, until the desired number of particles   has been reached. Recall that these intervals are necessary because the MCMC process is modifying only a single target at a time, as mentioned in Sect.&amp;nbsp; 4.1. Once all the particles have been drawn, the MAP estimate corresponds to choosing the particle with the highest repeat count. The addition of the Particle Filtering step to the MCMC core renders  less vulnerable to “unlucky” draws, in which an extremely low threshold is drawn in the last few iterations that allow a low-probability state to be accepted without having enough iterations left to recover. It should be noted that, in its original implementation,  does not incorporate any detector data, instead relying on knowledge of the entry point of any new targets and having a very discriminant appearance model for targets. As  was intended to track ants on a white surface where there is a single entry point with closed borders, such simplification is possible. On a human tracking situation where the borders are open, this becomes more difficult, and for this evaluation it has been implemented as a uniform entry field along the left, right and lower borders of the image (since the upper border is closed, and there are no off-border entry points). Therefore, we consider a variation annotated , where the drawing for new targets incorporates detector information, using the detectors outlined in Sect.&amp;nbsp;5. As in the case of MCMC, MCMC-PF is an inherently sequential process, which makes it poorly suited to optimization via parallelization. Our filtering scheme, called   from here on and detailed in Table&amp;nbsp; 3, combines Markov chain iterations and principles of the   algorithm [ 24].
Springer.tar//Springer//Springer\10.1007-s00138-013-0535-8.xml:Context-based person identification framework for smart video surveillance:Smart video surveillance Context information Entity resolution Person identification:  1 Introduction  Advances in sensing, networking, and computational technologies have allowed the possibility of creating sentient pervasive spaces wherein sensors embedded in physical environments are used to monitor its evolving state to improve the quality of our lives. There are numerous physical world domains in which sensors are used to enable new functionalities and/or bring new efficiencies including intelligent transportation systems, reconnaissance, surveillance systems, smart buildings, smart grid, and so on. In this paper, we focus on smart video surveillance (SVS) systems wherein video cameras are installed within buildings to monitor human activities [17, 18, 33]. Surveillance system could support variety of tasks: from building security to new applications such as locating/tracking people, inventory, or tasks like analysis of human activity in shared spaces (such as offices) to bring improvements on how the building is used. One of the key challenges in building smart surveillance systems is that of automatically extracting semantic information from the video streams [31, 32, 35, 36]. This semantic information may correspond to human activities, events of interest, and so on that can then be used to create a representation of the state of the physical world, e.g., a building. This semantic representation, when stored inside a sufficiently powerful spatio-temporal database, can be used to build variety of monitoring and/or analysis applications. Most of the current work in this direction focuses on computer vision techniques. Automatic detection of events from surveillance videos is a difficult challenge and the performance of current techniques often leaves a room for improvement. While event detection consists of multiple challenges, (e.g., activity detection, location determination, and so on), in this paper we focus on a particularly challenging task of  [3, 4]. The challenge of  (PI) consists of associating each subject that occurs in the video with a real-world person it corresponds to. In the domain of computer vision, the most direct way to identify a person is to perform  followed by , the accuracy of which is limited even when video data are of high quality, due to the large variation of illumination, pose, expression, and occlusion, etc. Thus, in the resource-constrained environments, where transmission delay, bandwidth restriction, and packet loss may prevent the capture of high quality data, face detection and recognition becomes more complex. We have experimented with Picasa’s face detector on our video dataset, 1 and found that it can detect faces in only 7 % of the cases and then among them it can recognize only 4 % of faces. Figure&amp;nbsp;  illustrates the example of frames in our video dataset, where only one face is successfully detected (solid-line rectangle) utilizing the current face detection techniques. Several reasons account for the low detection rate: (1) faces cannot be captured if people walk with their back to the cameras; (2) faces are too small to be detected when people are far away from cameras; (3) the large variation of people’s pose and expression brings more challenges to face detection. Thus the traditional face detection and recognition techniques are not sufficient to handle the poor-quality surveillance video data. To deal with the poor-quality video data and overcome the limitation of current face detection techniques, we shift our research focus to context-based approaches. Contextual data such as time, space, clothing, people co-occurrence, gait, and activities, are able to provide the additional cues for person identification. Consider the frames illustrated in Fig.&amp;nbsp; for example. Although only one face is detected and no recognition results are provided, the identities of all the subjects can be estimated by analyzing the contextual information. First, time and foreground color continuities split the eight frames into two sequences or shots. The first four frames construct the first shot, and the following four frames form the second shot, where subjects within each shot describe the same entities. Furthermore, some other contextual features reveal the high possibility that the two subjects in these shots are the same entity. For instance, they both share the similar clothing (red T-shirt and gray pants), they perform similar activities (walking in front of the same camera, though in opposite directions), and they have the same gaits (walking speed). Thus the context features help to reveal that the subjects in the eight frames very likely refer to the same entity. To identify this person, face recognition process is usually inevitable. However, the activity information can also provide extra cues to recognize people’s identity. In the above example, suppose that the first shot in Fig.&amp;nbsp; is the first shot of that day where a person enters the corner office which belongs to “Bob”. Then most probably this person is “Bob” because in most cases, the first person entering the office should have the key. Therefore, by analyzing contextual information even without face recognition results, we can predict that the very likely identity of subject in all the eight frames is “Bob”. The example demonstrates the essential role that contextual data play in the  issue for the low-quality video data. Another significant advantage of context information is its weaker sensitivity to video data quality as compared with that of face recognition. That makes context-driven approaches more robust and reliable when dealing with poor quality data. In this paper, we extend our previous work [39] to explore a novel approach to leverage contextual information, including time, space, clothing, people co-occurrence, gait, and activities to improve the performance of . To exploit contextual information, we connect the problem of person identification with a well-studied problem of  [5, 21, 26], which typically deals with textual data.  is a very active research area where many powerful and generic approaches have been proposed, some of which could potentially be applied to the person identification problem. In this paper, we first investigate methods for extracting and processing several different types of context features. We then demonstrate how to apply a relationship-based approach for entity resolution, called RelDC&amp;nbsp;[23], to the person identification problem. RelDC is an algorithmic framework for analyzing object features as well as inter-object relationships, to improve the quality of entity resolution. In this paper we will demonstrate how RelDC framework for  could be leveraged to solve a  problem that arises when analyzing video streams produced by cameras installed in the CS Department at UC Irvine. Our empirical evaluation demonstrates the advantage of the context-based solution over the traditional techniques, as well as its effectiveness and robustness. The proposed approach shows clear improvements over approaches that only exploit facial features. The improvement is even more pronounced for low-quality data, as it relies on contextual features that are less sensitive to deterioration of data quality. The rest of this paper is organized as follows: We start by introducing the related work in Sect.&amp;nbsp;2. Then in Sect.&amp;nbsp;3, we present the proposed approach for context based person identification. Section&amp;nbsp;4 demonstrates experiments and results. Finally, we conclude in Sect.&amp;nbsp;5 by highlighting key points of our work.    2 Related work  The conventional approaches for   are to first use face detection followed by face recognition. Figure&amp;nbsp;  illustrates the basic schema for person identification. Given a face frame, after locating faces via a face detector, the extracted faces are passed to a matcher which leverages the face recognition techniques to measure the similarities between the extracted faces and “gallery faces” (where true identities of people are known) to determine the identities of the extracted faces. In general, face detection is the first and essential component used in person identification. However, in our test dataset, only a small proportion of faces (7 %) could be detected using the current face detection techniques, and out of them very few (4 %) could be recognized, due to the poor quality of video data in our surveillance setting. The failure of detection for most faces makes it impossible to apply the subsequent face recognition process in 93 % of the cases. Hence, the task of achieving high-quality person identification becomes a challenge for video of poor quality. Face recognition is another active topic of research that has attracted significant attention in the past two decades. Most of the research efforts have focused on techniques for still images, especially face representation methods. Recently, descriptor-based face representation approaches have been proposed and proven to be effective. They include Local Binary Pattern (LBP)&amp;nbsp;[2] describing the micro-structure of faces, SIFT and Histogram of Oriented Gradients (HOG)&amp;nbsp;[10], and so on. These face recognition techniques are able to achieve good performance in controlled situations, but tend to suffer when dealing with uncontrolled conditions where faces are captured with a large variation in pose, illumination, expression, scale, motion blur, occlusion, etc. These nuisance factors may cause the differences in appearance between distinct shots of the same person to be greater than those between two people viewed under similar conditions. Thus leveraging context features could bring significant improvement on top of techniques that rely on low-level visual features only, especially in the context of surveillance videos. Compared with still images, videos often have more useful features and additional context information that can aid in face recognition. For example, a video sequence would often contain several images of the same entity, which potentially shows the entity’s appearance under different conditions. Surveillance videos usually have temporal and spatial information available, which still images do not always have. In addition, video frames are capable of storing the objects in different angles, which contain 3-D geometric information. To better leverage these properties, some face recognition algorithms have been proposed to operate on video data. They include using temporal voting to improve the identification rates, extracting 2-D or 3-D face structures from video sequences [12–14]. However, these methods do not fully exploit the context information and very few of them address the problem of integration of heterogeneous context features. In this paper, we propose to leverage heterogeneous contextual information to improve the performance of video-based face recognition. To integrate the heterogeneous contextual features together, we connect the problem of  with the well-studied  problem and apply our entity resolution RelDC framework to construct a relationship graph to resolve the corresponding person identification problem. High quality of data is a fundamental requirement for effective data analysis which is used by many scientific and decision-support applications to learn about the real-world and its phenomena [15, 16, 24]. However, many Information Quality (IQ) problems such as errors, duplicates, incompleteness, etc., exist in most real-world datasets. Among these IQ problems,  (also known as deduplication or record linkage) is among the most challenging and well-studied problem. It arises especially when dealing with raw textual data, or integrating multiple data sources to create a single unified database. The essence of ER problem is that the same real-world entities are usually referred to in different ways in multiple data sources, leading to ambiguity. For instances, the real-world person name ‘John Smith’ might be represented as ‘J. Smith’, or misspelled as ‘John Smitx’. Besides, two distinct individuals may be referred as the same representation, e.g., both ‘John Smith’ and ‘Jane Smith’ referred to as ‘J. Smith’. Therefore, the goal of ER is to resolve these entities by identifying the records representing the same entity. There are two main instances for ER problem:  [5, 21] and  [5, 26].  is a classification problem, with the goal of identifying the object that each reference refers to.  is a clustering problem, whose goal is to correctly group the representations that refer to the same object. We primarily will be interested in an instance of the lookup problem. Our research group at the University of California, Irvine, has also contributed significantly to the area of ER in the context of Project Sherlock@UCI, e.g., [8, 19, 20, 28–30, 38]. The most related work of our group is summarized next. To address the entity resolution problem, we have developed a powerful disambiguation engine called the  (RelDC) [6, 7, 21–23, 27]. RelDC is based on the observation that many real-world datasets are relational2 in nature, as they contain information not only about entities and their attributes, but also  among them as well as  associated with the relationships. RelDC provides a principled domain-independent methodology to exploit these relationships for disambiguation, significantly improving data quality. Relationship-based data cleaning (RelDC) works by representing and analyzing each dataset in the form of entity-relationship graph. In this graph, entities are represented as nodes and edges correspond to relationships among entities. The graph is augmented further to represent ambiguity in data. The augmented graph is then analyzed to discover interconnections, including indirect and long connections, between entities which are then used to make disambiguation decisions to distinguish between same/similar representations of different entities as well as to learn different representations of the same entity. RelDC is based on a simple principle that entities tend to cluster and form multiple relationships among themselves. After the construction of entity-relationship graphs, the algorithm computes the  between each uncertain reference and each of the reference’s potential “options” that entities it could refer to. For instance, reference ‘J. Smith’ might have two options: ‘John Smith’ and ‘Jane Smith’. The reference will be resolved to the option that has the strongest combination of the connection strength and the traditional feature-based similarity. Logically, the computation of the connection strength can be divided into two parts: first finding the connections which correspond to paths in the graph and then measuring the strength in the discovered connections. In general, many connections between a pair of nodes may exist. For efficiency, only the important paths are considered, e.g., -short simple paths. The strength of the discovered connections is measured by employing one of the connection strength models [21]. For instance, one model computes the connection strength of a path as the probability of following the path in the graph via a random walk. After the connection strength is computed, this problem is transformed into an optimization problem of determining the weights between each reference and each of reference’s option nodes. Once the weights are computed by solving the optimization problem, RelDC resolves the ambiguous reference to the option with the largest weight. Finally, the outcome of the disambiguation is used to create a regular (cleaned) database.    3 Context-based framework for person identification  Let   be the surveillance video dataset. The dataset contains   video frames   wherein motion has been detected. Let   denote the time stamp of each frame  . When a frame   contains just one subject, we will refer to the subject as  , or as  . Let   be the set of (known) people of interest that appear in our dataset. Then the goal of person identification is for each subject   to compute   which denotes the probability that   is person  , and correctly identify the person   that subject   corresponds to. If the subject is not in  , then the algorithm should output  . Table&amp;nbsp; 1 summarizes some of the notations throughout this paper. Figure&amp;nbsp;  illustrates an example of the person identification problem, where the goal is to determine whom the subject in each video frame refers to: “Bob” or “Alice”. We can observe that the entity resolution problem has a very similar goal, that is, to associate each uncertain reference to an object in the database with the real-world object. Hence in this paper we demonstrate how to apply one entity-resolution framework called RelDC to the problem of person identification. The framework will exploit the relationships between contextual features of subjects in the video surveillance to improve the quality of the person identification task. Figure&amp;nbsp;  illustrates the general framework of context-based person identification for surveillance videos. Given the stored videos from surveillance cameras, the framework first segments the frames with motion into shots based on temporal information. To facilitate person identification based on person faces the framework performs several preliminary steps such as face detection, extraction, facial representation. and recognition. It then extracts the contextual features including people’s clothing, attributes, gait, activities, etc. After the extraction of face and contextual features, the framework constructs the entity-relationship graph and then applies the entity resolution algorithm RelDC on the graph to perform the corresponding person identification task. In the following we discuss how to extract contextual features from surveillance videos and then leverage RelDC framework to integrate these features together to resolve the person identification problem. Contextual features can provide additional cues to facilitate video-based person identification, especially for poor-quality video data. In the following, we describe how to extract and leverage contextual features, such as people’s clothing, attribute, gait, activities, co-occurrence, and so on, to improve the performance of person identification. We first describe temporal segmentation which is an essential part in video processing. We segment videos into . Intuitively, subjects appearing in consecutive frames are likely to be the same person. Hence, we initially group frames into shots just based on the time continuity. But time continuity alone cannot guarantee person continuity. If the subjects’ color histograms of two consecutive frames are significantly different indicating potentially different people, the shot is split further at such break points. Suppose that we obtain a set of shots  after the video segmentation. Most of the time the frames that belong to the same shot describe the same entities. Thus the person identification task reduces from identifying the subjects in an image to identifying the subjects in a shot. We next describe how to extract contextual features for a shot. People’s clothing can be a good discriminative feature for distinguishing among people [ 11,  37]. Although people change their clothes across different days, they do not change it too often within shorter period of time, and hence the same clothing in such cases is often strong evidence that two images contain the same person. To accurately capture the clothing information of an individual in an image, we separate the person from the background by applying a background subtraction algorithm [ 4]. After color extraction processing, the foreground area is represented by a 64-dimensional vector, which consists of a 32-bin hue histogram, a 16-bin saturation histogram, and a 16-bin brightness histogram. Figure&amp;nbsp;  shows an example of the extracted foreground image and corresponding color histogram. The extracted clothing features can be used to compute the clothing-based similarity among subjects. For each pair of subjects   and  , let   and   be their clothing histograms and   and   by the timestamps when   and   have been captured in video. We can choose an appropriate similarity measure to compute the similarities between them, such as the cosine similarity. For instance, if we assume that people keep the same clothing during the same day, we can define        To compute the similarity of subjects from two shots, the algorithm selects a subject from a certain frame in a shot to represent the shot. Usually the algorithm chooses a frame towards the middle, which tends to capture the profile of the person better. Activities and events associated with subjects prove to be very relevant to the problem of person identification [9, 40]. The trajectory and walking direction can serve as a cue indicating the identity of the individual. For example, the activity of entering an office can provide strong evidence about the identity of the subject entering the office: it is likely to be either (one of the) person(s) who works in this office, or their collaborators and friends. Furthermore, considering the time of the activity in addition to the activity itself can often provide even better disambiguation power. For example, on any given weekday, the person who enters an office first on that day is likely to be the owner of the office. In addition, by analyzing past video data the behavior routines for different people can be extracted, which later can provide clues about the identify of subjects in video. For instance, if we discover that “Bob” is accustomed to entering the coffee room to drink his coffee at about 10 a.m. each weekday, then the subject who enters the coffee room at around 10 a.m. is possibly “Bob”. Therefore, subject activities can often provide additional evidence to recognize people. We now discuss how to extract and analyze certain people’s activities. To track the trajectory of a subject and obtain his activity information, we need to extract bounding box and centroid of the subject. To do that we consider three consecutive frames with the same object. We first compute the differences of the first two frames by subtraction and then compute the differences of the last two frames. By combining the two different parts, we get the location of objects. After obtaining the bounding box, we determine the centroid of subjects by averaging the points of x-axes and y-axes. The most common activity in surveillance dataset is walking. The walking direction (towards or away from the camera) is an important factor to predict the subsequent behavior of a person. The walking direction can be obtained automatically by analyzing the changes of the centroid between two consecutive frames in a shot. For example, as illustrated in Fig.&amp;nbsp; , by determining that the centroid of the subject is moving from the bottom to the top in the camera view, we can determine that this person is walking away from the camera. We focus on detecting simple regular type of behavior of people, including entering and exiting a room, walking through the corridor, standing still, and so on. These types of behavior can be determined by analyzing the bounding box of a person. For instance, for walking the algorithm focuses on the first and last frame in a shot, which we are called  and  frames. By analyzing the bounding box (BB) of a subject in the entrance frame, we could predict where the subject has come from. Similarly, the exit frame could tell us where this person is headed to. If we consider all the BBs in entrance and exit frames, we can find several locations in the camera view, where people are most likely to appear or disappear. These locations, denoted as  , can be automatically computed in an unsupervised way by clustering the centroid of entrance/exit BBs. Based on this analysis, we automatically obtain the entrance and exit point in an image. Figure&amp;nbsp;  demonstrates an example of the clustering result of the entrance and exit locations. After computing the set of entrance and exit locations , we compute the distance between them and determine the entrance and exit points in each shot. Suppose that in a shot  the subject  walks from location  to . Then we can denote the activity as . For each shot the algorithm extracts the activity information by performing the aforementioned process. We assume that two subjects with similar activities have a certain possibility to describe the same person. Thus based on this assumption, we connect the potentially same subjects through the similar activities. Suppose that for two subject   and   from shot   and shot  , respectively, the algorithm extract activity information   and  . We can define the activity similarity as follows:        In this equation, activities with the exact opposite entrance/exit points are defined to be equal, for example, the subject   with activity   and the subject   with activity   are considered to share the same activity. Thus the activity similarities can be leveraged to connect the subjects which share the same/similar activities. The intuition is that the identity of a person can be estimated by analyzing his activities. In general, given labeled past data we can compute priors such as , which correspond to the probability that the observed subject  is the real-world person , given that the subject participates in activity , such as entering/exiting a certain location. Similarly, we can compute  which also considers time. Gait is also a good feature to identify a particular person, because different people’s gaits are often different. For example, somebody might walk very fast or slow, somebody might walk with swinging arms or head. Thus by analyzing the characteristics of people’s gaits, we might be able to better predict the identity of one subject or the sameness of two subjects. For example, if the walking speed of two subjects differs significantly, then they might not refer to the same entity.
Springer.tar//Springer//Springer\10.1007-s00138-013-0536-7.xml:A new closed loop method of super-resolution for multi-view images:Mixed-resolution multi-view images Super-resolution Depth estimation:  1 Introduction  Single-view image super-resolution is a fundamental problem in low-level computer vision research and has been widely studied [1–3, 7–9, 12, 16, 19, 21, 22, 24, 27, 31]. As shown in [12, 22, 24], methods of this problem always belong to the following three categories: interpolation-based methods, reconstruction-based methods and learning-based methods. Interpolation-based methods are simple but may blur the edges [1, 7, 16]. Reconstruction-based methods benefit from different effective constraints or smoothness priors, which are consistent with some characteristics of image [2, 9, 19, 21]. Researchers impose these constraints or priors to the expected high-resolution target and formulate the super-resolution problem as an optimization problem. The performance of these methods is influenced by the effectiveness of the constraints or priors and the optimization techniques. Learning-based methods encode the relationship between high-resolution and low-resolution images in the training set [3, 8, 22, 24, 27]. They estimate the lost high-frequency details in the low-resolution image from the high-resolution images in the training set by using the above relationship. Selection of the training set and proper definition of the relationship are crucial to the performance of this kind of methods. Among these learning-based methods, advances in sparse coding have achieved outstanding performance in different applications [10, 25], including image super-resolution [31]. These above single-view image super-resolution methods may be directly used in stereo and multi-view applications [30, 33]. However, they share a disadvantage in that they do not utilize the correspondence of different views [4, 15, 29, 32]. Thus, the performance of the above methods may not meet the expectation. Recently, Garcia et al. [11] proposed an approach to increase the quality of the low-resolution view using the high-frequency information from adjacent high-resolution views, which is under the guidance of the known depth information. The depth map can be acquired directly with an active range finder, such as a time-of-flight camera (ToF camera). However, it is rather expensive and cannot be a common integral part of 3D devices, i.e., 3D mobile and consumer 3D camera. An alternative way is calculating the depth map using stereo matching algorithms. Brust et al. [4] proposed a method to render the right view from the left view by using the estimated depth information. However, they estimate the depth map from the the original high-resolution resolution left view and right view using hybrid recursive matching algorithm. It is actually not a mixed-resolution case. As is known, the quality of stereo matching result depends on the concurrence of high-frequency details in both views. Here comes the problem: if we only have mixed-resolution views (high-resolution view and its neighboring low-resolution views), the stereo matching algorithms may not achieve high-quality depth map. To get out of the predicament, we propose a closed loop method to enhance the resolution of low-resolution views and obtain a high-quality depth map simultaneously. Furthermore, different from [11], we will tackle the super-resolution problem in a more strict case, which is one high-resolution view along with its neighboring low-resolution views, and without pre-acquired depth information. Our method consists of two parts as shown in Fig. : part I, stereo matching and depth maps fusion; and part II, image super-resolution. The main idea of the proposed method can be summarized as follows: (i) We calculate several disparity maps between the reference high-resolution view and its neighboring low-resolution views. We can obtain several depth maps of the same scene. It means that we have several depth estimates of the same scene point. So, we can fuse them into a more reliable one using a 3D median filter. (ii) Under the guidance of the depth information, we can calculate the correspondence between the reference high-resolution view and its neighboring low-resolution views. Then, the lost details in the low-resolution views can be estimated from the corresponding ones in the high-resolution view. Moreover, since the nonlocal prior is very useful for image super-resolution, we impose the nonlocal constraint to the target super-resolution result to utilize similar details in the high-resolution view. (iii) After updating the low-resolution view with these super-resolution results, the quality of stereo matching results can be improved. Consequently, the more reliable the depth information, the better are the super-resolution results, and vice versa. We repeat these two steps until obtaining stable super-resolution results and depth maps. The proposed method can be used for 3D video data reduction and restoration, multi-view reconstruction and other video applications [11, 13, 14, 17, 18, 26]. For example, Hong et al. [13] present a novel scheme that is able to summarize the content of video search results by mining and threading “key” shots and achieve promising results. The proposed super-resolution method can be used to enhance the quality of input 3D videos and provide depth information for detecting more reliable and important “key” shots.    2 Related work  Nonlocal means (NLM) algorithm is originally proposed in [ 5], based on the observation that image content is likely to repeat itself within some neighborhood (See Fig.  ). It replaces every pixel with a weighted average of pixels in its nonlocal neighborhood. Mathematically,       where   is the pixel value at position   of a noisy image   is the filter output.   is the weight evaluated by using block-matching fit between patch centered at   and patch centered at  , which is in the nonlocal neighborhood  . The patch-wise form of NLM can be expressed as [ 6]:       where   is a vectorization patch extraction operator and   is vectored representation of the patch centered at  . While NLM is known as an efficient single-image denoising algorithm, Protter et al. [19] generalize it to super-resolution reconstruction. Their algorithm gives good results for video super-resolution by using this nonlocal prior along with total variation regularization. With the nonlocal prior, the algorithm benefits from exploiting the self-similarity of image, especially for multi-view images and videos input.    3 A closed loop method of depth estimation and image super-resolution  For the super-resolution problem in the mixed-resolution multi-view case (a registered high-resolution image  along with its several neighboring low-resolution views ), our goal is to obtain the high-resolution images  and a high-quality depth map simultaneously. In this paper, we focus on only one kind of multi-view configuration as follows: the high-resolution image  is the leftmost view and its neighboring low-resolution views  are the right views in order. The method for dealing with other configuration with a different order is analogous. As shown in Fig.  , our closed loop method consists of two parts. The first part aims at fusing several raw depth maps into a more reliable one, and is called stereo matching and depth maps fusion. In this part, those raw depth maps are computed from the disparity maps ( ), which are obtained by matching the reference view   with different views  . The second part is the image super-resolution process. Once we get the depth map, we can compute the disparity maps ( ) of view 2, 3, 4 and 5 relative to view 1. Then under the guidance of these disparity maps, we can estimate the lost high-resolution details of   from the high-resolution view  . In addition, we use the nonlocal prior to utilize the self-similarity in the high-resolution view  . After we get the super-resolution results , we use them to update the corresponding disparity maps and obtain a more reliable fused depth map. We repeat the loop several times for obtaining stable super-resolution results  and high-quality depth map simultaneously. For binocular camera, stereo matching is consistent with human’s inherent visual perception, in that we perceive depth based on the disparity of the corresponding point in the left eye and the right eye [ 23]. In our multi-view configuration, we can get several disparity maps of the reference view( ) relative to other views ( ). Hence, we can compute the depth map according to the following formula [ 23]:       where   is the focal length,   is the baseline between view 1 and view   is the disparity map of the reference view 1 relative to view  , and   is the   estimation of depth map of view 1. As we have pointed out before, stereo matching result depends on the concurrence of high-frequency details in both views. The more distinguished details these two views share, the more reliable the matching result is. But in our mixed-resolution super-resolution problem, neighboring views of the high-resolution reference view, namely, view 2, 3, 4 and 5, are in low resolution. Thus the disparity map   and the corresponding depth map   may not be so accurate. Luckily, we adopt the multi-view configuration described above. So, we have four estimates of the depth map of view 1, namely,  . We can fuse them into a more reliable estimation. Considering the depth map is locally smooth, we also use the local neighborhood information of each position for estimating. Here comes the 3D-median filter that we propose to use for achieving this goal. The 3D-median filter is an extension of the traditional 2D-median filter, and its additional dimension is the number of views in the multi-view configuration. As illustrated in Fig.  , for every position   on each depth map  , we firstly select a   patch   centered on   and stack them into a 3D array.   denotes the index set.   is the radius of the patch. (In this paper, we set   to 1.) Then we sort the elements in this 3D array and choose the median value as the estimation of depth value on  . Mathematically,            The 3D-median filter can efficiently eliminate the outliers and give a smooth and clear depth map. See Fig.   for a visual comparison. Once we get the depth map, we can use formula (3) inversely to compute the corresponding disparity map of view 1 relative to view . Then, we can interpolate it to get the disparity map  of view  relative to view 1. Then we can start the image super-resolution process, which is the second part of our closed loop method. In this paper, we formulate the super-resolution problem as an optimization problem by minimizing an energy function. This is a typical reconstruction-based method. We adopt the idea that the low-resolution image is the smoothed and down-sampled version of the high-resolution image. The degenerative process can be formulated as:  , where   is the down-sampling operator and   is the blurring operator. So, the first term of reconstruction energy function is the data term:       We know that the reference high-resolution view has plentiful details which are lost in the low-resolution view due to the above degenerative process. To use the information, we should know the correspondence between   and   image. Given that these two images are registered, we can calculate the correspondence using the disparity map. Every pixel   in   can find its corresponding pixel   at the same row in  . Their column indexes differ by a disparity value   (We use a linear interpolation technique to deal with the non-integer situation of the disparity value.). Considering all the pixels in  , we can give the following constraint which results from the disparity-based pixel mapping:       where   is a value of confidence about the disparity   which gives an account for how reliable   is. It is necessary since the depth map may not be so accurate, especially in the occlusion regions and non-overlapping regions of the two views. The value of   depends on two factors: whether the mapping result   exists; and when it exists, whether it is similar to  . In this paper, we measure the similarity by computing the Gaussian weighted distance between the local patch centered at   and the patch centered at  . Only when   exists and the weighted distance is smaller than a threshold, we set   to 1; else, we set it to 0. Beyond the above observation, we have known that nonlocal prior is useful for super-resolution as discussed in Sect.  2. This prior is very beneficial for resolving the super-resolution problem, because it means that we can use much more information to estimate the lost details. In our paper, we adopt the patch-wise form of nonlocal constraint [ 6]. Thus, we have the nonlocal constraint term as follows:            where   and   have the same meanings described in Sect.  2. By enforcing these three constraints to   together, the final energy function can be written as:       where   and   are two regularization parameters. Then, obtaining   becomes an optimization problem: The above optimization problem is indeed a quadratic optimization problem and can be solved by a gradient descent algorithm:       where               is a step size and   is the iterator index in gradient descent algorithm. However, the gradient descent algorithm seems time-consuming, because a patch-wise nonlocal means process is involved in every iteration for calculating the gradient. Considering the depth map may not be so accurate at the beginning, so we may only need a somewhat good super-resolution result to update timely the disparity map. Based on such an analysis, we propose a fast fusion method to approximately solve the above optimization problem. This method consists of three steps: ,  and . The  step corresponds to the term  in the optimization problem (Eq. (6)). The  step corresponds to the term  in the optimization problem (Eq. (7)) and the  step corresponds to the cooperation of these two terms, which is adjusted by the regularization parameters  and  in the optimization problem (Eq. (9)). Moreover, we use the super-resolution results obtained by the sparse coding method [31] to warm up our approach. Since the initial super-resolution result is consistent with the low-resolution input image, it actually corresponds to the first data term  in the optimization problem (Eq. (9)). Starting from the the disparity map   of view   relative to view 1, we can map the pixel value in the high-resolution view   to the corresponding position of the target super-resolution result  . Similar to the analysis about the mapping term in Sect.  3.3.1, this step can be formulated as:               We apply a nonlocal reconstruction process to the above mapping result to utilize the self-similarity of the high-resolution reference view  . Firstly, for every position   of the mapping result  , we determine the nonlocal neighborhood of the target position   of  . Then, we select out the patches within this nonlocal neighborhood, which are similar with the reference patch   centered on position   of  . Finally, we compute their weight sum as the estimation. Mathematically,            For every position  , we average all the corresponding pixel values which are involved in those overlapping patches as the final nonlocal reconstruction result  . Figure   illustrates the nonlocal reconstruction process. While the disparity-based mapping result may have a little of discontinuities or wrong mapping pixels, the nonlocal reconstruction process can smooth and eliminate them. But the nonlocal reconstruction process may also smooth out some tiny textures (see Fig.  ). To compensate for these losses, we fuse the disparity-based mapping result and nonlocal reconstruction result as the final estimation. Mathematically,       where   is a fusion parameter and subjected to  .    4 Experiments  To test the validity of the proposed method, we conducted a series of experiments on Middlebury 2005 Datasets1 [20] with scale factor 2, 4 and 8. We gave an objective evaluation against the bicubic interpolation method and sparse coding method [31] in terms of PSNR and SSIM (structural similarity) index [28] as well as subjective visual comparison. Among those seven views of each scene in the dataset, we selected views 1–5 for our experiments. The PSNR and SSIM were all calculated by averaging the corresponding values of these four views (view 2–5, view 1 is the high-resolution reference view). In our experiments, we used the results of sparse coding method [31] as the initial guess to warm up to our closed loop method. The parameters for the publicly available code2 are set according to [31]. The parameters in Eq. (7) are set according to [5]. The nonlocal neighborhood is set to be . The patch radius of the vectorization patch extraction operator  is 1. We conducted an experiment to determine the setting of parameter   in Eq. ( 14). For different settings of   ranging from  , we calculated the PSNRs of the different fusion results. The final results for scale factor 2, 4 and 8 are shown in Fig.  a–c, respectively. Every value on the curve is the average PSNR for view 2–5 of different test images in Middlebury 2005 Datasets. As can be seen, the highest PSNR is achieved when   is set as 0.5 for different scale factors. In the following experiments of this paper, we set   as 0.5. It has been pointed out in Sect.  3.3.2 that the   step corresponds to the cooperation of the mapping term and nonlocal term in Eq. ( 9), which is adjusted by the regularization parameters   and  . Therefore, parameter   in Eq. ( 14) indeed corresponds to the regularization parameter. When we set   as 0.5, we indeed consider the mapping term and nonlocal term equally. Tables  1 and  2 summarize the PSNRs and SSIMs of the three methods with different scale factors. It is clear that our approach consistently outperforms the bicubic interpolation method and sparse coding method for all the scale factors and images. Our approach achieves average gains of 2.47dB, 2.99dB and 1.42dB over the bicubic interpolation method for scale factor 2, 4 and 8, respectively. Compared with the sparse coding method, average PSNR gains are 0.83dB, 2.05dB and 0.79dB for scale factor 2, 4 and 8, respectively. The SSIMs of our method also reflect significant gains over the bicubic interpolation method and sparse coding method. The subjective visual comparisons also confirm the superiority of our method. Figure   shows the super-resolution results of Art (view 2) in the dataset with a scale factor 4. Figure   shows the super-resolution results of Reindeer (view 3) in the dataset with a scale factor 8. It can be seen that the results of our method are much more visually pleasing and have more and finer high-frequency details than the other two methods. The figures are best viewed in the electronic version of this paper. Besides the super-resolution results, our closed loop method can also obtain depth map simultaneously. To examine the quality of the depth results, we used the relative error of depth map (REoD) to measure how good a depth map is when compared with the ground truth. REoD was calculated as:       Here,   represents the ground truth depth map. Since we used the results of sparse coding method as the initial guess, we had the corresponding disparity maps using stereo matching algorithm. Then, we obtained four depth maps according to formula ( 3). We calculated the corresponding REoDs of these four initial depth maps and averaged them. The results are shown in Table  3, indicated by “initial”. Generally, after repeating one to three times, the proposed method achieved stable super-resolution results and depth maps. The REoDs of these depth maps are shown in Table  3, indicated by “proposed”. In addition, we matched the reference high-resolution view 1 with the ground truth of views 2–5 to get the corresponding depth maps and calculated the REoDs of these depth maps. They are shown in Table  3, indicated by “MoG” (matching results of ground truth views). These results can be used to compare with the results of our method. Furthermore, we can know the quality of the super-resolution results of our method from such a comparison indirectly. REoD values in Table  3 show that our method significantly improves the quality of the initial depth maps. It gives comparable results to “MoG” for scale factor 2 and 4. Although the values of our method for scale factor 8 are still a little higher, they are much lower than the values of the initial maps. Generally, these comparison results convince the effectiveness of our method on the aspect of obtaining a high-quality depth map. Figures   and   show the depth map results corresponding to the above super-resolution experiments in Figs.   and&amp;nbsp;  . It can be seen that our method improves the initial depth maps and gives comparable results to “MoG” and the ground truth. In addition, we show the super-resolution results and depth maps of each iteration for the test image   (view 2) in Middlebury 2005 Datasets in Fig.&amp;nbsp; . The scale factor is 4. It can be seen that the qualities of both super-resolution result and depth map are gradually improved with the increase of iterations. The final super-resolution result and depth map obtained by our proposed method show significant improvements over the initial ones according to the objective assessment indexes as well as visual comparison. For instance, the characters on the books in Fig.&amp;nbsp; c are clearer than the ones in Fig.&amp;nbsp; a, and are comparable with the ones in Fig.&amp;nbsp; d. Depth map shown in Fig.&amp;nbsp; g is much smoother than the initial one in Fig.&amp;nbsp; e, and is comparable with the ground truth in Fig.&amp;nbsp; h. These results convince the effectiveness of the closed loop idea of our proposed method: that each step can benefit from the improvement result in the other until obtaining stable results simultaneously.  
Springer.tar//Springer//Springer\10.1007-s00138-013-0537-6.xml:Localizing relevant frames in web videos using topic model and relevance filtering:Kernel density estimation Tag localization Web videos Topic model:  1 Introduction  With the rapid development of web technologies, numerous web videos associated with rich metadata are available on the Internet today. These metadata facilitate users to retrieve and share video corpus, by way of keyword-based video search. While video metadata such as video name, video tags, and so on greatly benefit the development of web videos, some issues also arise. In the current web video portals, videos are annotated with several tags, but usually these tags are tagged at the video level, i.e., they are the description of whole video content. In fact, many tags may only be related to some parts of frames in a video [1]. Such fact affects the video retrieval efficiency. For example, a video on sports news can be tagged with “basketball”, though only a few shots actually talk about “basketball”. When we use “basketball” as the keyword to search, this video may be returned to us. In such a scenario, we will need to watch the whole video to find our interested part. Thus, for more precise video search, we need to know which time periods or points of the video are actually related to the keywords, i.e., we need to localize the relevant frames in a video to a given concept. By localizing relevant frames, some multimedia research tasks can also be benefited. For example, in concept detection or semantic indexing of video corpus, training samples are needed to train a concept classifier [2, 3]. Due to the difficulty in collecting sufficient manual annotations, recently, researchers have begun to turn to online web resources such as  or  to get weekly annotated training samples [4–6]. However, the performance of trained classifiers is dramatically affected by the noisy tagged contents. If we can localize the time period of relevant shots or frames in videos, frames extracted from these time periods can serve as good training samples. Previous works such as [ 5,  6] represent video frames as vectors in a feature space and use non-parametric kernel densities to estimate a relevance score for each frame. This approach is based on the assumption that the relevant frames are clustered in feature space; thus, if a frame has short distance to all the positive samples while long distance to negative samples it would be thought to be likely positive, i.e., relevant to a given concept. However, due to the well-known semantic gap [ 7,  8] between low-level features and high-level semantics, neighboring samples in feature space are not necessarily semantically consistent; thus, there are many irrelevant frames mixed in the filtered results. In this paper, we enhance the relevance filtering performance from the semantic topic mining perspective. This is based on the observations that video frames can be separated into topics by mining the underlying latent semantics, and frames in the same topic are similar while some topics are more relevant to the given concept than other topics. Figure&amp;nbsp;  shows two topics for “basketball”. It can be seen that frames in the topic of Fig.&amp;nbsp; a are more relevant to those in Fig.&amp;nbsp; b. Thus by selecting the relevant topics, we can use them to guide us to remove the irrelevant frames and identify the relevant ones. In this paper, we propose a novel approach to localizing the relevant frames in videos. First, a probability framework called relevance filtering is applied to model content relevance and the initial relevant images are obtained. Then, top relevant images are selected as validation set to guide the later topic selection process. Finally, the selected topics are used to refine the initial relevant images to get the final result. Our main contributions are: (1) We combine the relevance filtering with the topic model. (2) We propose a new method to guide the topic selection. (3) The resulting relevant frames are more precise and diverse. We test our approach on databases Youtube22 [5] and DUT-WEBV [9], and the experiments show comparable or improved performance compared to former methods.    2 Related work  The task of localizing relevant frames for given tags, i.e., tag localization in videos has been studied in recent years. Similar work such as web image selection and classification is also being studied. By either identifying the most useful part or removing the noisy part, web resources can be best used. Recently, tag localization has attracted much research interests and many methods have been proposed. Ballan [10] proposed utilizing social knowledge to suggest and localize tags in web video. With the help of Flicker images, tags in YouTube videos can be localized and several tags are suggested for frames in videos. However in this method, additional data are required and the domain gap between Flicker and YouTube need to be considered. Tang et al. adapted semantic correlation into graph-based semisupervised learning [11, 12] and proposed propagation-based approaches to improve annotation performance. Ulgs et al. [5] proposed a probabilistic framework for modeling content relevance. This approach is based on a weighted kernel density model [5] and uses EM scheme to update the relevance score assigned to each training sample. Based on this approach, they proposed combining relevance filtering with active learning [6] and used little manual labels to help improve the performance of the model. Manually labeled samples are chosen by several strategies such as random, most relevant, uncertainty and density-weighted repulsion. Another approach also proposed by Adrain Ulgs used relaxed Multiple Instance Learning [13] to solve this problem. Videos are regarded as a bag of keyframe-related features. They assumed that training video is weakly labeled, i.e., the associated concept presence variables are observed, but which keyframes of a video are “critical” is not known. They used MIL-BPNET [14] approach as classifier for bags of samples instead of single sample. By feeding all samples to the network they can obtain a score for each sample. If the score of a sample is within the important fraction, this sample is thought to be relevant. Followed with the MIL framework, Meng Wang et al. proposed incorporating auxiliary knowledge from web [15, 16] to assign tags at the video shot level and their work points out a promising direction for solving the challenge tag localization problem. In the context of web image selection and classification, as pointed in [17, 18], most of the existing approaches which only adopt traditional Euclidean distance or variants are not enough, and multimodal information also needs to be considered [19]. Thus, Wang et al. [3] utilized both visual information of image and semantic information of tag and proposed a diverse relevance ranking scheme to obtain more relvant and diverse result. Another kind of approach using topic model such as PLSA [20–22] is proposed to select the most relevant images from web image sets. They are based on the observation that each concept can be divided into several semantic topics and some of them are more representative to the concept while others are not; therefore, selecting the representative topics can reduce the noise problem. One important problem with topic model is how to select the relevant topics. In [21], Fergus et al. construct a validation set by using query represented by six kinds of languages to search image search engines to obtain the top few result images. Their assumption is that only the first few images of the first page are likely to be good examples. After the validation set is constructed, it is used to select the most relevant topic among the learned topics by PLSA. Another topic selection approach proposed by Yanai [20] is similar to [21], while the difference is that in [21] the validation set is obtained by using HTML-text-based image selection. In [21] only one relevant topic is selected, while in [20] several topics can be relevant to the given concept. In this paper, we also produce a validation set using the result of relevance filtering to guide the topic selection. Topics obtained using LDA will be assgined a relevance score to the given concept and used to guide the identification of the relevant keyframes.    3 The proposed approach  Given a video V and keyframes  extracted from it using shot boundary detection algorithms, our goal is to automatically localize the most relevant shots or frames in the video with respect to a certain concept . To do this, we first estimate a relevance score  for each frame in the video set. Here, the positive video set for concept  is constructed by using a set of returned videos  from YouTube by issuing the concept as query keyword; and the negative video samples for concept  are the positive videos of other concepts. As mentioned above, only part of the positive videos are actually related to the concept, which means keyframes extracted from the positive videos may be false-positive samples. If we use  to represent the keyframe actually showing the given concepts, then the conditional probability  needs to be estimated, where  means frames from positive videos are not reliably labeled, while keyframes from negative videos are thought to be reliable labeled as . In previous works, images are selected by either feature distance-based method [ 5,  6] or topic model-based method [ 20,  21]. Feature distance-based methods consider image contents, while the topic model-based methods can model the underlying latent semantic information of images. In this paper we combine the advantages of both localizing the (most) relevant frames in web videos for a given concept, where kernel density estimation method (KDE) is used for visual relevance filtering and LDA is adopted to model the latent semantic cues. Figure&amp;nbsp;  shows an overview of our proposed approach. First, the concept keywords are used as queries to search YouTube and the returned videos are downloaded. Note that some videos, in which contents are either all related to the concept or not related to the concept at all, are not suitable for our task. We remove such kinds of videos. Next, keyframes are extracted from the remaining videos and, as expected, the extracted keyframes may be relevant, less relevant and even irrelevant to the concept. In the following step, relevance filtering and LDA are applied to the noisy keyframes to get the filtered results and topics, respectively. Then, we use the filtered result as validation set to select the relevant topics. Finally, the selected topics are used to refine the filtered result and the final results are obtained. Here, we adopt the framework proposed in [ 5] to identify initially relevant frames. In this framework, video frames are coded as vectors in feature space and non-parametric kernel densities are used to model the distribution of relevant and non-relevant frames. The relevance filtering is based on a weighted kernel density model which models the two class-condition densities  (concept presence) and  (concept absence) as follows.            In the above equation,   and   are parameterized by a vector  , where   is the relevance score for sample  , meaning the probability of being relevant. If   is high, sample   will have a strong influence on the density   but low influence on the density  .   and   are normalization constants. For a kernel function  , the well-known Epanechnikov kernel with Euclidean distance function and bandwidth   is used:              \(1_{||x-x'||&amp;lt;h} \) is an indicator function whose value is 1 if the condition  \(||x-x'||&amp;lt;h\) is satisfied, otherwise it is 0. The kernel function shows the correlation between   and   and the shorter the feature distance is, the more   will influence  . To calculate the relevance score  , we start with a   and then update the value   to   by the following iteration. The final  will be obtained until it converges. Here,  is prior for the relevant fraction . This procedure can identify the positive samples in feature space by assigning high score to samples close to positive samples and far away from negative samples, and low score to samples close to the negative samples in feature space. Topic model [ 20,  21,  23] is traditionally used for text semantic mining where documents are represented as histogram of words. The semantic meanings of learned topics are naturally represented by the words which have the most probability assigned to each topic. While for images, though there are no textual keywords to describe the images and we could only depend on the co-occurrence of visual features, the mined topics do reflect the semantic aspects of the given concept. For example, Fig.&amp;nbsp;  gives two mined topics from keyframes of concept “highway”. In Fig.&amp;nbsp; a, all the images are related to “highway” which are dominated with green things like grass, mountain or forest, while in Fig.&amp;nbsp; b the images are related to “highway” which are only the view of the road surfaces. Thus, though the learned topics cannot be described using accurate textual words, they actually show some latent semantic information. This motivates our work to use topic model on the image set to model their semantics and further to guide the localization of relevant frames to the given concept. For feature representation, we represent each image in a bag-of-feature manner. First, we segment the image into grids and for each grid the color and texture feature are extracted. Then, k-means algorithm is used to generate the cluster centers as codebooks. By assigning the feature of each grid to the closest center, each image is represented as a histogram of cluster centers. We then use LDA [ 23] to extract the latent semantic topics as its performance and efficiency have been widely validated [ 24,  25]. LDA is a generative probabilistic model assuming each image is represented as random mixtures over latent topics, where each topic is characterized by a distribution over codebooks. Given a corpus consisting of   samples, the following procedure is used to assign each sample to   topics:        Draw a multinomial  over  topics:           &amp;nbsp;                 For each topic                      Draw a multinomial                        &amp;nbsp;                 For each of the        words       :              Choose a topic          Choose a word                        &amp;nbsp;      Here,   is the parameter of the Dirichlet prior to the per-image topic distributions,   is the parameter of the Dirichlet prior on the per-topic word distribution,   is the topic distribution for images and   is the total number of visual words in all images. Given the parameters   and   , the joint distribution [ 23] of topic mixture  , a set of   topics   and a set of   words   is given as:       Learning the various distributions is a problem of Bayesian inference and we use the Gibbs sampling as an alternative inference techniques. After   for image   is estimated, we can assign image   to topic   with the maximum  . Note that  . Draw a multinomial  over  topics: For each topic       Draw a multinomial Draw a multinomial For each of the   words  :    Choose a topic     Choose a word Choose a topic Choose a word To select relevant topics from learned topics to the given concept, similar with previous works like [20, 21] a validation set is constructed. The key difference is that without using additional web images, we propose using the relevance filtering results to do this. We first apply the relevance filtering procedure using KDE as described in Sect.&amp;nbsp;3.2 to the video keyframes and then select the top  resulting samples which are expected to be more relevant to the concept as a validation set. For each of the  topics , a relevance score  is calculated to the concept. Our method to compute the topic relevance score is based on the assumption that the topic which is most likely assigned to the top   samples is supposed to be most relevant. Thus for the top   ranked samples, we can get the topic distribution   as described in Sect.&amp;nbsp; 3.2 and the probability of image assigned to topic   is also available represented as   Then for each topic  , the score   is computed as follows.       Here,   is the number of selected images in the validation set and we set its value to be the top   of all the training images in the experiment. The choice of   may greatly influence the score assignment. The value of   should not be too small; otherwise, some noisy keyframes may get an extremely high score after applying relevance filtering, while little enlargement of the validation set will get more reasonable and accurate score assignment for each topic. Usually, if   is within the reasonable range, topic scores can be assigned precisely. Thus, topics with higher   are selected as relevant, and we can select one or more topics as relevant, depending on the value of  . After applying the relevance filtering procedure on the training samples, the filtered result is obtained by ranking according to the relevance scores. However, in some cases, some less relevant samples can also get higher score since two classes of densities  (concept presence) and  may be both very small (i.e., the feature distances of these samples to positive samples and negative samples are both larger than ), so the resulting relevance score  (see Eq.&amp;nbsp;3) may be very large. Thus the feature distance-based method may be insufficient for distinguishing the positive and negative sample. In this paper, we propose using the topic model to refine or re-rank the filtering results. In relevant topics selection section, we have calculated the probability of being positive   for each topic using the top   samples after relevance filtering. “Positive topic” here means the latent topic which generates images relevant to the given concept, while “negative topic” means that the latent topic generates irrelevant images. Finally, a topic model-based relevance score for image   is computed by marginalizing over topics:       where   represents a single latent topic and   is the topic set. By combining the relevance score computed by relevance filtering   and topic-based relevant score  , the final relevance score   can be obtained as follows.       where   is an empirical weighting factor to balance the importance between relevance filtering and topic model. Usually, the weight   assigned to relevance filtering must be greater than that assigned to the topic model, because relevance filtering result also plays an important role in selecting topics. After the final relevance score is computed, we can get the final ranked keyframe list. Compared to the list generated using relevance filtering only, the proposed approach has several advantages: (1) with latent semantic information being considered, the scores for keyframes are more reasonable and accurate; and (2) noisy keyframes with extremely high scores can be successfully filtered.    4 Experiments  To validate the effectiveness of our proposed approach, we conduct experiments on two web video sets, i.e., Youtube22 [5, 6] and DUT-WEBV [9]. The first experiment shows that noisy keyframes with extremely high scores can be successfully filtered and thus lead to higher precision in the resulting top 100 and top 300 keyframes. The second experiment shows that as compared to the existing methods, our proposed approach can get higher localization precision on human annotated video database. We first evaluate our approach on a real-world web video dataset [5] which contains 22 concepts, and for each concept download 100 video clips by querying the video-sharing website . Similar to [5], we select four concepts including “basketball”, “golf”, “soccer” and “swimming” for evaluation. For each concept, the keyframe samples are extracted from videos using shot boundary detection algorithm and the noisy positive samples are constructed with samples of the concept, while negative samples are those from other concepts. For the feature representation of samples, we first divide the sample images into  grids, and for each patch, the color histogram and edge histogram features are extracted. Then following the bag-of-words approach, we conduct clustering on the two kinds of features, respectively, and for each kind of feature a 50-dimensional vocabulary is constructed. Finally, we concatenate the two bag-of-word representations into a 100-dimension feature vector to represent each sample. The following three steps are conducted to evaluate and compare three relevant frames’ localization strategies. We first test the automatic relevance filtering method using KDE proposed in [5] in Step 1, then test the relevant topic selection strategy based on the filtered results of Step 1 in Step 2. In Step 3, we evaluate the proposed combined filtering approach. We test this method similar to [ 5,  6], only that the feature representation is different. We use grid-based color histogram and edge histogram features in this paper. The bandwidth of   and relevance prior   are selected using cross-validation. The total number of video keyframes is 9,602. After the iteration converges, the relevance score for each sample is obtained and then used to rank the video frames. Figure&amp;nbsp;  shows the top 42 ranked frames for each of the four concepts. We can see that most of the resulting keyframes are highly relevant, while some irrelevant frames are also selected and have very high ranking because of the reason we explained in Sect.&amp;nbsp; 3.5 (irrelevant keframes are marked with red boxes). We apply LDA to the frames from positive videos as described in Sect.&amp;nbsp; 3.2. The topic number here is set to 5. After the topic distribution of each frame is obtained, we assign each frame to the topic with maximum probability. Then we select one or more most relevant topics to the given concept by using the relevance filtering result obtained in Step 1 as validation set. Fig.&amp;nbsp;  shows the results for topic selection. We can see that relevant topics and irrelevant topics are selected correctly. We refine the relevance filtering results obtained in Step 1 by the topic model. Here, we equally weight the importance of relevance score   and topic model score  , i.e., set   to 0.5. Figure&amp;nbsp;  shows the results of our approach. By comparing Figs.&amp;nbsp; – , we can see that some irrelevant frames are removed from the top 42 results, because their degree of membership in the relevant topic is low. Table&amp;nbsp; 1 gives the comparison results for two methods on P@100 and P@300, where P@N is the precision of the top   result frames. Whether keyframes are related to the concept are manually judged. From table&amp;nbsp;1, we can observe that our approach improves the precision of the top results, especially for the top 100 images. The P@100 for most of the concepts are lower than their P@300 when using relevance filtering only. The possible reason is that some irrelevant samples may get very high scores when they have little positive density and negative density simultaneously, as explained in Sect.&amp;nbsp;3.5. However, when combined with the topic model, such samples are successfully filtered, resulting in the relatively higher P@100 and lower P@300.
Springer.tar//Springer//Springer\10.1007-s00138-013-0538-5.xml:Detecting interaction above digital tabletops using a single depth camera:Depth sensing cameras Human–computer interaction Object detection Image segmentation:  1 Introduction  The field of digital tabletops is topic of a lively research community and is constantly growing&amp;nbsp;[3]. Much effort has been put into researching all kinds of gestures or combinations of input methods  the surface to realize natural interactive systems&amp;nbsp;[18]. However, the main means of interaction are still touch and tangible objects. The space&amp;nbsp; the surface has also gained attention of researchers. They mostly experiment with freehand gestures to be used as a separate means of interaction&amp;nbsp;[7, 12]. When it comes to interaction in the volume above the surface, many approaches focus on how to develop interaction techniques without examining the detection and tracking of hands themselves. This is usually accomplished by using a proprietary tracking solution where the user has to wear a glove or hat with markers attached, or by employing digital pens to interact with the digital content of the tabletop. While marker-based tracking solutions are useful to explore interaction techniques in a laboratory environment&amp;nbsp;[14], users might perceive such solutions as obtrusive and prefer wearing their casual clothes in real application scenarios. Especially, when a digital tabletop is integrated in everyday life, e.g., as a coffee table in private living environments or as an inquiry kiosk in public spaces, attaching markers would be cumbersome or even embarrass the user. Moreover, marker-based tracking tends to use expensive equipment and needs precise configuration, making it infeasible for every day applications. First attempts to realize marker-less interaction above digital tabletops have been made using depth cameras&amp;nbsp;[2, 4, 22]. However, detecting body parts is either not the aim of the approach&amp;nbsp;[22], limitations of the structure of the considered scene are made&amp;nbsp;[2] or only specific gestures are detected&amp;nbsp;[4]. In this paper, we present an unobtrusive system for arm and tangible object segmentation to detect the position of hands and tangible objects above a digital tabletop. With our approach, application developers are able to realize diverse interaction techniques that users can employ in everyday situations. We integrate our setup in a living room environment where we install a depth sensing camera above a digital table’s surface to monitor the scene from a top view. We make major enhancements to a previously proposed approach&amp;nbsp;[13] for arm segmentation to detect hands and tangible objects in this setup and present our adaption in detail. In addition, we describe a low effort method to estimate an optimal configuration of segmentation and detectionparameters. The paper is organized as follows. First, Sect.&amp;nbsp;2 gives an overview of related work before Sect.&amp;nbsp;3 describes the setup and the accuracy of the proposed system. Afterwards, Sect.&amp;nbsp;4 details the approach we take to detect interaction above a digital tabletop and gives a comparison to the original algorithm by Malassiotis and Strintzis&amp;nbsp;[13] regarding the merging of segments. Moreover, we present a low effort method to estimate a parameter configuration for segmentation and detection based on simulated annealing. Further, in Sect.&amp;nbsp;5, an evaluation of our approach regarding processing time, segmentation and detection, and repositioning accuracy of hands is given. Also, we discuss limitations of our approach and show first application results using our system. Finally, Sect.&amp;nbsp;6 concludes and describes future work that builds on our approach.    2 Related work  The field of interactive surfaces is topic of several research groups. In particular, the detection of interaction around the surface has gained attention recently. Usually, the approaches either investigate the space above the horizontal surfaces, the space in front of the vertical surfaces or a combination of both. Our approach also relates to a previous work that proposes systems to estimate the human pose and detect body parts from depth data. In the following, we first review previous work that focuses on extending interactive surfaces with depth. Second, we give an overview of approaches to detect body parts in depth data. In&amp;nbsp;[4], Hilliges et al. present two different rear projection-vision tabletop setups to detect interaction above a digital surface. In the first setup, the height of hands above the surface is approximated by their brightness in the image employing a standard camera. The second setup uses a depth sensing camera from behind a holographic screen to detect the exact height of hands above the surface. However, only specific interaction, e.g., a pinch gesture above the surface is proposed. Detecting human body parts is not subject of their work. In&amp;nbsp;[2], Benko introduces  combining a vertical screen with a depth camera to allow interaction in front of the screen. The interaction space is limited to a defined area where hand blobs are detected as the closest segments to the camera. Therefore, the approach is not directly applicable in a top-view setup. Furthermore, only one user can interact at a time, which suspends multi-user scenarios. Wilson and Benko&amp;nbsp;[22] present , a combination of horizontal and vertical digital surfaces, multiple projectors and depth cameras. In their setup, they allow for carrying a virtual object on the user’s hand. However, they do not distinguish between body parts but treat any part of a mesh representing the detected surface of the user equally. In&amp;nbsp;[21], Takeoka et al. present  that uses multi-layered infrared laser planes to detect interaction close to a tabletop’s surface. The system does not scale for larger interaction volumes above the surface and is unable to assign interaction to actual body parts. Annett et al.&amp;nbsp;[1] propose , which is also an extensive hardware setup composed of a digital tabletop and three rings of infrared-based proximity sensors, to detect the presence of users close to the tabletop. Employing this technology, they are able to assign touches on the surface to hands of users. The authors themselves pointed out that their system is error-prone in situations when multiple users cross paths. Furthermore, accurate positions of hands above the surface or distinct body postures cannot be determined. There exists a vast body of literature on vision-based approaches to realize marker-less human motion detection reviewed in&amp;nbsp;[16]. Since our approach bases on acquired depth data, we give a short overview of work focusing on the detection of body parts that involves depth data. Several approaches employ a fusion of RGB and depth data, and a human body model to detect and track a user’s body parts. Knoop et al. use an iterative closest point (ICP) algorithm to fit the body model into the data in&amp;nbsp;[10]. Apart from kinematic constraints, no other assumptions are made. However, the authors point out that the approach only works reliably if the whole body is visible. Further, a higher tracking accuracy comes at the cost of a growing number of ICP iterations, which increases the algorithm’s processing time. Jain et al. propose a Haar cascade-based detection of head and torso followed by a local fitting of limbs in&amp;nbsp;[6]. The approach uses a frontal face detection classifier to estimate the head’s position and then infers the upper body’s pose. With the launch of the Kinect1, the OpenNI SDK2 has become available to realize full body tracking in real-time by fitting a human skeleton into depth data. However, the system again only works if the user’s whole body is visible and standing upright. In&amp;nbsp;[15], Plagemann et al. propose an approach to detect human body parts in depth data by identifying geodesic extrema on the surface’s mesh. These extrema are classified as head, hands and feet using a classifier trained on depth images’ patches. To overcome self-occlusion problems occurring in this approach, Schwarz et al.&amp;nbsp;[19] employ optical flow to disconnect points lying on different body parts. To correctly assign body parts, both approaches assume that the user faces the camera to achieve a frontal view. All previously mentioned approaches have in common that they propose front-view setups and assume certain situations. Among others, they assume that the full body is visible, that the head is above other body parts, that arms are reaching towards the camera, or that the face is visible in the camera image. None of these assumptions can be made in a top-view scenario. As a consequence, the so far reviewed approaches are not applicable in our setup, which we will describe in the following section. Shotton et al.&amp;nbsp;[20] propose an approach to detect body parts from depth data without using any temporal or kinematic dependencies. Although they present a front-view setup where the whole body is visible, they do not make any assumptions that limit their approach to such setups. However, they need an extensive database of labeled training data to compute depth features for online detection of body parts. A less extensive approach is presented by Malassiotis and Strintzis&amp;nbsp;[13] that determines clusters in depth images and uses the respective eccentricities to detect arms. Because of its simplicity, we chose to build upon this approach and present our adaption in Sect.&amp;nbsp;4.    3 System  In this section, we propose a system to detect user input above and around a digital tabletop. We first present the setup and its calibration before we describe the accuracy that is to be expected of our approach. There exist various tracking solutions using markers that are attached to the user’s body offering a high tracking accuracy. However, Sect.&amp;nbsp;1 already points out that an obtrusive solution is not applicable in tabletop environments integrated in everyday situations. As interaction takes place in 3D space, 3D information of the scene has to be extracted first. There are numerous different solutions to measure 3D data. Although a setup with multiple cameras would reduce occlusion situations, the system would demand an extensive hardware installation. A passive stereo-vision system could be used to obtain a depth image of the scene. However, these systems lack in reliability because corresponding points need to be found in every frame which can be difficult, e.g., in homogeneous regions. We therefore employ a single active depth camera that uses infrared light to measure depth since this technology can achieve robust results at interactive frame rates. In a setup where only a single depth camera is present, the position and orientation of the camera have to be chosen carefully. In a tabletop setup, interaction is likely to be performed from all sides of the table. Installing the depth camera to obtain a side view of the scene would limit the system to only be usable from three sides. Furthermore, occlusions in multiple user scenarios might occur. Placing the camera inside the table would solve occlusion issues but is not applicable with most tabletop systems, as rear projecting screens reflect infrared light. Therefore, we decide to install the depth camera above the table to achieve a top view of the scene. One choice for a suitable active depth camera in our setup could be a Time-of-Flight (ToF) camera. ToF cameras are robust sensors to obtain depth images of the scene at interactive frame rates by measuring the time that the emitted infrared light needs to return to the sensor&amp;nbsp;[11]. However, ToF cameras are still expensive and provide a rather low-image resolution. Moreover, ToF cameras cannot be used with infrared-illuminated tabletop systems since these cameras employ highly pulsed infrared light. This light interferes with tabletop systems by outshining the table’s infrared light in the camera image making touch and tangible object recognition on the surface impossible. In contrast to ToF cameras, Microsoft’s Kinect determines the depth from a disparity map, which is obtained by an infrared light pattern constantly projected onto the scene. It captures a high-resolution image at 30 frames per second. The pattern only causes minor artifacts in the camera image of an infrared-illuminated table which can easily be removed employing standard filtering techniques. In addition, the Kinect is not affected by the table’s illumination during the detection of interaction above and around the surface. Taking the observations described above into account, our proposed system concludes in a top-view setup using a Kinect as shown in Fig.&amp;nbsp; . We investigate the scenario of a living room, where the coffee table is equipped with a digital surface with a height of approximately 0.55&amp;nbsp;&amp;nbsp;m. With a ceiling height of 2.50&amp;nbsp;&amp;nbsp;m, the camera is installed 1.88&amp;nbsp;&amp;nbsp;m above the surface and captures the whole surface of the table and an additional zone of around 0.3&amp;nbsp;&amp;nbsp;m around the surface. This setup allows for detecting interaction taking place above and closely around the tabletop system. In order to use interaction above the digital surface in combination with touch and tangible interaction, the depth data has to be aligned with the tabletop’s input coordinate system , which in our case is left-handed and has its origin at the upper left corner of the display. The - and -axes are defined by the display coordinate system and the -axis is defined perpendicular to the -plane pointing upwards from the table. From the raw depth data captured by the Kinect sensor, a point-set    is determined in the right-handed Kinect camera coordinate system   and needs to be transformed into the common coordinate system  . To obtain the transformation   that maps   into  , we establish an overdetermined linear system   with  , which can be solved in a least squares manner. Here,   are reference points in   and   are corresponding points in  . To determine   and  , we arrange nine calibration objects consisting each of a pole with a disc attached to both ends on the table surface as shown in Fig.&amp;nbsp; . To avoid degeneracy, we use objects with heights equally spaced between 5 and 21&amp;nbsp;&amp;nbsp;cm and arrange them in a way that the centers of the upper discs do not lie in a common plane (see Fig.&amp;nbsp; ). Placing the objects on predefined locations on the surface, we know the center’s coordinate of each disc in   and define these as reference points  . The order is defined by row starting in the upper left corner of the tabletop display. To achieve mapping of depth image coordinates and real world coordinates, we manually select the center of each object in the depth image in the same order to obtain  . To retrieve depth data, we use the Open Source library lib-freenect provided by the OpenKinect project 3, which was freely available soon after the Kinect was launched. The library delivers depth images with 11-&amp;nbsp;bit resolution where 1&amp;nbsp;&amp;nbsp;bit marks error pixels and 10&amp;nbsp;&amp;nbsp;bits encode the actual depth. OpenKinect also provides an approximation for converting the raw disparity values   into the metric system. The approximation is given by       where   is the depth value in meters and  . Using Eq.&amp;nbsp;( 1), a visible range results between 0.3 and 5.30&amp;nbsp;&amp;nbsp;m with a depth resolution that ranges from 0.0003 to 0.084 &amp;nbsp;m. Since the depth camera is installed 1.88 &amp;nbsp;m above the digital surface, we expect a depth resolution of at least 11 &amp;nbsp;mm in our setup. In&amp;nbsp;[ 8], an analysis of the Kinect’s accuracy is given resulting in a random error of depth measurements from a few millimeters to about 4 cm. To evaluate the depth precision with respect to our tabletop setup, we captured a planar cardboard in nine different regions and at four different distances to the surface. We define the distances to the surface in steps of 10 cm each, starting with 10 cm above the surface. Table&amp;nbsp; 1 shows the  -resolution of the raw data as the average number of points per cm  for the four distances. We average the data over 100 frames. In the closest distance to the table, we obtain approximately 11 points per cm . This restricts the minimum size of an object that can be captured theoretically to about 3.3&amp;nbsp;mm  &amp;nbsp;3.3 &amp;nbsp;mm. Figure&amp;nbsp;  shows the averaged standard deviations   of depth values for each zone and height, respectively. For clarification, the deviations are assigned to bars that are proportional in length and that are located in the nine regions of the table’s surface. One can observe that the depth noise tends to decrease, as height above the table increases, i.e., distance to the camera decreases, although a clear pattern throughout the zones cannot be established. These technical limitations have to be taken into account when designing the detection system. For instance, our system will probably not be able to detect fingers robustly as additionally noise can be expected due to lighting conditions and fast movements of the users.    4 Detection of arms and tangible objects  Our segmentation and detection algorithms build upon an approach presented by Malassiotis and Strintzis&amp;nbsp;[13] for the detection of arms in depth data. However, their approach is optimized for a front-view scenario and does not include tangible objects. Therefore, we adapt the described segmentation and detection approach to suit our previously described setup. In addition, we advance the proposed merging of segments using the segments’ regression planes. This results in two major improvements. Firstly, we can segment arms more robustly since we consider the shape and orientation of arms. Secondly, we are able to merge segments that belong to the same arm and are disconnected due to occlusion. In the following, we first describe how depth data is acquired and preprocessed before we briefly present Malassiotis and Strintzis’s work. We then propose our enhanced algorithm to merge segments and present how we adapted the detection of arms and tangibles to work in our tabletop environment. Finally, we describe a simple method to find the optimal parameter configuration for both segmentation and detection employing simulated annealing. Using the Kinect sensor, we obtain depth images consisting of 640  480 pixels. Since the depth image is computed based on the reflections of an infrared light pattern projected on the scene, distortion may occur due to bad lighting conditions or suboptimal reflection properties of the scene. In addition, the image may contain error pixels, meaning invalid depth values. To reduce distortion and to compensate for error pixels, we apply a median filter with radius  to the raw data. Since we aim to detect interaction that takes place above the digital surface, we segment all pixels as foreground that have a smaller depth than the distance to the surface. In the following, all algorithms are applied to the foreground only.
Springer.tar//Springer//Springer\10.1007-s00138-013-0540-y.xml:Triangular traffic signs detection based on RSLD algorithm:Advanced driver assistance systems Computer vision Traffic sign detection:  1 Introduction  Traffic signs allow the regulation and control of the traffic flow. These signs are important for road users: drivers or pedestrians. Although traffic signs are clearly visible along the roads, the driver can be distracted and miss signs, which can cause dangerous situations. Therefore, a traffic sign recognition (TSR) system assists the driver and gives him information about potentially dangerous situations. Two main approaches to TSR systems are proposed in the literature: the global positioning system (GPS)-based approach and the vision-based approach. The first one recognises the current traffic signs by extracting information from the GPS navigation data. The disadvantage of this approach is that the GPS map is neither complete nor up-to-date and there are numerous situations where the GPS signal is not available, e.g., in a tunnel. Therefore, the vision-based approach is necessary. This second approach uses an optical sensor (vehicle-mounted camera) and locates and recognises the traffic signs appearing in the captured frames. Recently, the MIPS (Modélisation, Intelligence, Processus et Systémes) laboratory has tried to merge both systems in order to design more robust TSR systems&amp;nbsp;[8]. This paper focuses on a computer vision approach. Two distinct steps usually compose TSR system by vision, which are detection and recognition. First, regions of interest (ROI) containing the candidate traffic signs are detected in the current frame. This detection is based on the sign characteristics, such as colour and shape, which help to distinguish traffic signs from other objects appearing in the road environment. Then, recognition or classification of the ROIs consists in identifying sign pictograms figuring in each ROI. Each traffic sign is characterised by its colour, shape and pictogram. In the literature numerous methods use colour information to extract the ROIs containing road signs. There are different colour spaces for segmentation, the one which is most often used is the hue-saturation-value (HSV) space [22, 25, 27]. A comparison between various segmentation techniques using different colour spaces for traffic sign detection is presented in&amp;nbsp;[13]. The results show that normalization improves performance and represents a low cost operation. Advanced driver assistance systems (ADAS) can contain up to three different applications: TSR, pedestrian and lane detection. Usually, pedestrian and lane detection use only a grey-scale camera and only TSR can use colour information. But colour information is sometimes perturbed by traffic sign surface reflectance which depends on the weather, daylight conditions and the age of signs. This is one of the reasons why a grey-scale method is proposed here, which offers the same performance as colour methods. The aim is to detect triangular traffic signs in a context of grey-scale images, using the original operator RANSAC symmetric lines detection (RSLD) which exploits the symmetry of a triangle. This paper is organised in three sections: Section&amp;nbsp;2 presents a state of the art of related studies. Section&amp;nbsp;3 details the proposed traffic sign detection and the results are presented in Sect.&amp;nbsp;4, followed by a conclusion and discussion on future research work.    2 Related studies  The methods proposed in the literature can be classified into two categories: the Hough-like approaches and the template matching approaches. The Hough transform is a popular tool in computing vision, e.g., for line detection and circle detection. For the detection of traffic signs, a voting procedure is carried out in a parameter space, from which the ROIs are obtained by computing the local-maxima in the parameter space. For the template matching approach, a set of shape sign templates is used for detection. Matching can be performed through correlation or by a classifier, such as the support vector machine (SVM). The following paragraphs describe recent algorithms, as well as colour or grey-scale images. Garcia-Garrido [12] uses the classic Hough algorithm for searching straight lines in order to detect triangular signs. The purpose is to detect three straight lines intersecting each other, forming a  angle. The advantage of the method proposed in&amp;nbsp;[12] is that it can work day and night with no change of the algorithm. This is possible because the edge points used by the Hough transform are obtained with an extended Canny filter. The two Canny-thresholds are adapted by the image histogram. In&amp;nbsp;[7], a colour segmentation is performed to obtain a binary image with selected regions of the candidate signs. A simple threshold method helps to perform a segmentation in the hue-saturation-intensity (HSI) space. The salient points related to the shape of the traffic sign are detected in the binary image by a detector of local binary features (DLBF)&amp;nbsp;[7]. The set of the salient points are clustered for relating single points, one at each corner. The author does not well explain how the corners are connected for the recognition of triangular shapes. The symmetric nature of triangular shapes is used in&amp;nbsp;[ 23]. An extended fast radial symmetry transform is proposed, the regular polygon detector (RPD), which computes the possible shape centroid locations into the grey-scale image. For regular polygons with   corners, the radius is defined as the perpendicular distance from an edge to the centroid. As for detecting the circle shape, each edge point votes for a potential radius along the line of the gradient vector. All votes are accumulated into a matrix and are used with the equiangular information of the regular polygons for computing the centroid of the shape. The vertex and bisector transformation (VBT) is defined in&amp;nbsp;[2]. This transformation is based on the image gradient and produces two arrays:  and , accumulating evidence of respectively the angle vertex and the angle bisector. The pair edge points  with non-parallel orientation gradient vote for the vertex, () in Fig.&amp;nbsp;a, to which they belong. This pair also votes for the bisector () and helps to obtain two accumulators ( and ). The centre of the triangular sign is represented by the local maxima of  and its corners are determined by the local maxima of  (see Fig.&amp;nbsp;c). Similar to VBT, a single target vote for upright triangles (STVUT) is proposed in&amp;nbsp;[16]. The difference from VBT is that the STUVT computes one accumulator only and the vote relates triple edge points with pairwise non-parallel orientations (see Fig.&amp;nbsp;b). To decrease the computational time of the voting process, a heuristic method is proposed to choose the appropriate triple edge points. Pair edge points  are chosen in the same row image with mirror orientations. The position of the third edge point  is between  and . Several preprocessing steps are proposed and evaluated in&amp;nbsp;[16] to increase the performance of the Hough-like methods. The preprocessing steps transform the colour image into a gradient image which assigns magnitude and orientation to significant pixels, e.g., red pixels which are related to warning traffic signs. Learned colour gradient (LCG) and expected orientations (EO) provide best results with RPD, VBT and STVUT&amp;nbsp;[16]. LCG produces the gradient image according to local colour distribution. EO is a similar step, but orientation gradient is introduced to enhance triangles detection. The colour and shape characteristics are used in&amp;nbsp;[11]. Colour segmentation helps to extract the ROIs containing possible traffic signs from the rest of the image. A quad-tree histogram is applied to segmentation which, uses recursively, a division of the image into quadrants until all elements are homogeneous. To classify the ROIs into a particular group of signs, e.g., triangular signs, the histograms of oriented gradients (HOG) is used as descriptor. A genetic algorithm is used in&amp;nbsp;[9] to search for possible signs in the image. First, an analysis of the hue and saturation components of the image helps to detect the regions which satisfy some colour restrictions. Based on the perimeters of these regions, the genetic algorithm performs a global search to detect the traffic signs where each individual is represented by a sign model with a certain position, a scale and a rotation. Two steps are proposed in&amp;nbsp;[30] for the detection of warning traffic signs. In the first step, a linear SVM classifier is used, and it is trained with the HOG features of all types of triangular traffic signs. To remove false detections, a blackhat filter [30] is applied to the image by emphasizing the dark pixels with high contrast in their local environment. Therefore, the border and the pictogram of the traffic sign are emphasised and the result is used to eliminate false detections. Alefs et al. [1] proposes a matching method based on an edge orientation histogram (EOH). Using the grey-scale image, a Gaussian pyramidal of three levels is created. At each level, a scanning window is used, and the matching measurements, between a set of templates and the region defined by the scanning window, are analyzed in order to determine the presence or absence of traffic signs. The matching measurements are based on edge-orientation histograms defined on different subparts which represent a template. The local contour pattern operator (LCP) is defined in&amp;nbsp; [19, 26] as a measurement over contour images to compute the local geometrical structures. For each contour point, obtained with the Canny filter, the local geometrical structure is represented by a LCP code-word. In [19, 26],  LCP code-words are used to represent 4 sets of orientations. The scanning windows are subdivided into subparts according to geometrical structures defining the triangular signs and circular signs. The four-bin histogram, related to the four sets of orientations, is computed on each subpart and used to detect the traffic signs. In&amp;nbsp;[19], the checking of the candidate signs is based on quantum features which are learned by AdaBoost algorithm. Similar to Haar-like features, quantum features are a simple intensity level comparison between pixels which are not adjacent. The method proposed here cannot be classified as a Hough-like approach or template-matching approach. It maybe be close to Hough-like approach because it is based on line fitting, but there is no voting process. The main idea is to use the principle of coordinates inversion combined with line detection to detect the symmetry of triangles. The interest of the method is to work on grey-scale images with a final algorithm easily implementable in real-time, and with performance similar to that of the latest methods published.    3 The method proposed  The authors propose a method to detect triangular traffic signs in four steps. This method is based on the Harris corner detector and an original algorithm, RSLD. Figure&amp;nbsp;  shows the four steps proposed in the algorithm. The method starts by computing the gradient image following   and   directions (Eq.&amp;nbsp; 1). The first step consists in computing the corners inside the images, using the well-known Harris detector to ensure the extraction of triangular traffic sign corners. The second step uses corner coding in order to select up and down corner candidates related to triangular shapes oriented upwards and downwards, respectively. The third step proposes the RSLD method for traffic sign triangular shape detection. The final step checks the presence of triangle baseline to confirm the triangular traffic sign detection. For the detection of triangle corners in images, the Harris detector&amp;nbsp;[ 15] is one of the most efficient methods with a low computation time. The Harris detector is based on the covariance matrix   (Eq.&amp;nbsp; 2). This matrix describes the gradient distribution in a local neighbourhood for each pixel. The neighbourhood depends on the Gaussian window   used. The eigenvalues,   and  , of this matrix represent the principal gradient components in two orthogonal directions. Both significant eigenvalues indicate the presence of two main directions in the neighbourhood, which can describe a corner at the corresponding position. Harris [ 15] proposes an alternative to eigenvalue computation with the criterion   described in Eq.&amp;nbsp; 3 where   is a scalar fixed at 0.04.                         The set of corners are computed by finding the local-maxima in the   matrix. An example of detection is shown in Fig.&amp;nbsp; a. The Harris detector is usually employed to compute geometric relations between images or in image registration. These methods associate feature vector descriptors to describe each corner in order to perform a comparison with representative vectors contained in a database&amp;nbsp;[ 20]. One major limitation of descriptors for traffic sign corners is the symmetric neighbourhood considered to create them. For example, the Harris feature vector descriptor (HFVD) proposed in&amp;nbsp;[ 29] is based on a circular neighbourhood partition. This approach is not adapted for traffic sign detection because the feature descriptor will not only include information of road signs but also of the background scene. This is the reason why an alternative to centered descriptors is proposed: coding the main directions attached to a corner in an appropriate neighbourhood. The aim is to detect up and down corners related to triangular shapes oriented upwards or downwards, respectively, using gradient images ( ) previously computed to identify both types of corners. A coded image is generated, by using Eq.&amp;nbsp; 4, where each pixel is coded according to its orientation. Intermediate calculations available in the Harris computation are used to define  \(A=(I_x^2&amp;gt;T), B=(I_y^2&amp;gt;T)\), and  \(C=(I_xI_y&amp;lt;0)\).   is a threshold used to select a significant gradient; Its value is set to  . Five classes   are defined. The non-edge pixels are coded by class  , and the edge pixels are coded following the four directions depicted in Fig.&amp;nbsp; . The diagonal edges are represented by classes   and  . Finally, the horizontal and vertical edges are represented by classes   and  .            The aim of corner coding is to select corner candidates which represent triangular traffic sign positions. This filtering is based on coded images. For each corner position reported in coded images, an up or down corner is detected according to class distribution. Two class distributions are proposed in adapted subregions. Figure&amp;nbsp; a shows subregions for up corner coding. In this case, a subregion concerns the neighbourhood under the corner position where the shape information of road signs must be found. For each corner, if class 2 is dominant in the South-West (SW) subregion and class 1 in the South-East (SE) subregion, then the corner is classified as an up corner. A similar process is applied for down corners in agreement with the subregions shown in Fig.&amp;nbsp; b. Figure&amp;nbsp;  illustrates the result of the corner coding step applied to a synthetic image, and Fig.&amp;nbsp; b on a road scene image. Figure&amp;nbsp;  shows the number of remaining triangular traffic sign candidates compared to the Harris corner number. The corner coding is able to select candidate corners only, and to significantly reduce the number of corners to be analyzed in the next steps, which should confirm that a corner represents a triangular traffic sign or not. The methods described in&amp;nbsp;[5, 21, 23, 24], detect the borders of traffic signs with a line detector and analyze the relative positions of these lines with different criteria such as symmetry, the distance to the border or the position of the intersection points. To reduce the computation time for line detection, the RSLD method based on RANSAC algorithm has been designed for this application. The RANSAC algorithm is a well-known, powerful method to fit a model in a set of  points containing data related to the model (inliers) and data which are not related to the model (outliers). It is an iterative algorithm, composed of two mains steps. The first step consists in generating an appropriate model based on  samples chosen randomly from a set of points. The second step evaluates the computed model for the  points of the dataset. The idea is to straighten the two diagonal sides of triangular shapes. This helps to detect only one segment instead of two with a RANSAC algorithm. This principle is illustrated in Fig.&amp;nbsp; a, b. For up corner candidates, a simple inversion of the  -coordinate applied on coded pixels of class 2 transforms the two diagonal segments into a single segment. For down corner candidates, the same inversion is performed with coded pixels of class 1. This approach converts the symmetry detection into a simple line detection while preserving the symmetry criteria. Indeed, if the corner candidate is really positioned at the top or bottom of a triangular shape, this method will produce a long segment centered around this corner. If the number of inliers is lower than a threshold  , the detected segment is then too short and the corner is not a true candidate. The threshold   is function of detection scale and its value is chosen from the receiver operating characteristics (ROC) curve as it is discussed in Sect.&amp;nbsp; 4. There are two advantages which justify this method: accuracy is better, depending on the outliers, and the number of computations is reduced as compared to other methods. Concerning the accuracy, this approach was compared with the separate detection of the two segments, using RANSAC to retrieve one or both of the segments. This comparison was performed by adding outliers to a synthetic image containing two segments representing the diagonal sides of a triangle. For each measurement, the RMSE (root-error-mean-square) geometric distance was computed between the segment(s) model and the segment(s) detected according to the level of the outliers added to the synthetic image. The comparison is performed between two approaches:    without the -coordinate inversion: detection of two segments using the RANSAC method for each segment. The adapted subregions were defined around the corner candidate (see Fig.&amp;nbsp;  with the -coordinate inversion: detection of a single segment with the RANSAC method applied to appropriate subregions after the -coordinate inversion process.  Figure&amp;nbsp;  shows an example of corresponding synthetic images with and without the  -coordinate inversion method. Inliers were generated from equations:   and   with Gaussian noise ( ) added to their coordinates. This noise represents perturbations, which generates realistic simulations. The measurements were performed according to outliers percentage added. For a given outlier percentage, the experiment was repeated 1,000 times to obtain representative results. without the -coordinate inversion: detection of two segments using the RANSAC method for each segment. The adapted subregions were defined around the corner candidate (see Fig.&amp;nbsp;) and RANSAC was applied to each region separately. with the -coordinate inversion: detection of a single segment with the RANSAC method applied to appropriate subregions after the -coordinate inversion process. The results of these experiments are reported in Fig.&amp;nbsp;. The possibility of easier detection of a long single segment instead of two short, ones is thus confirmed. The curve presenting the lower RMSE is the one where the -coordinate inversion was performed. Concerning the cost of computation, the number of iterations with and without the coordinate inversion was compared. The number of RANSAC iterations   (Eq.&amp;nbsp; 5) depends on the percentage of the inliers  , complexity of the model   and probability   that RANSAC gives a suitable model&amp;nbsp;[ 10]. For line detection, as   points are enough to model a line,  . The probability is chosen such that  , which is a commonly used value for RANSAC. With these values,   if  , and   if this percentage decreases to  . The RANSAC computation time   can be estimated using Eq.&amp;nbsp; 6 [ 6] where   and   represent, respectively, the calculation time for the two RANSAC steps: generation and evaluation.       The detection of diagonal triangle sides need the RANSAC algorithm to be applied two times in the subregions described in Fig.&amp;nbsp; a. First in subregion   and secondly in  . Here,   and   are unused subregions.   and   are considered to contain   samples each. The global computation cost   of this detection is then defined as the sum of the computation time for   and   subregions:            The RANSAC with  -coordinate inversion used here, is applied to subregions   and   and the number of points in the dataset is then  . The duration   of the method can be obtained by adding the  -coordinate inversion time   to  .       As the  -coordinate inversion is performed once, and not at each iteration, it can be concluded that  \(t_\mathrm{i} &amp;lt; K~t_\mathrm{g}\). In this case  \(T_{AD}~&amp;lt;~T_{CD}\), which shows that the proposed method is faster than two successive RANSAC segment detections. Figure&amp;nbsp;  compares the computation cost, with Matlab scripts, of both methods according to the outlier percentage. A region of interest having a   pixels is used for this simulation (see Fig.&amp;nbsp; ), with a realistic number of inliers  . The results confirm that the proposed method needs low computation time than two segments detection. In practice, for real images, the computation cost is reduced by  . The traffic sign scale in images depends on its distance from the camera. To use the present RSLD method efficiently, the region around the candidate corner where the inversion is applied must be defined precisely. The region size must correspond to the road sign’s dimensions and without depth information, it is impossible to estimate this size. A multi-scale detection process helps to solve this problem. First, a small region   is centered on the selected corner where the corresponding points for RANSAC algorithm are extracted, as shown in Fig.&amp;nbsp; . If no line is detected, there is no triangle related to this candidate corner. Otherwise, there remains the length of the line detected in  . This process is repeated in regions   to   until the length line   is stabilised, as shown in Fig.&amp;nbsp; . This multi-scale detection helps to estimate the resolution of potential triangles detected and gives the diagonal side length which is  . At this stage, only diagonal triangle sides are detected and it is necessary to confirm that this shape is a triangular traffic sign with a baseline detection. A typical example is given in Fig.&amp;nbsp; c. In road scene images, numerous objects present shapes which are similar to triangular traffic signs. With two diagonal sides and a symmetrical appearance, they can be detected thanks to the previous step (see Fig.&amp;nbsp; c). These objects are then considered as false positive detections and increase the computation time during the traffic sign recognition phase. The aim is to eliminate such false detections in a final step which consists in triangular shape baseline detection. This step exploits the length   obtained previously with the multi-scale detection. First, a horizontal subregion at   distance from the candidate corner position is defined, as illustrated in Fig.&amp;nbsp; . Secondly, as the sides of an equilateral triangle are equal, this subregion contains a baseline if the number of pixels from class   is  , where  . 
Springer.tar//Springer//Springer\10.1007-s00138-013-0541-x.xml:Intrinsic and extrinsic active self-calibration of multi-camera systems:Multi-camera systems Active calibration Self-calibration Pan-tilt zoom cameras:  1 Introduction  In the recent years multi-camera systems became increasingly important in computer vision. Many applications take advantage of multiple cameras observing a scene. Multi-camera systems become even more powerful if they consist of   cameras, ı.e. pan-tilt zoom cameras (Fig.&amp;nbsp; ). For many applications, however, the (active) multi-camera system needs to be calibrated, ı.e. the intrinsic and extrinsic parameters of the cameras have to be determined. In the context of mounting a camera on a pan-tilt unit, the calibration between these two is also of importance to transform the information obtained from images with different pan-tilt rotations into a common camera coordinate system. Intrinsic parameters of a camera can be estimated using a calibration pattern [38] or camera self-calibration methods for a rotating camera [1, 13, 16]. Frahm and Koch [12] and Bajramovic and Denzler [2] use different extents of rotation knowledge to improve the intrinsic self-calibration. Classical methods for extrinsic multi-camera calibration need a special calibration pattern [38] or user interaction like a moving LED in a dark room [7, 30]. From a practical point of view, however, a pure self-calibration is most appealing. Self-calibration in this context means that no artificial landmarks or user interaction is necessary. The cameras estimate their position only from the images they record. Examples for the self-calibration of  multi-camera systems are the works of Snavely et al. [29] or Läbe and Förstner [18]. Given several images they extract multi-image point correspondences and use these to incrementally calibrate the entire system. During each step a bundle adjustment [34] is performed. Martinec and Pajdla [22] weight relative poses by a measure based on the number of inliers found by RANSAC and also on the “importance” of a relative pose. Another example is graph-based approaches like the ones of Vergés-Llahí et al. [36] or Bajramovic and Denzler [3] which consider the uncertainty of the estimated relative pose of each camera pair. Most of these are not designed to handle the ambiguities that are created by the vast amount of different images a pan-tilt camera can record while changing its orientation. Furthermore, none of these uses active camera control in a beneficial way to increase the robustness of the calibration or to achieve a higher accuracy. Only few approaches for active extrinsic calibration can be found in the literature. Sinha and Pollefeys [28] suggest a method where each pan-tilt zoom camera builds a high-resolution panorama image. These images are used for relative pose estimation. However, these huge images typically contain many ambiguities which affect the extraction of correct point correspondences. Furthermore, they only discuss the relative pose estimation between two cameras and do not handle the problems that arise if more than two cameras need to be calibrated. The calibration method of Chippendale and Tobia [8] defines an observer camera which searches for the continuously moving other cameras. If the observer spots some other camera the relative pose between the two cameras is extracted by detecting the circle shape of the camera lens and tracking some special predefined camera motions. The applicability and accuracy of this method highly depends on the distance between the cameras. One of the most important problems in extrinsic camera self-calibration is extracting  point correspondences between two or more cameras. This problem is called wide baseline correspondence problem and several approaches can be found in the literature [20, 23]. However, if the cameras have very different viewpoints on a scene, projective influences and occlusions complicate or make it impossible to establish any correct point correspondences. Pan-tilt cameras can rotate and zoom to actively reduce these projective influences. In this paper, we tackle the problem of self-calibration by exploiting active camera control in a beneficial way. We adopt ideas of the uncertainty-based calibration and combine these with novel active methods for extrinsic camera calibration. Our approach covers the entire intrinsic and extrinsic self-calibration of multi-camera systems and is not restricted to some number of cameras. We implicitly handle ambiguities using a probabilistic formulation. Figure&amp;nbsp;  gives an overview of the different steps of our approach. Our method starts with the   self-calibration of each camera. We reach a robust intrinsic calibration for various zoom steps by exploiting the rotation knowledge provided by the pan-tilt units. During this calibration we also estimate the hand-eye rotation between pan-tilt unit and camera. In experiments we will show that ignoring this quite small rotation has a severe impact on the accuracy of the extrinsic calibration. The active   calibration consists of three steps (marked bold in Fig.&amp;nbsp; ):        We start with an initial coarse calibration which returns a set of relative pose candidates for each camera pair. The best pose for each pair is selected based on its probability. For that purpose we present an image based probability distribution for relative poses.         &amp;nbsp;           Given the initial poses, each camera pair rotates and zooms in a way that the points of view of the two cameras are as similar as possible. The perspective difference between the resulting camera images is comparatively small. This significantly simplifies the problem of establishing new point correspondences, which are used to re-estimate the relative poses.         &amp;nbsp;           In the final step, we use the relative poses and their probabilities to calibrate the complete multi-camera system from a minimal set of relative poses. We first determine a set of connected camera triplets that minimizes the uncertainty of the involved relative poses. In order to estimate the scale factors of the relative poses in these selected camera triplets, we require only two of the three relative poses and we do not need any triple-point correspondences. Instead, we control the cameras actively and apply our image-based probability distribution for relative poses. This reduces the number of relative poses necessary for the calibration and avoids the chance that an outlier point correspondence disturbs the calibration.         &amp;nbsp;      The obtained calibration then simplifies the problem of establishing multi-image point correspondences for a final bundle adjustment. We start with an initial coarse calibration which returns a set of relative pose candidates for each camera pair. The best pose for each pair is selected based on its probability. For that purpose we present an image based probability distribution for relative poses. Given the initial poses, each camera pair rotates and zooms in a way that the points of view of the two cameras are as similar as possible. The perspective difference between the resulting camera images is comparatively small. This significantly simplifies the problem of establishing new point correspondences, which are used to re-estimate the relative poses. In the final step, we use the relative poses and their probabilities to calibrate the complete multi-camera system from a minimal set of relative poses. We first determine a set of connected camera triplets that minimizes the uncertainty of the involved relative poses. In order to estimate the scale factors of the relative poses in these selected camera triplets, we require only two of the three relative poses and we do not need any triple-point correspondences. Instead, we control the cameras actively and apply our image-based probability distribution for relative poses. This reduces the number of relative poses necessary for the calibration and avoids the chance that an outlier point correspondence disturbs the calibration. Note that even though we present a self-contained method for intrinsic and extrinsic self-calibration of multi-camera systems, single parts of our approach, e.g. the suggested image-based probability distribution for relative poses or actively controlling cameras for the scale estimation, could also be applied on their own to find solutions for related problems. The main improvement over our preliminary conference publication [5] is that we use a provenly optimal uncertainty-based selection of the camera triplets instead of our prior greedy approach. Additionally, we exploit our obtained calibration to extract multi-image correspondences for a final bundle adjustment. Besides this extrinsic self-calibration, we present a method to estimate simultaneously the intrinsics and hand-eye rotation for each pan-tilt camera. To pinpoint the overall contribution of this paper in the context of multi-camera systems we provide a more detailed analysis of the results. This includes a comparison of our method with the self-calibration of the well-known Photo Tourism system of Snavely et al. [29]. The remainder of this paper is organized as follows: we introduce the camera model and some notation in Sect.&amp;nbsp;2. Section 3 presents our intrinsic calibration approach. The novel active extrinsic calibration method is described in Sect.&amp;nbsp;4 and we discuss application areas and restrictions of it in Sect.&amp;nbsp;5. In Sect.&amp;nbsp;6 we present experiments and results. Conclusions are given in Sect.&amp;nbsp;7.    2 Camera model and notation  A world point   is projected to the homogeneous image point   of camera  :       where the extrinsic camera parameters consist of a rotation   and a translation   is the pinhole matrix [ 14] and   means equality up to scale. The pan-tilt rotation       consist of the pan rotation  , the tilt rotation   and the hand-eye rotation   which is the rotation between the pan-tilt unit and the camera. Note that the camera’s extrinsic parameters   do not depend on the current pan-tilt rotation. We assume the intersection of the pan and tilt axes to be identical with the optical center of the camera. This assumption is reasonable if the camera is mounted similar to the one in Fig.&amp;nbsp;  (right). For a precise calibration the radial lens distortion should also be considered [ 31,  32]. We model the radial and tangential lens distortion similar to the model used by Heikkila and Silvén [ 17]. Furthermore, we use camera normalized image coordinates for the undistorted   and the distorted   image points. The undistorted image point is mapped to the distorted image point by a non-linear function  . As we are interested in correcting the radial distortion, we only need the inverse mapping, which we model as follows:            where   are the Euclidean coordinates of   are the parameters of the radial distortion model,   are the parameters of the tangential distortion model, and   is the squared distance between the distorted image point and the camera normalized image center.    3 Intrinsic rotational self-calibration  Taking two images of a 3D point   with constant intrinsics but different pan-tilt unit rotations   results in the image points  . The points   and   correspond. By eliminating  , we get [ 13,  16]:       where   is the relative camera rotation and   is called infinite homography [ 14]. Given   relative rotations of the camera (not all about the same axis), the self-calibration problem can be solved linearly [ 13,  16]. Based on Eq.&amp;nbsp;( 4), Bajramovic and Denzler [ 2] formulate a non-linear optimization problem which uses (partial) rotation knowledge to estimate  . We extend this approach. In addition to the pinhole parameters  , we also estimate the radial distortion parameters   and the hand-eye rotation  . For that purpose we exploit the rotation knowledge provided by the pan-tilt units. We define the following optimization problem:       with       and            where   is the Euclidean distance in the image plane of (undistorted) 2D points in homogeneous coordinates,   is the number of relative pan-tilt unit rotations   and   is the number of point correspondences   for rotation  . The relative rotations   are provided by the pan-tilt unit. For numerical reasons [ 14,  15], we normalize pixel coordinates to the range   by applying a translation and an isotropic scaling. The hand-eye rotation is represented by a rotation vector [ 21]. The optimization problem of Eq.&amp;nbsp;(5) is solved by applying a second-order Trust Region algorithm [9]. The initial solution for  is provided by a linear self-calibration method [13, 16]. Radial distortion parameters are initially set to . The hand-eye rotation is initialized as identity rotation . Point correspondences are established using KLT tracking [27]. For the following extrinsic calibration approach we need the intrinsic parameters of several zoom steps. Hence, we optimize the pinhole and radial distortion parameters of several cameras simultaneously. Note, that we estimate only one hand-eye rotation since this is not affected when changing the camera zoom. In order to reduce the number of optimization parameters and to increase the robustness, we furthermore assume that the aspect ratio and the principal point are identical for all zoom steps of the camera. While the first assumption is reasonable, the position of the principal point is slightly changed when the camera zoom changes [37]. However, the effect of a marginal incorrect principal point is negligibly small and we still achieve an accurate calibration of the multi-camera system (cf. experiments in Sect.&amp;nbsp;6.2.3).    4 Active extrinsic multi-camera calibration  During the calibration the pan-tilt camera will record images using different pan and tilt rotations. The information obtained from these images needs to be transformed into a common coordinate system. Hence, we suggest to use undistorted image points       which are normalized with respect to the intrinsics and the respective pan-tilt rotation. From this point on, when talking about the camera orientation and translation we actually mean  from (1) with no pan-tilt rotation . The relative pose between two cameras  and  is defined as . For each camera the intrinsic parameters for different zoom steps are assumed to be known (Sect.&amp;nbsp; 3). Each step of the suggested active extrinsic self-calibration exploits active camera control in a different way and to a different extent. The three main steps of our method are illustrated in the bottom row of Fig.&amp;nbsp; :        An initial relative pose estimation and an evaluation of these relative pose estimates (Sect.&amp;nbsp;4.3). During this evaluation the cameras are actively controlled to increase the similarity of the camera images. Furthermore, we suggest an image-based probability distribution of the relative poses.         &amp;nbsp;           The low perspective difference in the camera images significantly simplifies the problem of extracting point correspondences which are used to optimize the relative pose between each camera pair (Sect.&amp;nbsp;4.4).         &amp;nbsp;           In order to build up the final calibration from the pairwise relative poses, we select a minimal set of camera triplets based on the uncertainties of the relative poses. We tackle the problem of estimating consistent translation scales for each of these camera triplets by actively controlling the cameras. This active scaling approach allows us to estimate a missing relative pose in each camera triplet (Sect.&amp;nbsp;4.5). To the best of our knowledge this has not been done before.         &amp;nbsp;      Based on the obtained calibration multi-image correspondences for bundle adjustment (Sect.&amp;nbsp; 4.6) are extracted. In the following Sect.&amp;nbsp; 4.2, we briefly describe our common field of view detection [ 6] which is used at different points of the active extrinsic self-calibration. An initial relative pose estimation and an evaluation of these relative pose estimates (Sect.&amp;nbsp;4.3). During this evaluation the cameras are actively controlled to increase the similarity of the camera images. Furthermore, we suggest an image-based probability distribution of the relative poses. The low perspective difference in the camera images significantly simplifies the problem of extracting point correspondences which are used to optimize the relative pose between each camera pair (Sect.&amp;nbsp;4.4). In order to build up the final calibration from the pairwise relative poses, we select a minimal set of camera triplets based on the uncertainties of the relative poses. We tackle the problem of estimating consistent translation scales for each of these camera triplets by actively controlling the cameras. This active scaling approach allows us to estimate a missing relative pose in each camera triplet (Sect.&amp;nbsp;4.5). To the best of our knowledge this has not been done before. Common field of view detection consists of deciding which image pairs show a common part of the world. We will briefly describe a probabilistic method [6] which gave the best results in our experiments. Given two camera images, the difference of Gaussian detector [ 20] is used to detect interest points  . For each point  , the SIFT descriptor   is computed [ 20]. Based on the descriptors we construct a conditional correspondence probability distribution for each          where   is the inverse scale parameter of the exponential distribution,   is the Euclidean distance between the descriptors of the points   denotes the distance to the nearest neighbor of the point  . Each of the resulting conditional probability distributions   has to be normalized such that   holds. Note that other choices of this probability distribution are also feasible. The exact choice is not that crucial. Given the conditional probability distributions we can calculate the normalized joint entropy which is defined as            where   is the maximum joint entropy and   is a uniform distribution if no prior information about the interest points is available. A low joint entropy   indicates similar images. For further details the reader is referred to [ 6]. Given the intrinsic parameters, each camera rotates and records only as many images as necessary to cover its complete environment. For that purpose we determine the stepsize angle for the pan rotations       with   and where   is the image resolution, and   measures the angle between two vectors. In an analogous way we choose the tilt stepsize.
Springer.tar//Springer//Springer\10.1007-s00138-013-0542-9.xml:Anisotropic diffusion algorithm based on weber local descriptor for illumination invariant face verification:Weber local descriptor Retinex Face verification Anisotropic diffusion:  1 Introduction  Face recognition technology has evolved as a popular identification technique to perform verification of human identity. In the past three decades, it has become an important area in the research of the current face verification systems are designed for indoor, cooperative-user applications. However, even in these environments, the performance of face verification is compromised in accuracy by the changes in environmental illumination. Robust face verification under heterogeneous illumination conditions is still a challenging research topic. A variety of approaches have been proposed to solve illumination problem in face verification. The approaches based on Retinex theory have been shown a very good prospect for the illumination invariant feature extraction of face image. The earliest Retinex method was proposed by Jobson et al. [1]. This method was called single scale Retinex (SSR), using a single filter with Gaussian kernel. Then, Jobson et al. [2] extended SSR to multi-scale Retinex (MSR) by combining more than three low-pass filters of the logarithm of the original image with different cut-off frequencies for each filter. In case of strong shadows, the halo effects are visible at large illumination discontinuities of face images processed by SSR and MSR. Gross and Brajovic [3] introduced anisotropic diffusion (AD) algorithm, which can reduce the halo effects to a certain degree to estimate illumination. Wang et al. [4] proposed self-quotient image (SQI) using the weighted Gaussian filter to obtain better illumination estimation. SQI makes impressive improvement of performance for illumination problem with a side effect of causing edge sharpening. Chen et al. [5] proposed total variation quotient image (TVQI) improved SQI by utilizing the edge-preserving capability of the total variation model. The defect of TVQI is that there are large-area white spots in the processed images. Recently, Park et al. [6] presented extended Retinex method, in which the new conduction function was used for adaptive smoothing and an additional constraint was designed for more accurate description of real environments. It is superior to some typical illumination normalization methods. While the space gradient descriptor and the local in-homogeneity only reflect the absolute variation of gray value between the center pixel and its neighboring pixels. We consider that the relative variation may reflect the further changes in local image. Weber local descriptor (WLD) [7] inspired by Weber’s Law has been proved to be a simple, yet very powerful and robust local descriptor based on human visual perception. WLD consists of two components: differential excitation and orientation. The differential excitation component is a function of the ratio between the relative intensity differences of a current pixel against its neighbors and the intensity of the current pixel. Therefore, WLD can effectively reflect the difference of neighbors related to the current pixel. Inspired by the previous mentioned knowledge, we consider Weber local descriptor as the discontinuity measure in anisotropic diffusion algorithm to estimate the illumination of face image in this paper. Though WLD enhances the detail information of face image, it also enhances the edge of shadow. To eliminate this effect, we introduce a centre-symmetric logarithmic transformation to change the dynamic range of the image’s gray value. Then, we apply the proposed algorithm to extract the illumination invariant feature of face image for face verification. The experiments are executed on two public databases (CMU PIE [8] and CAS-PEAL [9]) and a self-built real-life face database. The experimental results show that the proposed method can obtain better performance than some typical methods on the face databases. The rest of this paper is organized as follows. In Sect. 2, face illumination model is reviewed. In Sect. 3, the proposed method and illumination invariant face verification scheme are described in detail. In Sect. 4, experiments are executed to demonstrate the performance of the proposed method. Finally, conclusions are discussed in Sect. 5.    2 Face Illumination Model  According to Lambertian convex surface model, a face image is expressed as follows:       In ( 1),   is mostly the reflectance of the scene and   is mostly the illumination. Hence, the illumination normalization for face verification can be achieved by estimating the illumination  . While it is impossible to estimate   from  , it is an ill-posed problem. A common assumption is that   varies much faster than  . Thus, in many methods,   is obtained by taking the difference between the logarithms of image   and its smooth version which serves as the estimation of  . The logarithmic function removes noise and makes the calculation easier. We call this category method generic quotient image. The basic diagram of generic quotient image method is shown in Fig.  . The key point of generic quotient image is how to estimate the illumination  from image . In this paper, we proposed another improved anisotropic diffusion (IAD) algorithm to estimate the illumination more accurately.    3 Anisotropic diffusion algorithm based on Weber local descriptor  In this section, the theory of traditional anisotropic diffusion algorithm, an improved conduction function and a discontinuity measure proposed by Park et al. [6] are reviewed. Then, the WLD [7] is analyzed and introduced into the anisotropic diffusion algorithm, and an anisotropic diffusion algorithm based on WLD is proposed. Finally, the illumination invariant face verification scheme is explained. Traditional anisotropic diffusion algorithm was introduced in pattern recognition to deal with the illumination problem of face recognition by Gross and Brajovic [ 3]. It can be expressed as follows:                 where  . In ( 2),  represents the  th iteration gray value of the pixel   is weight coefficient, and   represents four directions of north, south, west, east, respectively.   is conduction coefficient.   is conduction function where   represents the amount of discontinuity at each pixel   and   is a parameter. The core of traditional anisotropic diffusion algorithm is using the weighted sum of the neighborhood pixels around the center pixel to replace the gray value of the center pixel. To expand four directions to eight directions and simplify the implementation of the anisotropic diffusion algorithm, ( 2) is rewritten to ( 3).       The denominator of ( 3) represents a normalizing factor. To improve the performance of anisotropic diffusion algorithms effectively, different weights of the local image structure are needed to take into account. The weights are determined by the conduction function and the discontinuity measure. They will be discussed below. In [ 10], two common forms of conduction function are given as follows:                         Here,   is the amount of discontinuity and   is a parameter that determines the level of discontinuities. We utilize the conduction function model proposed by Park et al. [ 6]. The conduction function shown in ( 6) is used to improve the performance of anisotropic diffusion algorithm. For further details about the conduction function, please refer to [ 6]. Spatial gradient is a common local discontinuity measure in image processing. The spatial gradient of an image   at pixel   is defined as follows:       where  , and  . Applying the spatial gradient to the conduction function   can get main conduction coefficient as follows:       Park et al. [ 6] introduced local in-homogeneity as an auxiliary discontinuity measure. The local in-homogeneity   is defined in ( 9), a nonlinear transform and normalization of   are adopted in ( 10).                         In ( 9),   is a local neighborhood of the pixel   and   is the neighbor of  . In ( 10),   and   are the maximal and minimal values of   across the entire face image, respectively. We know that the spatial gradient and local in-homogeneity are the absolute variation of gray value between the center pixel and its neighboring pixels. However, they cannot reflect the variation relative to the background, while the relative variation may be more able to indicate the changes in local image. To overcome the drawback of the discontinuity measures mentioned above, we introduce Weber local descriptor [7] into the anisotropic diffusion algorithm to describe the local discontinuity. WLD has been proved to be an excellent local descriptor. It contains two components: differential excitation and orientation. WLD is inspired by Weber’s Law (also called Weber–Fechner law), which is a historically important psychological law quantifying the perception of change in a given stimulus. For a given pixel, Chen et al. [ 7] pointed out that the differential excitation is computed based on the ratio between the relative intensity differences of the center pixel against its neighbors (e.g.  ) and the center pixel. The intensity differences between its neighbors and a center pixel are used as the changes of the center pixel. For a face image influenced by illumination, we need to find the salient variation within an image to simulate the pattern perception of human being. WLD is a power descriptor and the computation steps of its differential excitation   are shown in Fig.  . The differences between the center pixel and its neighbors are calculated using the filter  . It can be expressed as follows:       where   is the center pixel,   is the   neighbor of   and  .   is the original image. The differential excitation of the center pixel   is computed by ( 12).       WLD describes face features by simulating a human sensing his/her surroundings, using the ratio between the intensity differences   and  , which has powerful representation ability for textures. For further details about Weber local descriptor, please refer to [ 7]. The impact of illumination on the face is uncertain. To extract illumination invariant facial feature image, anisotropic diffusion algorithm is used to estimate the illumination. Discontinuity measure is a main factor that influences the performance of anisotropic diffusion algorithm. To compensate the lack of the discontinuity measures proposed by Park et al. [ 6], we introduce the differential excitation of WLD to describe the relative variation. For a face image, it can be expressed in ( 13).       Then normalize the WLD as expressed in ( 14).       Applying   to the conduction function  can get the auxiliary conduction coefficient, as expressed in ( 15).       Conduction coefficient   can be obtained by combining   and  , as expressed in ( 16).       Figure   is the comparison of the face images processed by using different discontinuity measures. This paper introduces an illumination invariant face verification scheme shown in Fig.  . There are three parts of the scheme: face image preprocessing, illumination invariant feature image extraction and face verification. From Fig.  , we can see that the auxiliary description of the local discontinuity using WLD can enhance not only the detail information of face image but also the edge of shadow. Because of the shadow boundary can cause drastic change of WLD. So the original face images need to be preprocessed to reduce the influence of shadow boundary. Logarithmic transformation [ 11]   is often used to change the dynamic range of the image gray scale. The logarithmic transformation can stretch low gray value, but compress high gray value seriously. To solve the problem of excessive compression, we make a centre-symmetric transformation about the point   to the logarithmic curve. The centre-symmetric transformation could be expressed as follows:            To corresponding to the range of the image gray scale, we set  , so the above equation can be expressed as ( 18).            Figure   shows the curves of the logarithmic function   and the centre-symmetric transformation. From Fig.  , it can be shown that the centre-symmetric transformation can stretch low gray, compress high gray while keeping more high gray levels than the logarithmic function. Illumination invariant facial feature image extraction based on WLD is the core part of our face verification scheme. The main steps are as follows:          Compute  and  by (4) and (11) after setting parameters , and then compute weights  from  and  by&amp;nbsp; (12).         &amp;nbsp;                   Set maximal iteration number       , then perform iteration to update        as follows:                              Here       . The result of the iterative convolution is the estimated illumination       .              &amp;nbsp;           Illumination invariant feature image extraction.         &amp;nbsp;      In the above steps, we have estimated the illumination. Using the generic quotient image method, one can get an illumination invariant feature of the face image. It can be expressed as follows:        Figure   shows the processed images. From Fig.  , the preprocessed method can effectively adjust grayscale dynamic range of the images affected by shadow. Compute  and  by (4) and (11) after setting parameters , and then compute weights  from  and  by&amp;nbsp; (12). Set maximal iteration number  , then perform iteration to update   as follows:          Here  . The result of the iterative convolution is the estimated illumination  . Illumination invariant feature image extraction. This paper introduces a face verification scheme shown in Fig.  . Linear discriminant analysis/QR decomposition (LDA/QR) [ 12] is used to reduce facial features dimensions, cosine distance [ 13] is used to compute similarity and user-specific threshold value [ 14] is set as a threshold approach in the face verification algorithm.    4 Experiment and analysis  In this section, experiments are executed on two public face databases (CMU PIE and CAS-PEAL) and a self-built real-life face database to illustrate the effectiveness of the proposed method. We choose “C27” of CMU PIE which consists of 68 subjects with 1,428 images under illumination from 21 directions and the illumination subset of CAS-PEAL which consists of 233 subjects with 2,450 images under illumination from at least 9 conditions in our experiment. The self-built real-life face database contains 16 subjects, each with 4 kinds of light conditions. We randomly select 48 subjects as customers and the remaining 20 subjects as impostors in CMU PIE database, 160 subjects as customers and the remaining 73 subjects as impostors in CAS-PEAL database, 10 subjects as customers and the remaining 6 subjects as impostors in the self-built real-life face database. All face images from the three databases are properly aligned, cropped and resized to . We compare the proposed method with some other illumination invariant feature extraction methods including MSR [2], SQI [4], TVQI [5], anisotropic diffusion [3] and IAD [6] to verify the proposed method. In the proposed method, there are three parameters   and the iteration number  . They need to be chosen carefully. To choose appropriate   and  , we have executed lots of experiments.   and   have nonlinear relationship with the mean of each discontinuity. Among many possible candidates, and according to [ 6,  15],  and are set as follows:                         There is no strict theoretical basis for setting the iteration number  , hence we obtain it by experiment. Figure   shows the processed image changes as the iteration number   increasing, and Fig.   shows the relation between equal error rate (EER) and the iteration number  . As can be seen in Fig.&amp;nbsp; , when   is increasing, the local details have been enhanced and the processing time will increase with the increase of the number of iteration. In Fig.  , when   is larger than 15, the EER no longer drops but begins to fluctuate. Hence, we set   15. In this section, we evaluate the proposed method on the mentioned face databases by several evaluation criterions. The evaluation criteria are as follows: the proposed image, cosine similarity, receiver operating characteristic (ROC) curve and face verification rate. The processed images of several samples from CMUPIE, CAS-PEAL databases and the self-built real-life face database processed by different illumination invariant feature extraction methods are given in Fig.  . As can be seen from Fig.  , the visual effect of the processed image is slightly worse than IAD due to the preprocessing, but it is better than the other methods. The proposed method can retain the face details and weaken the impact of the shadow edge at the same time. Cosine similarity [ 13] evaluates intra-similarity of illumination invariant feature of face image processed by compared algorithms. This evaluation method can reflect the sensitivity of different illumination invariant feature extraction algorithms. Figure   shows the average cosine similarity of different illumination invariant feature extraction methods. From Fig.  , the cosine similarity of the proposed method is higher than other methods. It means that the proposed method is superior to other methods on robustness of illumination.
Springer.tar//Springer//Springer\10.1007-s00138-013-0544-7.xml:Multiple human tracking system for unpredictable trajectories:Cascade classifier Background subtraction Histogram of oriented gradients Collision detection Occlusion recovery Particle filter:  1 Introduction  The aim of automatic detection and tracking of human beings is a challenging issue, specially when dealing with surveillance systems. These procedures have to isolate every person in the scene to study his behavior. There are multiple solutions to this topic, from generic algorithms, which can track every moving object in the scene, to architectures specially developed to deal with persons. All these systems, when dealing with unpredictable trajectories, have the same bottleneck: target tracking. A lot of challenges arise in the aim of tracking every single object around the scene. Two are the most important issues: occlusions and collisions. An occlusion occurs when a person escapes from the camera control. This can occur if a target goes outside the camera range, or when is totally or partially hidden by any element of the scenario. The algorithm must have the ability to detect the occlusion and, if the target reenters again in the scene, to recover the previous identification. On the other hand, a collision occurs when two or more people cross in front of the camera, occluding part of the targets. The issue is similar with the occlusion, but it is more tricky. First, the system must detect which targets are occluded and which are not. So, when this problem is solved, the rest can be processed as occluded targets. In the collision case, if the system does not perform well, then it could derive into a switch identification problem. This issue must be avoided, since it is very difficult to recover the correct solution when it happens. There are many different approaches in the literature for target tracking. We can find two big groups: low-level and high-level methods. Low-level techniques, like optical-flow [21] or the Kalman filter [14] are simple and fast but they are too soft to work in complicated scenarios, since they have no possibility to recover a target when an occlusion event occurs. We focus on tracking of human beings in unstructured scenes, so we need to use high-level approaches to deal with the problems mentioned before. The main idea into the high-level systems is to add information a priori about the objects of interest. In particular, when dealing with people, information about the human shape is used. In first attempts, Haritaoglu et al. [12] used different cardboard models to represent the human body and located it using dynamic template matching. More recently, Dalai et al.&amp;nbsp;[9] introduced the histograms of oriented gradients (HOGs), which are used to train a SVM for each part of the body. This technique was also used by Felzenszwalb et al.&amp;nbsp;[11] to train any object in the scene. However, these methods try to detect every part in the body, many of which are occluded in crowded scenes. To solve this, Li et al. [16, 17] simplified the method, trying to locate only the omega shape created by the head and the shoulders. A HOG feature-based SVM is used to confirm every target previously located using a Viola–Jones type classifier [25], which improves the speed of the algorithm. A particle filter (PF) is used to track every detecting object in the scene. Based on [10] and using the omega-shape detection, Rodriguez et al. [20] improved the detection in crowded scenes including a density estimation parameter. However, the density information computation forces the algorithm to compute the HOG descriptor in every pixel image, making the system slower. Other different techniques focused in crowd analysis can be found in [28]. More recently, associated-based tracking systems were used. In [23], Starder et al. introduced geometric and long-term temporal constraints to increase the accuracy algorithm. Also they use trajectory filters to increase target identifications. Benfold et al. [4] combined a body and a head HOG detectors with simultaneous KLT tracking and Markov-Chain Monte-Carlo Data Association to estimate the most probable trajectories. Andriyenko et al. [1, 2] used a similar scheme. They infer the most usual path for every target, performing the matching by minimizing the energy related to each trajectory. Information about the interaction between subjects is included in [15] to increase the identification accuracy. A condition random field was also used by [26, 27] to produce discriminate descriptors, which are used to a better tracking dealing with partial occlusions and collisions. The problem with associated-based tracking techniques arises when targets operate with no usual behavior, for instance, a sport event, like football or basketball. When an offense player makes a crossover, or a fake movement, and the defender falls into the trap, an associated-based tracking would swap the identifications, as Fig.&amp;nbsp;  shows. Associated-based tracking techniques assume targets motions are stable, i.e., linear and constant speed in a short period, causing incapable of dealing with big and unpredictable movements like that in sports. It is very difficult to anticipate that kind of movements. So, a different technique must be used. To perform the tracking, techniques like particle filters [3] (PFs) or the Lucas–Kanade algorithm [22] are used, which are good methods to track isolated people. However, these algorithms tend to accumulate an error along successive detections, which often results in the loss of the target detection. Moreover, there are problems with occlusions and collisions in multiple-tracking scenarios, which rarely are solved using these methods. For instance, a target lost while walking towards the camera cannot be recovered using these techniques if it reappears walking in other direction. Hence, an ensemble of multiple techniques are required to solve these complicated situations. In this work, we present a strategy for human tracking under unpredictable trajectories, able to solve many of the problems mentioned before. Based on a previous work [6], this strategy is based on the omega-shaped descriptor. Some improvements to the Viola–Jones detection are made to increase the speed. A particle filter system, in combination with a linear filter to predict the next position, is used to perform the tracking. The detection procedure using The Viola–Jones and the HOG feature-based SVM are also used in the tracking system to reduce the error produced by the particle filter along the successive frames. A hierarchical architecture is created to deal with the problems associated with multiple-target tracking scenarios. Since the particle filter is used to track every person in the scene, it is disabled when either a collision or an occlusion event is detected. An ellipse representation is used to define the areas in which the lost target could be. Once a new target appears in one of these areas, it is compared against the lost target using a color-histogram representation. The use of this technique is mainly focused in camera locations far from the scene to analyze. The algorithm speed is highly increased when, regardless of the position in which the target is, there are only few differences between the size of his head. As Fig.&amp;nbsp;  shows, two different modules are included in our methodology. First, a detection system tries to seek and obtain every person in the scene. Later, the tracking system matches every detection with the targets previously created, adding new observations if necessary. This paper is organized as follows: Sect.&amp;nbsp;2 describes the method used to detect every human being in the scene; Sect.&amp;nbsp;3 describes the techniques used to perform the tracking, in combination with the collision and occlusion solvers; Sect.&amp;nbsp;4 introduces some tests and evaluations of the framework, showing the obtained results; finally, Sect.&amp;nbsp;5 offers conclusions and future work.    2 Human detection  In the first step of the algorithm, the system is going to detect every person in the scene. Our initial approach is similar to the original idea exposed in [17]: a Viola–Jones detector is combined with a HOG featured-based SVM to obtain a good agreement between accuracy and speed. Since the HOG descriptor process is quite expensive in terms of speed, the idea is to relax this step by restricting the positions in which we have to compute it. So, a Haar-like method, the Viola–Jones type classifier, is used to detect head-shoulders omega-shape feature. This method can work very fast, but the classification performance is poor. So, for every detection, the HOG descriptor is computed and evaluated in the SVM. In this initial approach, human detection system is reduced only to the position in the image for which they know a person could enter into the camera range, to increase the system speed. As mentioned before, we are going to use this system both in detection and tracking procedures, so we create a different approach. In Fig.&amp;nbsp;, we show the steps to perform people detection. We consider that a target should be tracked if it moves along the scene in some moment. Static targets are not considered until they move. Once this happens, the algorithm begins to track the target, even if it stops again. First, a background subtraction technique is used to detect the moving objects. Later, a Viola–Jones type classifier is applied in the regions where we detect movement. Finally, the HOG feature-based histogram is responsible for evaluating the Viola–Jones positive detections. A quick method for detecting motion was selected. Although optical flow is the commonly used technique in this case, we are only interested in the positions where a movement occurs, without taking into account neither orientation nor magnitude of the movement. Hence, knowing optical flow has a higher computational cost, we propose to use a background subtraction technique. Our idea is to use an algorithm able to be updated every frame. So, we choose the Mixture of Gaussians (MoG) algorithm described in [ 29]. We introduce a short window history (over 100 frames), to quickly consider as background-stopped targets. Figure&amp;nbsp;  shows an example of the results of this technique. Once we have detected all the moving pixels in the scene, we select the regions in which the Viola–Jones type classifier is going to be used. So, we split the foreground into different boxes. First, we remove the noise into the foreground regions. To do that, both open and close morphological operators are used, along with a minimum-area filter. Later, we perform a blob detection technique, with a simple four-connectivity algorithm. Finally, a bounding box is obtained for each blob. Each bounding box marks the regions in which the Viola–Jones type classifier is used. Due to the nature of background subtraction techniques, it is difficult to locate all moving pixels in the image, specially those which are in the person contour. Having this situation in mind, the size of each bounding box is increased to cope with this issue. The margin is related with the expected head size at the contour of the box. Furthermore, no margin is considered at the bottom, because of the nature of human movement. No positive head-shoulder shapes can be observed at the bottom of a bounding box, unless it matches with the bottom of the scene. Furthermore, if the overlap between two or more bounding boxes is high, we merge them to avoid recalculations. Figure&amp;nbsp;  shows an example of this technique. After the bounding boxes are located, the Viola–Jones type classifier is executed. In many implementations, the process consists in the classification of Haar features within the image. Multiple window sizes are used along the image. The patches which do not pass all the cascade classifiers are excluded as obvious non-head-shoulder image patches. We can improve the performance of this technique knowing the estimated head size in every pixel in the scene. According to [ 13], the object height in an image   follows the equation       being   the 3D object size,   the camera height,   the position in the image we are considering and   the horizon point. Knowing the average person head size   [ 18]), we decide to set the 3D head-shoulder size to  . The other parameters,   and   can be estimated using ground truth positions and heights of two detections in the image [ 20]. Using this idea, we can improve the system in two different ways: increasing the speed and reducing the false positive errors, avoiding patches with wrong sizes. As mentioned before, we also introduce knowledge about the human movement. We assume that, when a head movement occurs, it is also followed by the torso. Having this idea in mind, we choose a small rectangle at the bottom of each patch detected by the Viola–Jones cascade classifier. Its height is related with the height of the patch. If the number of pixels marked as foreground in that rectangle is low (over 10 times the head size), we discard that patch. Figure&amp;nbsp; b shows an example. Some results can be discarded without taking into account the HOG feature. Also good patches are deleted because of the absence of movement in the target, causing his torso to be marked as background. An example of this issue can be viewed at the bottom right of the figure. This is a problem not taken into account, because it is assumed we can find it and track it in previous frames, when in movement. Once we have obtained the Viola–Jones positive patches, a HOG feature-based histogram is used into a SVM to confirm the detection. We perform a classic HOG technique. First, we divide the patch into cells. Adjacent cells form a block, and a normalized histogram within the block for each cell included in it. Each histogram includes eight different orientation bins from 0 to 360&amp;nbsp;. HOG feature extraction details can be seen in [9]. As we can see, the HOG feature-based histogram creation has a lot of redundant operations. Hence, Porikli et al. [ 19] introduced the integral histogram, which highly increases the speed of the algorithm. Combining both Viola–Jones cascade classifier and HOG feature-based histogram, we can obtain a good balance between speed and accuracy. Although we considered the idea of computing the integral histogram only in the bounding boxes explained in Sect.&amp;nbsp; 2.2, we decided to discard it because we also need the histogram to perform the tracking, as it will be explained later. The integral histogram is computed along the whole scene. Examples of accepted and rejected patches can be seen in Fig.&amp;nbsp; . The combination of the HOG descriptor along with the Viola–Jones type classifier often results in many patches related with the same target. Thus, when we add new targets to the tracking system, we have to penalize overlapping detections.    3 Tracking system  Once we have detected persons in the scene, we do not instantiate new targets directly. First, we perform the tracking system to avoid the persons detected which are already tracked. As previously depicted in Fig.&amp;nbsp;, three different steps are performed into the tracking system: first, a distance matching is used between the targets previously tracked and the new positions detected by the Viola–Jones in combination with the HOG feature-based histogram; second, a particle filter is launched for every target which has not been tracked using the previous step; finally, the remaining individuals in the new frame are considered to become new targets. Furthermore, two more steps are included to detect both collision and occlusion events, providing a target recovery identification system. First, we develop an algorithm to predict the position of each target along the following frames. Although the Kalman Filter is a common technique used in this context, we propose the use of a linear filter, since it is a more efficient technique and shows a good performance under noisy images. A bunch of adalines is used to predict the velocity of each target position component. Examples of the performance of this technique can be seen in [5]. Although every Viola–Jones patch could have both different position and size, we will only take into account the parameters related with their position, since, as seen in Sect.&amp;nbsp;2.3, once we have the target predicted position, we can determine its size using the Eq.&amp;nbsp;1. Thus, we can reduce the computational cost. To track each target into the scene, we propose the use of a particle system. However, the particle system can accumulate an error along the frames, which can often be very difficult to recover from it. In Fig.&amp;nbsp; d, we can see how purple target its losing the head-shoulder position, resulting in a bad performance. Hence, it is interesting to add another solution that can correct, as far as possible, that accumulated error. Our solution proposes the inclusion of the people detection technique within the tracking system. With every patch detected, we create a group with all the possible targets that could fit in the new detection. More formally, if we have a patch in the new frame defined by  , where   and   are the coordinates of the patch center, we compute the target factor as                 where   is a set containing all the tracking objects detected (and not occluded) at time   is the predicted position of the centroid of the target   and   is the euclidean distance between the center of the two patches. Setting an small value   we can obtain good results. Using this equation, if we obtain that  , we assign the new patch to the target contained into the set. On the other hand, if  \(|T_{\mathbf{p}^{t+1}_i}| &amp;gt; 1\) a collision occurs. Handling of these events is discussed in Sect.&amp;nbsp; 3.4 in more detail. Finally, if no targets are contained into the set,   is set as candidate to be a new target. Targets that are not associated with any patch detected by the Viola–Jones type classifier are tracked using the next step. In Fig.&amp;nbsp; , we can see an example of how this technique is useful. However, we have to be very careful with the   parameter. In Fig.&amp;nbsp; b, we have a new patch related to the person with the orange shirt. As it has not been tracked yet, using a high value of   could change the identification of the person behind him, which has been already tracked, leading to a wrong situation. For this reason, a lower   value must be used   in our experiments). As mentioned before, also using the person detection system for tracking guarantees a good quality in the head-shoulder detection. However, our detection system idea is focused in locating the persons in the scene avoiding, as much as possible, false positive patches. As a result, the false negative rate is also high, causing the algorithm not to find everyone in the scene at all frames. To avoid the loss of targets, we propose a particle-based system. As object representation, given a patch  , we use the extracted local HOG features, using it to model its appearance  . For distance measurement we use the Bhattacharyya coefficient, which is proved as a good method for tracking non-rigid objects [ 8]. This coefficient is used to determine the similarity between two different observations. So, given a new observation   and an object representation  , corresponding to the target   at time  , the similarity between the two vectors is       being   the vector dimension. Higher values indicate better similarities between the observations. As in the tracking using the people detection technique, we will use the predicted position of each target to perform the tracking. As mentioned before, we use a linear filter to predict the position of the target in the new frame. However, due to unpredictable target movements, the new target position often differs a few pixels from the prediction. So, having a target   previously tracked, we can assume the new target position to be described as       being   the predicted position of the target   at time   and   is a Gaussian noise. In our approach, we add the Gaussian noise to the predicted position, generating a bunch of different particles. For each particle, we extract its HOG feature in the patch defined by the new position and the size computed using Eq.&amp;nbsp; 1, and compute the Bhattacharyya coefficient between that particle and the target model. We choose as the new position   as the particle which obtain the highest coefficient. Finally, we have to change the target model. To adapt the model to the changes along the frame, the object must be updated. Also, because of the possibility of a bad chosen particle, the model should maintain information about previous features. Hence, having a target model   and a new model  , corresponding with the HOG feature of the winning particle, the target model is updated following       where   is the learning parameter. In Fig.&amp;nbsp; , we can see an example about the particle dispersion. Four different states are defined:  ,  ,   and  , as Fig.&amp;nbsp;  shows. First, when a new target is instantiated, its state is initialized as  . A target in this state means that a person is detected in the scene, but there is not enough evidences to confirm the detection yet. While a target remains in this state, each new position has to be confirmed using the SVM, even if the tracking is done by the particle filter system. If the SVM cannot confirm the detection, the target changes its state to   and it is erased. If the number of times the target is confirmed by the SVM reaches a threshold , it changes its state to . In this state, no SVM evaluation is needed using the particle filter system. So, a different way is needed to detect when the target is lost. Three different possibilities could cause the system to loss a target: particle filter system bad accuracy, collisions and occlusions. Both collisions and occlusions will be explained later. To deal with the particle filter bad accuracy, we decided to set two different thresholds,  and , and an occlusion counter . The Bhattacharyya coefficient explained in Eq.&amp;nbsp;3 is used to measure the tracking quality. Every time the coefficient becomes higher than , or when the target is detected using the person detection tracking, we set . On the contrary, if \(S(\mathbf{O}^{t+1}, \hat{\mathbf{O}}^t) &amp;lt; \gamma _S\), we increase the  counter. If, after successive results below , we have that \(\rho _S &amp;gt; \tau _S\), we change its state to . This indicates that we cannot be sure about the position of the target. However, despite the fact that the tracker have lost the person, we continue to update the target position using the particle filter technique. Although the target position is not known, the tracker position typically remains close to the person being tracked. So, if the human detection system detects a person near the tracker position, and there are no other targets nearby, the target recovers its state from  to .
Springer.tar//Springer//Springer\10.1007-s00138-013-0545-6.xml:Action recognition using 3D DAISY descriptor:Action recognition 3D DAISY descriptor:  1 Introduction  Human action recognition is a hot research topic in computer vision for decades. Applications such as video surveillance, video indexing, and human–computer interaction require action recognition. Many approaches have been proposed, yet it remains a challenging task because the actions can be performed by different subjects in different scenarios. Typical variations include scale, view point, brightness, appearance, and cloth. Based on the features used by the researchers, approaches for human action recognition [1] can be broadly divided into four categories: space–time shape [7, 11, 18, 36, 39], optical flow [9, 10], trajectory [2, 23, 32], and local spatio-temporal interest point [8, 16, 17, 20, 22, 25, 28, 30, 31]. Gorelick et al. [11] regard the actions as 3D shapes induced by the silhouettes in the space–time volume. To work on the volumetric space–time action shape, the authors extend the method to analyze 3D shapes. They exploit the solution to the Poisson equation for extracting space–time features for shape representation and classification. Yilmaz and Shah [39] analyze spatial-temporal volume using the differential geometric surface properties to compute action descriptors capturing both spatial and temporal information. They then perform action recognition using these descriptors. Deng et al. [7] sample interest points along the silhouettes and combine them to represent the space–time shape. Although space–time shape-based methods demonstrate promising performance in action recognition, they require reasonable silhouette extraction for each frame, which is a difficult problem in realistic videos. Efros et al. [9] introduce a motion descriptor based on optical flow measurements in a stabilized figure-centric volume. The descriptors are then used to find -nearest-neighbor in a database of preclassified actions. Fathi and Mori [10] develop a method to build mid-level motion features which are constructed from low-level optical flow features. More specifically, mid-level motion features are weighted combinations of thresholded low-level features. The low-level features are similar to the motion features proposed in [9]. Optical flow is an important cue in motion estimation. However, features based on optical flows are sensitive to noises and computationally expensive. In [2], the authors apply theory of chaotic system to characterize the non-linear dynamics of human actions. Trajectories of reference joints are used to represent the non-linear dynamical system generating the action. Messing et al. [23] propose a generative mixture model for activity recognition using features based on the velocity history of tracked keypoints. In [32], the authors present a hierarchical framework to encode three levels of context information: point-level context, intra-trajectory context, and inter-trajectory context. The trajectory-based methods are effective. However, they assume that trajectories are long, noise free, or tracking known body landmarks. Our method belongs to the fourth category. We use the framework of local spatio-temporal descriptors together with bag-of-words models. This framework does not require background subtraction and object tracking and has proved to be effective by recent works [8, 16, 20, 22, 25, 30, 31]. In [30], Schüldt et al. construct video representations in terms of local space–time features and integrate such representations with SVM classification schemes for recognition. Dollár et al. [8] develop a detector which applies linear 2D Gaussian kernel in the spatial domain and 1D Gabor filter in the temporal domain. The detected interest points are quantized into visual-words whose statistical distributions are used to represent the action video. The following are the general steps of their methods: first a detector is used to find interest points with local maximum. Then the local region around the interest point is extracted and depicted by some kind of feature descriptor. Finally, the descriptors are clustered into visual-words, and the video is represented as the frequency histogram over the visual-words. The benefit of this framework is robust to noises, camera movement, and partial occlusion. Successful extraction of good features and discriminative power of the local descriptor are crucial to action recognition. In this paper, we develop two simple and effective feature extraction methods based on inter-frame difference. Using a threshold to filter the noise, the inter-frame differences of two consecutive frames are pixel changes along the temporal dimension whose variation is an important cue to motion recognition. So we identify the inter-frame difference as ROI (region of interesting). We first compute a bounding box that tightly contains the ROI and then divide the box into several equivalent grids. We extract either a random point in the interesting region at each grid or the center point of each grid. We refer these two ways as random sampling and uniform sampling, respectively. For the representation of interest points, we propose a novel local spatio-temporal descriptor, 3D DAISY, which is an extension of the recent DAISY descriptor [33]. DAISY is a local descriptor designed for dense matching in images and has proved to be discriminative and computationally efficient. The motivation to generalize DAISY to describe the features in a video is that a video can be regarded as a series of images concatenating together along the temporal dimension. To deal with the additional temporal domain in a video, Scovanner et al. [31] extended the SIFT [21] to 3D SIFT , and Willems et al. [37] extended the SURF [3] descriptor to ESURF. Inspired by their success, we extend DAISY [33] into the space–time volume and apply it for action recognition. Benefiting from the local descriptor together with bag-of-words model, our method is robust to cluster, tolerant to global deformation of the body shapes, and can cope with certain camera motion and illumination changes. In the experiment, we apply our method on two widely used KTH [30] and WEIZMANN [11] action datasets and show the comparable results to the state of the art. The rest of the paper is organized as follows: section&amp;nbsp;2 introduces the local spatio-temporal descriptor that generalizes DAISY from 2D images to 3D videos. Section&amp;nbsp;3 introduces our feature extraction method. We show the experimental results in Sect.&amp;nbsp;4, and section&amp;nbsp;5 concludes this paper.    2 3D DAISY descriptors  Our 3D DAISY descriptor is inspired by a new local descriptor, DASIY [ 33], which is designed for dense matching. DAISY replaces weighted sums of gradient norms used by earlier descriptors such as SIFT [ 21] and GLOH [ 24] by convolutions of original image with several oriented derivatives of Gaussian filters. The convolution is defined as       where   is a Gaussian kernel,   is the orientation of the derivative, and  . The convolution results,  , are referred as convolved orientation maps. The oriented derivatives of the image   are referred as the orientation maps. The convolved orientation maps can be efficiently achieved by computing these convolutions recursively. Let   be the vector made of the values at location   in the convolved orientation maps:       where   denotes the  -convolved orientation map at direction  . These vectors are normalized as  . The full DAISY descriptor   for location   is then defined as a concatenation of   vectors, and can be written as            where   is the location with distance   from   in the direction given by   when the directions are quantized into   values. The 2D DAISY uses a circular grid instead of SIFT’s regular one [24]. To extend the 2D DAISY descriptor to deal with the additional temporal domain without losing its merits, we extend the circular grid to spherical voxels. As shown in Fig.&amp;nbsp; , we design our 3D DAISY descriptor as three concentric spheres with radius   (red),   (green), and   (blue). The interest point is the center of the spheres. Let the center of spheres be  , four planes  , and   intersect the 3 spheres, resulting in 12 circles. The circles are denoted by  , where   is the radius of the circle, and   is the angle of the circle plane with respect to the image plane  . The sequence of the 12 circles is  ,  ,  ,  ,  ,  . We evenly sample eight points on these circles. Without utilizing the redundant samples, a 3D DAISY descriptor has 79 samples. The reason why we choose four planes to construct the 3D DAISY descriptor is that in [ 38], the authors have analyzed the main parameters of 2D DAISY and concluded that by considering both computational complexity and storage cost, two rings would be a better choice for 2d images. We want to keep this setting in each plane, and thus four planes are selected as the default parameter. Motion cues are valuable for representing an action. Therefore, it is necessary to incorporate motion information into the feature vector,  , for each of the 79 samples. In our design, we use temporal gradients,  , to represent the motion cue because the gradient information proves to be powerfully discriminative [ 3,  21,  31,  33,  37] and meanwhile easy to compute. To compute a discriminative vector at location  , we have two alternatives. One way is similar to 2D DAISY. We compute the gradients along   orientations with specified Gaussian Kernels denoted by       where   denotes the  . In 2D DAISY descriptor, derivatives of eight orientations are computed. The absolute values of derivatives towards the two opposite directions at a point are equal. So   is used to remove the redundant derivatives. In our 3D DAISY, we use   because there is no redundance due to the orientation of the gradients are  ,  ,  . Another way is omitting the convolution step and using the orientation map   directly, The motivation for extending Eqs.&amp;nbsp;( 3)–( 5) is that if we simply use Eq.&amp;nbsp;( 4) to build our proposed descriptor, the step of convolution is likely to be oversmoothed since both the pixels in the spatial and temporal domains are contributing to the convolution. Thus we propose to use Eq.&amp;nbsp;( 5) to construct the descriptor. In the experiment, we compare those two different descriptors. The next steps of the two alternatives are similar. For the approach using convolved orientation map, the vector,  , made of values at circle   is            where   is a location in the circle   from   along the direction given by  .   denotes the rotation of the circle plane with respect to the image plane  , and   indicates the angle of the   sample point in a circle. Each plane has the same six samples in the   axis as marked by black larger dots in Fig.&amp;nbsp; b, c. So when  , the samples locate in the   axis,  ,  ,  ,  ,  , and  , should be omitted since these six points are already included in the plane  . For the alternative using oriented map, one just simply replaces   by            Different from 2D DAISY, we do not normalize   (or  ), because the measurements of gradients in spatial and temporal domains are different. The vector made at the center point   is   or  . We represent them by  . The 3D-DAISY is finally denoted by            In our implementation, we use the radii   and   and  . For convenience we refer the descriptors computed with convolved orientation map as 3D  and refer those computed using orientation map as 3D . The 3D  descriptor can be computed much faster than 3D  since the omitting of convolution step. In the experiment, both of them have been evaluated, and we will demonstrate the 3D  is not only much faster, but also more discriminative.    3 Feature extraction  Successful feature extraction is important to action recognition. Many works were proposed for interest point detection, such as widely used Harris3D detector [17], and Cuboids detector [8]. Harris3D is the spatio-temporal extension of Harris Corner Detector. Cuboid detector [8] is proposed by Dollár et al. The authors apply linear 2D Gaussian kernel in the spatial domain and 1D Gabor filter in the temporal domain. Interest points with local maxima of the response function are extracted. However, Harris3D and Cuboids detector are ineffective when encountering slow movement or smooth movement. In this situation, the detected features are likely to be too sparse resulting in trouble in a recognition framework, as observed by Lowe [21]. In this paper, we propose two simple and effective extraction methods based on inter-frame differences. The inter-frame difference is caused by the pixel change along the temporal dimension and is an important cue of motion information. First, we compute the inter-frame differences and use a threshold to filter the noise. We identify the region where inter-frame differences occur as ROI (region of interesting). A bounding box that tightly contains the ROI is computed. To uniformly extract features in ROI, we divide it into several grids that equal in sizes. After the grid division, we either extract a random point in the ROI in each gird, or extract the center point of each grid. The first option is referred as random sampling, while the second one is referred as uniform sampling. In our experiments, we use the configuration of   grids. Figure&amp;nbsp;  shows the examples of the two feature extraction methods. Different from the detectors such as Harris3D and Cuboids detectors, we do not compute a response function at each pixel, so our methods are simpler and faster. We evaluate the performance of our detectors and compare them with Harris3D and Cuboids detectors in Sect.&amp;nbsp;4.3.    4 Experiment  We carried out our experiments on four action datasets: KTH, WEIZMANN, Youtube, and UT-interaction. In Sects.&amp;nbsp;4.1 and 4.2 we introduce the datasets and classifiers, respectively. In Sect.&amp;nbsp;4.3 we demonstrate the discriminative power of the 3D DAISY descriptor and show the effectiveness of our detector. In Sect.&amp;nbsp;4.4, we compare our method with the state of the art. In Sect.&amp;nbsp;4.5, YouTube action data set [19] is used to demonstrate the robustness of our descriptor. In Sect.&amp;nbsp;4.6, we test our descriptor in multi-persons action on UT-interaction dataset [29]. In Sect.&amp;nbsp;4.7, our method is compared with 3D SIFT [31]. The  [30] contains six human action classes: boxing, hand clapping, hand waving, jogging, running, and walking. Each action class is performed by 25 persons in 4 different environments: outdoors, outdoors with scale variations, outdoors with different clothes, and indoors. The database contains 599 videos. All sequences were taken over homogeneous backgrounds by a static camera with 25fps rate. The spatial resolution of the videos are  pixels. Each sequence have a length of four seconds on average. For the KTH dataset, we use 16 subjects for training and the rest 9 subjects for testing. When constructing the vocabularies, we set . Following the setup in [35], we cluster a subset of  randomly selected training descriptors to limit the complexity and initialize the -means eight times to increase precision. We train a multi-class classifier and report the average accuracy over all classes. The  [11] contains 93 videos of 9 people performing 10 action classes: running, walking, jumping, bending, jumping-jacks, jumping in place, gallop sideways, one-hand waving, two-hands waving, and skipping. All the videos are captured in the same environment, whose spatial resolution is . The maximum length of these videos is 146 frames, the minimum is 28 frames, and average length of all the videos is about 61 frames. For the WEIZMANN dataset, we apply leave-one-out cross-validation. In each iteration one subject is used for testing and the rest eight subjects are used for training. When constructing the vocabularies, we set , where  is the number of training samples. Similar to the KTH dataset, we train a multi-class classifier and report the average accuracy over all classes. The   [ 19] has the following properties: (1) a mix of steady and shaky cameras, (2) cluttered background, (3) variations in object scales, (4) varied view point, (5) varied illumination, and (6) low resolution. It contains   categories: basketball ( ), volleyball spiking ( ), trampoline jumping ( ), soccer juggling ( ), horse-back riding ( ), cycling ( ), diving, swinging, golf swinging ( ), tennis swinging ( ), and walking with a dog ( ). Figure&amp;nbsp;  shows some examples of this action dataset. Leave-one-out cross-validation is employed and in each iteration one subject is used for testing and the rest subjects are used for training. Similar to the KTH dataset, we train a multi-class classifier and report the average accuracy over all classes. The   [ 29] contains   classes of human–human interactions:  ,  ,  ,  ,  , and  . Several participants with more than   different clothing conditions appear in the videos. This dataset has two sets: the set   is composed of ten video sequences taken on a parking lot. The videos of the set   are taken with slightly different zoom rates, and their backgrounds are mostly static with little camera jitter. The set   (the other   sequences) are taken on a lawn in a windy day. Background is moving slightly (e.g. tree moves), and they contain more camera jitters. Each set has a different background, scale, and illumination. Some examples in this dataset are shown in Fig.&amp;nbsp; . In this dataset, we use tenfold leave-one-out cross validation: for each round, we leave one among ten sequences for the testing and use the other nine for training. Similar to the KTH dataset, we train a multi-class classifier and report the average accuracy over all classes. For classification, we use SVM [ 5] with   kernel [ 41],       where   and   denote histograms,   is   distance between   and   defined in Eq.&amp;nbsp;( 6), and   is average distance [ 41]. In this section, we demonstrate the recognition results using 3D DAISY as local spatial-temporal descriptors. As mentioned in Sect.&amp;nbsp;2.2, we compare the result under the 3D  computed using convolved oriented map, and 3D  computed using oriented map. We know that the location of a pixel in a video sequence must be integer, but the locations of 79 sample points of a 3D DAISY descriptor may have decimal parts. When encountering the location with fraction decimal parts, we either use truncation which rounds a number to the nearest integer less than or equal to it, or use trilinear interpolation to compute vector made at this location. In the experiment, we use the two detectors proposed in this paper, and compare them with the widely used Harri3D detector [17], and Cuboids detector [8]. We extract 8 interest points per frame for each of our detectors, while extracting 200 points per video using Harris3D detector or Cuboids detector. The accuracy of the recognition on WEIZMANN and KTH dataset are illustrated in Tables&amp;nbsp; 1 and  2. Statistical analysis can be computed using the data in the two Tables. We first compare the two descriptors, 3D   and 3D  . In KTH dataset, the 3D   descriptor improves the accuracy   on average compared with the 3D   descriptor. In WEIZMANN, the average improvement is up to  . The main reason might be that the videos are over smoothed after the convolution with both spatial and temporal pixels, especially when the video is in low resolution such as WEIZMANN and KTH action datasets. Next we compare the performance of the four detectors. In KTH dataset, uniform random sampling gets the best accuracy on average  , which is 9.77 and 1.26&amp;nbsp;% higher than Harris3D detector and Cuboids detector, respectively. In WEIZMANN dataset, uniform random sampling also gets the best average accuracy, and its improvement compared with Harris3D detector and Cuboids detector is 15.05 and 1.35&amp;nbsp;%, respectively. Finally, we compare results under truncation and interpolation. On average, the difference of their performance is very small. Figure&amp;nbsp;  gives intuitive comparison between the 3D   and 3D   descriptors, truncation and interpolation, and the four detectors. Since the time spent on computing gradient is much less than computing convolution, the overall computational time computing Eq.&amp;nbsp;( 1) is mainly spent on the convolution. We conclude that 3D   is not only more discriminative, but also much more faster than 3D  . The best recognition result of our approach in KTH dataset is 94.05&amp;nbsp;%, and in WEIZMANN is 92.47&amp;nbsp;%. The measurement using confusion matrices are displayed in Fig.&amp;nbsp; . The KTH, WEIZMANN are popular action datasets available in the internet. A lot of papers have evaluated their methods on the two datasets. As shown in Table&amp;nbsp; 3, our recognition rate, 94.05&amp;nbsp;% on KTH database, is very close to one of the best reported results, 94.53&amp;nbsp;% [ 15]. Meanwhile, promising result, 92.47&amp;nbsp;%, is also obtained on the WEIZMANN database. Our recognition result demonstrates the discriminative power of 3D DAISY descriptor under the standard bag-of-the-words framework. Note that not all the methods listed in Table&amp;nbsp; 3 are based on standard bag-of-words framework. For example, Liu et al. [ 20] and Kovashka et al. [ 15] learn a richer vocabulary for bag-of-words based action recognition. For another example, Fathi [ 10] use features based on optical flows in a different evaluation framework from our bag-of-words. Since KTH and WEIZERMANN datasets are relatively simple, to further evaluate the robustness of our proposed descriptor, we try the proposed 3D DAISY descriptor on the complex and challenging YouTube action dataset. We extract 200 feature points from each video and randomly select 100,000 feature points from all the videos to construct a 4,000-center dictionary. Then, the SVM with chi-square kernel is used to classify the actions. Finally, a leave-one-out strategy is used to select the training set and testing set. Figure&amp;nbsp;  shows the recognition accuracy of each action. To make a fair comparison, we only compare with the original static and motion features which are proposed in [ 19]. In [ 19], they use cuboids [ 16] as their motion descriptor and SIFT [ 21] as their static feature descriptor. Our recognition accuracy is   which is slightly better than   using motion feature descriptor and   using static feature descriptors as reported in [ 19]. Figure&amp;nbsp;  shows the confusion table for action classification using our framework. Actions  ,   and   have relatively lower accuracy. After careful analysis, we found that the action   is easy to be wrongly labeled as a very similar action  . The better performance by Liu et al. [ 19] is partially benefited from the facts that the differences of those two objects, i.e. bike and horse are quite distinct from the static feature description, 2D SIFT.
Springer.tar//Springer//Springer\10.1007-s00138-013-0546-5.xml:A nonlocal energy minimization approach to brain image segmentation with simultaneous bias field estimation and denoising:Membership functions Energy functional Bias field Nonlocal regularization Image segmentation:  1 Introduction  Image segmentation plays an important role in medical image analysis. Accurate classification of brain images into three main cerebral tissues (namely, grey matter (GM), white matter (WM) and cerebrospinal fluid (CSF)) will facilitate computer aided disease diagnosis structurally, such as Alzheimer’s disease (AD) [1] and Parkinson’s disease (PD) [2]. When dealing with magnetic resonance imaging (MRI) brain data, bias field, rich structures and heavy noise make the segmentation a difficult problem. Although many methods have been proposed to solve one or two aspects of the problem, there is little work to take all aspects into account. Bias field, also known as intensity inhomogeneity, usually manifests itself as a smooth spatially varying function. It alters image intensities to be not constant for the same tissue. Although it is hardly noticeable to human observers, most medical image analysis methods are sensitive to the spurious intensity variations. During last two decades, numerous approaches have been proposed to correct the intensity inhomogeneity. They are mainly divided into two groups. The first one is parametric algorithms [3–7]. In those methods, the bias field is constructed as various smooth functions that naturally embody its smooth spatial effect. For example, it is modelled as a linear combination of polynomial [3], B-spline [4], orthogonal polynomial [5], or Legendre polynomial [6] and so on. The bias field estimating process can be achieved by optimizing those parameters used for representing the bias field, while the number of parameters is much smaller than the dimensionality of the image space. In this case, the parameters are usually estimated by using a maximum likelihood (ML) [7, 8] or maximum a posteriori (MAP) [9, 10] approach. The expectation maximization (EM) algorithm is normally used for the optimization process. The second one is nonparametric method [11–16, 20]. This kind of method does not require any prior knowledge on the intensity probability distribution. The bias field is usually integrated into an energy functional, and ML or MAP can also be used here. By minimizing the objective functional, the estimation of the bias field can be obtained automatically. More about intensity inhomogeneity correction, one can refer to [21] and the references therein. In the following, we will review some related works that motivate our idea of this paper. Recently, Li et al. [16] proposed a coherent local intensity clustering (CLIC) criterion for segmenting images with non-uniform intensity. An energy minimization method was given to simultaneously estimate the reflectance image and the illumination. In their work, the illumination can be regarded as the bias field and the reflectance image is assumed to be piecewise constant. By observing that the variation of the bias field in a local neighborhood of a pixel is small, they approximated the bias field in the neighborhood by the value at the center point. Thus, they defined a local data term with a kernel function for clustering the image. In order to reduce the influence of the noise, they added a membership regularization term by the total variation. By minimizing their functional, the nonparametric bias field is confirmed to be smooth as a result of the convolution with a symmetric smooth kernel function. Their method has been verified to be able to segment images with non-uniform intensity caused by spatial variations in illumination, and also work for brain MRI. Later, some extended works have been done based on this idea such as [17–19]. While with total variation regularization of the membership function, it does not work well in texture-rich regions which are often seen in brain images. Brain structures play a very important role in disease analysis and diagnosis, so it is necessary to preserve these structures during the segmentation. Recently, nonlocal (NL) methods [22–24, 26] have been paid more attention for their superiority in dealing with textures in image processing. They can dig out the similarity of structures in an image, which is used for conserving image structures while denoising and clustering. In [27], Caldairou et al. proposed to integrate the nonlocal regularization into the FCM segmentation methodology. Their experiments showed that it is good at preserving brain structures. But it does not estimate the bias field due to the construction of their data term, which may result in that it is not suitable for images with heavy noise. Although FCM-based methods have been extended by incorporating various information such as multiple channels [28], spatial regularization [29], bias field approximation [21] and so on, they still do a weak job in the presence of bias field and heavy noise as pointed out in [24], since they only consider the spatial correction. The pre-denoising method may be a good remedy for the heavy noise. However, how much we should smooth the image will be not easy to determine. Moreover, some denoising methods may lead to inaccurate segmentations. Our idea is different from the idea of denoising preprocessing. We will integrate the denoising into our variational framework such that it reduces the influence of the noise and hence improves the segmentation accuracy. To avoid the loss of image structures, the nonlocal regularization will be a good choice. The nonlocal operator is firstly proposed by Buades et al. [25] to denoise based on a nonlocal averaging of all pixels in the image. Also in [26], Gilboa and Osher proposed a nonlocal ROF model to denoise, which keeps structures better comparing to original ROF model since it makes the structural correction by the nonlocal weight operator. Notice this, we will add a nonlocal regularization to the image intensity for reducing the influence of the noise while partitioning the image. In this paper, by incorporating the nonlocal regularization mechanism in the CLIC formulation, we propose a nonlocal energy minimization method for simultaneous image segmentation, bias field estimation and noise reduction with structures-preserved. It combines the advantages of the local clustering criterion [15], nonlocal membership functional regularization [24] and nonlocal way to denoise [26]. We do segmentation, bias field estimation and noise reduction alternatively to enhance one another. Our proposed functional composes of four terms: a local data fitting term resulting from spatially smoothing weighted local-region fuzzy energy, two nonlocal regularization terms to enforce the smoothness and maintain the structures of final label image, and a fidelity term to ensure the final clean image close to the original one. As a result, our proposed method outperforms or does equal results comparing to some other methods. The rest of this paper is organized as follows: In Sect.&amp;nbsp;2, we present the segmentation problem for intensity inhomogeneous images, and then give a brief review of CLIC based model as well as nonlocal denoising and segmentation methods after an introduction of nonlocal operators. Section&amp;nbsp;3 details the proposed method for brain MRI segmentation. Minimization of the proposed energy functional is presented in Sect.&amp;nbsp;4. In Sect.&amp;nbsp;5, the experimental results on synthetic and clinical brain images and the comparisons to other methods are shown to demonstrate the advantages of our method. And in last section, we will discuss the contribution of this work and present the future directions for development.    2 Background  Based on the theory of the lightness perception, it is common to model an observed MR image with intensity inhomogeneity as a multiplicative component:       where   are the observed noisy image, bias field, approximated piecewise constant (PC) image and noise located at  , respectively. Also we denote   as the total pixels in  . The goal of image segmentation is to partition the image domain   into disjointed regions  . For brain images segmentation, the basic assumptions of ( 1) are stated as follows:          (A1).         for  for  with  for  and          (A2).       The bias field  is smooth;        (A3).         is additive zero-means Gaussian noise. for  for  with  for  and The bias field  is smooth; is additive zero-means Gaussian noise. As a result,   Here, each   is a membership function of the corresponding region   such that            and  . So the estimation of the PC image   from the observed image   can be done by seeking the membership functions   associated with regions   and the constants  . And image segmentation results can be obtained by the final membership functions. For convenience, we denote respectively and . To be able to easily solve variational problems, it is required to relax the non-convex constrain of  (2) to the convex constrain  as usual. Thus, each  can be explained as the possibility of  belonging to the cluster . By assuming the local clustering criterion that   for   and   for  , where   and   is the radius of the neighborhood, Li et al. [ 15] proposed to minimize the energy function based on the CLIC criterion as follow       where       and       Here   is an exact convex penalty function that forces   lies in  ,   denotes the fuzzy degree of the membership function  , and   is a weighting function, which decreases when the distance from the reference point   to the central point   increases. It is often chosen as a truncated Gaussian kernel       where   is a normalized constant such that       and   controls the radial scope of the function. The authors introduced such weight to control its influence on the clustering criterion function: the further the distance is, the less the contribution is. In (3), the first term is a standard local intensity clustering criterion [5, 15–18, 31], which implies local intensities consistency, and it can be applied for local intensity classification and bias correction [16]. This clustering function characterizes how well  gives a classification of each intensity within its neighborhood into  clusters or classes with the corresponding centroid. Smaller value of clustering function  means better classification. The second term is used for the regularization of membership functions considering the inherent smoothness of . The authors used an exact convex penalty function and Lagrange multiplier method to turn a constrained problem into an unconstrained one. This model, denoted as  model for convenience, does a good segmentation in the presence of bias field and noise. Yet with total variation regularization of membership functions, it does not work well enough in texture-rich regions, which are often seen in brain images. In this section, we introduce some definitions and notations of nonlocal operators. Many of them are borrowed from Gilboa-Osher in [26] and Zhou-Scholkopf in [32]. Let   and   be a real function. Assume   is a nonnegative symmetric weight function, i.e.,  , for example, it can be chosen as       where   is the distance between image patches located at   and  ,   is a Gaussian function with standard deviation   and   is a positive constant which acts as a scale parameter. Then the NL gradient of an image   is defined as       The standard   inner product between two   vectors   at point   is defined by:            which gives the norm of a NL vector   at point   as follows:       Hence, the norm of the NL gradient of a function   at point   is given by:       The NL divergence operator can be defined by the standard adjoint relation with the NL gradient as follows:            which defines the NL divergence of the NL vector   at point   as follows:       And the NL Laplacian operator can be defined by            Note that in order to get the standard Laplacian definition which relates to the graph Laplacian, we need a factor of 1/2. In the following sections, we will detail the updates of variables using the discrete forms of the NL gradient, divergence and Laplacian operators, which can be respectively represented as:                                      where   denotes the intensity at pixel   in the image ( );   is a weight between pixels   and  , which is the discrete version of  . In [ 26], Gilboa and Osher proposed to minimize the following   to denoise:       where       and       The main advantage of this nonlocal form for image processing is its ability to handle both structures (geometric parts) and textures within the same framework. It keeps better structures than the original ROF model [ 30] for image denoising. In [ 27], Caldairou et al. proposed to minimize the following nonlocal weighted energy function       with            and       where   is the nonlocal weight,   is the neighbor of pixel   with radius  ,   and   is the centroid of the  th component.   is used to perform a N-classes segmentation with spatially variant local centroid { } for any  . The nonlocal regularization   is taken to smooth the current segmentation map in an adaptive and flexible manner. If the neighborhoods of two pixels   and   are similar, then the weight   is large, and thus it is more probable that these pixels belong to the same cluster. By computing the weight matrix for all  , and then updating   and   alternatively, they got pleasant segmentation results for brain images.    3 Our proposed model  In order to deal with noisy brain MR images with rich structures, we propose a nonlocal energy minimization approach defined as follows:            where   is the restored image,   are constants that balance the effects of local energy term, regularization terms and fidelity term, and   is supposed to be a real number greater than 1. The local energy term   takes the advantage of coherent local clustering criterion, which is used for better clustering and bias field estimation as that in [ 16]. The nonlocal regularization terms are used for better keeping the structures of the classification and the image itself since the nonlocal weight uses the similarity between image patches, while TV norm only uses the information of neighboring pixels, resulting in a poor structure preservation. We also update the nonlocal weight   with updated   in order to get more reliable relationship or exact relationship between blocks. It must be noted that the weight   used in two regularization terms may be distinct, because they are used for different purposes. While in this paper, for the sake of simplicity, we consider for the same pair  , both weights   have the same value. So image segmentation with well-preserved structures, denoising and the estimation of the bias field can be achieved simultaneously by solving the following constrained nonlocal energy minimization problem:       where   is the space of all the membership functions, i.e.,    4 Minimization of \(\mathcal {J}_\mathrm{NL}(U,B,C,I)\)  We have defined a constrained nonlocal energy minimization problem in Eq.(26). In this section, we will give an alternating method to solve this problem in detail. Firstly, for fixed I, the minimization subproblem with respect to   is            The energy functional with respect to the variable   or   or   is convex given the other two variables fixed. So we can alternatively update these variables. Secondly, for fixed  , the energy functional with respect to   is also convex, i.e.,            Consequently, we can alternatively solve these subproblems ( 28) and ( 29) until convergence. To minimize the subproblem ( 28), we need the following four steps:        Calculate the nonlocal weight  with .         &amp;nbsp;                 Given        and       , there is a unique minimizer of Eq. (      28) with respect to       , which is in the form denoted by       . By the Lagrange multiplier method, the constrained minimization problem (      28) can be transformed to an unconstrained one                             Then the saddle point of this problem can be obtained by the following equations:                                            and                             Let                                            we have        and then combining the condition of       , we get                             Particularly the discrete form of        is given by                                            for       .              &amp;nbsp;                 Given       ,        and       , the optimal        that minimizes the energy in (      28) can be equally obtained by minimizing       . Denoted by        be the minimizer, it can be gotten by                             where        denotes the convolution operator.              &amp;nbsp;                 Given       ,        and       , the optimal       , denoted by       , which also can be gotten by minimizing       , can be updated as                             where        and       . Note that convolutions with a kernel        in the expression of        intrinsically confirm the smoothness of the bias field. Therefore, we get a smooth bias field without any additional smooth procedure as in [      15,       16].              &amp;nbsp; Calculate the nonlocal weight  with . Given   and  , there is a unique minimizer of Eq. ( 28) with respect to  , which is in the form denoted by  . By the Lagrange multiplier method, the constrained minimization problem ( 28) can be transformed to an unconstrained one         Then the saddle point of this problem can be obtained by the following equations:              and         Let              we have   and then combining the condition of  , we get         Particularly the discrete form of   is given by              for  . Given  ,   and  , the optimal   that minimizes the energy in ( 28) can be equally obtained by minimizing  . Denoted by   be the minimizer, it can be gotten by         where   denotes the convolution operator. Given  ,   and  , the optimal  , denoted by  , which also can be gotten by minimizing  , can be updated as         where   and  . Note that convolutions with a kernel   in the expression of   intrinsically confirm the smoothness of the bias field. Therefore, we get a smooth bias field without any additional smooth procedure as in [ 15,  16].
Springer.tar//Springer//Springer\10.1007-s00138-013-0547-4.xml:Image forgery detection using steerable pyramid transform and local binary pattern:Steerable pyramid transform Image splicing Local binary pattern Image forgery detection:  1 Introduction  The application of digital imaging has been increased in recent years due to the availability of low-cost and high resolution digital cameras, and user friendly digital imaging software. This has considerable benefits in areas like electronic and print media, world wide web, security and surveillance, and insurance industry, to name a few. However, although user friendly image editing tools make our life easier, they also raise serious authentication issues due to their mishandling by some people. It has never been easier to manipulate images in order to gain illegal advantages or to make false propaganda using forged images. Therefore, correctly detecting image forgeries is of growing interest in the research community. Most image forgeries are performed using a copy-move procedure. In this procedure, some part in an image is copied and pasted to either another part of the same image or a different image. If the copy-move procedure involves the same image, then it is called copy-move forgery; if it involves more than one images, it is referred to as splicing. Before pasting the copied part, it can geometrically be transformed by rotation, scaling, etc. to fit in the location being moved to. To hide the traces of forgery, different types of post- processing are applied to the forged region such as blurring, adding noise, etc. Research in image forgery detection mainly started early in 2000 [1]. Many techniques have been reported since then to detect copy-move forgery or splicing. These techniques can be divided mainly in two categories: active and passive (or blind) [4]. Active techniques, assume that the image contains a watermark; in this case, forgery is detected if the extracted watermark does not match the original watermark. However, this technique has limited applications because in most the cases we do not have information about the watermark, if it exists at all. This limitation has led to the development of blind techniques that do not depend on an explicit watermark. The output of forgery detection could be of two types: (i) classifying an image as authentic or forged (no localization), and (ii) localizing the forged region, if the image is not authentic. In this paper, we propose a blind technique with the goal of classifying a test image as authentic or forged. Most forgery detection methods are block-based [1, 2]. Fridrich et al. [1] and Huang et al. [2] used the discrete cosine transform (DCT) coefficients to create a feature vector from the blocks of an image and applied block matching. Multi-resolution techniques, such as the discrete wavelet transform (DWT), has been utilized in several other methods [3, 4]. An alternative to block matching is using SIFT features for matching [5]. In image splicing detection, most works have been evaluated using the Columbia dataset [6] and the CASIA dataset [7]. Ng et al. [8] used higher order moments of the image spectrum to detect image splicing, while geometric invariants and the camera response function were used in [9]. Shi et al. [10] proposed statistical features based on 1D and 2D moments, and transition probability features based on Markov chain in the DCT domain for image splicing detection. In CASIA v2.0 database, their method has achieved 84.86&amp;nbsp;% accuracy. Later, He et al. [11] improved the method by combining transition probability features in the DCT and DWT domains. For classification, they used support vector machine (SVM) with recursive feature elimination (RFE). Their method achieved 89.76&amp;nbsp;% accuracy on the CASIA v2.0 database. The transition probability features extracted from the Cb chrominance channel of an edge-thresholded image were proposed in [12]. Using the CASIA v2.0 database, the method achieved 95.6&amp;nbsp;% accuracy, however, they did not use the full database. The same method achieved 89.23&amp;nbsp;% accuracy using the Columbia database. In a recent method [13], a chroma-like channel was designed for image splicing detection, improving the performance of [12] to 93.14&amp;nbsp;%. All of the methods reviewed above, except [11], do not fully utilize the scale and orientation information in an image. Even in [11], where DWT coefficients were used, DWT filtering was only performed in the horizontal and vertical directions. In this paper, an image forgery detection method based on SPT and LBP is proposed. SPT involves filtering at different scales and orientations. In our method, SPT is being applied on the chrominance components of a color image and LBP features are extracted from the resulted subbands. The LBP histograms of all the subbands are concatenated to form a feature vector which is then given to an SVM for classification. Our objective is to determine whether a given image is forged or not rather than localizing the forgery in the image. The rest of this paper is organized as follows: In Sect.&amp;nbsp;2, the proposed method is described. Section&amp;nbsp;3 presents and discusses our experimental results. Finally, Sect.&amp;nbsp;4 contains our conclusions and directions for future work.    2 Proposed method  Figure&amp;nbsp;  shows a block diagram of the proposed method for image forgery detection. First, given a color image, we convert it into the YCbCr chrominance space, where Y is the luminance, and Cb and Cr are the chrominance components. Cb and Cr are the blue difference and the red difference, respectively. The luminance channel is more sensitive to the human eye than the chrominance channels; however, it has been shown that the chrominance channels are more suitable for forgery detection [ 12,  13]. In general, the content of an image is too strong to hide tampering clues. The luminance channel mostly describes the image content, while the chrominance channels emphasize the weak signal content of the image. The edge irregularity caused by image tampering can be noticed in chrominance channels. Therefore, we concentrate only on the Cb and Cr channels. The usage of red, green, and blue channels could be another option, but it does not consider the relationship between the channels. Also, there are many other color spaces such as HSV, XYZ,  * * *, CMY, etc. that could be utilized, however, the emphasis of this current work is to extract appropriate features (steerable filters and LBP) for image forgery detection. In the second step, SPT is applied to each chrominance component; the output is a number of subbands that are translation and rotation invariant. In the third step, we perform feature extraction by applying LBP on each SPT subband. LBP is a powerful local texture descriptor, which has shown impressive performance in the literature [ 16]. LBP histograms from different subbands are concatenated to form a feature vector. An optional feature selection step is applied next to reduce the number of features, as well as to enhance the performance of the system. In the final step, SVM is utilized to determine if the input image is forged or not. SPT is a powerful linear multi-scale, multi-orientation image decomposition technique. It was developed to overcome the limitations of orthogonal separable wavelet decompositions. The steerable pyramid analysis functions are dilated and rotated versions of a single directional wavelet [ 14,  15]. SPT is used here because the trace of the forgery can be found in different scales or orientations of an image. In the zero scale ( ) of SPT, a lowpass filter   and a highpass filter   are applied to the image. In the first scale ( ),   is decomposed into  -oriented   subbands using directional operators   and a lowpass subband ( ). In the second scale ( ),   (sub-sampled by a factor of 2) is again divided into  -oriented subbands and a lowpass subband ( ). This procedure continues recursively until a required number of scales have been obtained. In order to avoid aliasing during sub-sampling, the constraint on the   filter is as follows:            To avoid amplitude distortion, the transfer function (frequency response) of the system should be unity as follows:       The relationship between two successive low pass filters in terms of frequency is as follows:       Figure&amp;nbsp;  illustrates the transfer functions of the filters  ,  , four oriented (  = 4) bandpass filters ( ,  ,  ,  ), and  . Here, a 3-scale, 4-orientation SPT is used to represent an image by a total of 12 subbands with a wide range of scale and orientation variety. In addition, we use the residual lowest subband as well as the residual highest subband (i.e., total number of subbands used is 14). Figure&amp;nbsp;  shows the block diagram of the SPT system. The motivation of using translation and rotation invariant SPT in the proposed method is that the SPT is a multi-resolution technique, where the given image is decomposed into subbands having different resolutions (scales) and frequencies, and various orientations. The trace of forgery, which cannot be noticed in the test image, can be found in these subbands. Though another multi-resolution technique, DWT, is used before in image forgery detection, there is no orientation filtering involved in DWT (it has only horizontal and vertical filtering). After decomposing the chrominance components of an image into several SPT subbands, LBP is applied on each subband to extract a set of features. LBP is a texture descriptor that labels each pixel in the image by thresholding the neighborhood pixels with the center pixel and considering the result as a binary number (see Fig.&amp;nbsp; ). Then, the texture can be described by the histogram of these label values [ 16]. This is called the basic LBP operator, where it is calculated in a rectangular window. LBP can also be extracted in a circular neighborhood ( ,  ), where   is the number of neighbors and   is the radius of the neighborhood. In this work, we have experimented both with the basic and circular LBP using   = 8 and   = 1. The normalized LBP histogram is used as a feature vector for the corresponding subband. The histogram has 256 bins corresponding to 256 gray values. The LBP histograms from all the subbands are concatenated to produce the final feature vector. The LBP is used in the proposed method for two reasons. First, the number of coefficients from all the subbands of the SPT is prohibitively large to feed into a classifier. For example, if an image size is , then the SPT illustrated in Fig.&amp;nbsp; will produce  coefficients at  subband, four  coefficients at scale 1, four  coefficients at scale 2, four  coefficients at scale 3, and  coefficients at the lowest frequency subband. Second, we are interested in texture, because when forging an image, the original texture is distorted. Therefore, the texture pattern can be a good indicator of forgery detection. The LBP (which is a good texture descriptor) histogram can encode texture differences at different scales and orientations of the steerable pyramid transformed image. Feature selection is very important because of the high dimensionality and complicated distribution of data. The length of the feature vector in our case is more than 3,500 while using all subbands of the SPT and the basic LBP. A significant number of the features may not be important, and thereby can be called irrelevant, for classification. Eliminating such irrelevant features can reduce system complexity, and data analysis and processing time. Many feature selection methods can be used such as Fisher, local learning based (LLB), RFE, zero norm minimization and others. In the fourth step of the proposed method, a combination of two different data reduction techniques (i.e., -norm [17] and LLB [18]), is used for feature selection. -norm ranks the features based on class separability constraints, while LLB removes features that contain redundant information. In our implementation, -norm is applied first, followed by the LLB. The threshold for LLB was set to . It should be mentioned that besides -norm and LLB, we investigated other feature selection algorithms such as the Fisher discrimination ratio and -norm. However, we found the performances of -norm and LLB to be better than the other two algorithms. We have also found that their combination works better than when applied individually. Therefore, we adopted combining -norm with LLB in the proposed method. In the final step of the proposed method, we apply SVM classification with an radial basis function (RBF) kernel to classify an image as authentic or forged. SVM is widely used for data classification and it is known for its high prediction capabilities in many applications, especially for binary classes. RBF kernel was chosen for SVM as it is more general than other kernels (especially linear one) and usually it produces better accuracy and has less restriction than other kernels.    3 Experiments  In our experiments, we used the following three databases: CASIA v1.0 [7], CASIA v2.0 [7], and Columbia color DVMM [6]. First, we performed an extensive set of experiments to evaluate the proposed method using the CASIA v1.0 database. Then, we performed additional experiments using the CASIA v2.0 and Columbia color databases. The CASIA v1.0 dataset contains 800 authentic and 921 forged color images of which 459 are copy-move forged, and the remaining are spliced. Different geometric transforms, such as scaling and rotation, have been applied on the forged images. All the images have a size of  pixels, and they are in JPEG format. The CASIA v2.0 database is an extension of the CASIA v1.0 database. In particular, it consists of 7,491 authentic and 5,123 forged color images in JPEG, BMP, and TIFF format, where image sizes vary from  to  pixels. The Columbia color image database consists of 183 authentic and 180 spliced images in TIFF format. The image size is , and no post-processing was applied to the forged image. All the forged images are spliced images. Image features were extracted using the chrominance channels, SPT and LBP. The LIBSVM toolbox [19] was used for classification. The optimal parameter values of the SVM ( and ) were automatically set by an intensive grid search process using the training set after applying feature selection. The performance of the proposed method is given in terms of accuracy, sensitivity, specificity, and area under curve (AUC) averaged over a tenfold cross-validation. In tenfold cross-validation, the authentic images set and the forged images set are randomly divided into ten equal groups each. In each iteration, nine groups each from the authentic images and the forged images are used for training, while the remaining are for testing. Therefore, at the end of ten iterations, all the ten groups are tested. There is no overlapping between the training set and the testing set in one iteration. Feature selection and the optimization of the SVM parameters are done with the training set. The final accuracy is obtained by averaging ten accuracies of the folds. To assess the performance of the proposed method, we performed a large number of experiments by considering many combinations of different image representations (grayscale, Y, Cr and Cb), SPT subbands and LBP parameters. In the case of LBP, the basic LBP and the LBP with ‘u2’ parameter (maximum two bits transitions) performed the best; however, the results reported here are with the basic LBP. In this section, we investigate the effect of different SPT subbands on image forgery detection. We use CASIA v1.0 database for this investigation. The following (a)–(f) results are without feature selection.         (    )           In this experiment, we studied the performance of three different subbands, at different scales, but having the same orientation (first orientation,  ). The feature vector length in this case was 256 (i.e., the number of bins in the LBP histogram). Figure&amp;nbsp; a shows the performance of the proposed method assuming different channels. In all channels, the first scale ( ) has the highest accuracy. The accuracy of the first scale in the Cr channel is 87.6&amp;nbsp;%. The accuracy of the same channel decreases to 78.6&amp;nbsp;% in the second scale, followed by 66.8&amp;nbsp;% in the third scale. The same pattern occurs in the other channels. The performances of Cr and Cb are comparable, and they are better than the performance in Y and gray.         (    )           In this experiment, we investigated the performance of four different subbands having four different orientations ( ,  ,  , and  ), but at the same scale ( ). Figure&amp;nbsp; b shows the results; in both chrominance channels, the first orientation has the highest accuracy, which is 87.6&amp;nbsp;% for Cr and 85.8&amp;nbsp;% for Cb. However, there is no clear trend observed in the performance of different orientations.         (    )           In this experiment, we studied the effect of each SPT scale on the performance of the proposed method. The histograms of the four oriented subbands within each scale are concatenated to form a feature vector of length  . Figure&amp;nbsp; c shows the accuracy of the three scales in different channels. The best accuracy, which is 90.4&amp;nbsp;%, was obtained by the   scale in Cr channel. It is noteworthy that using the Cr channel, the same scale with a single orientation ( ) has an accuracy of 87.6&amp;nbsp;%. Based on this observation, we conclude that all four orientation subbands are necessary for improving accuracy.         (    )           In this experiment, the histograms of the three scales at each orientation were concatenated to form a feature vector of length  . Figure&amp;nbsp; d shows the accuracyof the four orientations in different channels. Orientation 1 ( ) and orientation 3 ( ), gave the best results. . Cr and Cb yielded an accuracy of 87.3 and 86.3&amp;nbsp;%, respectively, in  .         (    )           In this experiment, we studied the performance of combining all the SPT subbands by concatenating the corresponding LBP histograms. The length of the feature vector in this case was  . The results are shown in Fig.&amp;nbsp; . The accuracies of the four channels were 91.16&amp;nbsp;% for Cr, 89.19&amp;nbsp;% for Cb, 63.9&amp;nbsp;% for Y, and 63.6&amp;nbsp;% for gray. It is clear that the combination of all subbands achieves higher accuracy than using the individual subband for each channel. Moreover, Cr and Cb have better performance compared to Y and gray.      Table&amp;nbsp; 1 shows the performance of the proposed method using feature selection on the CASIA v1.0 database. As mentioned earlier, the features are selected using a combination of  -norm and LLB. Using feature selection, the number of features was reduced from 3,584 to 480, on average. Our experimental results indicate that the chrominance channels performed better than the luminance channel or gray. For example, the Cr channel had the best mean accuracy of 94.89&amp;nbsp;% (standard deviation, SD = 1.12), followed by 94.24&amp;nbsp;% accuracy using the Cb channel. The sensitivity, specificity, and AUC were also high for these two channels. On the other hand, Y and gray had only 66.74 and 64.65&amp;nbsp;% accuracy, respectively. In the case of Cr, feature selection improved the accuracy from 91.16&amp;nbsp;% (see Fig.&amp;nbsp; ) to 94.89&amp;nbsp;%. Figure&amp;nbsp;  shows the ROC curve for the four channels using feature selection. We have also analyzed the performance of individual SPT subbands using feature selection. Table&amp;nbsp; 2 shows the subband contributions (%) for the Cr and Cb channels. As it can be observed, the highest frequency subband contributes the most (33.54&amp;nbsp;% for Cr and 34.37&amp;nbsp;% for Cb) as a single subband. In terms of scale contribution, scale 1 contributes the most (46.04&amp;nbsp;% for Cr and 45.84&amp;nbsp;% for Cb), while scale 2 and scale 3 contribute the least. We have performed additional experiments to investigate the performance of the proposed method on splicing and copy-move forgery separately. In the ‘Sp’ folder of the CASIA v1.0 database, two types of forgeries can be found: splicing (involving two images) and copy-move (involving one image). While performing experiments on splicing detection, we removed all the copy-move forged images from the dataset; similarly, we excluded all the spliced images for the copy-move detection experiments. The results are reported in Table&amp;nbsp; 3 for the chrominance channels only, using feature selection. As it can be observed, the accuracy of splicing detection (95.09&amp;nbsp;% for Cr) and that of copy-move detection (95.21&amp;nbsp;% for Cr) were very close, indicating that the proposed method is not biased towards any specific type of image forgery. Table&amp;nbsp; 4 shows the performance of the proposed method in CASIA v2.0 database. As we have found that the Y channel and gray scale do not give satisfactory results, we have not included them in the table. The proposed method achieved 97.33&amp;nbsp;% accuracy using the Cr and Cb channels without feature selection. When using feature selection, performance did not increase; however, it feature selection decreases the number of features to around one-tenth. Figure&amp;nbsp;  shows the ROC curve for the two chrominance channels using feature selection. We have also performed experiments with different types of forgery separately to see whether the proposed method is biased to any particular type or not. Figure&amp;nbsp;  shows the accuracy (%) using the Cr and Cb channels with feature selection with CASIA v2.0. Apart from splicing and copy-move detection, we have also performed experiments assuming geometric transformations, namely, rotation, scaling, deformation, and no transformation (copy-move forgery without any geometric transformations). A similar approach as described in the previous section was utilized, that is, in the case of splicing detection, we removed all the copy-move forged images from the dataset; in the case of copy-move detection, we removed all the spliced images from the dataset. The other four experiments (i.e., assuming geometric transformation) were performed in the same manner. In all cases, accuracy ranges between 93.22 and 97.67&amp;nbsp;%, where the best accuracy was obtained in the case of splicing detection using the Cr channel. Therefore, we can conclude that the proposed method works well for detecting both splicing and copy-move forgery with and without geometric transformations. Table&amp;nbsp; 5 shows the accuracy (%) of the proposed method with and without feature selection using Columbia color database. The Cb channel performs better than the Cr channel on this database. The proposed method, after applying feature selection, yields 96.39&amp;nbsp;% accuracy using the Cb channel, and 94.17&amp;nbsp;% accuracy using the Cr channel. It is interesting to note (not shown here) that all selected features came from the residual highest frequency subband of SPT. The performance of the proposed method was also compared to that of some other recent methods. Table&amp;nbsp; 6 shows the comparisons using the three databases used in our experiments. The results of the other methods were obtained from the corresponding papers. Only the best results of these methods are reported in the table. The method in [ 20] involves a modified run-length run-number technique on the chrominance channels, while the methods in [ 10,  11,  13] were described briefly in Sect.&amp;nbsp; 1. According to our knowledge, these methods have achieved the best results so far on these databases. However, the authors of these methods reported results on only one database, either CASIA v1.0, CASIA v2.0, or Columbia color; and also no implementation code was provided. Therefore, in Table&amp;nbsp; 6, we show results of these four methods for the corresponding databases only. As Table&amp;nbsp; 6 illustrates, the proposed method outperforms the methods of [ 10,  11] on CASIA v2.0 and the method of [ 13] in the Columbia color database; the performance of the proposed method is slightly better than that of the method in [ 20].    Acknowledgments   
Springer.tar//Springer//Springer\10.1007-s00138-013-0548-3.xml:Inductive hierarchical nonnegative graph embedding for “verb–object” image classification:Graph embedding Inductive Hierarchical Verb–object:  1 Introduction  Image understanding and classification applications have been wildly researched for decades. Most existing image classification methods focus on handling images with only “object” concepts&amp;nbsp;[2, 4, 13, 14], such as “horse”, “bike” and “tree” etc. However, in real-world, a huge number of images contain “verb–object” concepts, such as “ride horse”, “repair boat”, and “cut tree”, rather than only “object” concepts. These kinds of “verb–object” concept images could represent more abundant semantic meaning while having much smaller size comparing with videos&amp;nbsp;[6, 7, 22]. Hence, research on “verb–object” concept images, like image classification, is significant and essential. However, by using traditional image representation techniques, each “verb–object” concept could be treated as a whole “object”. The embedded information in it will be ignored. In such a way, the classification performance would be discounted. It is observed that, some concepts, like “ride horse” and “feed horse”, “repair boat” and “row boat”, “cut tree” and “plant tree”, are sharing the same “object” part. Figure&amp;nbsp;  illustrates several images under a set of “verb–object” concepts sharing same “object” but different “verb” parts. Intuitively, this kind of set of concepts very likely share a common latent information or pattern in images, which can be helpful for image classification. Motivated by these observations, we try to more effectively interpret “verb–object” concepts images. We assume that those “verb–object” concept images which share the same “object” but different “verb” part have a specific hierarchical structure, which can be utilized for image classification. By applying this hierarchical structure, one “verb–object” concept can be used not only to separate a set of concepts from other concepts which have different “object” parts, but also to discriminated itself from concepts in this set by the different “verb” parts. For example, images of “carry bike” can help to discriminate images of “repair bike” from images of “feed horse”, while itself should be classified from images of “repair bike”. We regard this structure as hierarchical structure and utilize it in classification. Our previous work, hierarchical nonnegative graph embedding (HNGE) [19], has utilized hierarchical structure together with nonnegative graph embedding algorithm to preform “verb–object” concept image classification, and has achieved a remarkable classification accuracy. In that work, we reconstructed the testing sample with the training samples and then used the derived reconstruction coefficients to combine the encoding coefficient vectors of training samples. However, in its subsequent work, we found that it suffered from an out-of-sample extension problem, that is, how to easily and accurately transform each new testing sample into its low-dimensional nonnegative representation. This problem could also be described as how to obtain the encoding coefficient vector for the feature vectors of testing samples, without increasing computational cost or violating the basic nonnegative assumption. To tackle this out-of-sample extension problem, in this paper, we reformulate the problem, extend the HNGE, and propose a new algorithm, named inductive hierarchical nonnegative graph embedding (IHNGE). This approach could not only combine hierarchical structure with nonnegative graph embedding to preform effective “verb–object” concept image classification, but also propose a conventional and effective way for out-of-sample extension in classification. We conduct experiments on a web image corpus composed of 9000 images on 45 “verb–object” concepts. The experimental results demonstrate the effectiveness of our approach. The contributions of our work can be summarized into threefold:          The rest of the paper is organized as follows: in Sect.&amp;nbsp; 2, we introduce the related work. We elaborate our IHNGE and its formulation in Sect.&amp;nbsp; 3. Experimental results are reported in Sect.&amp;nbsp; 4. Finally, we give conclusions in Sect.&amp;nbsp; 5. We utilize the hierarchical information to extend the nonnegative graph embedding, which is proved to be suitable for image classification within hierarchical structure. Based on the HNGE, we propose the IHNGE, which brings in the inductive matrix to deal with the problems of high computational cost and nonnegative assumption satisfaction within the testing procedure in image classification. We use the IHNGE to tackle the classification of “verb–object” concept images, and have obtained a remarkable classification accuracy.    2 Related work  Nonnegative and sparse representation techniques have been well researched in recent decades to find nonnegative basis of data features with few nonzero elements [9]. A pioneer work for such a purpose is nonnegative matrix factorization (NMF) [12]. It decomposes the data features matrix as the arithmetical product of two matrices which possess only nonnegative elements. Generally, NMF belongs to the techniques of feature extraction and dimensionality reduction, as it results in a dimension-reduced representation of the primal data features [16]. In recent years, NMF&amp;nbsp;[8] and its variants, localized NMF (LNMF)&amp;nbsp;[15], convex NMF (CNMF)&amp;nbsp;[3], and Fisher NMF&amp;nbsp;[23], have been proved effective in many applications. Some work extended and applied NMF in many different fields, such as face and object recognition [18, 27], biomedical applications [5, 10], color science [17], and so on. Recently, beyond the original nonnegative data factorization, Yan et al.&amp;nbsp;[24] proposed a graph embedding framework which provided a unified formulation for dimensionality reduction, and possessed the algorithmic properties in convergency, sparsity, and classification power. Yang et al. [25] extended this work and proposed an approach, named nonnegative graph embedding (NGE), to obtain customized nonnegative data factorization by simultaneously realizing the specific purpose characterized by the intrinsic and penalty graphs. This work was further refined by Wang et al. [20] with the efficient multiplicative updating rule, namely multiplicative nonnegative graph embedding (MNGE). MNGE achieved a satisfactory performance, however, it lacked the direct way to obtain the encoding coefficient vector for the new testing sample. Our previous work&amp;nbsp;[19] HNGE also suffered from this out-of-sample extension problem, which is solved by IHNGE in this paper. A similar work was proposed by Zhang et al. [28], which tried to simultaneously learn a set of classifiers for “verb–object” concepts in the same group. However, in this paper, we do not focus on designing classifiers, but focus on feature extraction and representation, which refer to nonnegative and sparse representation techniques. Besides, our work seems to be similar to human action recognition, like [26], as “verb–object” concepts look like actions. However, the main difference between our work and human action recognition is that human action recognition treats every action as unrelated to each others while our IHNGE considers “verb–object” concepts having hierarchical structure to explore a new layer of linkage among actions. Moreover, in human action recognition, an action does not always have an object, while IHNGE focuses on handling images containing both “verb” and “object”.    3 Inductive hierarchical nonnegative graph embedding  In this section, we elaborate IHNGE and formulate the problem within the framework of nonnegative data decomposition. Before introducing the inductive nonnegative graph embedding, we first list the notations used in this paper here. Let  denote the data sample set, in which  denotes the feature descriptor of the th sample and  is number of total samples. Here we assume that the matrix  is nonnegative. Let  be the dimension of the desired dimension-reduced feature space, the task of our data factorization is to derive a nonnegative basis matrix  and a nonnegative encoding coefficient matrix , while the data matrix  can be approximated as the product of matrices  and . In this paper, we utilize the following rule to facilitate presentation: for any matrix ,  denotes the th row vector of , its corresponding lowercase version  denotes the th column vector of .  denotes the element of  at the th row and th column, and  means that  has  rows and  columns. Practically, we believe that the “verb–object” concept images contain hierarchical structure. Figure&amp;nbsp;  illustrates the hierarchical structure in “verb–object” concept images. As shown, we divide whole “verb–object” concepts into two levels. On the first level, those “verb–object” concepts containing the same “object” are treated as a class, here “class” is as the same meaning as “group” or “set” mentioned above. On the second level, those “verb–object” concepts in the same class on the first level are divided into sub-classes, according to the different “verb” parts they have. Although the final aim of our classification is to discriminate all the sub-classes on the second level, we do not directly perform classification on only second level. Instead, as shown in Fig.&amp;nbsp; , on one hand, we enlarge interclass distance for classes on the first level. On the other hand, we reduce intraclass distance and enlarge interclass distance for sub-classes on the second level. These two steps are performed simultaneously while the one on the first level will compensate the one on the seconde level and improve the performance on the final “verb–object” concept classification. Figure&amp;nbsp; shows an illustration of this procedure. Suppose “repair car”, “drive car”, “wash face” and “make up face” are four illustrative sub-classes on the second level in our dataset, while “repair car” and “drive car” belong to the same class “car” on the first level and “wash face” and “make up face” belong to another same class “face” on the first level. Our goal is to classify four sub-classes “repair car”, “drive car”, “wash face” and “make up face”. However, classification in our IHNGE is not directly conducted by only treating these four as different classes. As Fig.&amp;nbsp; shown, for an illustrative sample in sub-class “repair car”, on one hand, we treat it as a sample in class “car” on the first level and tend to separate it from all samples in class “face” by enlarging the interclass distance on first level. In this step, all samples in anther sub-class “drive car” but same class “car” will contribute to classify this illustrative sample. On the other hand, we tend to separate it from those samples in same class “car” but different sub-class “drive car”, by reducing the intraclass distance on second level and enlarging interclass distance on second level. Specifically, these two steps are integrated into the our IHNGE model and performed simultaneously in hierarchical graph embedding process. To achieve our goal, we make use of inductive nonnegative data decomposition together with HNGE. The target of inductive nonnegative data decomposition and the purpose of HNGE coexist harmoniously and do not mutually compromise, while satisfying those distance requirements in two levels. We elaborate each part as well as the unified one in the following. Nonnegative matrix factorization (NMF) factorizes the data matrix   into one lower-rank nonnegative basis matrix   and one nonnegative coefficient matrix  . Its usual objective function is shown as follows, which was also be utilized in&amp;nbsp;[ 19]:       The most important improvement of IHNGE different from HNGE&amp;nbsp;[ 19] is that, IHNGE imposes the extra constraint that the coefficient of each data point lies within the subspace spanned by the column vectors of an inductive matrix. Theoretically, based on the coefficient matrix  , we assume that the coefficient vector   can be derived by linear transformation from the feature descriptor of sample  , shown as follows:       or:       where   indicates the inductive matrix which transforms the  -dimensional feature vector into the  -dimensional feature vector. Then the objective function of NMF in&amp;nbsp;( 1) can be refined as:       Based on this assumption, we can easily obtain the encoding coefficient vector   for a new testing sample  . After getting the inductive matrix  , we have:       The inductive matrix   can ensure the nonnegative ability of vector   required by NMF, while avoiding increasing computational cost when testing new samples. As proposed by Yan et al. [24], most dimensionality reduction algorithms can be explained within a unified framework, called graph embedding&amp;nbsp;[24]. The purpose of graph embedding-based algorithm is characterized by the so-called intrinsic and penalty graphs. In this paper, we formulate our IHNGE within the general graph embedding framework&amp;nbsp;[24]. Specifically, to serves for graph embedding, we first divide the coefficient matrix   into two parts, namely, 1        where  ,  \(d&amp;lt;N\), denotes the desired low-dimensional representations for the training data on parent classes, and  . As existing hierarchical structure, we then divide the matrix   into two parts, namely,       where  ,  \(r&amp;lt;d\), denotes the desired low-dimensional representations for the training data on child classes, and  . Therefore, to clearly reveal the structure, we rewrite matrix   as:       Meanwhile, to facilitate presentation, we define matrix   as the combination of   and  :       where  . Correspondingly, the basis matrix   is also divided as:            and:            where  ,  ,   and  . The combination of   and   is:            where  . As  , inductive matrix   is also divided as:       and       where  ,  ,   and  . The combination of   and   is denoted as:       where  . Let   be an undirected weighted graph with vertex set   and similarity matrix  . Each element of the real symmetric matrix   measures similarity between a pair of vertices. The Laplacian matrix   and diagonal matrix   of a graph   are defined as:       Graph embedding generally involves an intrinsic graph  , which characterizes the favorite relationship among the data samples, and a penalty graph  , which characterizes the unfavorable relationship among the data samples. Correspondingly, penalty graph   also has its Laplacian matrix   and diagonal matrix  . In our work, we assume that the sample data set has two levels. Therefore, there are an intrinsic graph and a penalty graph on the first level, and an intrinsic graph and a penalty graph on the second level, respectively. Let  be the intrinsic graph on the first level,  be the penalty graph on the first level,  be the intrinsic graph on the second level, and  be the penalty graph on the second level. Their Laplacian matrices and diagonal matrices are , , , , , , , and , respectively. According to graph embedding, the target of graph preserving on the first level is:       As   are considered as the complementary space of  . From the complementary property between   and  , the objective is transformed into:       On the second level, our two targets of graph preserving are given as:       As   are considered as the complementary space of  . From the complementary property between   and  , the second objective above is transformed into: To achieve the objectives in Eqs.&amp;nbsp; ( 4), ( 18), ( 19), and ( 20) which are required for HNGE, we have the unified objective function as:            where   is a positive parameter to balance the two parts for graph embedding and data reconstruction, and is always set as 1 empirically. From the definitions of Laplacian matrix and diagonal matrix, together with definition in Eq.&amp;nbsp;( 3), we have:                                      Furthermore, as   is the basis matrix, it is natural to require that each column vector of   is normalized, that is  . But this constraint makes the optimization problem much more complicated. Hence, we compensate the norms of the bases into the coefficient matrix and rewrite&amp;nbsp;( 22), ( 23), and ( 24) as:                                      where the matrix  ,  , and  , where   denotes the  th column vector of matrix By combining equations above, the final objective function is then reformulated as: As the final objective function is biquadratic and generally there does not exist closed-form solution, we use an iterative procedure to get the nonnegative solution. Most iterative procedures for solving high-order optimization problems transform the original intractable problem into a set of tractable sub-problems, and finally obtain the convergence to a local optimum. Our proposed iterative procedure also follows this philosophy and optimizes  and  alternately. For a fixed matrix  , the objective function in&amp;nbsp;( 28) with respect to basis matrix   can be rewritten as:            where:        and  , operator   indicates the element-wise matrix multiplication,   indicates the identity matrix. We integrate the nonnegative constraints into the objective function with respect to  , and set   as the Lagrange multiplier for constraint  . Set Matrix  , where  . Then the Lagrange   is defined as:            By setting the partial derivation of   with respect to   as zero,            Along with the Karush–Kuhn–Tucker (KKT) condition&amp;nbsp;[ 11] of  , we obtain the following equation:            Then we set:    where:        and:        Equation&amp;nbsp;( 32) can be rewritten as:            which leads to the following update rule for  :
Springer.tar//Springer//Springer\10.1007-s00138-013-0549-2.xml:Pedestrian detection based on sparse coding and transfer learning:Transfer learning Pedestrian detection Sparse coding SVM HOG:  1 Introduction  Due to the wide use of surveillance cameras in public places, a large number of videos need to be processed. However, it is impractical to manually analyze these data due to expensive labor costs. Intelligent video surveillance is an effective way to solve this problem. Pedestrian detection is an essential and significant task in an intelligent surveillance system, which aims to automatically find and localize all pedestrians in each frame from a video sequence. As a key procedure of pedestrian tracking, action recognition, personal identification and unusual events detection, it can also be used in many other applications, such as human computer interfaces and vehicle navigation. In the past decades, significant progress has been made on pedestrian detection [ 1– 3], especially the appearance-based approaches [ 4– 6] based on large-scale training sets became more and more popular and have achieved great success. However, it is still difficult to train a generic appearance-based pedestrian detector which works robustly in a great variety of scenes. For example, it was shown that detection rate of the popular HOG pedestrian detector trained on the INRIA data set dropped significantly when being tested on the Caltech benchmark data set [ 7]. Our experiment results shown in Fig.&amp;nbsp; a also confirm this conclusion. To reach high accuracy in all kinds of scenes, generic pedestrian detector not only requires a huge training set to cover a very large variety of viewpoints, resolutions, lighting conditions, illuminations and backgrounds across different scenes which needs impractically amounts of manual labeling efforts, but also a very complex model to handle so many variations which are very hard to be found. Therefore, training and tuning a well-performed pedestrian detector for one or several particular scenes are more practical. For example, we manually labeled 2,000 positive samples and 12,000 negative samples to get the detection results in Fig.&amp;nbsp; b. However, the required manual labeling efforts for each particular scene are time-consuming, tedious and even impractical. To alleviate such manual labeling efforts, some solutions have been proposed in recent years. For example, Levin et al. [ 8] adopted co-training framework, Roth et al. [ 9] trained multiple classifiers based on grid structure, Nair and clark [ 10] and Wu and Nevatia [ 11] used online learning framework. However, these existing approaches are based on ad hoc rules and their trained detectors may have risk of drifting [ 12]. Transfer learning framework is another emerging direction to reduce the manual labeling efforts mainly by re-using existing labeled source samples to train scene-specific detectors [ 13]. In recent years, this approach has achieved great progress. However, there are still two main challenges unaddressed well. In one hand, it has been proved by Meng and Xiaogang [ 14] that including a set of concretely labeled target samples is essential to train excellent scene-specific detectors in such transfer learning frameworks. Currently, these target samples are mainly manually labeled. Therefore, the first challenge is how to get a set of concrete labeled samples from target scenes totally without manually labeling efforts. In another hand, the existing huge source samples are also valuable to be included to train scene-specific detectors since there are some similarities between them and target scene. But not all labeled source samples can contribute to improve the performance of scene-specific detector. On the contrary, the performance of trained scene-specific detector may be reduced when including the outliers for training [ 15]. How to avoid the negative effect of outliers of source samples is another challenge for transfer learning frameworks. This paper’s goal is to solve both these two problems while adopting transfer learning framework to train scene-specific pedestrian detectors. To completely avoid the manually labeling efforts, we choose to select training samples directly from the detection results of the existing generic pedestrian detector on target scenes. Since the detection results are not absolutely correct, our challenges are how to make sure the accuracy of selected samples and keep the selected sample set big enough. Two techniques are applied for these challenges. At first, besides appearance, we adopt some context information in the surveillance video, such as motion, size to filter the initial detected results. It has been proved by Meng and Xiaogang [14] that such context information can help to get those samples whose labels and confidence values we are very sure about. To differentiate the filtered samples with other samples, we call them as target templates. Unfortunately, the set of target templates is usually not big enough to train an excellent scene-specific detector. Therefore, our second technique is adjusting the confidence value of each target sample according to its relevancy to target templates so that its confidence value is more close to its real confidence value. The adjusted confidence value is used as weight of each target sample later while training scene-specific detector, which has been proved by Meng et al. [12] to be very useful to solve the drifting problem. We can use similar technology to exclude the outliers while including labeled source samples to train scene-specific detectors. It is intuitive and reasonable to set larger weights to those samples which are more close to the target scene while including them to train scene-specific detector. Different to target samples, we directly use the calculated relevancy between each source sample and target templates as the weight. Inspired by the success of sparse coding applied to computer vision and machine learning [16], we propose to use sparse coding to calculate such relevancy. For sparse coding, each sample is sparsely decomposed into a set of data basis. This sparse decomposition of a sample reflects its true neighborhood structure and provides a relevancy measure between the sample and the basis. Through setting different basis, we can not only adjust the weight for target samples, but also calculate the weight for source samples and exclude outliers. For example, when we calculate weights of source samples, we use source samples as basis and calculate the sparse coding of all target templates. According to sparse coding theory, the summing sparse coding corresponding to each base represents its contribution to reconstruct all target templates, so it can be used as the weight. Combining all the previous methods to solve the above challenges, we conduct a transfer learning framework for pedestrian detection. The experimental results on public video surveillance datasets show that our method significantly improves the performance compared with the generic pedestrian detector and other re-weighting method. Moreover, the results of our approach are comparable to those of manual labeling method. The rest of the paper is organized as follows. In Sect.&amp;nbsp;2, related work is reviewed. Section&amp;nbsp;3 provides details of the proposed transferred pedestrian detector based on sparse coding. Experiments are described in Sect.&amp;nbsp;4, and Conclusion is presented in Sect.&amp;nbsp;5.    2 Related work  In the past few decades, numerous pedestrian detection algorithms have been proposed. The two main frameworks into which most algorithms fall are (1) training a detector (general or scene-specific) with enormous labeled samples; and (2) training a scene-specific detector automatically starts from a general detector. As early works, Oren et al. [1] and Papageorgiou and Poggio [17] used Haar wavelets features and SVM to detect objects. Viola et al. [18] combined generalized Haar wavelets and AdaBoost into pedestrian detection. By introducing HOG descriptors into pedestrian detection, Dalal and Triggs [4] achieved great success. Mohan et al. [19] and Mikolajczyk et al. [20] detect pedestrians as a combination of parts. Wu and Nevatia [21] used a set of hard coded mid-level features, called “edgelets”, and Bayesian combination to detect pedestrian. In recent years, Dollár et al. [5] introduced a combined multiple feature, called as “Chntr”, performed well in pedestrian detection, Felzenszwalb et al. [22] used discriminatively trained part-based models to perform detection. All these works have achieved great success, but rely much on the enormous labeled training samples. However, manual labeling work is cost and even unacceptable for some applications. Therefore, many researches turn to automatically label a small set of samples by making use of the detection results in the specific scene which come from original general detector and different kinds of other information. Background subtraction results are used to help labeling target samples in Nair and Clark [10] in its on-line learning framework. But the background subtraction results are very sensitive to lighting variations and scene clusters. Rosenberg et al. [23] selected target samples according to a training data selection metric that is defined independently of the detector rather than the selection metric that is based on the detection confidence generated by the detector. However, this method relies on the score of the selection metric which is insufficiently reliable. Meng and Xiaogang [14] integrated multiple cues of motions, path models, locations, sizes and appearance to select target samples. Its improvement mainly comes from path models, so it can only be used in scenes with paths. This work attempts the pedestrian detection using sparse coding in the framework of transfer learning to automatically train scene-specific detector. Target samples are automatically obtained through a general discriminative detector. And sparse coding is used to weight the samples in the final training set, which can suppress the drifting problem when using the unreliable target samples and outlier problem when using the unconformable distribution of source samples. Transfer learning aims to retain and reuse previously learned knowledge [24]. Early related works mainly depend on the fact that there are a small set of manually labeled samples (target samples) from the target-specific scene. With this assumption, a common way to get high detection accuracy in a specific scene is training a new detector by mixing a large set of original training samples (source samples) and the target samples. Wu and Dietterich [25] demonstrated that the source samples can used to improve accuracy by taking them as auxiliary data although they are drawn from a different distribution than the target scene. Based on Adaboost, Dai et al. [26] proposed a learning framework called TrAdaBoost to improve accuracy by decreasing the weights of wrongly predicted samples during retraining process if they belong to source dataset. The variation of view point is handled in Pang et al. [27] by a feature-representation transfer approach to adapt weights of multiple weak classifiers. To get the target samples totally without manual labeling effort. Meng et al. [12] proposed to use detector trained on the source samples to get target samples, and use KNN-based graph to weight these target samples and source samples for handling the impact of drifting. However, the graphs constructed by KNN often fail to capture piecewise linear relevancy between data samples in the same class [28] and also cannot handle situations where an adaptive neighborhood is required. Although our method also uses transfer learning framework to combine weighted source samples and weighted target samples into the final process of training scene-specific detector, the weighting process is quite different from that of Meng et al. [12]. In our method, weight for each source sample is calculated by summing the sparse coding and corresponding confidence value of each target template, which can naturally represent the distribution of the target templates. However, in Wang et al.’s method, KNN is used to calculate the weight, which will lead to over-adaptation [29] (large weights are exclusively allocated to very few samples and thus failing to code the real distribution of the target templates). In addition, unlike Wang et al.’s method which use KNN-based label propagation to get the weight for target samples which is more likely to be influenced by data noises, the sparse coding-based weighting method introduced by our method is well suited to handle different data, especially to partial occlusion. The sparse representation theory has shown that sparse signals can be exactly reconstructed from a small number of linear measurements [30]. It has gained increasing attention due to its success in various computer vision applications. A pioneering attempt was conducted to use the sparse representation theory for face recognition [28]. Experimental results showed that the method is superior to other state-of-the-art methods, especially under occlusion. Sparse coding can be seen as a part of sparse representation theory, and has been also used in many applications. Yang et al. [31] extended spatial pyramid matching method by generalizing vector quantization to sparse coding and gained better results in image classification. Raina et al. [32] proposed a transfer learning framework, called self-taught learning to learn feature representation from unlabeled data. All the above works used sparse coding as new feature representation to perform their tasks. Recently, sparse coding is also used in other ways. In Mei and Ling’s [33] L1 tracker, each target candidate is represented as a linear combination of the template set composed of both target templates and trivial templates. The assumption is that a good target candidate can be sparsely represented by both the target templates and trivial templates. In their work, sparse coding is not only used to decide the label of the target candidate, but also used to update the weight for them. Elhamifar et al. [34] represented each sample into several other samples in the training set and then used the summing sparse coding of each sample to choose the representatives. Inspired by their work, we propose to use sparse coding to estimate the relevancy between source samples and target templates as well as between target samples and target templates. Each sample is naturally related to several target templates by sparse coding. Thus, the weight of each sample by summing the sparse coding is expected to present its relevancy with the target scene.    3 Our method  Figure&amp;nbsp;  shows the overall flowchart of our transfer learning framework. It starts with a generic pedestrian detector using HOG + SVM trained on a general source dataset  . In the source dataset, each source sample   is labeled as   ( .   indicates positive samples and   indicates negative samples. An unlabeled video sequence is captured from the target scene. Once the general detector is applied to the target sequence, a target dataset   is obtained by selecting the target samples whose confidence value   is positive. The context information and clustering technology help to filter out the false positive samples and construct the target template dataset   from  . Each target sample   is re-weighted by   according to its relevancy with the target templates, and each source sample   is re-weighted by   according to its relevancy with the target templates. A new scene-specific detector   is trained on both   and   given   and   under the transferred SVM in Eq. ( 9). The details are given in the subsections below. Just as we mentioned above, there are many false labeled samples when directly scanning the target video frames with generic pedestrian detector at multiple scales. To select a set of target templates whose labels are most likely to be correct, we adopt several filters and clustering technology to filter out those falsely labeled samples and get the target templates. Some selected target templates are shown in Fig.&amp;nbsp; . To select positive target templates, we use several filters to filter out those samples in   which are potential false positives. Those filters include motion, size and appearance.      Pedestrians are usually walking in video surveillance. Therefore, it is reasonable to filter out those detected positive samples if they keep stationary for a long time. Also it does not matter for selecting target templates even the true pedestrians are filtered out. Based on this observation, most false positive samples on the background can be filtered out by a parameter defined as the ratio of moving pixels and total pixels in the detection windows. Moving pixels can be obtained using background subtraction models like Vibe [35]. According to our experiments, most false positive samples on the background can be filtered out by setting the threshold value as 0.2.     For a specific target scene, the size range of pedestrians is fixed especially for eagle-eye perspective. We can take use of this character to filter out those false positive samples whose sizes are out of this range. We construct the histograms of the sizes of the detected windows and choose the size range to cover 80&amp;nbsp;% detected positive windows to get the positive target templates.     The generic detector is based on HOG features representing appearance, so the confidence value itself can be used as a filter. Usually detection windows on pedestrians have high confidence value. Therefore, the confidence value can be used to choose true positive target samples. We set the threshold to select out about 10&amp;nbsp;% of target samples with highest confidence values as positive target templates. Pedestrians are usually walking in video surveillance. Therefore, it is reasonable to filter out those detected positive samples if they keep stationary for a long time. Also it does not matter for selecting target templates even the true pedestrians are filtered out. Based on this observation, most false positive samples on the background can be filtered out by a parameter defined as the ratio of moving pixels and total pixels in the detection windows. Moving pixels can be obtained using background subtraction models like Vibe [35]. According to our experiments, most false positive samples on the background can be filtered out by setting the threshold value as 0.2. For a specific target scene, the size range of pedestrians is fixed especially for eagle-eye perspective. We can take use of this character to filter out those false positive samples whose sizes are out of this range. We construct the histograms of the sizes of the detected windows and choose the size range to cover 80&amp;nbsp;% detected positive windows to get the positive target templates. The generic detector is based on HOG features representing appearance, so the confidence value itself can be used as a filter. Usually detection windows on pedestrians have high confidence value. Therefore, the confidence value can be used to choose true positive target samples. We set the threshold to select out about 10&amp;nbsp;% of target samples with highest confidence values as positive target templates. We use the samples with lower confidence values and the filtered out samples as candidates to select the negative target templates. These candidates are close to the decision boundary and may be misclassified by the detector. They are known as hard examples in [4, 22] which are proved to be very helpful to train a better detector. To select the negative target templates, we assumed that it is uncommon for pedestrians to stay at the same location for a long time just as above. On the other hand, if a background patch is misclassified as pedestrian, similar patterns tend to repeatedly appear at the same location and be misclassified over a long period. So we cluster those candidates on the locations, sizes and HOG features across frames. Larger clusters across many frames are selected as negative target templates. Although our target samples are detected out by the generic detector, there are still some samples being falsely labeled. Therefore, there is risk of drifting issue if we directly include them to train scene-specific detector. To avoid such risk of drifting, we choose to assign different weights to different target samples. The basic idea is that if a sample is more (less) confident to be positive or negative, it should be assigned larger (smaller) positive or negative weight so that the cost of misclassifying it will be larger (smaller). The most common and intuitive way to weight a target sample is directly using its detected confidence value as its weight. However, it cannot well solve the drifting issue since sometimes the weight itself is not absolutely correct. For example, as shown in Fig&amp;nbsp; , some pedestrians are labeled as positive samples with very low positive confidence value, and there exist some false positive samples with positive confidence value. Therefore, we need to adjust the confidence value of each target sample before it is used as weight according to the relevancy between the target sample and target templates. We calculate such relevancy by sparse coding method. In our method, the target templates   plus an identity matrix   are used as basis. Adding the identity matrix as part of basis is motivated by the work in [ 36] to handle part-occlusion issue. The basic idea is that if a sample is part-occluded, it can be reconstructed from samples which are not occluded and identity vector which indicates the location of occlusion. According to sparse coding theory, all target samples   can be reconstructed by the basis   as the following equation:       where   is the relevancy matrix and   means all elements in   are non-negative. Since a target sample and target templates are from the same target scene, it can be efficiently represented by a very small part of target templates which lead us to solve Eq.&amp;nbsp;( 1) in a sparse way like:       where   means the number of non-zeros in  . Since the   norm is NP-hard, we further relax the   norm to   norm as the following equation so that it can be efficiently solved by the online learning methods proposed in [ 37].       Each element  of   is further normalized as Eq.&amp;nbsp;( 4). and then get the relevancy matrix (sparse coding matrix) between target samples and target templates.       We propose if two samples are related with  , their confidence values should also be related with  . We update the initial confidence value of each target sample using the sparse coding as follows:       The weighting process with some examples is shown in Fig.&amp;nbsp; . Some weighted results are shown in Fig.&amp;nbsp; b. We can see that those detected target samples with high weights are true pedestrians, while those detected target samples with negative weights are not pedestrians. As we mentioned above, some source samples are better match the target dataset while others can be seen as outliers. Therefore, they should be assigned different weights according to their relevancy with the target dataset for re-training a scene-specific detector. To select and weight the source samples, we model the relevancy between source samples   and target templates   in a form of reconstruction process as follows:       where   is the relevancy matrix and   means all elements in   are non-negative. Equation ( 6) means each target template can reconstruct by source samples with weight  . We model the reconstruction to be sparse because we only hope the most relevant source sample to be chosen. So we impose the sparsity on  , and then solve Eq.&amp;nbsp;( 6) in a similar way as:       After normalizing  , we can get an array of coefficients for each source sample, which can be seen as relevant degree between each source sample and the target dataset. Since the target templates are not manually labeled, some samples may be not labeled precisely. Therefore, besides this array of coefficients, the confidence value   of each target templates is also included when computing the final weights. The final weight   for each source sample is calculated by       The weighting process is shown in Fig.&amp;nbsp;  and some source samples with different weights are shown in Fig.&amp;nbsp; a. We can see that those source samples with larger weights are more similar to samples of the target scene, while those source samples with small or even zero weights are obviously outliers of target scene. After getting the weighted source samples and target samples, we use them all as new training set and train a new scene-specific model on SVM with an objective function showed as Eq.&amp;nbsp;( 9), where   and   represent the weights of source samples and target samples, which have the most importance for the performance of the newly trained pedestrian detector. In this equation,  and   are weights and bias to be learned with   source samples and   target samples. These samples are represented by HOG features   and   and labeled as   and  , respectively.   is a pre-set penalty parameter.   and   are slack variables of source samples and target samples. As we state in Sects.&amp;nbsp;3.3 and 3.4, we use different methods to calculate weights of target samples and source samples. We do this mainly because we have different purposes to weight them. For source samples, the major goal of weighting is to exclude the outliers and find those samples which are most related to target scene, so we use the source samples to reconstruct the target templates. If the summing sparse code of a source sample is zero, which means that this sample has totally no use to reconstruct any of the target templates, it is an outlier and should be excluded from the final training set. On the other hand, if the summing sparse code of a source sample is larger, which means that this sample is more useful to reconstruct the target templates, it is more related to the target scene and should be assigned a larger weight. For target samples, the major goal of weighting is to adjust their confidence values because we are not very sure about their initial confidence values obtained by the general detector. The main idea of our method is to adjust confidence value of each target sample closer to confidence values of those samples in target templates, whose confidence values we are very sure about and can be used to reconstruct the corresponding target sample. Therefore, we choose to use target templates and identity matrix to reconstruct all target samples and adjust the weight of the each target sample according to its relevancy matrix with the target templates and the confidence values of target templates. To make the whole process more efficient, we adopt two optimization strategies. Firstly, we use the 13-dimensional HOG feature [22] as the input to calculate the relevancy matrix for the following reasons: (1) HOG feature is the most effective way to describe the pedestrian [4, 22] and it is invariance to illumination by contrast-normalization of the local responses [4]. (2) It is more efficient to calculate the relevancy matrix using 13-dimensional HOG features as it is shorter than the pixel gray values and the origin 36-dimension HOG feature [4]. Secondly, in the training stage, we adopt standard LINEARSVM [38] and use the calculated weights as the penalty factor for each training sample. Using LINEARSVM, we can train a model very fast. Furthermore, the detection speed is not related to the number of training samples when using the trained model to perform detection.    4 Experiments  Three public datasets, INRIA [ 4], VIRAT [ 39] and PETS09 1, are used in our experiments. From the distribution of some randomly selected positive samples shown in Fig.&amp;nbsp; , we can see that these three datasets are quite different. INRIA is a public pedestrian dataset including lots of positive and negative samples taken in different scenes. We use it as the generic dataset to train a generic HOG + SVM pedestrian detector. Two target scenes are used in our experiments, which are selected from VIRAT (Fig.&amp;nbsp; a) and PETS09 (Fig.&amp;nbsp; b), respectively. The scene selected from VIRAT is recorded in a street. It is very challenging to accurately detect out pedestrians in such scenes because there are both moving pedestrians and vehicles with many occlusions. To get scene-specific detectors trained by manually labeled samples for comparison, we manually labeled 2,000 positive samples and 12,000 negative samples from 5,500 frames for the target scene selected from VIRAT, and 2,570 positive samples and 12,000 negative samples from 1,000 frames for the target scene selected from PETS09. Note that all these manually labeled samples are thoroughly not used to train our scene-specific detectors. Some statistics of the datasets are listed in Table&amp;nbsp; 1. PASCAL criterion is adopted in our experiments to distinguish true positive pedestrians from false positive ones. A detected positive pedestrian is treated as correct only if area overlap of its window and a ground truth window exceeds 50&amp;nbsp;%. We use DET curve which plots miss rate versus false positive per image (FPPI) as the evaluation metric. For DET curve, lower curve means better performance of detection. In the following description, we assume that FPPI is 0.1 when we talk about detection rate and FPPI is not specified. There are several parameters need to be set in our method. (1) Regularization parameter  for sparse coding. Many previous works [37] and our experiments show that there is no direct analytic link between the value of  and the corresponding effective sparsity, so we just set it as its default value (0.15). (2) Penalty parameters  in all SVM models. We adopt SVM cross-validation [38] to decide the value for each test model automatically. (3) Threshold parameters for the filters to choose target templates. Generally, for these threshold parameters, higher value means that we can filter our more false labeled samples and precision of selected target templates can be higher. However, according to the experimental results of [34], at least about 2&amp;nbsp;% of target samples should be chosen as target templates so that target samples can be well reconstructed by target templates. In this paper, the values of these threshold parameters are set empirically as stated in the Sect.&amp;nbsp;3.2 to match all these requirements.
Springer.tar//Springer//Springer\10.1007-s00138-013-0550-9.xml:Mixture of Merged Gaussian Algorithm using RTDENN:Image motion analysis Mixture of Gaussian model Object detection Real-Time Dynamic Ellipsoidal neural network Computer vision Motion detection Object segmentation Video surveillance:  1 Introduction  Security is an issue of great concern in our society and there is an increasing demand of surveillance systems. Typically, video surveillance systems have been used as a passive mechanism to record and store the data for later review in the event of an incident. These surveillance systems require constant human supervision, resulting in a significant cost increase. Computer vision is a widely developed research area, oriented to a broad range of applications and nowadays, research in this topic pursues the development of a completely autonomous and smart surveillance system capable of identifying threatening situations and managing alarms automatically. Video processing is broadly used for surveillance systems, as well as for many different applications, in view of the large volume of information it can provide, its low cost and its robustness [17]. More specifically, motion detection in the recorded scene is the first step in order to take subsequent actions [4, 9] and it is therefore an essential task in many applications [28]. In order to achieve successful motion detection, a foreground/background segmentation algorithm must be able to discriminate relevant motion from possible changes in the scene, such as the movement of trees due to the wind or changes in lighting. Moreover, the time required for the execution of the algorithm must enable real-time implementation. This paper presents a foreground/background segmentation algorithm called Mixture of Merged Gaussian Algorithm (MMGA) with the aim of achieving a substantial improvement in execution time to enable real-time implementation, without compromising the reliability and accuracy of the segmentation. The proposed algorithm is based on a combination of the probabilistic model of the Mixture of Gaussian Model (MGM) [22], which has proved to be one of the most successful methods to date, and the learning process of Real-Time Dynamic Ellipsoidal Neural Network (RTDENN) model [18–20]. The paper is structured as follows. First, an extensive literature review of segmentation solutions is presented to introduce previous segmentation solutions. Section 4 describes the proposed algorithm, explaining the MGM and the RTDENN concepts used to develop the MMGA. Then, the different tests performed and the results obtained are shown in Sect.&amp;nbsp;5. Finally, the conclusions are drawn in the last section.    2 Previous work  Present segmentation solutions can be classified into frame difference [12], image descriptors [24], optical flow analysis [26] and background subtraction models [7]. All of these methods have different advantages and weak aspects so that they may be more suitable for different applications. In security systems, real-time implementation and robustness of detection are the key aspects. The frame difference algorithm is clearly the simplest and less time consuming method, making it very interesting for real-time application. Nevertheless, it is not robust enough, thus failing to accurately detect moving objects in many cases [5]. Image descriptors need training samples. The system must create its own representative samples and these may vary depending on camera position. Moreover, this technique uses a complex algorithm, besides the need of patterns creation. Thus, it is not suitable to be used in real-time applications [27]. The authors of [11, 13, 14] state the optical flow technique uses complex algorithms that are too computationally costly for real-time implementation. Background subtraction techniques are only suitable in environments with static cameras [25], therefore, unacceptable for applications such as those in aerial vehicles [26]. However, this restriction may be acceptable in most security systems, and, since it is faster than other solutions, it has proven to be one of the most successful methods up to date for such applications. The Mixture of Gaussians Model (MGM) [22] is an interesting and largely used background subtraction technique. Piccardi [15] states that MGM is the background subtraction technique with the best results in an environment with limited resources. Nevertheless, many authors have pointed out that MGM can be outperformed in computational time, proposing other background subtraction techniques. In [10], to avoid using costly operations, the authors propose to model the background with a codebook, where a set of codewords are assigned to each pixel and new values are assigned to one of the codewords. Depending on their values, each codeword will model a foreground or background object. Zivkovic [29] implements a way to adapt the learning rate to the number of samples, as well as a new way to calculate the weight of each Gaussian in the model. Gaussians with insignificant weight are eliminated, allowing the algorithm to reduce the processing time. Yet other technique to improve the computational time with respect to MGM is to reduce the detection area. The Static and Dynamic Gates Model (SDGM) reduces it by means of dynamic gates [1] while in the model proposed by [2], the image is segmented in regions thanks to meanshift algorithm. As stated in [23], the updating process in background subtraction techniques consumes a significant amount of computational time. Therefore, these techniques typically use low-resolution images. In order solve this, a model with a modified, faster update process is proposed in this paper.    3 RTDENN  This section presents a brief theoretical description of the RTDENN model [18–20], focusing on the training algorithm, and more specifically on the merger of two neurons, which will be essential in the update process. RTDENN is conceived as a neural network model that models an -dimensional space . It can be divided into sub-workspaces or regions . Let  be the th sample value of the variable monitored. Each  belongs to . The RTDENN consists of a set of neurons  where each neuron is specialized in a particular sub-space . Every set of sample vectors included in every sub-workspace  represented by neuron  can be characterized by the number of sample that belongs to its region , the mean of the values of these samples  and the covariance matrix . RTDENN is suitable for real-time implementation thanks to the update process of the parameters. Every time   a new set of samples excite the network  , it is updated taking into account the number   of new samples, the mean   and the covariance matrix   of these new values:            where   the value of element   of   and   the value of element   of  . In the original work, a forgotten factor   based on [ 21] is introduced in these equations to reduce the importance of the old samples compared to the new ones, in a similar way to the learning factor   in MGM algorithm. The excitation degree of each neuron is estimated using the inverse of the square root of the Mahalanobis distance between the mean of the sample vectors which excite the neuron and each neuron:                         The excitation level is used to know which of the neurons should be updated: the neuron   that satisfies Eq. ( 3). The more excited neuron within a threshold   is the neuron updated. If none surpass the threshold, a new neuron   is added to the RTDENN. This neuron has a number of samples  , mean   and covariance matrix  . Then an analysis and merger of close neurons in the RTDENN is performed. This step is an important feature because it allows to reduce the number of neurons and thus to save memory, resources and calculation time. The fusion tries to find close neurons   and   using Eqs.&amp;nbsp;( 4) and ( 5), that is neurons with very similar sub-workspaces.                         The parameters of the merged neuron are calculated using the Eq.&amp;nbsp;( 1). If the new region obtained is too wide, the merger does not take place to avoid that the region of a single neuron occupies the whole domain  . A new merged neuron is considered to be acceptable if it achieves:    4 Proposed algorithm: MMGA  This paper proposes an adaptive approach for a segmentation algorithm based on the probabilistic background model of MGM and the learning process of the RTDENN for the update of the model. This approach models the background scene through a sum of Gaussians, which allows the model to deal with different backgrounds. This means that cyclical changes will be considered to be part of the background. For instance, in the case of swinging tree leaves, the background model would consider both the color of the leaves and the color of the sky as background. Furthermore, the background model is updated so that it can adapt itself to permanent changes in the scene. Taking the previous example, the model would take darker colors of green and blue for the background as the night falls. The background model is composed by a mixture of Gaussians to represent each pixel, similarly to MGM. The segmentation is performed at every frame by comparing each pixel value to its corresponding background model in order to determine whether the pixel belongs to the background or, on the contrary, to the foreground. Nevertheless, the update of the model is performed adopting a similar solution to the RTDENN training algorithm, which is very different from the update of the background model used by MGM. The best feature of the MMGA is the reduction of computing time without losing accuracy with respect to MGM. The MMGA is further explained throughout the following subsections, focusing on the main aspects of the algorithm, namely the creation and update of the background model and the background/foreground segmentation. A pixel is represented by its value   at instant  , which is a vector with the   components given by the channels of the input video (e.g.: one component for gray scale videos, or three different components for RGB videos), as expressed in Eq.&amp;nbsp;( 7), where   is the value of channel   of vector  .       The temporal series of values   of a pixel over a certain period of time is considered a stochastic process, and is stored by the algorithm to build a sample of size  . Thus, a new Gaussian   is created with three parameters: number of observations ( ), mean ( ) and covariance ( ). The number of observations associated to this Gaussian is  . The mean   and covariance   are estimated by the unbiased estimators sample mean and sample covariance given by Eqs.&amp;nbsp;( 8), ( 9) and ( 10) for the fitting of the sample data in a Gaussian distribution. Input video channels are considered to be independent, so that the covariance matrices for the Gaussians are diagonal matrices with the variance of each input channel in the main diagonal, as expressed in ( 9). This assumption leads to a remarkable reduction of the computational cost with no significant effect on the accuracy and reliability of the segmentation results, as explained in [ 22]. However, contrary to MGM method, the proposed model permits different variance for each channel, thus allowing the use of different color spaces, such as HSV (used in [ 3]) and color and brightness (used in [ 8,  10]).                                      The process of data storing is successively repeated, so that every   frames, a new Gaussian   is created for each pixel. The background model is updated to take into account the information provided by the new Gaussian  . To accomplish the data accommodation, this new Gaussian   is compared to the Gaussians that comprise the background model at that instant. This comparison will result on either merging the new Gaussian with an existing one, or directly incorporating the new Gaussian into the model if the required conditions for the merger are not satisfied. The model may be composed of a maximum number of Gaussians  , so that when the model is already composed by   Gaussians and a new Gaussian must be included, the latter will substitute the Gaussian that has the least number of observations associated, which is equivalent to the weight in MGM. But, contrarily to MGM, the background model is updated with a new Gaussian once every   frames, instead of considering a new value pixel at every frame. The update of the background model is based on the learning algorithms of RTDENN, where the neurons comprising a network excited by a set of samples are updated depending on the excitation level, which is the inverse of the square root of the Mahalanobis distance between the mean of the sample vectors which excite the neuron and each neuron. If the excitation level surpasses a certain threshold, the new data is merged into the neuron; otherwise, a new neuron with the new data is created and added to the RTDENN. The merger of close neurons is an important feature because it allows to reduce the number of neurons and thus save memory, resources and calculation time. In the proposed model, the merger of the new Gaussian   created with the data of the last   observations and the Gaussian   that was already part of the background model will be developed only under the two following conditions:        The merging Gaussians  and  must comprise very close regions.         &amp;nbsp;           The region of the resulting Gaussian  must be not too large.         &amp;nbsp;      The proximity of two Gaussians is measured by the Mahalanobis distance, as in Eq.&amp;nbsp;( 4), which defines the distance from Gaussian   and Gaussian   as stated in Eq.&amp;nbsp;( 11):            The first condition of proximity is checked using the square of the Mahalanobis distance,  , which has a Chi-Square distribution. The hypothesis test given by Eq.&amp;nbsp;( 12) is carried out, where   is the number of degrees of freedom, which corresponds to the number of input channels, and   is the significance level of the test, usually taken as  . If the result of the test is positive, the parameters of the merged Gaussian   are calculated with Eqs.&amp;nbsp;( 13), ( 14) and ( 15):                                                   These equations are similar to the merger equations of RTDENN, but these include the learning rate of MGM  . The merging Gaussians  and  must comprise very close regions. The region of the resulting Gaussian  must be not too large. The second condition to ensure the precision required by the system is related to the dispersion of the data provided by the observations that constitute the Gaussian. To evaluate this condition, the variances of the new Gaussian are compared to a maximum set by the parameter  , as stated in Eq.&amp;nbsp;( 16).            If both Gaussians   and   satisfy also this condition, the merger is completed, substituting the existing Gaussian   by the resulting Gaussian   in the background model. and , as explained in [18], are the main parameters of the RTDENN training process. The former deals with precision, while the latter has to do with execution time. On the one hand,  is a limit of noise acceptance. On the other hand, high values of  are expected to reduce execution time, in detriment of segmentation accuracy. Although theoretically  is important to avoid Gaussian with excessively high standard deviations, it can be seen experimentally that the value does not modify the results significantly. For a scale of value of  to  has been set to . Another parameter is introduced into the model to solve some problems in very static background and low noise cameras, where the standard deviation becomes too low. The parameter  is the lower limit for the covariance of the Gaussian distribution that comprise the model. Since input video channels have been considered to be independent from each other, the covariance is composed only by the standard deviation for each channel, limited by . The use of minimum values for covariance has already been explained in other works as [8]. A low value of , makes the model very precise, but without any ability of generalization, while high values of  make the model very imprecise. A pixel will be part of the background if its value is contained in the most probable scene given by the background model, as is proposed in MGM [ 22]. The most probable scene is a set   of Gaussians that models the background. In order to determine  , the Gaussians are sorted in decreasing order of relevance. Relevance in the background model is given by parameter  , which considers the relative number of observations associated to a Gaussian   and its standard deviation, as stated in Eq.&amp;nbsp;( 17). This set   includes at least a percentage   of the total data accounted for by the neural network, as can be seen in Eq.&amp;nbsp;( 18).                         Low values of threshold   would lead to a unimodal most probable background scene, i.e. composed by a single Gaussian, which would be unable to deal with repetitive, irrelevant motion in the background. In contrast, very high values of   would result in a most probable scene comprising multiple distributions, which would cause a transparency effect. The pixel under consideration is part of the background if there is a connection with one of the Gaussians in the set  . This connection is considered in terms of distance to the mean of the Gaussian, which must not exceed a maximum value proportional to the standard deviation of the Gaussian, with proportionality constant as stated in Eq.&amp;nbsp;( 19).       The parameter   determines the maximum distance of a pixel value to the mean of the Gaussian distribution so that the pixel is considered part of the background. The value assigned to this parameter may have a significant impact on the resulting segmentation, since it is key for the categorization of pixels. The optimal value depends on several factors, such as the degree of disturbance in the input video, the lighting, the colors in the scene, etc. Higher degrees of noise will require higher values of   so that small changes in the background are not considered to be relevant motion. Darker areas have usually a lower degree of variability than lighter zones. If no connection is detected to any of the Gaussians in , the pixel analyzed is not a part of the background, so the corresponding output will be set to foreground. 
